{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2759349"
                        ],
                        "name": "M. Luko\u0161evi\u010dius",
                        "slug": "M.-Luko\u0161evi\u010dius",
                        "structuredName": {
                            "firstName": "Mantas",
                            "lastName": "Luko\u0161evi\u010dius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Luko\u0161evi\u010dius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32384143"
                        ],
                        "name": "D. Popovici",
                        "slug": "D.-Popovici",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Popovici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Popovici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5631525"
                        ],
                        "name": "H. Jaeger",
                        "slug": "H.-Jaeger",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jaeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10707214"
                        ],
                        "name": "U. Siewert",
                        "slug": "U.-Siewert",
                        "structuredName": {
                            "firstName": "Udo",
                            "lastName": "Siewert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Siewert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 269
                            }
                        ],
                        "text": "\u2026of duration Tp with a similar frequency makeup was generated and smoothly embedded into g(t) at random locations, using suitable soft windowing techniques for the embedding to make sure that no novel (high) frequency components were created in the process (details in Lukos\u030cevic\u030cius et al. (2006))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 181778004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "473987dbaeddef2b46dfd6acebe82063f64af2a5",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Echo State Networks (ESNs) is a recent simple and powerful approach to training recurrent neural networks (RNNs). In this report we present a modification of ESNs - time warping invariant echo state networks (TWIESNs) that can effectively deal with time warping in dynamic pattern recognition. The standard approach to classify time warped input signals is to align them to candidate pro- totype patterns by a dynamic programming method and use the alignment cost as a classification criterion. In contrast, we feed the original input signal into specifically designed ESNs which intrinsically are invariant to time warping in the input. For this purpose, ESNs with leaky integrator neurons are required, which are here presented for the first time, too. We then explain the TWIESN architecture and demonstrate their functioning on very strongly warped, synthetic data sets."
            },
            "slug": "Time-Warping-Invariant-Echo-State-Networks-Luko\u0161evi\u010dius-Popovici",
            "title": {
                "fragments": [],
                "text": "Time Warping Invariant Echo State Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This report presents a modification of ESNs - time warping invariant echo state networks (TWIESNs) that can effectively deal with time warped in dynamic pattern recognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5631525"
                        ],
                        "name": "H. Jaeger",
                        "slug": "H.-Jaeger",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jaeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 77
                            }
                        ],
                        "text": "This simple idea leads to likewise simple offline (Jaeger, 2001) and online (Jaeger, 2003) learning algorithms, sometimes amazingly accurate models (Jaeger & Haas, 2004), and may also be realized in vertebrate brains (Stanley, Li, & Dan, 1999; Mauk & Buonomano, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7420805,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24547f720f472dd92870c1a7c4cb8bb450307f27",
            "isKey": false,
            "numCitedBy": 557,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Echo state networks (ESN) are a novel approach to recurrent neural network training. An ESN consists of a large, fixed, recurrent \"reservoir\" network, from which the desired output is obtained by training suitable output connection weights. Determination of optimal output weights becomes a linear, uniquely solvable task of MSE minimization. This article reviews the basic ideas and describes an online adaptation scheme based on the RLS algorithm known from adaptive linear systems. As an example, a 10-th order NARMA system is adaptively identified. The known benefits of the RLS algorithms carry over from linear systems to nonlinear ones; specifically, the convergence rate and misadjustment can be determined at design time."
            },
            "slug": "Adaptive-Nonlinear-System-Identification-with-Echo-Jaeger",
            "title": {
                "fragments": [],
                "text": "Adaptive Nonlinear System Identification with Echo State Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An online adaptation scheme based on the RLS algorithm known from adaptive linear systems is described, as an example, a 10-th order NARMA system is adaptively identified."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3318461"
                        ],
                        "name": "M. Gagliolo",
                        "slug": "M.-Gagliolo",
                        "structuredName": {
                            "firstName": "Matteo",
                            "lastName": "Gagliolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gagliolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11745761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75479012461814fd176556a56b32c2392462aef5",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, gradient-based LSTM recurrent neural networks (RNNs) solved many previously RNN-unlearnable tasks. Sometimes, however, gradient information is of little use for training RNNs, due to numerous local minima. For such cases, we present a novel method: EVOlution of systems with LINear Outputs (Evolino). Evolino evolves weights to the nonlinear, hidden nodes of RNNs while computing optimal linear mappings from hidden state to output, using methods such as pseudo-inverse-based linear regression. If we instead use quadratic programming to maximize the margin, we obtain the first evolutionary recurrent support vector machines. We show that Evolino-based LSTM can solve tasks that Echo State nets (Jaeger, 2004a) cannot and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
            },
            "slug": "Training-Recurrent-Networks-by-Evolino-Schmidhuber-Wierstra",
            "title": {
                "fragments": [],
                "text": "Training Recurrent Networks by Evolino"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that Evolino-based LSTM can solve tasks that Echo State nets cannot and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-basedLSTM."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37729423"
                        ],
                        "name": "Ulf D. Schiller",
                        "slug": "Ulf-D.-Schiller",
                        "structuredName": {
                            "firstName": "Ulf",
                            "lastName": "Schiller",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ulf D. Schiller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27551792"
                        ],
                        "name": "J. Steil",
                        "slug": "J.-Steil",
                        "structuredName": {
                            "firstName": "Jochen",
                            "lastName": "Steil",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Steil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 132
                            }
                        ],
                        "text": "A closer analytical investigation and/or optimization schemes of reservoir dynamics has attracted the attention of several authors (Schiller & Steil, 2005; M.C., Xu, & Principe, accepted 2006; Schmidhuber, Gomez, Wierstra, & Gagliolo, 2006, in press; Zant, Becanovic, Ishii, Kobialka, & Plo\u0308ger,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16133861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d605e91c1fa0f3649acd5cab7fdcdbfb6b195cf",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Analyzing-the-weight-dynamics-of-recurrent-learning-Schiller-Steil",
            "title": {
                "fragments": [],
                "text": "Analyzing the weight dynamics of recurrent learning algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 36
                            }
                        ],
                        "text": "To say the truth, we relied very much on our general confidence that ESN performance is quite robust with respect to changes in global control parameters, and invested some care only in the optimization of the leaking rate a, whereas the other parameters were settled in a rather offhand fashion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 81
                            }
                        ],
                        "text": "The \u201cfigure 8\u201d generation task is a perennial exercise for RNNs (for example, see (Pearlmutter, 1995) (Zegers & Sundareshan, 2003) and references therein)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1400872,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32e97eef94beacace020e79322cef0e1e5a76ee0",
            "isKey": false,
            "numCitedBy": 607,
            "numCiting": 302,
            "paperAbstract": {
                "fragments": [],
                "text": "Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continues with some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. The author presents some simulations, and at the end, addresses issues of computational complexity and learning speed."
            },
            "slug": "Gradient-calculations-for-dynamic-recurrent-neural-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Gradient calculations for dynamic recurrent neural networks: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones and presents some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39287553"
                        ],
                        "name": "M. C. Ozturk",
                        "slug": "M.-C.-Ozturk",
                        "structuredName": {
                            "firstName": "Mustafa",
                            "lastName": "Ozturk",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Ozturk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116458436"
                        ],
                        "name": "Dongming Xu",
                        "slug": "Dongming-Xu",
                        "structuredName": {
                            "firstName": "Dongming",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongming Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143961030"
                        ],
                        "name": "J. Pr\u00edncipe",
                        "slug": "J.-Pr\u00edncipe",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Pr\u00edncipe",
                            "middleNames": [
                                "Carlos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pr\u00edncipe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12887835,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "243fe0614483d201aae12bb8f41078b0af56e43a",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The present design of echo state network (ESN) parameters relies on the selection of the maximum eigenvalue of the linearized system around zero. However, this has been found far from optimal for function approximation. This letter presents a function approximation perspective to better understand the operation of ESNs and proposes an information-theoretic measure, the average entropy of echo states, to assess the \u201crichness\u201d of the ESN dynamics. Furthermore, it provides a new interpretation of the ESN dynamics rooted in system theory as a combination of linearized systems where their poles move according to the input signal dynamics. With this interpretation, we will be able to a priori design ESNs with uniform pole distributions covering the frequency spectrum optimally. With adaptive read-outs, the designed ESN can be used as a general infrastructure to represent information in time. 1Correspondence address: Mustafa C. Ozturk, Computational NeuroEngineering Laboratory, Department of Electrical and Computer Engineering, University of Florida, Gainesville, FL 32611, Tel: (352)392-2682, Fax: (352)392-0044"
            },
            "slug": "Analysis-and-Design-of-Echo-State-Networks-for-Ozturk-Xu",
            "title": {
                "fragments": [],
                "text": "Analysis and Design of Echo State Networks for Function Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A function approximation perspective is presented to better understand the operation of ESNs and an information-theoretic measure, the average entropy of echo states, is proposed to assess the \u201crichness\u201d of the ESN dynamics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238582"
                        ],
                        "name": "D. Buonomano",
                        "slug": "D.-Buonomano",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Buonomano",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Buonomano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2550598,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "0611161a8a63884b3c66fb2acc2ea57db6b123c3",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural dynamics within recurrent cortical networks is an important component of neural processing. However, the learning rules that allow networks composed of hundreds or thousands of recurrently connected neurons to develop stable dynamical states are poorly understood. Here I use a neural network model to examine the emergence of stable dynamical states within recurrent networks. I describe a learning rule that can account both for the development of stable dynamics and guide networks to states that have been observed experimentally, specifically, states that instantiate a sparse code for time. Across trials, each neuron fires during a specific time window; by connecting the neurons to a hypothetical set of output units, it is possible to generate arbitrary spatial-temporal output patterns. Intertrial jitter of the spike time of a given neuron increases as a direct function of the delay at which it fires. These results establish a learning rule by which cortical networks can potentially process temporal information in a self-organizing manner, in the absence of specialized timing mechanisms."
            },
            "slug": "A-learning-rule-for-the-emergence-of-stable-and-in-Buonomano",
            "title": {
                "fragments": [],
                "text": "A learning rule for the emergence of stable dynamics and timing in recurrent networks."
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A neural network model is used to examine the emergence of stable dynamical states within recurrent networks and establishes a learning rule by which cortical networks can potentially process temporal information in a self-organizing manner, in the absence of specialized timing mechanisms."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of neurophysiology"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39287553"
                        ],
                        "name": "M. C. Ozturk",
                        "slug": "M.-C.-Ozturk",
                        "structuredName": {
                            "firstName": "Mustafa",
                            "lastName": "Ozturk",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Ozturk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116458436"
                        ],
                        "name": "Dongming Xu",
                        "slug": "Dongming-Xu",
                        "structuredName": {
                            "firstName": "Dongming",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongming Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143961030"
                        ],
                        "name": "J. Pr\u00edncipe",
                        "slug": "J.-Pr\u00edncipe",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Pr\u00edncipe",
                            "middleNames": [
                                "Carlos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pr\u00edncipe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20816736,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a9e1c2007adc59d83503054430051e47a515953",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "The design of echo state network (ESN) parameters relies on the selection of the maximum eigenvalue of the linearized system around zero (spectral radius). However, this procedure does not quantify in a systematic manner the performance of the ESN in terms of approximation error. This article presents a functional space approximation framework to better understand the operation of ESNs and proposes an information-theoretic metric, the average entropy of echo states, to assess the richness of the ESN dynamics. Furthermore, it provides an interpretation of the ESN dynamics rooted in system theory as families of coupled linearized systems whose poles move according to the input signal dynamics. With this interpretation, a design methodology for functional approximation is put forward where ESNs are designed with uniform pole distributions covering the frequency spectrum to abide by the richness metric, irrespective of the spectral radius. A single bias parameter at the ESN input, adapted with the modeling error, configures the ESN spectral radius to the input-output joint space. Function approximation examples compare the proposed design methodology versus the conventional design."
            },
            "slug": "Analysis-and-Design-of-Echo-State-Networks-Ozturk-Xu",
            "title": {
                "fragments": [],
                "text": "Analysis and Design of Echo State Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A functional space approximation framework is presented to better understand the operation of ESNs and an information-theoretic metric, the average entropy of echo states, is proposed to assess the richness of the ESN dynamics."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247053"
                        ],
                        "name": "W. Maass",
                        "slug": "W.-Maass",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Maass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Maass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792142"
                        ],
                        "name": "T. Natschl\u00e4ger",
                        "slug": "T.-Natschl\u00e4ger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Natschl\u00e4ger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Natschl\u00e4ger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754307"
                        ],
                        "name": "H. Markram",
                        "slug": "H.-Markram",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Markram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Markram"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1045112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0535dedb8607d83cd2614317c99913378e89e26",
            "isKey": false,
            "numCitedBy": 2875,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology."
            },
            "slug": "Real-Time-Computing-Without-Stable-States:-A-New-on-Maass-Natschl\u00e4ger",
            "title": {
                "fragments": [],
                "text": "Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks, based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5631525"
                        ],
                        "name": "H. Jaeger",
                        "slug": "H.-Jaeger",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jaeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547639"
                        ],
                        "name": "H. Haas",
                        "slug": "H.-Haas",
                        "structuredName": {
                            "firstName": "Harald",
                            "lastName": "Haas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Haas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 198
                            }
                        ],
                        "text": "\u2026data are deterministic or come from a very low-noise process, we feel comfortable with using larger N \u2013 for instance, when learning to predict a chaotic process from almost noise-free training data (Jaeger & Haas, 2004), we used a 1,000 unit reservoir with 2,000 time steps of training data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 149
                            }
                        ],
                        "text": "This simple idea leads to likewise simple offline (Jaeger, 2001) and online (Jaeger, 2003) learning algorithms, sometimes amazingly accurate models (Jaeger & Haas, 2004), and may also be realized in vertebrate brains (Stanley, Li, & Dan, 1999; Mauk & Buonomano, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2184251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d073966e48ffb6dccde1e4eb3f0380c10c6a766",
            "isKey": false,
            "numCitedBy": 2523,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude."
            },
            "slug": "Harnessing-Nonlinearity:-Predicting-Chaotic-Systems-Jaeger-Haas",
            "title": {
                "fragments": [],
                "text": "Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A method for learning nonlinear systems, echo state networks (ESNs), which employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115401300"
                        ],
                        "name": "Hsing-Hen Chen",
                        "slug": "Hsing-Hen-Chen",
                        "structuredName": {
                            "firstName": "Hsing-Hen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsing-Hen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552960"
                        ],
                        "name": "Yee-Chun Lee",
                        "slug": "Yee-Chun-Lee",
                        "structuredName": {
                            "firstName": "Yee-Chun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yee-Chun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17076103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1ae0fb208fd389d2ff723e5442f9ca7896cb0a4",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We proposed a model of Time Warping Invariant Neural Networks (TWINN) to handle the time warped continuous signals. Although TWINN is a simple modification of well known recurrent neural network, analysis has shown that TWINN completely removes time warping and is able to handle difficult classification problem. It is also shown that TWINN has certain advantages over the current available sequential processing schemes: Dynamic Programming(DP)[1], Hidden Markov Model(HMM)[2], Time Delayed Neural Networks(TDNN) [3] and Neural Network Finite Automata(NNFA)[4]. \n \nWe also analyzed the time continuity employed in TWINN and pointed out that this kind of structure can memorize longer input history compared with Neural Network Finite Automata (NNFA). This may help to understand the well accepted fact that for learning grammatical reference with NNFA one had to start with very short strings in training set. \n \nThe numerical example we used is a trajectory classification problem. This problem, making a feature of variable sampling rates, having internal states, continuous dynamics, heavily time-warped data and deformed phase space trajectories, is shown to be difficult to other schemes. With TWINN this problem has been learned in 100 iterations. For benchmark we also trained the exact same problem with TDNN and completely failed as expected."
            },
            "slug": "Time-Warping-Invariant-Neural-Networks-Sun-Chen",
            "title": {
                "fragments": [],
                "text": "Time Warping Invariant Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Analysis has shown that TWINN completely removes time warping and is able to handle difficult classification problem, and has certain advantages over the current available sequential processing schemes."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5631525"
                        ],
                        "name": "H. Jaeger",
                        "slug": "H.-Jaeger",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jaeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 72
                            }
                        ],
                        "text": "Leaky integrator ESNs were in passing introduced in (Jaeger, 2001) and (Jaeger, 2002b); fragments of what will be reported here appeared first in a technical report (Lukos\u030cevic\u030cius, Popovici, Jaeger, & Siewert, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 55
                            }
                        ],
                        "text": "Rationale: due to the ESNs short-term-memory capacity (Jaeger, 2002a), there is hope that the last extended state incorporates information from earlier points in the sequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 166
                            }
                        ],
                        "text": "Findings: Best results (of about 2 test misclassifications) achieved with networks of size 1,000 (about 9,000 trained parameters) with regularization by state noise (Jaeger, 2002b); no clear difference between standard and leaky integrator ESNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 192593367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a10990aab66ffaf6bfd3fe582c42c93a9e406fa7",
            "isKey": false,
            "numCitedBy": 377,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This tutorial is a worked-out version of a 5-hour course originally held at AIS in September/October 2002. It has two distinct components. First, it contains a mathematically-oriented crash course on traditional training methods for recurrent neural networks, covering back-propagation through time (BPTT), real-time recurrent learning (RTRL), and extended Kalman filtering approaches (EKF). This material is covered in Sections 2 \u2013 5. The remaining sections 1 and 6 \u2013 9 are much more gentle, more detailed, and illustrated with simple examples. They are intended to be useful as a stand-alone tutorial for the echo state network (ESN) approach to recurrent neural network training."
            },
            "slug": "A-tutorial-on-training-recurrent-neural-networks-,-Jaeger",
            "title": {
                "fragments": [],
                "text": "A tutorial on training recurrent neural networks , covering BPPT , RTRL , EKF and the \" echo state network \" approach - Semantic Scholar"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This tutorial is a worked-out version of a 5-hour course originally held at AIS in September/October 2002, and contains a mathematically-oriented crash course on traditional training methods for recurrent neural networks, covering back-propagation through time, real-time recurrent learning (RTRL), and extended Kalman filtering approaches (EKF)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8992604"
                        ],
                        "name": "E. Levin",
                        "slug": "E.-Levin",
                        "structuredName": {
                            "firstName": "Esther",
                            "lastName": "Levin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Levin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115311"
                        ],
                        "name": "R. Pieraccini",
                        "slug": "R.-Pieraccini",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Pieraccini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Pieraccini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2606242"
                        ],
                        "name": "E. Bocchieri",
                        "slug": "E.-Bocchieri",
                        "structuredName": {
                            "firstName": "Enrico",
                            "lastName": "Bocchieri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bocchieri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18864457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3673a263d6bc1af93f2b00fdc5e002df501b0f26",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, much interest has been generated regarding speech recognition systems based on Hidden Markov Models (HMMs) and neural network (NN) hybrids. Such systems attempt to combine the best features of both models: the temporal structure of HMMs and the discriminative power of neural networks. In this work we define a time-warping (TW) neuron that extends the operation of the formal neuron of a back-propagation network by warping the input pattern to match it optimally to its weights. We show that a single-layer network of TW neurons is equivalent to a Gaussian density HMM-based recognition system, and we propose to improve the discriminative power of this system by using back-propagation discriminative training, and/or by generalizing the structure of the recognizer to a multi-layered net. The performance of the proposed network was evaluated on a highly confusable, isolated word, multi speaker recognition task. The results indicate that not only does the recognition performance improve, but the separation between classes is enhanced also, allowing us to set up a rejection criterion to improve the confidence of the system."
            },
            "slug": "Time-Warping-Network:-A-Hybrid-Framework-for-Speech-Levin-Pieraccini",
            "title": {
                "fragments": [],
                "text": "Time-Warping Network: A Hybrid Framework for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A time-warping neuron is defined that extends the operation of the formal neuron of a back-propagation network by warping the input pattern to match it optimally to its weights."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775909"
                        ],
                        "name": "M. Strickert",
                        "slug": "M.-Strickert",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Strickert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Strickert"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7751069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c2f72a7bdd853e31640e4252b9953ddc6b8c9d9",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 179,
            "paperAbstract": {
                "fragments": [],
                "text": "This work investigates the self-organizing representation of temporal data in prototypebased neural networks. Extensions of the supervised learning vector quantization (LVQ) and the unsupervised self-organizing map (SOM) are considered in detail. The principle of Hebbian learning through prototypes yields compact data models that can be easily interpreted by similarity reasoning. In order to obtain a robust prototype dynamic, LVQ is extended by neighborhood cooperation between neurons to prevent a strong dependence on the initial prototype locations. Additionally, implementations of more general, adaptive metrics are studied with a particular focus on the built-in detection of data attributes involved for a given classification task. For unsupervised sequence processing, two modifications of SOM are pursued: the SOM for structured data (SOMSD) realizing an efficient back-reference to the previous best matching neuron in a triangular low-dimensional neural lattice, and the merge SOM (MSOM) expressing the temporal context as a fractal combination of the previously most active neuron and its context. The first SOMSD extension tackles data dimension reduction and planar visualization, the second MSOM is designed for obtaining higher quantization accuracy. The supplied experiments underline the data modeling quality of the presented methods."
            },
            "slug": "Self-organizing-neural-networks-for-sequence-Strickert",
            "title": {
                "fragments": [],
                "text": "Self-organizing neural networks for sequence processing"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "This work investigates the self-organizing representation of temporal data in prototypebased neural networks with a particular focus on the built-in detection of data attributes involved for a given classification task through Extensions of the supervised learning vector quantization (LVQ) and the unsupervised self- Organizing map (SOM)."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247053"
                        ],
                        "name": "W. Maass",
                        "slug": "W.-Maass",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Maass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Maass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066881594"
                        ],
                        "name": "Prashant Joshi",
                        "slug": "Prashant-Joshi",
                        "structuredName": {
                            "firstName": "Prashant",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prashant Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 936284,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "c866911b61bcf4f1e2f0c43fe54034ba362ef9f7",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "It has previously been shown that generic cortical microcircuit models can perform complex real-time computations on continuous input streams, provided that these computations can be carried out with a rapidly fading memory. We investigate the computational capability of such circuits in the more realistic case where not only readout neurons, but in addition a few neurons within the circuit, have been trained for specific tasks. This is essentially equivalent to the case where the output of trained readout neurons is fed back into the circuit. We show that this new model overcomes the limitation of a rapidly fading memory. In fact, we prove that in the idealized case without noise it can carry out any conceivable digital or analog computation on time-varying inputs. But even with noise, the resulting computational model can perform a large class of biologically relevant real-time computations that require a nonfading memory. We demonstrate these computational implications of feedback both theoretically, and through computer simulations of detailed cortical microcircuit models that are subject to noise and have complex inherent dynamics. We show that the application of simple learning procedures (such as linear regression or perceptron learning) to a few neurons enables such circuits to represent time over behaviorally relevant long time spans, to integrate evidence from incoming spike trains over longer periods of time, and to process new information contained in such spike trains in diverse ways according to the current internal state of the circuit. In particular we show that such generic cortical microcircuits with feedback provide a new model for working memory that is consistent with a large set of biological constraints. Although this article examines primarily the computational role of feedback in circuits of neurons, the mathematical principles on which its analysis is based apply to a variety of dynamical systems. Hence they may also throw new light on the computational role of feedback in other complex biological dynamical systems, such as, for example, genetic regulatory networks."
            },
            "slug": "Computational-Aspects-of-Feedback-in-Neural-Maass-Joshi",
            "title": {
                "fragments": [],
                "text": "Computational Aspects of Feedback in Neural Circuits"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The application of simple learning procedures to a few neurons enables generic cortical microcircuits with feedback to represent time over behaviorally relevant long time spans, to integrate evidence from incoming spike trains over longer periods of time, and to process new information contained in such spike trains in diverse ways according to the current internal state of the circuit."
            },
            "venue": {
                "fragments": [],
                "text": "PLoS Comput. Biol."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144919173"
                        ],
                        "name": "P. Zegers",
                        "slug": "P.-Zegers",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Zegers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Zegers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782897"
                        ],
                        "name": "M. Sundareshan",
                        "slug": "M.-Sundareshan",
                        "structuredName": {
                            "firstName": "Malur",
                            "lastName": "Sundareshan",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sundareshan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 101
                            }
                        ],
                        "text": "The \u201cfigure 8\u201d generation task is a perennial exercise for RNNs (for example, see (Pearlmutter, 1995) (Zegers & Sundareshan, 2003) and references therein)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14806010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fb7f8149c7e5a0ee3aeacfcbd77d022f4a26cb5",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 87,
            "paperAbstract": {
                "fragments": [],
                "text": "Generation of desired trajectory behavior using neural networks involves a particularly challenging spatio-temporal learning problem. This paper introduces a novel solution, i.e., designing a dynamic system whose terminal behavior emulates a prespecified spatio-temporal pattern independently of its initial conditions. The proposed solution uses a dynamic neural network (DNN), a hybrid architecture that employs a recurrent neural network (RNN) in cascade with a nonrecurrent neural network (NRNN). The RNN generates a simple limit cycle, which the NRNN reshapes into the desired trajectory. This architecture is simple to train. A systematic synthesis procedure based on the design of relay control systems is developed for configuring an RNN that can produce a limit cycle of elementary complexity. It is further shown that a cascade arrangement of this RNN and an appropriately trained NRNN can emulate any desired trajectory behavior irrespective of its complexity. An interesting solution to the trajectory modulation problem, i.e., online modulation of the generated trajectories using external inputs, is also presented. Results of several experiments are included to demonstrate the capabilities and performance of the DNN in handling trajectory generation and modulation problems."
            },
            "slug": "Trajectory-generation-and-modulation-using-dynamic-Zegers-Sundareshan",
            "title": {
                "fragments": [],
                "text": "Trajectory generation and modulation using dynamic neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A novel solution to the trajectory modulation problem, i.e., online modulation of the generated trajectories using external inputs, is presented and it is shown that a cascade arrangement of this RNN and an appropriately trained NRNN can emulate any desired trajectory behavior irrespective of its complexity."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2715753"
                        ],
                        "name": "G. Stanley",
                        "slug": "G.-Stanley",
                        "structuredName": {
                            "firstName": "Garrett",
                            "lastName": "Stanley",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Stanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115374542"
                        ],
                        "name": "F. Li",
                        "slug": "F.-Li",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Li",
                            "middleNames": [
                                "Fei"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058327310"
                        ],
                        "name": "Y. Dan",
                        "slug": "Y.-Dan",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Dan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Dan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15452913,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "06df1ed332d76e3c48404efe04551acc7d88e30e",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "A major challenge in studying sensory processing is to understand the meaning of the neural messages encoded in the spiking activity of neurons. From the recorded responses in a sensory circuit, what information can we extract about the outside world? Here we used a linear decoding technique to reconstruct spatiotemporal visual inputs from ensemble responses in the lateral geniculate nucleus (LGN) of the cat. From the activity of 177 cells, we have reconstructed natural scenes with recognizable moving objects. The quality of reconstruction depends on the number of cells. For each point in space, the quality of reconstruction begins to saturate at six to eight pairs of on and off cells, approaching the estimated coverage factor in the LGN of the cat. Thus, complex visual inputs can be reconstructed with a simple decoding algorithm, and these analyses provide a basis for understanding ensemble coding in the early visual pathway."
            },
            "slug": "Reconstruction-of-Natural-Scenes-from-Ensemble-in-Stanley-Li",
            "title": {
                "fragments": [],
                "text": "Reconstruction of Natural Scenes from Ensemble Responses in the Lateral Geniculate Nucleus"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A linear decoding technique is used to reconstruct spatiotemporal visual inputs from ensemble responses in the lateral geniculate nucleus of the cat to provide a basis for understanding ensemble coding in the early visual pathway."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of Neuroscience"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3296970"
                        ],
                        "name": "T. V. D. Zant",
                        "slug": "T.-V.-D.-Zant",
                        "structuredName": {
                            "firstName": "Tijn",
                            "lastName": "Zant",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. V. D. Zant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735405"
                        ],
                        "name": "V. Becanovic",
                        "slug": "V.-Becanovic",
                        "structuredName": {
                            "firstName": "Vlatko",
                            "lastName": "Becanovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Becanovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145418778"
                        ],
                        "name": "K. Ishii",
                        "slug": "K.-Ishii",
                        "structuredName": {
                            "firstName": "Kazuo",
                            "lastName": "Ishii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ishii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70054166"
                        ],
                        "name": "H. Kobialka",
                        "slug": "H.-Kobialka",
                        "structuredName": {
                            "firstName": "Hans-Ulrich",
                            "lastName": "Kobialka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kobialka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775016"
                        ],
                        "name": "P. Pl\u00f6ger",
                        "slug": "P.-Pl\u00f6ger",
                        "structuredName": {
                            "firstName": "Paul-Gerhard",
                            "lastName": "Pl\u00f6ger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pl\u00f6ger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59832410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebe9413574d68fc866cd062cfcb33346eea2b302",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Finding-good-Echo-State-Networks-to-control-an-Zant-Becanovic",
            "title": {
                "fragments": [],
                "text": "Finding good Echo State Networks to control an underwater robot using evolutionary computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5631525"
                        ],
                        "name": "H. Jaeger",
                        "slug": "H.-Jaeger",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jaeger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 63
                            }
                        ],
                        "text": "Several equivalent formulations of this property are given in (Jaeger, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026ESNs for (i) learning very slow dynamical systems and re-playing the learnt system at different speeds, (ii) classifying of relatively\nslow and noisy time series (the Japanese Vowel dataset \u2013 here we obtain a zero test error rate), and (iii) recognizing strongly time-warped dynamical patterns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 107
                            }
                        ],
                        "text": "In order to find a good value for a, we carried out a tenfold cross-validation on the training data for various values of a, for 25 independently created 4- unit ESNs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 116
                            }
                        ],
                        "text": "Periodic signals of rugged shape and modest length (up to about 50 time points) have been trained on standard ESNs (Jaeger, 2001) with success, but longer, smoothly and slowly changing targets like a 200-point \u201clazy\u201d eight we could never master with standard ESNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 53
                            }
                        ],
                        "text": "Leaky integrator ESNs were in passing introduced in (Jaeger, 2001) and (Jaeger, 2002b); fragments of what will be reported here appeared first in a technical report (Lukos\u030cevic\u030cius, Popovici, Jaeger, & Siewert, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 147
                            }
                        ],
                        "text": "In Section 2 we provide the system\nequations and point out basic stability conditions \u2013 amounting to algebraic criteria for the echo state property (Jaeger, 2001) in leaky integrator ESNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 140
                            }
                        ],
                        "text": "The proof is a straightforward demonstration that the linearized ESN with zero input is instable around the zero state when |\u03bb|max > 1, see (Jaeger, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 191
                            }
                        ],
                        "text": "In Section 4 we will be concerned with the attractor stability of ESNs trained as pattern generators; but here we will start with a more basic stability property of ESNs, namely the echo state property."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 51
                            }
                        ],
                        "text": "This simple idea leads to likewise simple offline (Jaeger, 2001) and online (Jaeger, 2003) learning algorithms, sometimes amazingly accurate models (Jaeger & Haas, 2004), and may also be realized in vertebrate brains (Stanley, Li, & Dan, 1999; Mauk & Buonomano, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 73
                            }
                        ],
                        "text": "The idea that gave birth to the twin pair of echo state networks (ESNs) (Jaeger, 2001) and liquid state machines (LSMs) (Maass, Natschlaeger, & Markram, 2002) is simple."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We first observe that optimizing \u03b4 is by and large a non-issue."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 56
                            }
                        ],
                        "text": "The proof (a streamlined version of the proof given in (Jaeger, 2001)) is given in the Appendix."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 45
                            }
                        ],
                        "text": "Then the continuous-time dynamics of a leaky integrator ESN is given by\nx\u0307 = 1\nc\n( \u2212a x + f(Winu + Wx + Wfby) ) , (1)\ny = g(Wout[x ; u]), (2)\nwhere c > 0 is a time constant global to the ESN, a > 0 is the reservoir neuron\u2019s leaking rate (we assume a uniform leaking rate for simplicity), f is a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 150
                            }
                        ],
                        "text": "For leaky integrator ESNs, a sufficient and a necessary condition for the echo state property are known, which we cite here from an early techreport (Jaeger, 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 73
                            }
                        ],
                        "text": "Standardly echo state networks (ESNs) are built from simple additive units with a sigmoid activation function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15467150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8430c0b9afa478ae660398704b11dca1221ccf22",
            "isKey": false,
            "numCitedBy": 1959,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The report introduces a constructive learning algorithm for recurrent neural networks, which modifies only the weights to output units in order to achieve the learning task. key words: recurrent neural networks, supervised learning Zusammenfassung. Der Report f\u00fchrt ein konstruktives Lernverfahren f\u00fcr rekurrente neuronale Netze ein, welches zum Erreichen des Lernzieles lediglich die Gewichte der zu den Ausgabeneuronen f\u00fchrenden Verbindungen modifiziert. Stichw\u00f6rter: rekurrente neuronale Netze, \u00fcberwachtes Lernen"
            },
            "slug": "The''echo-state''approach-to-analysing-and-training-Jaeger",
            "title": {
                "fragments": [],
                "text": "The''echo state''approach to analysing and training recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The report introduces a constructive learning algorithm for recurrent neural networks, which modifies only the weights to output units in order to achieve the learning task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3002354"
                        ],
                        "name": "M. Mauk",
                        "slug": "M.-Mauk",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mauk",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mauk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238582"
                        ],
                        "name": "D. Buonomano",
                        "slug": "D.-Buonomano",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Buonomano",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Buonomano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 275
                            }
                        ],
                        "text": "\u2026can process temporal information has been approached in a fashion that is related in spirit to ESNs/LSMs. Precise timing phenomena can be explained as emerging from the network dynamics as such, without the necessity of special timing mechanisms like clocks or delay lines (Mauk & Buonomano, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026ESNs for (i) learning very slow dynamical systems and re-playing the learnt system at different speeds, (ii) classifying of relatively\nslow and noisy time series (the Japanese Vowel dataset \u2013 here we obtain a zero test error rate), and (iii) recognizing strongly time-warped dynamical patterns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 244
                            }
                        ],
                        "text": "This simple idea leads to likewise simple offline (Jaeger, 2001) and online (Jaeger, 2003) learning algorithms, sometimes amazingly accurate models (Jaeger & Haas, 2004), and may also be realized in vertebrate brains (Stanley, Li, & Dan, 1999; Mauk & Buonomano, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1860792,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "95a11e0c0763268c891e9450f1f09397b6e3ad00",
            "isKey": false,
            "numCitedBy": 825,
            "numCiting": 145,
            "paperAbstract": {
                "fragments": [],
                "text": "A complete understanding of sensory and motor processing requires characterization of how the nervous system processes time in the range of tens to hundreds of milliseconds (ms). Temporal processing on this scale is required for simple sensory problems, such as interval, duration, and motion discrimination, as well as complex forms of sensory processing, such as speech recognition. Timing is also required for a wide range of motor tasks from eyelid conditioning to playing the piano. Here we review the behavioral, electrophysiological, and theoretical literature on the neural basis of temporal processing. These data suggest that temporal processing is likely to be distributed among different structures, rather than relying on a centralized timing area, as has been suggested in internal clock models. We also discuss whether temporal processing relies on specialized neural mechanisms, which perform temporal computations independent of spatial ones. We suggest that, given the intricate link between temporal and spatial information in most sensory and motor tasks, timing and spatial processing are intrinsic properties of neural function, and specialized timing mechanisms such as delay lines, oscillators, or a spectrum of different time constants are not required. Rather temporal processing may rely on state-dependent changes in network dynamics."
            },
            "slug": "The-neural-basis-of-temporal-processing.-Mauk-Buonomano",
            "title": {
                "fragments": [],
                "text": "The neural basis of temporal processing."
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is suggested that, given the intricate link between temporal and spatial information in most sensory and motor tasks, timing and spatial processing are intrinsic properties of neural function, and specialized timing mechanisms such as delay lines, oscillators, or a spectrum of different time constants are not required."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of neuroscience"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685310"
                        ],
                        "name": "F. Itakura",
                        "slug": "F.-Itakura",
                        "structuredName": {
                            "firstName": "Fumitada",
                            "lastName": "Itakura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Itakura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 109
                            }
                        ],
                        "text": "The most widely used technique for dealing with time-warped patterns is probably dynamic time warping (DTW) (Itakura, 1975) and its modifications."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61601418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbf2b0948ec73e21f6d5a67b22a31a20d503cc9e",
            "isKey": false,
            "numCitedBy": 1244,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "A computer system is described in which isolated words, spoken by a designated talker, are recognized through calculation of a minimum prediction residual. A reference pattern for each word to be recognized is stored as a time pattern of linear prediction coefficients (LPC). The total log prediction residual of an input signal is minimized by optimally registering the reference LPC onto the input autocorrelation coefficients using the dynamic programming algorithm (DP). The input signal is recognized as the reference word which produces the minimum prediction residual. A sequential decision procedure is used to reduce the amount of computation in DP. A frequency normalization with respect to the long-time spectral distribution is used to reduce effects of variations in the frequency response of telephone connections. The system has been implemented on a DDP-516 computer for the 200-word recognition experiment. The recognition rate for a designated male talker is 97.3 percent for telephone input, and the recognition time is about 22 times real time."
            },
            "slug": "Minimum-prediction-residual-principle-applied-to-Itakura",
            "title": {
                "fragments": [],
                "text": "Minimum prediction residual principle applied to speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A computer system is described in which isolated words, spoken by a designated talker, are recognized through calculation of a minimum prediction residual through optimally registering the reference LPC onto the input autocorrelation coefficients using the dynamic programming algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583597"
                        ],
                        "name": "J. Kolen",
                        "slug": "J.-Kolen",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kolen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kolen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2492622"
                        ],
                        "name": "S. C. Kremer",
                        "slug": "S.-C.-Kremer",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kremer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. C. Kremer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 124881417,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "bf9bece374ca57b826fc4e51bd2f156da11b5b73",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction Learning in Networks with Fixed Points Computing the Gradient Without Assuming a Fixed Point Some Simulations Stability and Perturbation Experiments Other Non-Fixed-Point Techniques Learning with Scale Parameters Conclusion \n]]>"
            },
            "slug": "Gradient-Calculations-for-Dynamic-Recurrent-Neural-Kolen-Kremer",
            "title": {
                "fragments": [],
                "text": "Gradient Calculations for Dynamic Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction Learning in Networks with Fixed Points Computing the Gradient Without Assuming a Fixed Point Some Simulations Stability and Perturbation Experiments Other Non-Fixed-Point Techniques Learning with Scale Parameters Conclusion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2935285"
                        ],
                        "name": "M. Hinder",
                        "slug": "M.-Hinder",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Hinder",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hinder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38557491"
                        ],
                        "name": "T. Milner",
                        "slug": "T.-Milner",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Milner",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Milner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1918047,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "f68c886fdf737236af8ef485dd3ad1e668a01737",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The equilibrium point hypothesis (EPH) was conceived as a means whereby the central nervous system could control limb movements by a relatively simple shift in equilibrium position without the need to explicitly compensate for task dynamics. Many recent studies have questioned this view with results that suggest the formation of an internal dynamics model of the specific task. However, supporters of the EPH have argued that these results are not incompatible with the EPH and that there is no reason to abandon it. In this study, we have tested one of the fundamental predictions of the EPH, namely, equifinality. Subjects learned to perform goal\u2010directed wrist flexion movements while a motor provided assistance in proportion to the instantaneous velocity. It was found that the subjects stopped short of the target on the trials where the magnitude of the assistance was randomly decreased, compared to the preceding control trials (P= 0.003), i.e. equifinality was not achieved. This is contrary to the EPH, which predicts that final position should not be affected by external loads that depend purely on velocity. However, such effects are entirely consistent with predictions based on the formation of an internal dynamics model."
            },
            "slug": "The-Case-for-an-Internal-Dynamics-Model-versus-in-Hinder-Milner",
            "title": {
                "fragments": [],
                "text": "The Case for an Internal Dynamics Model versus Equilibrium Point Control in Human Movement"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It was found that the subjects stopped short of the target on the trials where the magnitude of the assistance was randomly decreased, compared to the preceding control trials, contrary to the EPH, which predicts that final position should not be affected by external loads that depend purely on velocity."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773173"
                        ],
                        "name": "Mineichi Kudo",
                        "slug": "Mineichi-Kudo",
                        "structuredName": {
                            "firstName": "Mineichi",
                            "lastName": "Kudo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mineichi Kudo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20722878"
                        ],
                        "name": "J. Toyama",
                        "slug": "J.-Toyama",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Toyama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Toyama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782129"
                        ],
                        "name": "M. Shimbo",
                        "slug": "M.-Shimbo",
                        "structuredName": {
                            "firstName": "Masaru",
                            "lastName": "Shimbo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Shimbo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7351806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d162178b86a0704de5c8cec35088559593806956",
            "isKey": false,
            "numCitedBy": 114,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multidimensional-curve-classification-using-regions-Kudo-Toyama",
            "title": {
                "fragments": [],
                "text": "Multidimensional curve classification using passing-through regions"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit. Lett."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50206577"
                        ],
                        "name": "P. Geurts",
                        "slug": "P.-Geurts",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Geurts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Geurts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "Various techniques have been applied to this problem (Kudo, Toyama, & Shimbo, 1999) (Geurts, 2001) (Barber, 2003) (Strickert, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15706973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c8c63ee0da654f7ae3cc296cc5d8fc3200a433b",
            "isKey": false,
            "numCitedBy": 327,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose some new tools to allow machine learning classifiers to cope with time series data. We first argue that many time-series classification problems can be solved by detecting and combining local properties or patterns in time series. Then, a technique is proposed to find patterns which are useful for classification. These patterns are combined to build interpretable classification rules. Experiments, carried out on several artificial and real problems, highlight the interest of the approach both in terms of interpretability and accuracy of the induced classifiers."
            },
            "slug": "Pattern-Extraction-for-Time-Series-Classification-Geurts",
            "title": {
                "fragments": [],
                "text": "Pattern Extraction for Time Series Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "It is argued that many time-series classification problems can be solved by detecting and combining local properties or patterns in time series, and a technique is proposed to find patterns which are useful for classification."
            },
            "venue": {
                "fragments": [],
                "text": "PKDD"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32931438"
                        ],
                        "name": "Michael R. Buehner",
                        "slug": "Michael-R.-Buehner",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Buehner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael R. Buehner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50322323"
                        ],
                        "name": "P. Young",
                        "slug": "P.-Young",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Young",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 103
                            }
                        ],
                        "text": "A tighter sufficient condition for the echo state property in standard sigmoid ESNs has been given in (Buehner & Young, 2006); it remains to be transferred to leaky-integrator ESNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 664843,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72cf11b0605826db1d692143b105291150d6d418",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "This letter provides a brief explanation of echo state networks (ESNs) and provides a rigorous bound for guaranteeing asymptotic stability of these networks. The stability bounds presented here could aid in the design of echo state networks that would be applicable to control applications where stability is required"
            },
            "slug": "A-tighter-bound-for-the-echo-state-property-Buehner-Young",
            "title": {
                "fragments": [],
                "text": "A tighter bound for the echo state property"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The stability bounds presented here could aid in the design of echo state networks that would be applicable to control applications where stability is required."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145617808"
                        ],
                        "name": "D. Barber",
                        "slug": "D.-Barber",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Barber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Barber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 100
                            }
                        ],
                        "text": "Various techniques have been applied to this problem (Kudo, Toyama, & Shimbo, 1999) (Geurts, 2001) (Barber, 2003) (Strickert, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5555048,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b87a2680357fcbc39d01796c53fcd98dc2abf3f8",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of latent/hidden variable Dynamic Bayesian Networks is constrained by the complexity of marginalising over latent variables. For this reason either small latent dimensions or Gaussian latent conditional tables linearly dependent on past states are typically considered in order that inference is tractable. We suggest an alternative approach in which the latent variables are modelled using deterministic conditional probability tables. This specialisation has the advantage of tractable inference even for highly complex non-linear/non-Gaussian visible conditional probability tables. This approach enables the consideration of highly complex latent dynamics whilst retaining the benefits of a tractable probabilistic model."
            },
            "slug": "Dynamic-Bayesian-Networks-with-Deterministic-Latent-Barber",
            "title": {
                "fragments": [],
                "text": "Dynamic Bayesian Networks with Deterministic Latent Tables"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work suggests an alternative approach in which the latent variables are modelled using deterministic conditional probability tables, which has the advantage of tractable inference even for highly complex non-linear/non-Gaussian visible conditional probability Tables."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747298"
                        ],
                        "name": "R. Duin",
                        "slug": "R.-Duin",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Duin",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 79
                            }
                        ],
                        "text": "Following the tentative advice given in (Kittler, Hatef, Duin, & Matas, 1998) (Duin, 2002), we opted for the mean of the individual votes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1632078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8518c1bae658ff2d4fb8ba41e307a681c1559f7",
            "isKey": false,
            "numCitedBy": 385,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "When more than a single classifier has been trained for the same recognition problem the question arises how this set of classifiers may be combined into a final decision rule. Several fixed combining rules are used that depend on the output values of the base classifiers only. They are almost always suboptimal. Usually, however, training sets are available. They may be used to calibrate the base classifier outputs, as well as to build a trained combining classifier using these outputs as inputs. It depends on various circumstances whether this is useful, in particular whether the training set is used for the base classifiers as well and whether they are overtrained. We present an intuitive discussion on the use of trained combiners, relating the question of the choice of the combining classifier to a similar choice in the area of dissimilarity based pattern recognition. Some simple examples are used to illustrate the discussion."
            },
            "slug": "The-combining-classifier:-to-train-or-not-to-train-Duin",
            "title": {
                "fragments": [],
                "text": "The combining classifier: to train or not to train?"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "An intuitive discussion on the use of trained combiners is presented, relating the question of the choice of the combining classifier to a similar choice in the area of dissimilarity based pattern recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Object recognition supported by user interaction for service robots"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398220613"
                        ],
                        "name": "B. Farhang-Boroujeny",
                        "slug": "B.-Farhang-Boroujeny",
                        "structuredName": {
                            "firstName": "Behrouz",
                            "lastName": "Farhang-Boroujeny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Farhang-Boroujeny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 58688997,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "e9500f1ddf1409ab84dd8d4bbc52c8f386f201aa",
            "isKey": false,
            "numCitedBy": 982,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nAdaptive filtering is an advanced and growing field in signal processing. A filter is a transmission network used in electronic circuits for the selective enhancement or reduction of specified components of an input signal. Filtering is achieved by selectively attenuating those components of the input signal which are undesired, relative to those which it is desired to enhance. This comprehensive book is both a valuable student resource and a useful technical reference for signal processing engineers in industry. The author is experienced in teaching graduates and practicing engineers and the text offers good theoretical coverage complemented by plenty of application examples."
            },
            "slug": "Adaptive-Filters:-Theory-and-Applications-Farhang-Boroujeny",
            "title": {
                "fragments": [],
                "text": "Adaptive Filters: Theory and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This comprehensive book is both a valuable student resource and a useful technical reference for signal processing engineers in industry."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712517"
                        ],
                        "name": "L. Rabiner",
                        "slug": "L.-Rabiner",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Rabiner",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Rabiner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Another common approach to time-warped recognition uses hidden Markov models (HMMs) (Rabiner, 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13618539,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "8fe2ea0a67954f1380b3387e3262f1cdb9f9b3e5",
            "isKey": false,
            "numCitedBy": 24870,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests. The fabric is defined of voids having depth as well as width and length. The fabric is usable as a material from which to form clothing for wear, or bed coverings, or sleeping bags, etc., besides use simply as a netting."
            },
            "slug": "A-tutorial-on-hidden-Markov-models-and-selected-in-Rabiner",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Hidden Markov Models and Selected Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The fabric comprises a novel type of netting which will have particular utility in screening out mosquitoes and like insects and pests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145801638"
                        ],
                        "name": "J. Kittler",
                        "slug": "J.-Kittler",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Kittler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kittler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31887966"
                        ],
                        "name": "M. Hatef",
                        "slug": "M.-Hatef",
                        "structuredName": {
                            "firstName": "Mohamad",
                            "lastName": "Hatef",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hatef"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747298"
                        ],
                        "name": "R. Duin",
                        "slug": "R.-Duin",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Duin",
                            "middleNames": [
                                "P.",
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145564537"
                        ],
                        "name": "Jiri Matas",
                        "slug": "Jiri-Matas",
                        "structuredName": {
                            "firstName": "Jiri",
                            "lastName": "Matas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiri Matas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1991617,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "5c85308415a544df7f9ac0657971f996fafff99a",
            "isKey": false,
            "numCitedBy": 5753,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a common theoretical framework for combining classifiers which use distinct pattern representations and show that many existing schemes can be considered as special cases of compound classification where all the pattern representations are used jointly to make a decision. An experimental comparison of various classifier combination schemes demonstrates that the combination rule developed under the most restrictive assumptions-the sum rule-and its derivatives consistently outperform other classifier combinations schemes. A sensitivity analysis of the various schemes to estimation errors is carried out to show that this finding can be justified theoretically."
            },
            "slug": "Combining-classifiers-Kittler-Hatef",
            "title": {
                "fragments": [],
                "text": "Combining classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A common theoretical framework for combining classifiers which use distinct pattern representations is developed and it is shown that many existing schemes can be considered as special cases of compound classification where all the pattern representations are used jointly to make a decision."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 13th International Conference on Pattern Recognition"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444394"
                        ],
                        "name": "E. Ziegel",
                        "slug": "E.-Ziegel",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ziegel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ziegel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46701966,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "e41ba5dc12c79a64dfa905c0328f95976252ffe0",
            "isKey": false,
            "numCitedBy": 12747,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Chapter 11 includes more case studies in other areas, ranging from manufacturing to marketing research. Chapter 12 concludes the book with some commentary about the scienti\u008e c contributions of MTS. The Taguchi method for design of experiment has generated considerable controversy in the statistical community over the past few decades. The MTS/MTGS method seems to lead another source of discussions on the methodology it advocates (Montgomery 2003). As pointed out by Woodall et al. (2003), the MTS/MTGS methods are considered ad hoc in the sense that they have not been developed using any underlying statistical theory. Because the \u201cnormal\u201d and \u201cabnormal\u201d groups form the basis of the theory, some sampling restrictions are fundamental to the applications. First, it is essential that the \u201cnormal\u201d sample be uniform, unbiased, and/or complete so that a reliable measurement scale is obtained. Second, the selection of \u201cabnormal\u201d samples is crucial to the success of dimensionality reduction when OAs are used. For example, if each abnormal item is really unique in the medical example, then it is unclear how the statistical distance MD can be guaranteed to give a consistent diagnosis measure of severity on a continuous scale when the larger-the-better type S/N ratio is used. Multivariate diagnosis is not new to Technometrics readers and is now becoming increasingly more popular in statistical analysis and data mining for knowledge discovery. As a promising alternative that assumes no underlying data model, The Mahalanobis\u2013Taguchi Strategy does not provide suf\u008e cient evidence of gains achieved by using the proposed method over existing tools. Readers may be very interested in a detailed comparison with other diagnostic tools, such as logistic regression and tree-based methods. Overall, although the idea of MTS/MTGS is intriguing, this book would be more valuable had it been written in a rigorous fashion as a technical reference. There is some lack of precision even in several mathematical notations. Perhaps a follow-up with additional theoretical justi\u008e cation and careful case studies would answer some of the lingering questions."
            },
            "slug": "The-Elements-of-Statistical-Learning-Ziegel",
            "title": {
                "fragments": [],
                "text": "The Elements of Statistical Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Chapter 11 includes more case studies in other areas, ranging from manufacturing to marketing research, and a detailed comparison with other diagnostic tools, such as logistic regression and tree-based methods."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 173
                            }
                        ],
                        "text": "We will deal with the problem of recognition (detection plus classification) of time-warped patterns in a background signal, and follow the approach originally proposed in (Sun et al., 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Time warping invariant neural"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "press). Training recurrent networks by evolino. Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": "press). Training recurrent networks by evolino. Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 132
                            }
                        ],
                        "text": "A closer analytical investigation and/or optimization schemes of reservoir dynamics has attracted the attention of several authors (Schiller & Steil, 2005; M.C., Xu, & Principe, accepted 2006; Schmidhuber, Gomez, Wierstra, & Gagliolo, 2006, in press; Zant, Becanovic, Ishii, Kobialka, & Plo\u0308ger,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Analyzing the weight"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Short term memory in echo state networks ( GMD - Report No . 152 ) . GMD - German National Research Institute for Computer Science"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multidimensional curve clas"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 72
                            }
                        ],
                        "text": "Leaky integrator ESNs were in passing introduced in (Jaeger, 2001) and (Jaeger, 2002b); fragments of what will be reported here appeared first in a technical report (Lukos\u030cevic\u030cius, Popovici, Jaeger, & Siewert, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 55
                            }
                        ],
                        "text": "Rationale: due to the ESNs short-term-memory capacity (Jaeger, 2002a), there is hope that the last extended state incorporates information from earlier points in the sequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 166
                            }
                        ],
                        "text": "Findings: Best results (of about 2 test misclassifications) achieved with networks of size 1,000 (about 9,000 trained parameters) with regularization by state noise (Jaeger, 2002b); no clear difference between standard and leaky integrator ESNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tutorial on training recurrent neural networks, covering BPPT, RTRL, EKF and the echo state network approach (GMD Report No. 159)"
            },
            "venue": {
                "fragments": [],
                "text": "Fraunhofer Institute AIS"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 114
                            }
                        ],
                        "text": "Various techniques have been applied to this problem (Kudo, Toyama, & Shimbo, 1999) (Geurts, 2001) (Barber, 2003) (Strickert, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 261,
                                "start": 244
                            }
                        ],
                        "text": "These tiny networks (number of trainable df\u2019s: D \u2217 (N + K) \u2217 9 \u2248 460 for D = 3, N = 4) achieved the best test error rate under this design option, with a bit less than 6 misclassifications, which amounts to the current best from the literature (Strickert, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Self-organizing neural networks for sequence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 72
                            }
                        ],
                        "text": "Leaky integrator ESNs were in passing introduced in (Jaeger, 2001) and (Jaeger, 2002b); fragments of what will be reported here appeared first in a technical report (Lukos\u030cevic\u030cius, Popovici, Jaeger, & Siewert, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 55
                            }
                        ],
                        "text": "Rationale: due to the ESNs short-term-memory capacity (Jaeger, 2002a), there is hope that the last extended state incorporates information from earlier points in the sequence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 166
                            }
                        ],
                        "text": "Findings: Best results (of about 2 test misclassifications) achieved with networks of size 1,000 (about 9,000 trained parameters) with regularization by state noise (Jaeger, 2002b); no clear difference between standard and leaky integrator ESNs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Short term memory in echo state networks (GMD-Report No. 152)"
            },
            "venue": {
                "fragments": [],
                "text": "GMD - German National Research Institute for Computer Science"
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 40,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Optimization-and-applications-of-echo-state-with-Jaeger-Luko\u0161evi\u010dius/a10ec7cc6c42c7780ef631c038b16c49ed865038?sort=total-citations"
}