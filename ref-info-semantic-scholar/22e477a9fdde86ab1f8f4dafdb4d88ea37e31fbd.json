{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2570381"
                        ],
                        "name": "Brody Huval",
                        "slug": "Brody-Huval",
                        "structuredName": {
                            "firstName": "Brody",
                            "lastName": "Huval",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brody Huval"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156632012"
                        ],
                        "name": "Tao Wang",
                        "slug": "Tao-Wang",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25629078"
                        ],
                        "name": "David J. Wu",
                        "slug": "David-J.-Wu",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2301680"
                        ],
                        "name": "Bryan Catanzaro",
                        "slug": "Bryan-Catanzaro",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Catanzaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan Catanzaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "Currently, these workloads are mostly executed on multi\u00adcores using SIMD [41], on GPUs [5], or on FPGAs \n[3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "Currently, these workloads are mostly executed on multicores using SIMD [41], on GPUs [5], or on FPGAs [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8604637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1208ac421cf8ff67b27d93cd19ae42b8d596f95",
            "isKey": false,
            "numCitedBy": 679,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloudlike computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks."
            },
            "slug": "Deep-learning-with-COTS-HPC-systems-Coates-Huval",
            "title": {
                "fragments": [],
                "text": "Deep learning with COTS HPC systems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents technical details and results from their own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI, and shows that it can scale to networks with over 11 billion parameters using just 16 machines."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715548"
                        ],
                        "name": "Mark Z. Mao",
                        "slug": "Mark-Z.-Mao",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Mao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Z. Mao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Currently, these workloads are mostly executed on multi\u00adcores using SIMD [41], on GPUs [5], or on FPGAs \n[3]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": "On 10 of the largest layers found in recent CNNs and DNNs, this accelerator is \n117.87x faster and 21.08x more energy-ef.cient (including main memory accesses) on aver\u00adage than an 128-bit \nSIMD core clocked at 2GHz."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "Note that the architecture can implement either per-image or batch processing [41], only the generated layer control code would change."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "Currently, these workloads are mostly executed on multicores using SIMD [41], on GPUs [5], or on FPGAs [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 223
                            }
                        ],
                        "text": "Copyright &#38;#169; 2014 \nACM 978-1-4503-2305-5/14/03. . . $15.00. http://dx.doi.org/10.1145/http://dx.doi.org/10.1145/2541940.2541967 \n neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD \nproces\u00adsor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15196840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbeaa499e10e98515f7e1c4ad89165e8c0677427",
            "isKey": true,
            "numCitedBy": 692,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in deep learning have made the use of large, deep neural networks with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden, even for modern CPUs. For this reason, GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout, batching of the computation, the use of SSE2 instructions, and particularly leverage SSSE3 and SSE4 fixed-point instructions which provide a 3\u00d7 improvement over an optimized floating-point baseline. We use speech recognition as an example task, and show that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10\u00d7 speedup over an unoptimized baseline and a 4\u00d7 speedup over an aggressively optimized floating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware."
            },
            "slug": "Improving-the-speed-of-neural-networks-on-CPUs-Vanhoucke-Senior",
            "title": {
                "fragments": [],
                "text": "Improving the speed of neural networks on CPUs"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper uses speech recognition as an example task, and shows that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10\u00d7 speedup over an unoptimized baseline and a 4\u00d7 speed up over an aggressively optimized floating-point baseline at no cost in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40894913"
                        ],
                        "name": "Tian-ping Chen",
                        "slug": "Tian-ping-Chen",
                        "structuredName": {
                            "firstName": "Tian-ping",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tian-ping Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7377735"
                        ],
                        "name": "Yunji Chen",
                        "slug": "Yunji-Chen",
                        "structuredName": {
                            "firstName": "Yunji",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunji Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2492970"
                        ],
                        "name": "M. Duranton",
                        "slug": "M.-Duranton",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Duranton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Duranton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145461473"
                        ],
                        "name": "Qi Guo",
                        "slug": "Qi-Guo",
                        "structuredName": {
                            "firstName": "Qi",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qi Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2048185"
                        ],
                        "name": "Atif Hashmi",
                        "slug": "Atif-Hashmi",
                        "structuredName": {
                            "firstName": "Atif",
                            "lastName": "Hashmi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Atif Hashmi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704076"
                        ],
                        "name": "M. Lipasti",
                        "slug": "M.-Lipasti",
                        "structuredName": {
                            "firstName": "Mikko",
                            "lastName": "Lipasti",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lipasti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3354588"
                        ],
                        "name": "Andrew Nere",
                        "slug": "Andrew-Nere",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Nere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Nere"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055117356"
                        ],
                        "name": "Shi Qiu",
                        "slug": "Shi-Qiu",
                        "structuredName": {
                            "firstName": "Shi",
                            "lastName": "Qiu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shi Qiu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69343681"
                        ],
                        "name": "M. Sebag",
                        "slug": "M.-Sebag",
                        "structuredName": {
                            "firstName": "Mich\u00e8le",
                            "lastName": "Sebag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sebag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731764"
                        ],
                        "name": "O. Temam",
                        "slug": "O.-Temam",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Temam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Temam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 257
                            }
                        ],
                        "text": "This trend even starts to percolate in our community where it turns out that about half of the benchmarks of PARSEC [2], a suite partly introduced to highlight the emergence of new types of applications, can be implemented using machine-learning algorithms [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8027303,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf7e0f0e113a1e29be5f02dabb21531592028f26",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent technology trends have indicated that, although device sizes will continue to scale as they have in the past, supply voltage scaling has ended. As a result, future chips can no longer rely on simply increasing the operational core count to improve performance without surpassing a reasonable power budget. Alternatively, allocating die area towards accelerators targeting an application, or an application domain, appears quite promising, and this paper makes an argument for a neural network hardware accelerator. After being hyped in the 1990s, then fading away for almost two decades, there is a surge of interest in hardware neural networks because of their energy and fault-tolerance properties. At the same time, the emergence of high-performance applications like Recognition, Mining, and Synthesis (RMS) suggest that the potential application scope of a hardware neural network accelerator would be broad. In this paper, we want to highlight that a hardware neural network accelerator is indeed compatible with many of the emerging high-performance workloads, currently accepted as benchmarks for high-performance micro-architectures. For that purpose, we develop and evaluate software neural network implementations of 5 (out of 12) RMS applications from the PARSEC Benchmark Suite. Our results show that neural network implementations can achieve competitive results, with respect to application-specific quality metrics, on these 5 RMS applications."
            },
            "slug": "BenchNN:-On-the-broad-potential-application-scope-Chen-Chen",
            "title": {
                "fragments": [],
                "text": "BenchNN: On the broad potential application scope of hardware neural network accelerators"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Software neural network implementations of 5 RMS applications from the PARSEC Benchmark Suite are developed and evaluated and it is highlighted that a hardware neural network accelerator is indeed compatible with many of the emerging high- performance workloads, currently accepted as benchmarks for high-performance micro-architectures."
            },
            "venue": {
                "fragments": [],
                "text": "2012 IEEE International Symposium on Workload Characterization (IISWC)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678776"
                        ],
                        "name": "Zidong Du",
                        "slug": "Zidong-Du",
                        "structuredName": {
                            "firstName": "Zidong",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zidong Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740501"
                        ],
                        "name": "K. Palem",
                        "slug": "K.-Palem",
                        "structuredName": {
                            "firstName": "Krishna",
                            "lastName": "Palem",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Palem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2647448"
                        ],
                        "name": "L. Avinash",
                        "slug": "L.-Avinash",
                        "structuredName": {
                            "firstName": "Lingamneni",
                            "lastName": "Avinash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Avinash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731764"
                        ],
                        "name": "O. Temam",
                        "slug": "O.-Temam",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Temam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Temam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7377735"
                        ],
                        "name": "Yunji Chen",
                        "slug": "Yunji-Chen",
                        "structuredName": {
                            "firstName": "Yunji",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunji Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7514065"
                        ],
                        "name": "Chengyong Wu",
                        "slug": "Chengyong-Wu",
                        "structuredName": {
                            "firstName": "Chengyong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengyong Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 137
                            }
                        ],
                        "text": "The latter property has also been leveraged to trade application accuracy for energy efficiency through hardware neural processing units [9, 11]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15206416,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87e037c174b9896de6a2afa209a57306dc661a64",
            "isKey": false,
            "numCitedBy": 105,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, inexact computing has been increasingly regarded as one of the most promising approaches for reducing energy consumption in many applications that can tolerate a degree of inaccuracy. Driven by the principle of trading tolerable amounts of application accuracy in return for significant resource savings - the energy consumed, the (critical path) delay and the (silicon) area being the resources - this approach has been limited to certain application domains. In this paper, we propose to expand the application scope, error tolerance as well as the energy savings of inexact computing systems through neural network architectures. Such neural networks are fast emerging as popular candidate accelerators for future heterogeneous multi-core platforms, and have flexible error tolerance limits owing to their ability to be trained. Our results based on simulated 65nm technology designs demonstrate that the proposed inexact neural network accelerator could achieve 43.91%-62.49% savings in energy consumption (with corresponding delay and area savings being 18.79% and 31.44% respectively) when compared to existing baseline neural network implementation, at the cost of an accuracy loss (quantified as the Mean Square Error (MSE) which increases from 0.14 to 0.20 on average)."
            },
            "slug": "Leveraging-the-error-resilience-of-machine-learning-Du-Palem",
            "title": {
                "fragments": [],
                "text": "Leveraging the error resilience of machine-learning applications for designing highly energy efficient accelerators"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes to expand the application scope, error tolerance as well as the energy savings of inexact computing systems through neural network architectures, and demonstrates that the proposed inexact neural network accelerator could achieve 43.91%-62.49% savings in energy consumption."
            },
            "venue": {
                "fragments": [],
                "text": "2014 19th Asia and South Pacific Design Automation Conference (ASP-DAC)"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "22919539"
                        ],
                        "name": "K. Fan",
                        "slug": "K.-Fan",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Fan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Fan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1942300"
                        ],
                        "name": "M. Kudlur",
                        "slug": "M.-Kudlur",
                        "structuredName": {
                            "firstName": "Manjunath",
                            "lastName": "Kudlur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kudlur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31613624"
                        ],
                        "name": "Ganesh S. Dasika",
                        "slug": "Ganesh-S.-Dasika",
                        "structuredName": {
                            "firstName": "Ganesh",
                            "lastName": "Dasika",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ganesh S. Dasika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721289"
                        ],
                        "name": "S. Mahlke",
                        "slug": "S.-Mahlke",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Mahlke",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mahlke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6544521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1209ca733b9ab0ad12b637319fbbec1758c00a7f",
            "isKey": false,
            "numCitedBy": 78,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "New media and signal processing applications demand ever higher performance while operating within the tight power constraints of mobile devices. A range of hardware implementations is available to deliver computation with varying degrees of area and power efficiency, from general-purpose processors to application-specific integrated circuits (ASICs). The tradeoff of moving towards more efficient customized solutions such as ASICs is the lack of flexibility in terms of hardware reusability and programmability. In this paper, we propose a customized semi-programmable loop accelerator architecture that exploits the efficiency gains available through high levels of customization, while maintaining sufficient flexibility to execute multiple similar loops. A customized instance of the loop accelerator architecture is generated for a particular loop and then the data and control paths are proactively generalized in an efficient manner to increase flexibility. A compiler mapping phase is then able to map other loops onto the same hardware. The efficiency of the programmable accelerator is compared with non-programmable accelerators and with the OpenRISC 1200 general purpose processor. The programmable accelerator is able to achieve up to 34x better power efficiency and 30x better area efficiency than a simple general purpose processor, while trading off as little as 2x power and area efficiency to the non-programmable accelerator."
            },
            "slug": "Bridging-the-computation-gap-between-programmable-Fan-Kudlur",
            "title": {
                "fragments": [],
                "text": "Bridging the computation gap between programmable processors and hardwired accelerators"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes a customized semi-programmable loop accelerator architecture that exploits the efficiency gains available through high levels of customization, while maintaining sufficient flexibility to execute multiple similar loops."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 15th International Symposium on High Performance Computer Architecture"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731764"
                        ],
                        "name": "O. Temam",
                        "slug": "O.-Temam",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Temam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Temam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 171
                            }
                        ],
                        "text": "However, the aforementioned trends have already been iden\u00adti.ed by a number of researchers who have \nproposed accel\u00aderators implementing Convolutional Neural Networks [3] or Multi-Layer Perceptrons [38]; \naccelerators focusing on other domains, such as image processing, also propose ef.cient implementations \nof some of the primitives used by machine\u00adlearning algorithms, such as convolutions [33]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 92
                            }
                        ],
                        "text": "This approach has been recently used for perceptron or spike-based hardware neural networks [30, 38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 250
                            }
                        ],
                        "text": "Our study not only confirms previous observations that dedicated storage is key for achieving good performance and power [14], but it also highlights that, beyond exploiting locality at the level of registers located close to computational operators [33, 38], considering memory as a prime-order concern can profoundly affect accelerator design."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 327
                            }
                        ],
                        "text": "However, recently, due to simultaneous trends in applications, machine-learning and technology constraints, hardware neural networks have been increasingly considered as potential accelerators, either for very dedicated functionalities within a processor, such as branch prediction [1], or for their fault-tolerance properties [15, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 199
                            }
                        ],
                        "text": "However, all these works have first, and successfully, focused on efficiently implementing the computational primitives but they either voluntarily ignore memory transfers for the sake of simplicity [33, 38], or they directly plug their computational accelerator to memory via a more or less sophisticated DMA [3, 13, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 41
                            }
                        ],
                        "text": "As previously proposed in the literature [23, 38], the sigmoid of NFU-3 (for classifier and convolutional layers) can be efficiently implemented using piecewise linear interpolation (f(x) = ai \u00d7 x + bi, x \u2208 [xi;xi+1]) with negligible loss of accuracy (16 segments are sufficient) [24], see Figure 9."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "[14] report an energy ratio of about 500x, and 974x has been reported for a small Multi-Layer Perceptron [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "For instance, an execution time of 15ns and an energy reduction of 974x over a core has been reported for a 90-10-10 (90 inputs, 10 hidden, 10 outputs) perceptron [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "However, the aforementioned trends have already been identified by a number of researchers who have proposed accelerators implementing Convolutional Neural Networks [3] or Multi-Layer Perceptrons [38]; accelerators focusing on other domains, such as image processing, also propose efficient implementations of some of the primitives used by machinelearning algorithms, such as convolutions [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11265441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32c3d778d8cce464b3ad3de277666295f3a0b02a",
            "isKey": true,
            "numCitedBy": 156,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Due to the evolution of technology constraints, especially energy constraints which may lead to heterogeneous multi-cores, and the increasing number of defects, the design of defect-tolerant accelerators for heterogeneous multi-cores may become a major micro-architecture research issue. Most custom circuits are highly defect sensitive, a single transistor can wreck such circuits. On the contrary, artificial neural networks (ANNs) are inherently error tolerant algorithms. And the emergence of high-performance applications implementing recognition and mining tasks, for which competitive ANN-based algorithms exist, drastically expands the potential application scope of a hardware ANN accelerator. However, while the error tolerance of ANN algorithms is well documented, there are few in-depth attempts at demonstrating that an actual hardware ANN would be tolerant to faulty transistors. Most fault models are abstract and cannot demonstrate that the error tolerance of ANN algorithms can be translated into the defect tolerance of hardware ANN accelerators. In this article, we introduce a hardware ANN geared towards defect tolerance and energy efficiency, by spatially expanding the ANN. In order to precisely assess the defect tolerance capability of this hardware ANN, we introduce defects at the level of transistors, and then assess the impact of such defects on the hardware ANN functional behavior. We empirically show that the conceptual error tolerance of neural networks does translate into the defect tolerance of hardware neural networks, paving the way for their introduction in heterogeneous multi-cores as intrinsically defect-tolerant and energy-efficient accelerators."
            },
            "slug": "A-defect-tolerant-accelerator-for-emerging-Temam",
            "title": {
                "fragments": [],
                "text": "A defect-tolerant accelerator for emerging high-performance applications"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is empirically show that the conceptual error tolerance of neural networks does translate into the defect tolerance of hardware neural networks, paving the way for their introduction in heterogeneous multi-cores as intrinsically defect-tolerant and energy-efficient accelerators."
            },
            "venue": {
                "fragments": [],
                "text": "2012 39th Annual International Symposium on Computer Architecture (ISCA)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752242"
                        ],
                        "name": "S. Chakradhar",
                        "slug": "S.-Chakradhar",
                        "structuredName": {
                            "firstName": "Srimat",
                            "lastName": "Chakradhar",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chakradhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329060"
                        ],
                        "name": "M. Sankaradass",
                        "slug": "M.-Sankaradass",
                        "structuredName": {
                            "firstName": "Murugan",
                            "lastName": "Sankaradass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sankaradass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101580"
                        ],
                        "name": "V. Jakkula",
                        "slug": "V.-Jakkula",
                        "structuredName": {
                            "firstName": "Venkata",
                            "lastName": "Jakkula",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Jakkula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2468150"
                        ],
                        "name": "S. Cadambi",
                        "slug": "S.-Cadambi",
                        "structuredName": {
                            "firstName": "Srihari",
                            "lastName": "Cadambi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cadambi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 167
                            }
                        ],
                        "text": "\u30a2\u30af\u30bb\u30e9\u30ec\u30fc\u30bf\u306f\u4e3b\u306b\u3001\u30b3\u30f3\u30c8\u30ed\u30fc\u30eb\u30ed\u30b8\u30c3\u30af(CP)\u3001 \u5165\u529b\u30cb\u30e5\u30fc\u30ed\u30f3\u3092\u4fdd\u6301\u3059\u308b\u5165\u529b\u30d0\u30c3\u30d5\u30a1(NBin)\u3001\u30d1\u30a4\u30d7\u30e9 \u30a4\u30f3\u5316\u3055\u308c\u305f\u6f14\u7b97\u5668 (NFU-1,2,3)\u3001\u30b7\u30ca\u30d7\u30b9\u7d50\u5408\u8377\u91cd\u3092\u4fdd \u6301\u3059\u308b\u30d0\u30c3\u30d5\u30a1(SB)\u3001\u51fa\u529b\u30cb\u30e5\u30fc\u30ed\u30f3\u3092\u4fdd\u6301\u3059\u308b\u51fa\u529b\u30d0\u30c3 \u30d5\u30a1(NBout)\u3067\u69cb\u6210\u3055\u308c\u3066\u3044\u308b\u3002DMA\u30a8\u30f3\u30b8\u30f3 (direct memory access)[3]\u3092\u30d0\u30c3\u30d5\u30a1\u306b\u53d6\u308a\u4ed8\u3051\u308b\u3053\u3068\u3067\u30e1\u30e2\u30ea \u3068\u30d0\u30c3\u30d5\u30a1\u306e\u30c7\u30fc\u30bf\u8ee2\u9001\u3068\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8a08\u7b97 \u51e6\u7406\u3092\u30aa\u30fc\u30d0\u30fc\u30e9\u30c3\u30d7\u3067\u304d\u308b\u3088\u3046\u306b\u306a\u308a\u3001\u30e1\u30e2\u30ea\u30a2\u30af\u30bb \u30b9\u306e\u6027\u80fd\u3092\u5411\u4e0a\u3055\u305b\u308b\u3053\u3068\u304c\u3067\u304d\u308b\u3002\u307e\u305f\u3001\u30d0\u30c3\u30d5\u30a1\u4e0a\u306e \u30c7\u30fc\u30bf\u3092\u518d\u5229\u7528\u3059\u308b\u3053\u3068\u3067\u30e1\u30e2\u30ea\u30a2\u30af\u30bb\u30b9\u306e\u30a8\u30cd\u30eb\u30ae\u30fc \u3092\u524a\u6e1b\u3067\u304d\u308b\u3002"
                    },
                    "intents": []
                }
            ],
            "corpusId": 3350152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1c4e2fa071046569a05e9cfdf13496d094025dd",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional neural networks (CNN) applications range from recognition and reasoning (such as handwriting recognition, facial expression recognition and video surveillance) to intelligent text applications such as semantic text analysis and natural language processing applications. Two key observations drive the design of a new architecture for CNN. First, CNN workloads exhibit a widely varying mix of three types of parallelism: parallelism within a convolution operation, intra-output parallelism where multiple input sources (features) are combined to create a single output, and inter-output parallelism where multiple, independent outputs (features) are computed simultaneously. Workloads differ significantly across different CNN applications, and across different layers of a CNN. Second, the number of processing elements in an architecture continues to scale (as per Moore's law) much faster than off-chip memory bandwidth (or pin-count) of chips. Based on these two observations, we show that for a given number of processing elements and off-chip memory bandwidth, a new CNN hardware architecture that dynamically configures the hardware on-the-fly to match the specific mix of parallelism in a given workload gives the best throughput performance. Our CNN compiler automatically translates high abstraction network specification into a parallel microprogram (a sequence of low-level VLIW instructions) that is mapped, scheduled and executed by the coprocessor. Compared to a 2.3 GHz quad-core, dual socket Intel Xeon, 1.35 GHz C870 GPU, and a 200 MHz FPGA implementation, our 120 MHz dynamically configurable architecture is 4x to 8x faster. This is the first CNN architecture to achieve real-time video stream processing (25 to 30 frames per second) on a wide range of object detection and recognition tasks."
            },
            "slug": "A-dynamically-configurable-coprocessor-for-neural-Chakradhar-Sankaradass",
            "title": {
                "fragments": [],
                "text": "A dynamically configurable coprocessor for convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This is the first CNN architecture to achieve real-time video stream processing (25 to 30 frames per second) on a wide range of object detection and recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ISCA"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2408151"
                        ],
                        "name": "P. Merolla",
                        "slug": "P.-Merolla",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Merolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merolla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2100344"
                        ],
                        "name": "J. Arthur",
                        "slug": "J.-Arthur",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Arthur",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Arthur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3349882"
                        ],
                        "name": "Filipp Akopyan",
                        "slug": "Filipp-Akopyan",
                        "structuredName": {
                            "firstName": "Filipp",
                            "lastName": "Akopyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Filipp Akopyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39536844"
                        ],
                        "name": "N. Imam",
                        "slug": "N.-Imam",
                        "structuredName": {
                            "firstName": "Nabil",
                            "lastName": "Imam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Imam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144576833"
                        ],
                        "name": "R. Manohar",
                        "slug": "R.-Manohar",
                        "structuredName": {
                            "firstName": "Rajit",
                            "lastName": "Manohar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manohar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944330"
                        ],
                        "name": "D. Modha",
                        "slug": "D.-Modha",
                        "structuredName": {
                            "firstName": "Dharmendra",
                            "lastName": "Modha",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Modha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 92
                            }
                        ],
                        "text": "This approach has been recently used for perceptron or spike-based hardware neural networks [30, 38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 91
                            }
                        ],
                        "text": "While several of these neuromorphic architectures have been applied to computational tasks [30, 43], the specific bio-inspired information representation (spiking neural networks) they rely on may not be competitive with stateof-the-art neural networks, though this remains an open debate at the threshold between neuroscience and machinelearning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5277100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41d044095628119bda85189d7b4e8acd7bd8d79e",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The grand challenge of neuromorphic computation is to develop a flexible brain-like architecture capable of a wide array of real-time applications, while striving towards the ultra-low power consumption and compact size of the human brain\u2014within the constraints of existing silicon and post-silicon technologies. To this end, we fabricated a key building block of a modular neuromorphic architecture, a neurosynaptic core, with 256 digital integrate-and-fire neurons and a 1024\u00d7256 bit SRAM crossbar memory for synapses using IBM's 45nm SOI process. Our fully digital implementation is able to leverage favorable CMOS scaling trends, while ensuring one-to-one correspondence between hardware and software. In contrast to a conventional von Neumann architecture, our core tightly integrates computation (neurons) alongside memory (synapses), which allows us to implement efficient fan-out (communication) in a naturally parallel and event-driven manner, leading to ultra-low active power consumption of 45pJ/spike. The core is fully configurable in terms of neuron parameters, axon types, and synapse states and is thus amenable to a wide range of applications. As an example, we trained a restricted Boltzmann machine offline to perform a visual digit recognition task, and mapped the learned weights to our chip."
            },
            "slug": "A-digital-neurosynaptic-core-using-embedded-memory-Merolla-Arthur",
            "title": {
                "fragments": [],
                "text": "A digital neurosynaptic core using embedded crossbar memory with 45pJ per spike in 45nm"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work fabricated a key building block of a modular neuromorphic architecture, a neurosynaptic core, with 256 digital integrate-and-fire neurons and a 1024\u00d7256 bit SRAM crossbar memory for synapses using IBM's 45nm SOI process, leading to ultra-low active power consumption."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE Custom Integrated Circuits Conference (CICC)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37116969"
                        ],
                        "name": "R. Hameed",
                        "slug": "R.-Hameed",
                        "structuredName": {
                            "firstName": "Rehan",
                            "lastName": "Hameed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hameed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2872576"
                        ],
                        "name": "W. Qadeer",
                        "slug": "W.-Qadeer",
                        "structuredName": {
                            "firstName": "Wajahat",
                            "lastName": "Qadeer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Qadeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40402375"
                        ],
                        "name": "M. Wachs",
                        "slug": "M.-Wachs",
                        "structuredName": {
                            "firstName": "Megan",
                            "lastName": "Wachs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wachs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096950929"
                        ],
                        "name": "Omid Azizi",
                        "slug": "Omid-Azizi",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Azizi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omid Azizi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344899"
                        ],
                        "name": "A. Solomatnikov",
                        "slug": "A.-Solomatnikov",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Solomatnikov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Solomatnikov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152840677"
                        ],
                        "name": "Benjamin C. Lee",
                        "slug": "Benjamin-C.-Lee",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Lee",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145326337"
                        ],
                        "name": "S. Richardson",
                        "slug": "S.-Richardson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Richardson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700331"
                        ],
                        "name": "C. Kozyrakis",
                        "slug": "C.-Kozyrakis",
                        "structuredName": {
                            "firstName": "Christoforos",
                            "lastName": "Kozyrakis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kozyrakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144764327"
                        ],
                        "name": "M. Horowitz",
                        "slug": "M.-Horowitz",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Horowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Horowitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 121
                            }
                        ],
                        "text": "Our study not only confirms previous observations that dedicated storage is key for achieving good performance and power [14], but it also highlights that, beyond exploiting locality at the level of registers located close to computational operators [33, 38], considering memory as a prime-order concern can profoundly affect accelerator design."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 298
                            }
                        ],
                        "text": "First, while the chunk of input neurons is loaded from NBin and used to compute the partial sums, it would be inefficient to let the partial sum exit the NFU pipeline and then re-load it into the pipeline for each entry of the NBin buffer, since data transfers are a major source of energy expense [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Accelerators can range from processors tuned for certain tasks, to ASIC-like circuits such as H264 [14], or more flexible accelerators capable of targeting a broad range of, but not all, tasks [12, 44] such as QsCores [42], or accelerators for image processing [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] report an energy ratio of about 500x, and 974x has been reported for a small Multi-Layer Perceptron [38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3165696,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f59cd7ebe88df0a4da66657faa4d4a50c7c36004",
            "isKey": true,
            "numCitedBy": 464,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "Due to their high volume, general-purpose processors, and now chip multiprocessors (CMPs), are much more cost effective than ASICs, but lag significantly in terms of performance and energy efficiency. This paper explores the sources of these performance and energy overheads in general-purpose processing systems by quantifying the overheads of a 720p HD H.264 encoder running on a general-purpose CMP system. It then explores methods to eliminate these overheads by transforming the CPU into a specialized system for H.264 encoding. We evaluate the gains from customizations useful to broad classes of algorithms, such as SIMD units, as well as those specific to particular computation, such as customized storage and functional units. The ASIC is 500x more energy efficient than our original four-processor CMP. Broadly applicable optimizations improve performance by 10x and energy by 7x. However, the very low energy costs of actual core ops (100s fJ in 90nm) mean that over 90% of the energy used in these solutions is still \"overhead\". Achieving ASIC-like performance and efficiency requires algorithm-specific optimizations. For each sub-algorithm of H.264, we create a large, specialized functional unit that is capable of executing 100s of operations per instruction. This improves performance and energy by an additional 25x and the final customized CMP matches an ASIC solution's performance within 3x of its energy and within comparable area."
            },
            "slug": "Understanding-sources-of-inefficiency-in-chips-Hameed-Qadeer",
            "title": {
                "fragments": [],
                "text": "Understanding sources of inefficiency in general-purpose chips"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The sources of these performance and energy overheads in general-purpose processing systems are explored by quantifying the overheads of a 720p HD H.264 encoder running on a general- Purpose CMP system and exploring methods to eliminate these overheads by transforming the CPU into a specialized system for H. 264 encoding."
            },
            "venue": {
                "fragments": [],
                "text": "ISCA"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49618465"
                        ],
                        "name": "Daniel Larkin",
                        "slug": "Daniel-Larkin",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Larkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Larkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693451"
                        ],
                        "name": "Andrew Kinane",
                        "slug": "Andrew-Kinane",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Kinane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Kinane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98536322"
                        ],
                        "name": "N. O'Connor",
                        "slug": "N.-O'Connor",
                        "structuredName": {
                            "firstName": "Noel",
                            "lastName": "O'Connor",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. O'Connor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 80
                            }
                        ],
                        "text": ", 8 bits or even less) have almost no impact on the accuracy of neural networks [8, 17, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 280
                            }
                        ],
                        "text": "As previously proposed in the literature [23, 38], the sigmoid of NFU-3 (for classifier and convolutional layers) can be efficiently implemented using piecewise linear interpolation (f(x) = ai \u00d7 x + bi, x \u2208 [xi;xi+1]) with negligible loss of accuracy (16 segments are sufficient) [24], see Figure 9."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aee60c94b8c59e377634ae8e9190249d5a81e29d",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 176,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of accelerating large artificial neural networks (ANN), whose topology and weights can evolve via the use of a genetic algorithm. The proposed digital hardware architecture is capable of processing any evolved network topology, whilst at the same time providing a good trade off between throughput, area and power consumption. The latter is vital for a longer battery life on mobile devices. The architecture uses multiple parallel arithmetic units in each processing element (PE). Memory partitioning and data caching are used to minimise the effects of PE pipeline stalling. A first order minimax polynomial approximation scheme, tuned via a genetic algorithm, is used for the activation function generator. Efficient arithmetic circuitry, which leverages modified Booth recoding, column compressors and carry save adders, is adopted throughout the design."
            },
            "slug": "Towards-Hardware-Acceleration-of-Neuroevolution-for-Larkin-Kinane",
            "title": {
                "fragments": [],
                "text": "Towards Hardware Acceleration of Neuroevolution for Multimedia Processing Applications on Mobile Devices"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This paper addresses the problem of accelerating large artificial neural networks (ANN), whose topology and weights can evolve via the use of a genetic algorithm, and proposes a digital hardware architecture capable of processing any evolved network topology, whilst providing a good trade off between throughput, area and power consumption."
            },
            "venue": {
                "fragments": [],
                "text": "ICONIP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2048185"
                        ],
                        "name": "Atif Hashmi",
                        "slug": "Atif-Hashmi",
                        "structuredName": {
                            "firstName": "Atif",
                            "lastName": "Hashmi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Atif Hashmi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3354588"
                        ],
                        "name": "Andrew Nere",
                        "slug": "Andrew-Nere",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Nere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Nere"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110226723"
                        ],
                        "name": "James J. Thomas",
                        "slug": "James-J.-Thomas",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Thomas",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James J. Thomas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704076"
                        ],
                        "name": "M. Lipasti",
                        "slug": "M.-Lipasti",
                        "structuredName": {
                            "firstName": "Mikko",
                            "lastName": "Lipasti",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lipasti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 335,
                                "start": 327
                            }
                        ],
                        "text": "However, recently, due to simultaneous trends in applications, machine-learning and technology constraints, hardware neural networks have been increasingly considered as potential accelerators, either for very dedicated functionalities within a processor, such as branch prediction [1], or for their fault-tolerance properties [15, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13960593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "741092a808d3d8b98448011d1e70ba31efb346de",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "The desire to create novel computing systems, paired with recent advances in neuroscientific understanding of the brain, has led researchers to develop neuromorphic architectures that emulate the brain. To date, such models are developed, trained, and deployed on the same substrate. However, excessive co-dependence between the substrate and the algorithm prevents portability, or at the very least requires reconstructing and retraining the model whenever the substrate changes. This paper proposes a well-defined abstraction layer -- the Neuromorphic instruction set architecture, or NISA -- that separates a neural application's algorithmic specification from the underlying execution substrate, and describes the Aivo framework, which demonstrates the concrete advantages of such an abstraction layer. Aivo consists of a NISA implementation for a rate-encoded neuromorphic system based on the cortical column abstraction, a state-of-the-art integrated development and runtime environment (IDE), and various profile-based optimization tools. Aivo's IDE generates code for emulating cortical networks on the host CPU, multiple GPGPUs, or as boolean functions. Its runtime system can deploy and adaptively optimize cortical networks in a manner similar to conventional just-in-time compilers in managed runtime systems (e.g. Java, C#).\n We demonstrate the abilities of the NISA abstraction by constructing a cortical network model of the mammalian visual cortex, deploying on multiple execution substrates, and utilizing the various optimization tools we have created. For this hierarchical configuration, Aivo's profiling based network optimization tools reduce the memory footprint by 50% and improve the execution time by a factor of 3x on the host CPU. Deploying the same network on a single GPGPU results in a 30x speedup. We further demonstrate that a speedup of 480x can be achieved by deploying a massively scaled cortical network across three GPGPUs. Finally, converting a trained hierarchical network to C/C++ boolean constructs on the host CPU results in 44x speedup."
            },
            "slug": "A-case-for-neuromorphic-ISAs-Hashmi-Nere",
            "title": {
                "fragments": [],
                "text": "A case for neuromorphic ISAs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A well-defined abstraction layer -- the Neuromorphic instruction set architecture, or NISA -- is proposed that separates a neural application's algorithmic specification from the underlying execution substrate, and the Aivo framework is described, which demonstrates the concrete advantages of such an abstraction layer."
            },
            "venue": {
                "fragments": [],
                "text": "ASPLOS XVI"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2872576"
                        ],
                        "name": "W. Qadeer",
                        "slug": "W.-Qadeer",
                        "structuredName": {
                            "firstName": "Wajahat",
                            "lastName": "Qadeer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Qadeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37116969"
                        ],
                        "name": "R. Hameed",
                        "slug": "R.-Hameed",
                        "structuredName": {
                            "firstName": "Rehan",
                            "lastName": "Hameed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hameed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788630"
                        ],
                        "name": "O. Shacham",
                        "slug": "O.-Shacham",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Shacham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Shacham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72076105"
                        ],
                        "name": "P. Venkatesan",
                        "slug": "P.-Venkatesan",
                        "structuredName": {
                            "firstName": "Preethi",
                            "lastName": "Venkatesan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Venkatesan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700331"
                        ],
                        "name": "C. Kozyrakis",
                        "slug": "C.-Kozyrakis",
                        "structuredName": {
                            "firstName": "Christoforos",
                            "lastName": "Kozyrakis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kozyrakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144764327"
                        ],
                        "name": "M. Horowitz",
                        "slug": "M.-Horowitz",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Horowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Horowitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 394,
                                "start": 390
                            }
                        ],
                        "text": "However, the aforementioned trends have already been identified by a number of researchers who have proposed accelerators implementing Convolutional Neural Networks [3] or Multi-Layer Perceptrons [38]; accelerators focusing on other domains, such as image processing, also propose efficient implementations of some of the primitives used by machinelearning algorithms, such as convolutions [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 250
                            }
                        ],
                        "text": "Our study not only confirms previous observations that dedicated storage is key for achieving good performance and power [14], but it also highlights that, beyond exploiting locality at the level of registers located close to computational operators [33, 38], considering memory as a prime-order concern can profoundly affect accelerator design."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 199
                            }
                        ],
                        "text": "However, all these works have first, and successfully, focused on efficiently implementing the computational primitives but they either voluntarily ignore memory transfers for the sake of simplicity [33, 38], or they directly plug their computational accelerator to memory via a more or less sophisticated DMA [3, 13, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 261
                            }
                        ],
                        "text": "Accelerators can range from processors tuned for certain tasks, to ASIC-like circuits such as H264 [14], or more flexible accelerators capable of targeting a broad range of, but not all, tasks [12, 44] such as QsCores [42], or accelerators for image processing [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3117823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d50d7cd78126cd4df5dda62f1a75c89d095bc8a5",
            "isKey": true,
            "numCitedBy": 165,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on the trade-off between flexibility and efficiency in specialized computing. We observe that specialized units achieve most of their efficiency gains by tuning data storage and compute structures and their connectivity to the data-flow and data-locality patterns in the kernels. Hence, by identifying key data-flow patterns used in a domain, we can create efficient engines that can be programmed and reused across a wide range of applications. We present an example, the Convolution Engine (CE), specialized for the convolution-like data-flow that is common in computational photography, image processing, and video processing applications. CE achieves energy efficiency by capturing data reuse patterns, eliminating data transfer overheads, and enabling a large number of operations per memory access. We quantify the tradeoffs in efficiency and flexibility and demonstrate that CE is within a factor of 2-3x of the energy and area efficiency of custom units optimized for a single kernel. CE improves energy and area efficiency by 8-15x over a SIMD engine for most applications."
            },
            "slug": "Convolution-engine:-balancing-efficiency-&-in-Qadeer-Hameed",
            "title": {
                "fragments": [],
                "text": "Convolution engine: balancing efficiency & flexibility in specialized computing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Convolution Engine, specialized for the convolution-like data-flow that is common in computational photography, image processing, and video processing applications, is presented and it is demonstrated that CE is within a factor of 2-3x of the energy and area efficiency of custom units optimized for a single kernel."
            },
            "venue": {
                "fragments": [],
                "text": "ISCA"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089272"
                        ],
                        "name": "R. Monga",
                        "slug": "R.-Monga",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Monga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Monga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145139947"
                        ],
                        "name": "Matthieu Devin",
                        "slug": "Matthieu-Devin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Devin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Devin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "On 10 of the largest layers found in recent CNNs and DNNs, this accelerator is \n117.87x faster and 21.08x more energy-ef.cient (including main memory accesses) on aver\u00adage than an 128-bit \nSIMD core clocked at 2GHz."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "DNNs and CNNs are strongly related, they especially differ \nin the presence and/or nature of convolutional layers, see later."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "In Section 2, we .rst provide a primer on recent machine-learning techniques and introduce the main layers \ncomposing CNNs and DNNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "CONV2* 200 200 18 18 8 8 Detection of faces in YouTube videos (DNN) [26], largest NN to date (Google) CONV3 32 32 4 4 108 200 Traffic sign identification for car navigation (CNN) [36] POOL3 32 32 4 4 100 CLASS3 - - - - 200 100 CONV4 32 32 7 7 16 512 Google Street View house numbers (CNN) [35] CONV5* 256 256 11 11 256 384 Multi-Object recognition in natural images (DNN) [16], winner 2012 ImageNet competition POOL5 256 256 2 2 256 -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 198
                            }
                        ],
                        "text": "This is especially important in the domain of machine-learning where there is a clear trend towards scaling up the size of neural networks in order to achieve better accuracy and more functionality [16, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "In section 4, we explain why an ASIC implementation \nof large-scale CNNs or DNNs cannot be the same as the straightforward ASIC implementation of small NNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "This is characteristic of CNNs, while kernels can be specific to each point of the output feature map in DNNs [26], we then use the term private kernels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "This subset of neural networks includes both Deep Neural Networks (DNNs) [25] and \nConvolutional Neural Networks (CNNs) [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 210
                            }
                        ],
                        "text": "Due to recent progress in machinelearning, certain types of neural networks, especially Deep Neural Networks [25] and Convolutional Neural Networks [27] have become state-of-the-art machine-learning techniques [26] across a broad range of applications such as web search [19], image analysis [31] or speech recognition [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 16
                            }
                        ],
                        "text": "private kernels [26, 35], and the machine-learning importance of having private instead of shared kernels remains unclear."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "For DNNs with private kernels, this is not possible as the total number of synapses are in the tens or hundreds of millions (the largest network to date has a billion synapses [26])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "However, recent state-of-the-art \nCNNs and DNNs are characterized by their large size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "At the same time, a small set of machine-learning algorithms (especially Convo\u00adlutional \nand Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 188
                            }
                        ],
                        "text": "In this study, we investigate an \naccelerator design that can accommodate the most popular state-of-the-art algorithms, i.e., Convolutional \nNeural Networks (CNNs) and Deep Neu\u00adral Networks (DNNs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "In summary, our main contributions are the following: A synthesized (place \n&#38; route) accelerator design for large-scale CNNs and DNNs, the state-of-the-art machine\u00adlearning \nalgorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "In this study, we design an accelerator for large-scale \nCNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and \nenergy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206741597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72e93aa6767ee683de7f001fa72f1314e40a8f35",
            "isKey": true,
            "numCitedBy": 2112,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of building high-level, class-specific feature detectors from only unlabeled data. For example, is it possible to learn a face detector using only unlabeled images? To answer this, we train a deep sparse autoencoder on a large dataset of images (the model has 1 billion connections, the dataset has 10 million 200\u00d7200 pixel images downloaded from the Internet). We train this network using model parallelism and asynchronous SGD on a cluster with 1,000 machines (16,000 cores) for three days. Contrary to what appears to be a widely-held intuition, our experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not. Control experiments show that this feature detector is robust not only to translation but also to scaling and out-of-plane rotation. We also find that the same network is sensitive to other high-level concepts such as cat faces and human bodies. Starting from these learned features, we trained our network to recognize 22,000 object categories from ImageNet and achieve a leap of 70% relative improvement over the previous state-of-the-art."
            },
            "slug": "Building-high-level-features-using-large-scale-Le-Ranzato",
            "title": {
                "fragments": [],
                "text": "Building high-level features using large scale unsupervised learning"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Contrary to what appears to be a widely-held intuition, the experimental results reveal that it is possible to train a face detector without having to label images as containing a face or not."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2677903"
                        ],
                        "name": "R. S. Amant",
                        "slug": "R.-S.-Amant",
                        "structuredName": {
                            "firstName": "Ren\u00e9e",
                            "lastName": "Amant",
                            "middleNames": [
                                "St."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. S. Amant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755281"
                        ],
                        "name": "D. Jim\u00e9nez",
                        "slug": "D.-Jim\u00e9nez",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Jim\u00e9nez",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jim\u00e9nez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144859824"
                        ],
                        "name": "D. Burger",
                        "slug": "D.-Burger",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Burger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 282
                            }
                        ],
                        "text": "However, recently, due to simultaneous trends in applications, machine-learning and technology constraints, hardware neural networks have been increasingly considered as potential accelerators, either for very dedicated functionalities within a processor, such as branch prediction [1], or for their fault-tolerance properties [15, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14553490,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab64c8e0db0147d9b7bc66df136e3debed3ae16b",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Shrinking transistor sizes and a trend toward low-power processors have caused increased leakage, high per-device variation and a larger number of hard and soft errors. Maintaining precise digital behavior on these devices grows more expensive with each technology generation. In some cases, replacing digital units with analog equivalents allows similar computation to be performed at higher speed and lower power. The units that can most easily benefit from this approach are those whose results do not have to be precise, such as various types of predictors. We introduce the Scaled Neural Predictor (SNP), a highly accurate prediction algorithm that is infeasible in a purely digital implementation, but can be implemented using analog circuitry. Our analog implementation, the Scaled Neural Analog Predictor (SNAP), uses current summation in place of the expensive digital dot-product computation required in neural predictors. We show that the analog predictor can outperform digital neural predictors because of the reduced cost, in power and latency, of the key computations. The SNAP circuit is able to produce an accuracy nearly equivalent to an infeasible digital neural predictor that requires 128 additions per prediction. The analog version, however, can run at 3 GHz, with the analog portion of the prediction computation requiring approximately 7 milliwatts at a 45 nm technology, which is small compared to the power required for the table lookups in this and conventional predictors."
            },
            "slug": "Low-power,-high-performance-analog-neural-branch-Amant-Jim\u00e9nez",
            "title": {
                "fragments": [],
                "text": "Low-power, high-performance analog neural branch prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work introduces the Scaled Neural Predictor (SNP), a highly accurate prediction algorithm that is infeasible in a purely digital implementation, but can be implemented using analog circuitry, and shows that the analog predictor can outperform digital neural predictors because of the reduced cost, in power and latency."
            },
            "venue": {
                "fragments": [],
                "text": "2008 41st IEEE/ACM International Symposium on Microarchitecture"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256269"
                        ],
                        "name": "C. Farabet",
                        "slug": "C.-Farabet",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Farabet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Farabet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940285"
                        ],
                        "name": "B. Martini",
                        "slug": "B.-Martini",
                        "structuredName": {
                            "firstName": "Berin",
                            "lastName": "Martini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Martini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079955555"
                        ],
                        "name": "B. Corda",
                        "slug": "B.-Corda",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Corda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Corda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2447628"
                        ],
                        "name": "Polina Akselrod",
                        "slug": "Polina-Akselrod",
                        "structuredName": {
                            "firstName": "Polina",
                            "lastName": "Akselrod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Polina Akselrod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889774"
                        ],
                        "name": "E. Culurciello",
                        "slug": "E.-Culurciello",
                        "structuredName": {
                            "firstName": "Eugenio",
                            "lastName": "Culurciello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Culurciello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "Interestingly, machine-learning researchers who have recently dipped into hardware accelerators [13] have made the same choice."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 134
                            }
                        ],
                        "text": "However, the aforementioned trends have already been iden\u00adti.ed by a number of researchers who have \nproposed accel\u00aderators implementing Convolutional Neural Networks [3] or Multi-Layer Perceptrons [38]; \naccelerators focusing on other domains, such as image processing, also propose ef.cient implementations \nof some of the primitives used by machine\u00adlearning algorithms, such as convolutions [33]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Others have proposed ASIC implementations of Convolutional Neural Networks [13], or of other custom neural network algorithms [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "On 10 of the largest layers found in recent CNNs and DNNs, this accelerator is \n117.87x faster and 21.08x more energy-ef.cient (including main memory accesses) on aver\u00adage than an 128-bit \nSIMD core clocked at 2GHz."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "DNNs and CNNs are strongly related, they especially differ \nin the presence and/or nature of convolutional layers, see later."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "In Section 2, we .rst provide a primer on recent machine-learning techniques and introduce the main layers \ncomposing CNNs and DNNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 311,
                                "start": 307
                            }
                        ],
                        "text": "In order to assess the performance of the SIMD core, we also implemented a standard C++ version of the different benchmark layers presented below, and on average (geometric mean), we observed that the SIMD core provides Layer Nx Ny Kx Ky Ni No Description CONV1 500 375 9 9 32 48 Street scene parsing (CNN) [13], (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "In section 4, we explain why an ASIC implementation \nof large-scale CNNs or DNNs cannot be the same as the straightforward ASIC implementation of small NNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 307,
                                "start": 304
                            }
                        ],
                        "text": "However, all these works have .rst, and suc\u00adcessfully, focused on ef.ciently implementing \nthe computa\u00adtional primitives but they either voluntarily ignore memory transfers for the sake of simplicity \n[33, 38], or they directly plug their computational accelerator to memory via a more or less sophisticated \nDMA [3, 13, 21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 310
                            }
                        ],
                        "text": "However, all these works have first, and successfully, focused on efficiently implementing the computational primitives but they either voluntarily ignore memory transfers for the sake of simplicity [33, 38], or they directly plug their computational accelerator to memory via a more or less sophisticated DMA [3, 13, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "This subset of neural networks includes both Deep Neural Networks (DNNs) [25] and \nConvolutional Neural Networks (CNNs) [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "However, recent state-of-the-art \nCNNs and DNNs are characterized by their large size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "At the same time, a small set of machine-learning algorithms (especially Convo\u00adlutional \nand Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "Neuflow [13] is an accelerator for fast and low-power implementation of the feed-forward paths of CNNs for vision systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 45
                            }
                        ],
                        "text": "Others have \nproposed ASIC implementations of Convolutional Neural Networks [13], or of other custom neural network \nalgo\u00adrithms [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 176
                            }
                        ],
                        "text": "This trend in application comes together with a \nthird and equally remarkable trend in machine-learning where a small number of techniques, based on neural \nnetworks (especially Convo\u00adlutional Neural Networks [27] and Deep Neural Networks [16]), have been proved \nin the past few years to be state-of\u00adthe-art across a broad range of applications [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 156
                            }
                        ],
                        "text": "In this study, we investigate an \naccelerator design that can accommodate the most popular state-of-the-art algorithms, i.e., Convolutional \nNeural Networks (CNNs) and Deep Neu\u00adral Networks (DNNs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "In summary, our main contributions are the following: A synthesized (place \n&#38; route) accelerator design for large-scale CNNs and DNNs, the state-of-the-art machine\u00adlearning \nalgorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "In this study, we design an accelerator for large-scale \nCNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and \nenergy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 851574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204710a6a6d935150b5b16daf74493dea6d1b7a2",
            "isKey": true,
            "numCitedBy": 349,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms \u2014 neuFlow \u2014 and a dataflow compiler \u2014 luaFlow \u2014 that transforms high-level flow-graph representations of these algorithms into machine code for neuFlow. This system was designed with the goal of providing real-time detection, categorization and localization of objects in complex scenes, while consuming 10 Watts when implemented on a Xilinx Virtex 6 FPGA platform, or about ten times less than a laptop computer, and producing speedups of up to 100 times in real-world applications. We present an application of the system on street scene analysis, segmenting 20 categories on 500 \u00d7 375 frames at 12 frames per second on our custom hardware neuFlow."
            },
            "slug": "NeuFlow:-A-runtime-reconfigurable-dataflow-for-Farabet-Martini",
            "title": {
                "fragments": [],
                "text": "NeuFlow: A runtime reconfigurable dataflow processor for vision"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms \u2014 neuFlow \u2014 and a dataflow compiler \u2014 luaFlow \u2014 that transforms high-level flow-graph representations of these algorithms into machine code for neu Flow are presented."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011 WORKSHOPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398987690"
                        ],
                        "name": "Ahmed Al-Maashri",
                        "slug": "Ahmed-Al-Maashri",
                        "structuredName": {
                            "firstName": "Ahmed",
                            "lastName": "Al-Maashri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmed Al-Maashri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723845"
                        ],
                        "name": "M. DeBole",
                        "slug": "M.-DeBole",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "DeBole",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. DeBole"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144490976"
                        ],
                        "name": "M. Cotter",
                        "slug": "M.-Cotter",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Cotter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cotter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2916636"
                        ],
                        "name": "N. Chandramoorthy",
                        "slug": "N.-Chandramoorthy",
                        "structuredName": {
                            "firstName": "Nandhini",
                            "lastName": "Chandramoorthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Chandramoorthy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409958726"
                        ],
                        "name": "Yang Xiao",
                        "slug": "Yang-Xiao",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733274"
                        ],
                        "name": "N. Vijaykrishnan",
                        "slug": "N.-Vijaykrishnan",
                        "structuredName": {
                            "firstName": "Narayanan",
                            "lastName": "Vijaykrishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Vijaykrishnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143804422"
                        ],
                        "name": "C. Chakrabarti",
                        "slug": "C.-Chakrabarti",
                        "structuredName": {
                            "firstName": "Chaitali",
                            "lastName": "Chakrabarti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chakrabarti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[29] have also investigated the implementation of another neural network model, the bio-inspired HMAX for vision processing, using a set of custom accelerators arranged around a switch fabric; in the article, the authors allude to locality optimizations across different orientations, which are roughly the HMAX equivalent of feature maps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14878401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29fc8380eccdd1b4e2f8d9dd8faac6a2c3906230",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Video analytics introduce new levels of intelligence to automated scene understanding. Neuromorphic algorithms, such as HMAX, are proposed as robust and accurate algorithms that mimic the processing in the visual cortex of the brain. HMAX, for instance, is a versatile algorithm that can be repurposed to target several visual recognition applications. This paper presents the design and evaluation of hardware accelerators for extracting visual features for universal recognition. The recognition applications include object recognition, face identification, facial expression recognition, and action recognition. These accelerators were validated on a multi-FPGA platform and significant performance enhancement and power efficiencies were demonstrated when compared to CMP and GPU platforms. Results demonstrate as much as 7.6X speedup and 12.8X more power-efficient performance when compared to those platforms."
            },
            "slug": "Accelerating-neuromorphic-vision-algorithms-for-Al-Maashri-DeBole",
            "title": {
                "fragments": [],
                "text": "Accelerating neuromorphic vision algorithms for recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This paper presents the design and evaluation of hardware accelerators for extracting visual features for universal recognition and demonstrates significant performance enhancement and power efficiencies when compared to CMP and GPU platforms."
            },
            "venue": {
                "fragments": [],
                "text": "DAC Design Automation Conference 2012"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115771308"
                        ],
                        "name": "Muhammad Mukaram Khan",
                        "slug": "Muhammad-Mukaram-Khan",
                        "structuredName": {
                            "firstName": "Muhammad",
                            "lastName": "Khan",
                            "middleNames": [
                                "Mukaram"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Muhammad Mukaram Khan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34274343"
                        ],
                        "name": "D. Lester",
                        "slug": "D.-Lester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lester",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3085921"
                        ],
                        "name": "L. Plana",
                        "slug": "L.-Plana",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Plana",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Plana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516402"
                        ],
                        "name": "Alexander D. Rast",
                        "slug": "Alexander-D.-Rast",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Rast",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander D. Rast"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149169978"
                        ],
                        "name": "Xin Jin",
                        "slug": "Xin-Jin",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3168663"
                        ],
                        "name": "E. Painkras",
                        "slug": "E.-Painkras",
                        "structuredName": {
                            "firstName": "Eustace",
                            "lastName": "Painkras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Painkras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144409615"
                        ],
                        "name": "S. Furber",
                        "slug": "S.-Furber",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Furber",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Furber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 218
                            }
                        ],
                        "text": "While many implementations of hardware neurons and neural networks have been investigated in the past two decades [18], the main purpose of hardware neural networks has been fast modeling of biological neural networks [20, 34] for implementing neurons with thousands of connections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5203122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa6509ebe8c0264da3ce7e1c5794100d25ff1997",
            "isKey": false,
            "numCitedBy": 269,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "SpiNNaker is a novel chip - based on the ARM processor - which is designed to support large scale spiking neural networks simulations. In this paper we describe some of the features that permit SpiNNaker chips to be connected together to form scalable massively-parallel systems. Our eventual goal is to be able to simulate neural networks consisting of 109 neurons running in dasiareal timepsila, by which we mean that a similarly sized collection of biological neurons would run at the same speed. In this paper we describe the methods by which neural networks are mapped onto the system, and how features designed into the chip are to be exploited in practice. We will also describe the modelling and verification activities by which we hope to ensure that, when the chip is delivered, it will work as anticipated."
            },
            "slug": "SpiNNaker:-Mapping-neural-networks-onto-a-chip-Khan-Lester",
            "title": {
                "fragments": [],
                "text": "SpiNNaker: Mapping neural networks onto a massively-parallel chip multiprocessor"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The methods by which neural networks are mapped onto the system, and how features designed into the chip are to be exploited in practice are described to ensure that, when the chip is delivered, it will work as anticipated."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153698680"
                        ],
                        "name": "Sheng Li",
                        "slug": "Sheng-Li",
                        "structuredName": {
                            "firstName": "Sheng",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheng Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2575874"
                        ],
                        "name": "Jung Ho Ahn",
                        "slug": "Jung-Ho-Ahn",
                        "structuredName": {
                            "firstName": "Jung Ho",
                            "lastName": "Ahn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jung Ho Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14956868"
                        ],
                        "name": "Richard D. Strong",
                        "slug": "Richard-D.-Strong",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Strong",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard D. Strong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830240"
                        ],
                        "name": "J. Brockman",
                        "slug": "J.-Brockman",
                        "structuredName": {
                            "firstName": "Jay",
                            "lastName": "Brockman",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Brockman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740142"
                        ],
                        "name": "D. Tullsen",
                        "slug": "D.-Tullsen",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Tullsen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tullsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715454"
                        ],
                        "name": "N. Jouppi",
                        "slug": "N.-Jouppi",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Jouppi",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Jouppi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "For the SIMD baseline, we use the GEM5+McPAT [28] combination."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207177144,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "823575dce41618aef0674f95f52234df3dc20894",
            "isKey": false,
            "numCitedBy": 2351,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces McPAT, an integrated power, area, and timing modeling framework that supports comprehensive design space exploration for multicore and manycore processor configurations ranging from 90nm to 22nm and beyond. At the microarchitectural level, McPAT includes models for the fundamental components of a chip multiprocessor, including in-order and out-of-order processor cores, networks-on-chip, shared caches, integrated memory controllers, and multiple-domain clocking. At the circuit and technology levels, McPAT supports critical-path timing modeling, area modeling, and dynamic, short-circuit, and leakage power modeling for each of the device types forecast in the ITRS roadmap including bulk CMOS, SOI, and double-gate transistors. McPAT has a flexible XML interface to facilitate its use with many performance simulators. Combined with a performance simulator, McPAT enables architects to consistently quantify the cost of new ideas and assess tradeoffs of different architectures using new metrics like energy-delay-area2 product (EDA2P) and energy-delay-area product (EDAP). This paper explores the interconnect options of future manycore processors by varying the degree of clustering over generations of process technologies. Clustering will bring interesting tradeoffs between area and performance because the interconnects needed to group cores into clusters incur area overhead, but many applications can make good use of them due to synergies of cache sharing. Combining power, area, and timing results of McPAT with performance simulation of PARSEC benchmarks at the 22nm technology node for both common in-order and out-of-order manycore designs shows that when die cost is not taken into account clustering 8 cores together gives the best energy-delay product, whereas when cost is taken into account configuring clusters with 4 cores gives the best EDA2P and EDAP."
            },
            "slug": "McPAT:-An-integrated-power,-area,-and-timing-for-Li-Ahn",
            "title": {
                "fragments": [],
                "text": "McPAT: An integrated power, area, and timing modeling framework for multicore and manycore architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Combining power, area, and timing results of McPAT with performance simulation of PARSEC benchmarks at the 22nm technology node for both common in-order and out-of-order manycore designs shows that when die cost is not taken into account clustering 8 cores together gives the best energy-delay product, whereas when cost is taking into account configuring clusters with 4 cores gives thebest EDA2P and EDAP."
            },
            "venue": {
                "fragments": [],
                "text": "2009 42nd Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775019"
                        ],
                        "name": "S. Dr\u0103ghici",
                        "slug": "S.-Dr\u0103ghici",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Dr\u0103ghici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dr\u0103ghici"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 80
                            }
                        ],
                        "text": ", 8 bits or even less) have almost no impact on the accuracy of neural networks [8, 17, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6922202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbd6540d673d023e69183513f80a9d323aa4821e",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 165,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-the-capabilities-of-neural-networks-using-Dr\u0103ghici",
            "title": {
                "fragments": [],
                "text": "On the capabilities of neural networks using limited precision weights"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110033447"
                        ],
                        "name": "Joo-Young Kim",
                        "slug": "Joo-Young-Kim",
                        "structuredName": {
                            "firstName": "Joo-Young",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joo-Young Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2141319767"
                        ],
                        "name": "Minsu Kim",
                        "slug": "Minsu-Kim",
                        "structuredName": {
                            "firstName": "Minsu",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minsu Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47090124"
                        ],
                        "name": "Seungjin Lee",
                        "slug": "Seungjin-Lee",
                        "structuredName": {
                            "firstName": "Seungjin",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seungjin Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3216445"
                        ],
                        "name": "Jinwook Oh",
                        "slug": "Jinwook-Oh",
                        "structuredName": {
                            "firstName": "Jinwook",
                            "lastName": "Oh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinwook Oh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3079378"
                        ],
                        "name": "Kwanho Kim",
                        "slug": "Kwanho-Kim",
                        "structuredName": {
                            "firstName": "Kwanho",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kwanho Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678901"
                        ],
                        "name": "H. Yoo",
                        "slug": "H.-Yoo",
                        "structuredName": {
                            "firstName": "Hoi-Jun",
                            "lastName": "Yoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yoo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[21] and consists of 128 SIMD processors of 16 PEs each; the architecture is significantly larger and implements a specific neural vision model (neither CNNs nor DNNs), but it can achieve 60 frame/sec (real-time) multi-object recognition for up to 10 different objects."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 307,
                                "start": 304
                            }
                        ],
                        "text": "However, all these works have .rst, and suc\u00adcessfully, focused on ef.ciently implementing \nthe computa\u00adtional primitives but they either voluntarily ignore memory transfers for the sake of simplicity \n[33, 38], or they directly plug their computational accelerator to memory via a more or less sophisticated \nDMA [3, 13, 21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 126
                            }
                        ],
                        "text": "Others have proposed ASIC implementations of Convolutional Neural Networks [13], or of other custom neural network algorithms [21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 321,
                                "start": 310
                            }
                        ],
                        "text": "However, all these works have first, and successfully, focused on efficiently implementing the computational primitives but they either voluntarily ignore memory transfers for the sake of simplicity [33, 38], or they directly plug their computational accelerator to memory via a more or less sophisticated DMA [3, 13, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3857871,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a0e781acb1b7511cee9444fcda775855e87cb02",
            "isKey": true,
            "numCitedBy": 126,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "A 201.4 GOPS real-time multi-object recognition processor is presented with a three-stage pipelined architecture. Visual perception based multi-object recognition algorithm is applied to give multiple attentions to multiple objects in the input image. For human-like multi-object perception, a neural perception engine is proposed with biologically inspired neural networks and fuzzy logic circuits. In the proposed hardware architecture, three recognition tasks (visual perception, descriptor generation, and object decision) are directly mapped to the neural perception engine, 16 SIMD processors including 128 processing elements, and decision processor, respectively, and executed in the pipeline to maximize throughput of the object recognition. For efficient task pipelining, proposed task/power manager balances the execution times of the three stages based on intelligent workload estimations. In addition, a 118.4 GB/s multi-casting network-on-chip is proposed for communication architecture with incorporating overall 21 IP blocks. For low-power object recognition, workload-aware dynamic power management is performed in chip-level. The 49 mm2 chip is fabricated in a 0.13 \u00bfm 8-metal CMOS process and contains 3.7 M gates and 396 KB on-chip SRAM. It achieves 60 frame/sec multi-object recognition up to 10 different objects for VGA (640 \u00d7 480) video input while dissipating 496 mW at 1.2 V. The obtained 8.2 mJ/frame energy efficiency is 3.2 times higher than the state-of-the-art recognition processor."
            },
            "slug": "A-201.4-GOPS-496-mW-Real-Time-Multi-Object-With-Kim-Kim",
            "title": {
                "fragments": [],
                "text": "A 201.4 GOPS 496 mW Real-Time Multi-Object Recognition Processor With Bio-Inspired Neural Perception Engine"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "In the proposed hardware architecture, three recognition tasks (visual perception, descriptor generation, and object decision) are directly mapped to the neural perception engine, 16 SIMD processors including 128 processing elements, and decision processor and executed in the pipeline to maximize throughput of the object recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Journal of Solid-State Circuits"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696563"
                        ],
                        "name": "H. Esmaeilzadeh",
                        "slug": "H.-Esmaeilzadeh",
                        "structuredName": {
                            "firstName": "Hadi",
                            "lastName": "Esmaeilzadeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Esmaeilzadeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145719923"
                        ],
                        "name": "Adrian Sampson",
                        "slug": "Adrian-Sampson",
                        "structuredName": {
                            "firstName": "Adrian",
                            "lastName": "Sampson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adrian Sampson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717411"
                        ],
                        "name": "L. Ceze",
                        "slug": "L.-Ceze",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ceze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ceze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144859824"
                        ],
                        "name": "D. Burger",
                        "slug": "D.-Burger",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Burger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52943205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15b275f0421c606f5903532e9964b140cbb2f878",
            "isKey": false,
            "numCitedBy": 335,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "This work proposes an approximate algorithmic transformation and a new class of accelerators, called neural processing units (NPUs). NPUs leverage the approximate algorithmic transformation that converts regions of code from a Von Neumann model to a neural model. NPUs achieve an average 2.3\u00d7 speedup and 3.0\u00d7 energy savings for general-purpose approximate programs. This new class of accelerators shows that significant performance and efficiency gains are possible when the abstraction of full accuracy is relaxed in general-purpose computing."
            },
            "slug": "Neural-Acceleration-for-General-Purpose-Approximate-Esmaeilzadeh-Sampson",
            "title": {
                "fragments": [],
                "text": "Neural Acceleration for General-Purpose Approximate Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "NPUs leverage the approximate algorithmic transformation that converts regions of code from a Von Neumann model to a neural model and shows that significant performance and efficiency gains are possible when the abstraction of full accuracy is relaxed in general-purpose computing."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Micro"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2138905"
                        ],
                        "name": "J. Schemmel",
                        "slug": "J.-Schemmel",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Schemmel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schemmel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2900301"
                        ],
                        "name": "J. Fieres",
                        "slug": "J.-Fieres",
                        "structuredName": {
                            "firstName": "Johannes",
                            "lastName": "Fieres",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fieres"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36245042"
                        ],
                        "name": "K. Meier",
                        "slug": "K.-Meier",
                        "structuredName": {
                            "firstName": "Karlheinz",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Meier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 218
                            }
                        ],
                        "text": "While many implementations of hardware neurons and neural networks have been investigated in the past two decades [18], the main purpose of hardware neural networks has been fast modeling of biological neural networks [20, 34] for implementing neurons with thousands of connections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9574706,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "706491314ee0b0a879db41054b570af656f72922",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a novel design of an artificial neural network tailored for wafer-scale integration. The presented VLSI implementation includes continuous-time analog neurons with up to 16 k inputs. A novel interconnection and routing scheme allows the mapping of a multitude of network models derived from biology on the VLSI neural network while maintaining a high resource usage. A single 20 cm wafer contains about 60 million synapses. The implemented neurons are highly accelerated compared to biological real time. The power consumption of the dense interconnection network providing the necessary communication bandwidth is a critical aspect of the system integration. A novel asynchronous low-voltage signaling scheme is presented that makes the wafer-scale approach feasible by limiting the total power consumption while simultaneously providing a flexible, programmable network topology."
            },
            "slug": "Wafer-scale-integration-of-analog-neural-networks-Schemmel-Fieres",
            "title": {
                "fragments": [],
                "text": "Wafer-scale integration of analog neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel asynchronous low-voltage signaling scheme is presented that makes the wafer-scale approach feasible by limiting the total power consumption while simultaneously providing a flexible, programmable network topology."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE International Joint Conference on Neural Networks (IEEE World Congress on Computational Intelligence)"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2588465"
                        ],
                        "name": "J. L. Holt",
                        "slug": "J.-L.-Holt",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Holt",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. L. Holt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145159381"
                        ],
                        "name": "Jenq-Neng Hwang",
                        "slug": "Jenq-Neng-Hwang",
                        "structuredName": {
                            "firstName": "Jenq-Neng",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jenq-Neng Hwang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 80
                            }
                        ],
                        "text": ", 8 bits or even less) have almost no impact on the accuracy of neural networks [8, 17, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1353891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b119dc8a4e38824d73a9f88179935a96e001aaf",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Through parallel processing, low precision fixed point hardware can be used to build a very high speed neural network computing engine where the low precision results in a drastic reduction in system cost. The reduced silicon area required to implement a single processing unit is taken advantage of by implementing multiple processing units on a single piece of silicon and operating them in parallel. The important question which arises is how much precision is required to implement neural network algorithms on this low precision hardware. A theoretical analysis of error due to finite precision computation was undertaken to determine the necessary precision for successful forward retrieving and back-propagation learning in a multilayer perceptron. This analysis can easily be further extended to provide a general finite precision analysis technique by which most neural network algorithms under any set of hardware constraints may be evaluated. >"
            },
            "slug": "Finite-Precision-Error-Analysis-of-Neural-Network-Holt-Hwang",
            "title": {
                "fragments": [],
                "text": "Finite Precision Error Analysis of Neural Network Hardware Implementations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A theoretical analysis of error due to finite precision computation was undertaken to determine the necessary precision for successful forward retrieving and back-propagation learning in a multilayer perceptron."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Computers"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32837403"
                        ],
                        "name": "J. Bergstra",
                        "slug": "J.-Bergstra",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "This subset of neural networks includes both Deep Neural Networks (DNNs) [25] and Convolutional Neural Networks (CNNs) [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "[16]), have been proved in the past few years to be state-ofthe-art across a broad range of applications [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 45
                            }
                        ],
                        "text": "This subset of neural networks includes both Deep Neural Networks (DNNs) [25] and \nConvolutional Neural Networks (CNNs) [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Due to recent progress in machinelearning, certain types of neural networks, especially Deep Neural Networks [25] and Convolutional Neural Networks [27] have become state-of-the-art machine-learning techniques [26] across a broad range of applications such as web search [19], image analysis [31] or speech recognition [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 91
                            }
                        ],
                        "text": "At the same time, a small set of machine-learning algorithms (especially Convo\u00adlutional \nand Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 215
                            }
                        ],
                        "text": "This trend in application comes together with a \nthird and equally remarkable trend in machine-learning where a small number of techniques, based on neural \nnetworks (especially Convo\u00adlutional Neural Networks [27] and Deep Neural Networks [16]), have been proved \nin the past few years to be state-of\u00adthe-art across a broad range of applications [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 166
                            }
                        ],
                        "text": "In this study, we investigate an \naccelerator design that can accommodate the most popular state-of-the-art algorithms, i.e., Convolutional \nNeural Networks (CNNs) and Deep Neu\u00adral Networks (DNNs)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14805281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "isKey": true,
            "numCitedBy": 983,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks."
            },
            "slug": "An-empirical-evaluation-of-deep-architectures-on-of-Larochelle-Erhan",
            "title": {
                "fragments": [],
                "text": "An empirical evaluation of deep architectures on problems with many factors of variation"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A series of experiments indicate that these models with deep architectures show promise in solving harder learning problems that exhibit many factors of variation."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "On 10 of the largest layers found in recent CNNs and DNNs, this accelerator is \n117.87x faster and 21.08x more energy-ef.cient (including main memory accesses) on aver\u00adage than an 128-bit \nSIMD core clocked at 2GHz."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "DNNs and CNNs are strongly related, they especially differ \nin the presence and/or nature of convolutional layers, see later."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "In Section 2, we .rst provide a primer on recent machine-learning techniques and introduce the main layers \ncomposing CNNs and DNNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "CONV2* 200 200 18 18 8 8 Detection of faces in YouTube videos (DNN) [26], largest NN to date (Google) CONV3 32 32 4 4 108 200 Traffic sign identification for car navigation (CNN) [36] POOL3 32 32 4 4 100 CLASS3 - - - - 200 100 CONV4 32 32 7 7 16 512 Google Street View house numbers (CNN) [35] CONV5* 256 256 11 11 256 384 Multi-Object recognition in natural images (DNN) [16], winner 2012 ImageNet competition POOL5 256 256 2 2 256 -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "In section 4, we explain why an ASIC implementation \nof large-scale CNNs or DNNs cannot be the same as the straightforward ASIC implementation of small NNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "This subset of neural networks includes both Deep Neural Networks (DNNs) [25] and \nConvolutional Neural Networks (CNNs) [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "However, recent state-of-the-art \nCNNs and DNNs are characterized by their large size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "At the same time, a small set of machine-learning algorithms (especially Convo\u00adlutional \nand Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 156
                            }
                        ],
                        "text": "In this study, we investigate an \naccelerator design that can accommodate the most popular state-of-the-art algorithms, i.e., Convolutional \nNeural Networks (CNNs) and Deep Neu\u00adral Networks (DNNs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "In summary, our main contributions are the following: A synthesized (place \n&#38; route) accelerator design for large-scale CNNs and DNNs, the state-of-the-art machine\u00adlearning \nalgorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "In this study, we design an accelerator for large-scale \nCNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and \nenergy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7593950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ab0de951cc9cdf16887b1f841f8da6affc9c0de",
            "isKey": true,
            "numCitedBy": 652,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply Convolutional Networks (ConvNets) to the task of traffic sign classification as part of the GTSRB competition. ConvNets are biologically-inspired multi-stage architectures that automatically learn hierarchies of invariant features. While many popular vision approaches use hand-crafted features such as HOG or SIFT, ConvNets learn features at every level from data that are tuned to the task at hand. The traditional ConvNet architecture was modified by feeding 1st stage features in addition to 2nd stage features to the classifier. The system yielded the 2nd-best accuracy of 98.97% during phase I of the competition (the best entry obtained 98.98%), above the human performance of 98.81%, using 32\u00d732 color input images. Experiments conducted after phase 1 produced a new record of 99.17% by increasing the network capacity, and by using greyscale images instead of color. Interestingly, random features still yielded competitive results (97.33%)."
            },
            "slug": "Traffic-sign-recognition-with-multi-scale-Networks-Sermanet-LeCun",
            "title": {
                "fragments": [],
                "text": "Traffic sign recognition with multi-scale Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work applies Convolutional Networks (ConvNets) to the task of traffic sign classification as part of the GTSRB competition, and yields the 2nd-best accuracy above the human performance."
            },
            "venue": {
                "fragments": [],
                "text": "The 2011 International Joint Conference on Neural Networks"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711244"
                        ],
                        "name": "R. J. Vogelstein",
                        "slug": "R.-J.-Vogelstein",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Vogelstein",
                            "middleNames": [
                                "Jacob"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J. Vogelstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1406451016"
                        ],
                        "name": "U. Mallik",
                        "slug": "U.-Mallik",
                        "structuredName": {
                            "firstName": "Udayan",
                            "lastName": "Mallik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Mallik"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717958"
                        ],
                        "name": "J. Vogelstein",
                        "slug": "J.-Vogelstein",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Vogelstein",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vogelstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2702388"
                        ],
                        "name": "G. Cauwenberghs",
                        "slug": "G.-Cauwenberghs",
                        "structuredName": {
                            "firstName": "Gert",
                            "lastName": "Cauwenberghs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cauwenberghs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 91
                            }
                        ],
                        "text": "While several of these neuromorphic architectures have been applied to computational tasks [30, 43], the specific bio-inspired information representation (spiking neural networks) they rely on may not be competitive with stateof-the-art neural networks, though this remains an open debate at the threshold between neuroscience and machinelearning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1188141,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3e2ffe453c476e8c84bccb740191628703dbad88",
            "isKey": false,
            "numCitedBy": 231,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "A mixed-signal very large scale integration (VLSI) chip for large scale emulation of spiking neural networks is presented. The chip contains 2400 silicon neurons with fully programmable and reconfigurable synaptic connectivity. Each neuron implements a discrete-time model of a single-compartment cell. The model allows for analog membrane dynamics and an arbitrary number of synaptic connections, each with tunable conductance and reversal potential. The array of silicon neurons functions as an address-event (AE) transceiver, with incoming and outgoing spikes communicated over an asynchronous event-driven digital bus. Address encoding and conflict resolution of spiking events are implemented via a randomized arbitration scheme that ensures balanced servicing of event requests across the array. Routing of events is implemented externally using dynamically programmable random-access memory that stores a postsynaptic address, the conductance, and the reversal potential of each synaptic connection. Here, we describe the silicon neuron circuits, present experimental data characterizing the 3 mm times 3 mm chip fabricated in 0.5-mum complementary metal-oxide-semiconductor (CMOS) technology, and demonstrate its utility by configuring the hardware to emulate a model of attractor dynamics and waves of neural activity during sleep in rat hippocampus"
            },
            "slug": "Dynamically-Reconfigurable-Silicon-Array-of-Spiking-Vogelstein-Mallik",
            "title": {
                "fragments": [],
                "text": "Dynamically Reconfigurable Silicon Array of Spiking Neurons With Conductance-Based Synapses"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The silicon neuron circuits are described, experimental data characterizing the 3 mm times 3 mm chip fabricated in 0.5-mum complementary metal-oxide-semiconductor (CMOS) technology is presented, and its utility is demonstrated by configuring the hardware to emulate a model of attractor dynamics and waves of neural activity during sleep in rat hippocampus."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738667"
                        ],
                        "name": "S. Yehia",
                        "slug": "S.-Yehia",
                        "structuredName": {
                            "firstName": "Sami",
                            "lastName": "Yehia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yehia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2506136"
                        ],
                        "name": "Sylvain Girbal",
                        "slug": "Sylvain-Girbal",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Girbal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sylvain Girbal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144703557"
                        ],
                        "name": "Hugues Berry",
                        "slug": "Hugues-Berry",
                        "structuredName": {
                            "firstName": "Hugues",
                            "lastName": "Berry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hugues Berry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731764"
                        ],
                        "name": "O. Temam",
                        "slug": "O.-Temam",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Temam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Temam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 193
                            }
                        ],
                        "text": "Accelerators can range from processors tuned for certain tasks, to ASIC-like circuits such as H264 [14], or more flexible accelerators capable of targeting a broad range of, but not all, tasks [12, 44] such as QsCores [42], or accelerators for image processing [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6639861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b94022fcce12ae099ab45e72d7b71e4d2bd72",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "While parallelism and multi-cores are receiving much attention as a major scalability path, customization is another, orthogonal and complementary, scalability path which can target not easily parallelizable programs or program sections. The key assets of customization are cost and power efficiency. The key limitation of customization is flexibility. However, we argue that there is no perfect balance between efficiency and flexibility, each system vendor may want to strike a different such balance. In this article, we present a method for achieving any desired balance between flexibility and efficiency by automatically combining any set of individual customization circuits into a larger compound circuit. This circuit is significantly more cost efficient than the simple union of all target circuits, and is configurable to behave as any of the target circuits, while avoiding the routing and configuration cost overhead of FPGAs. The more individual circuits are included, the larger the number of applications which can potentially benefit from this compound customization circuit, realizing flexibility at a minimal cost. Moreover, we observe that the compound circuit cost does not increase in proportion to the number of target applications, due to the wide range of common data-flow and control-flow patterns in programs. Currently, the target individual circuits correspond to loops, like most accelerators in embedded systems, but the aggregation method can accommodate circuits of any size. Using the UTDSP benchmarks and accelerators coupled with an embedded PowerPC405 processor, we show that this approach can yield an average performance improvement of 2.97, while the corresponding synthesized aggregate accelerator is 3 time smaller than the sum of individual accelerators for each target benchmark."
            },
            "slug": "Reconciling-specialization-and-flexibility-through-Yehia-Girbal",
            "title": {
                "fragments": [],
                "text": "Reconciling specialization and flexibility through compound circuits"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article presents a method for achieving any desired balance between flexibility and efficiency by automatically combining any set of individual customization circuits into a larger compound circuit, which is significantly more cost efficient than the simple union of all target circuits."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE 15th International Symposium on High Performance Computer Architecture"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127604"
                        ],
                        "name": "Soumith Chintala",
                        "slug": "Soumith-Chintala",
                        "structuredName": {
                            "firstName": "Soumith",
                            "lastName": "Chintala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumith Chintala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "On 10 of the largest layers found in recent CNNs and DNNs, this accelerator is \n117.87x faster and 21.08x more energy-ef.cient (including main memory accesses) on aver\u00adage than an 128-bit \nSIMD core clocked at 2GHz."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "DNNs and CNNs are strongly related, they especially differ \nin the presence and/or nature of convolutional layers, see later."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "In Section 2, we .rst provide a primer on recent machine-learning techniques and introduce the main layers \ncomposing CNNs and DNNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 289
                            }
                        ],
                        "text": "CONV2* 200 200 18 18 8 8 Detection of faces in YouTube videos (DNN) [26], largest NN to date (Google) CONV3 32 32 4 4 108 200 Traffic sign identification for car navigation (CNN) [36] POOL3 32 32 4 4 100 CLASS3 - - - - 200 100 CONV4 32 32 7 7 16 512 Google Street View house numbers (CNN) [35] CONV5* 256 256 11 11 256 384 Multi-Object recognition in natural images (DNN) [16], winner 2012 ImageNet competition POOL5 256 256 2 2 256 -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "In section 4, we explain why an ASIC implementation \nof large-scale CNNs or DNNs cannot be the same as the straightforward ASIC implementation of small NNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 113
                            }
                        ],
                        "text": "This subset of neural networks includes both Deep Neural Networks (DNNs) [25] and \nConvolutional Neural Networks (CNNs) [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 16
                            }
                        ],
                        "text": "private kernels [26, 35], and the machine-learning importance of having private instead of shared kernels remains unclear."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "However, recent state-of-the-art \nCNNs and DNNs are characterized by their large size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "At the same time, a small set of machine-learning algorithms (especially Convo\u00adlutional \nand Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 156
                            }
                        ],
                        "text": "In this study, we investigate an \naccelerator design that can accommodate the most popular state-of-the-art algorithms, i.e., Convolutional \nNeural Networks (CNNs) and Deep Neu\u00adral Networks (DNNs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "In summary, our main contributions are the following: A synthesized (place \n&#38; route) accelerator design for large-scale CNNs and DNNs, the state-of-the-art machine\u00adlearning \nalgorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "In this study, we design an accelerator for large-scale \nCNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and \nenergy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6788752,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f7f9aba0a6a966ce04e29e401ea28f9eae82f02",
            "isKey": true,
            "numCitedBy": 458,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We classify digits of real-world house numbers using convolutional neural networks (ConvNets). Con-vNets are hierarchical feature learning neural networks whose structure is biologically inspired. Unlike many popular vision approaches that are hand-designed, ConvNets can automatically learn a unique set of features optimized for a given task. We augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establish a new state-of-the-art of 95.10% accuracy on the SVHN dataset (48% error improvement). Furthermore, we analyze the benefits of different pooling methods and multi-stage features in ConvNets. The source code and a tutorial are available at eblearn.sf.net."
            },
            "slug": "Convolutional-neural-networks-applied-to-house-Sermanet-Chintala",
            "title": {
                "fragments": [],
                "text": "Convolutional neural networks applied to house numbers digit classification"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work augmented the traditional ConvNet architecture by learning multi-stage features and by using Lp pooling and establishes a new state-of-the-art of 95.10% accuracy on the SVHN dataset (48% error improvement)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 21st International Conference on Pattern Recognition (ICPR2012)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 206
                            }
                        ],
                        "text": "This trend in application comes together with a third and equally remarkable trend in machine-learning where a small number of techniques, based on neural networks (especially Convolutional Neural Networks [27] and Deep Neural Networks"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 134
                            }
                        ],
                        "text": "However, the aforementioned trends have already been iden\u00adti.ed by a number of researchers who have \nproposed accel\u00aderators implementing Convolutional Neural Networks [3] or Multi-Layer Perceptrons [38]; \naccelerators focusing on other domains, such as image processing, also propose ef.cient implementations \nof some of the primitives used by machine\u00adlearning algorithms, such as convolutions [33]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 243
                            }
                        ],
                        "text": "To illustrate and further confirm that notion, we trained and tested multi-layer perceptrons on data sets from the UC Irvine Machine-Learning repository, see Figure 12, and on the standard MNIST machine-learning benchmark (handwritten digits) [27], see Table 1, using both 16-bit fixed-point and 32bit floating-point operators; we used 10-fold cross-validation for testing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "This subset of neural networks includes both Deep Neural Networks (DNNs) [25] and Convolutional Neural Networks (CNNs) [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 82
                            }
                        ],
                        "text": "This subset of neural networks includes both Deep Neural Networks (DNNs) [25] and \nConvolutional Neural Networks (CNNs) [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "Due to recent progress in machinelearning, certain types of neural networks, especially Deep Neural Networks [25] and Convolutional Neural Networks [27] have become state-of-the-art machine-learning techniques [26] across a broad range of applications such as web search [19], image analysis [31] or speech recognition [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 45
                            }
                        ],
                        "text": "Others have \nproposed ASIC implementations of Convolutional Neural Networks [13], or of other custom neural network \nalgo\u00adrithms [21]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 176
                            }
                        ],
                        "text": "This trend in application comes together with a \nthird and equally remarkable trend in machine-learning where a small number of techniques, based on neural \nnetworks (especially Convo\u00adlutional Neural Networks [27] and Deep Neural Networks [16]), have been proved \nin the past few years to be state-of\u00adthe-art across a broad range of applications [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 125
                            }
                        ],
                        "text": "In this study, we investigate an \naccelerator design that can accommodate the most popular state-of-the-art algorithms, i.e., Convolutional \nNeural Networks (CNNs) and Deep Neu\u00adral Networks (DNNs)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 64294544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f42b865e20e61a954239f421b42007236e671f19",
            "isKey": true,
            "numCitedBy": 3561,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique. Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called Graph Transformer Networks (GTN), allows such multi-module systems to be trained globally using Gradient-Based methods so as to minimize an overall performance measure. Two systems for on-line handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of Graph Transformer Networks. A Graph Transformer Network for reading bank check is also described. It uses Convolutional Neural Network character recognizers combined with global training techniques to provides record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day."
            },
            "slug": "GradientBased-Learning-Applied-to-Document-Haykin-Kosko",
            "title": {
                "fragments": [],
                "text": "GradientBased Learning Applied to Document Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Various methods applied to handwritten character recognition are reviewed and compared and Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To illustrate and further confirm that notion, we trained and tested multi-layer perceptrons on data sets from the UC Irvine Machine-Learning repository, see Figure 12, and on the standard MNIST machine-learning benchmark (handwritten digits) [27], see Table 1, using both 16-bit fixed-point and 32bit floating-point operators; we used 10-fold cross-validation for testing."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Due to recent progress in machinelearning, certain types of neural networks, especially Deep Neural Networks [25] and Convolutional Neural Networks [27] have become state-of-the-art machine-learning techniques [26] across a broad range of applications such as web search [19], image analysis [31] or speech recognition [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This trend in application comes together with a third and equally remarkable trend in machine-learning where a small number of techniques, based on neural networks (especially Convolutional Neural Networks [27] and Deep Neural Networks"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This subset of neural networks includes both Deep Neural Networks (DNNs) [25] and Convolutional Neural Networks (CNNs) [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": true,
            "numCitedBy": 35623,
            "numCiting": 246,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 322,
                                "start": 319
                            }
                        ],
                        "text": "Due to recent progress in machinelearning, certain types of neural networks, especially Deep Neural Networks [25] and Convolutional Neural Networks [27] have become state-of-the-art machine-learning techniques [26] across a broad range of applications such as web search [19], image analysis [31] or speech recognition [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6299466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a3c74c7b11ad5635570932577cdde2a3f7a6a5c",
            "isKey": false,
            "numCitedBy": 1192,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, pre-trained deep neural networks (DNNs) have outperformed traditional acoustic models based on Gaussian mixture models (GMMs) on a variety of large vocabulary speech recognition benchmarks. Deep neural nets have also achieved excellent results on various computer vision tasks using a random \u201cdropout\u201d procedure that drastically improves generalization error by randomly omitting a fraction of the hidden units in all layers. Since dropout helps avoid over-fitting, it has also been successful on a small-scale phone recognition task using larger neural nets. However, training deep neural net acoustic models for large vocabulary speech recognition takes a very long time and dropout is likely to only increase training time. Neural networks with rectified linear unit (ReLU) non-linearities have been highly successful for computer vision tasks and proved faster to train than standard sigmoid units, sometimes also improving discriminative performance. In this work, we show on a 50-hour English Broadcast News task that modified deep neural networks using ReLUs trained with dropout during frame level training provide an 4.2% relative improvement over a DNN trained with sigmoid units, and a 14.4% relative improvement over a strong GMM/HMM system. We were able to obtain our results with minimal human hyper-parameter tuning using publicly available Bayesian optimization code."
            },
            "slug": "Improving-deep-neural-networks-for-LVCSR-using-and-Dahl-Sainath",
            "title": {
                "fragments": [],
                "text": "Improving deep neural networks for LVCSR using rectified linear units and dropout"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Modelling deep neural networks with rectified linear unit (ReLU) non-linearities with minimal human hyper-parameter tuning on a 50-hour English Broadcast News task shows an 4.2% relative improvement over a DNN trained with sigmoid units, and a 14.4% relative improved over a strong GMM/HMM system."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 53
                            }
                        ],
                        "text": "On 10 of the largest layers found in recent CNNs and DNNs, this accelerator is \n117.87x faster and 21.08x more energy-ef.cient (including main memory accesses) on aver\u00adage than an 128-bit \nSIMD core clocked at 2GHz."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "DNNs and CNNs are strongly related, they especially differ \nin the presence and/or nature of convolutional layers, see later."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "In Section 2, we .rst provide a primer on recent machine-learning techniques and introduce the main layers \ncomposing CNNs and DNNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 376,
                                "start": 372
                            }
                        ],
                        "text": "CONV2* 200 200 18 18 8 8 Detection of faces in YouTube videos (DNN) [26], largest NN to date (Google) CONV3 32 32 4 4 108 200 Traffic sign identification for car navigation (CNN) [36] POOL3 32 32 4 4 100 CLASS3 - - - - 200 100 CONV4 32 32 7 7 16 512 Google Street View house numbers (CNN) [35] CONV5* 256 256 11 11 256 384 Multi-Object recognition in natural images (DNN) [16], winner 2012 ImageNet competition POOL5 256 256 2 2 256 -"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 198
                            }
                        ],
                        "text": "This is especially important in the domain of machine-learning where there is a clear trend towards scaling up the size of neural networks in order to achieve better accuracy and more functionality [16, 26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "In section 4, we explain why an ASIC implementation \nof large-scale CNNs or DNNs cannot be the same as the straightforward ASIC implementation of small NNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[16]), have been proved in the past few years to be state-ofthe-art across a broad range of applications [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "This subset of neural networks includes both Deep Neural Networks (DNNs) [25] and \nConvolutional Neural Networks (CNNs) [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "However, recent state-of-the-art \nCNNs and DNNs are characterized by their large size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "At the same time, a small set of machine-learning algorithms (especially Convo\u00adlutional \nand Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 188
                            }
                        ],
                        "text": "In this study, we investigate an \naccelerator design that can accommodate the most popular state-of-the-art algorithms, i.e., Convolutional \nNeural Networks (CNNs) and Deep Neu\u00adral Networks (DNNs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "In summary, our main contributions are the following: A synthesized (place \n&#38; route) accelerator design for large-scale CNNs and DNNs, the state-of-the-art machine\u00adlearning \nalgorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "In this study, we design an accelerator for large-scale \nCNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and \nenergy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14832074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1366de5bb112746a555e9c0cd00de3ad8628aea8",
            "isKey": true,
            "numCitedBy": 6242,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
            },
            "slug": "Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava",
            "title": {
                "fragments": [],
                "text": "Improving neural networks by preventing co-adaptation of feature detectors"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1851278"
                        ],
                        "name": "M. Holler",
                        "slug": "M.-Holler",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Holler",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Holler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053498368"
                        ],
                        "name": "S. Tam",
                        "slug": "S.-Tam",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Tam",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69997181"
                        ],
                        "name": "H. Castro",
                        "slug": "H.-Castro",
                        "structuredName": {
                            "firstName": "Hernan",
                            "lastName": "Castro",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Castro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7642007"
                        ],
                        "name": "R. Benson",
                        "slug": "R.-Benson",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Benson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Benson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "such as the Intel ETANN [18] at the beginning of the 1990s, not because neural networks were already large at the time, but because hardware resources (number of transistors) were naturally much more scarce."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "While many implementations of hardware neurons and neural networks have been investigated in the past two decades [18], the main purpose of hardware neural networks has been fast modeling of biological neural networks [20, 34] for implementing neurons with thousands of connections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17020463,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "8941022cbbb4f99e878d5e342269f067f8596a77",
            "isKey": false,
            "numCitedBy": 434,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of floating-gate nonvolatile memory technology for analog storage of connection strengths, or weights, has previously been proposed and demonstrated. The authors report the analog storage and multiply characteristics of a new floating-gate synapse and further discuss the architecture of a neural network which uses this synapse cell. In the architecture described 8192 synapses are used to interconnect 64 neurons fully and to connect the 64 neurons to each of 64 inputs. Each synapse in the network multiplies a signed analog voltage by a stored weight and generates a differential current proportional to the product. Differential currents are summed on a pair of bit lines and transferred through a sigmoid function, appearing at the neuron output as an analog voltage. Input and output levels are compatible for ease in cascade-connecting these devices into multilayer networks. The width and height of weight-change pulses are calculated. The synapse cell size is 2009 mu m/sup 2/ using 1- mu m CMOS EEPROM technology.<<ETX>>"
            },
            "slug": "An-electrically-trainable-artificial-neural-network-Holler-Tam",
            "title": {
                "fragments": [],
                "text": "An electrically trainable artificial neural network (ETANN) with 10240 'floating gate' synapses"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The authors report the analog storage and multiply characteristics of a new floating-gate synapse and further discuss the architecture of a neural network which uses this synapse cell, using 1- mu m CMOS EEPROM technology."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145595812"
                        ],
                        "name": "Ganesh Venkatesh",
                        "slug": "Ganesh-Venkatesh",
                        "structuredName": {
                            "firstName": "Ganesh",
                            "lastName": "Venkatesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ganesh Venkatesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39862527"
                        ],
                        "name": "J. Sampson",
                        "slug": "J.-Sampson",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Sampson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sampson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39758083"
                        ],
                        "name": "Nathan Goulding",
                        "slug": "Nathan-Goulding",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Goulding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Goulding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3232544"
                        ],
                        "name": "Sravanthi Kota Venkata",
                        "slug": "Sravanthi-Kota-Venkata",
                        "structuredName": {
                            "firstName": "Sravanthi",
                            "lastName": "Venkata",
                            "middleNames": [
                                "Kota"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sravanthi Kota Venkata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38303344"
                        ],
                        "name": "M. Taylor",
                        "slug": "M.-Taylor",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Taylor",
                            "middleNames": [
                                "Bedford"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143846212"
                        ],
                        "name": "S. Swanson",
                        "slug": "S.-Swanson",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Swanson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Swanson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 218
                            }
                        ],
                        "text": "Accelerators can range from processors tuned for certain tasks, to ASIC-like circuits such as H264 [14], or more flexible accelerators capable of targeting a broad range of, but not all, tasks [12, 44] such as QsCores [42], or accelerators for image processing [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15488082,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "1087e2e1244665c9574ab5914ae13c6c88bcc096",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Transistor density continues to increase exponentially, but power dissipation per transistor is improving only slightly with each generation of Moore's law. Given the constant chip-level power budgets, this exponentially decreases the percentage of transistors that can switch at full frequency with each technology generation. Hence, while the transistor budget continues to increase exponentially, the power budget has become the dominant limiting factor in processor design. In this regime, utilizing transistors to design specialized cores that optimize energy-per-computation becomes an effective approach to improve system performance."
            },
            "slug": "QSCORES:-Trading-dark-silicon-for-scalable-energy-Venkatesh-Sampson",
            "title": {
                "fragments": [],
                "text": "QSCORES: Trading dark silicon for scalable energy efficiency with quasi-specific cores"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "While the transistor budget continues to increase exponentially, the power budget has become the dominant limiting factor in processor design and utilizing transistors to design specialized cores that optimize energy-per-computation becomes an effective approach to improve system performance."
            },
            "venue": {
                "fragments": [],
                "text": "2011 44th Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2275326"
                        ],
                        "name": "Christian Bienia",
                        "slug": "Christian-Bienia",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Bienia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Bienia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109967380"
                        ],
                        "name": "Sanjeev Kumar",
                        "slug": "Sanjeev-Kumar",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjeev Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685479"
                        ],
                        "name": "J. Singh",
                        "slug": "J.-Singh",
                        "structuredName": {
                            "firstName": "Jaswinder",
                            "lastName": "Singh",
                            "middleNames": [
                                "Pal"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49243317"
                        ],
                        "name": "Kai Li",
                        "slug": "Kai-Li",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 116
                            }
                        ],
                        "text": "This trend even starts to percolate in our community where it turns out that about half of the benchmarks of PARSEC [2], a suite partly introduced to highlight the emergence of new types of applications, can be implemented using machine-learning algorithms [4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 109
                            }
                        ],
                        "text": "This trend even starts to percolate in our community where it turns out that about half of the benchmarks \nof PARSEC [2], a suite partly introduced to highlight the emergence of new types of applications, can \nbe implemented using machine-learning algorithms [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10043111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "588dda88f15f1622ee7de7631f2824c23eea60df",
            "isKey": false,
            "numCitedBy": 3372,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents and characterizes the Princeton Application Repository for Shared-Memory Computers (PARSEC), a benchmark suite for studies of Chip-Multiprocessors (CMPs). Previous available benchmarks for multiprocessors have focused on high-performance computing applications and used a limited number of synchronization methods. PARSEC includes emerging applications in recognition, mining and synthesis (RMS) as well as systems applications which mimic large-scale multithreaded commercial programs. Our characterization shows that the benchmark suite covers a wide spectrum of working sets, locality, data sharing, synchronization and off-chip traffic. The benchmark suite has been made available to the public."
            },
            "slug": "The-PARSEC-benchmark-suite:-Characterization-and-Bienia-Kumar",
            "title": {
                "fragments": [],
                "text": "The PARSEC benchmark suite: Characterization and architectural implications"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This paper presents and characterizes the Princeton Application Repository for Shared-Memory Computers (PARSEC), a benchmark suite for studies of Chip-Multiprocessors (CMPs), and shows that the benchmark suite covers a wide spectrum of working sets, locality, data sharing, synchronization and off-chip traffic."
            },
            "venue": {
                "fragments": [],
                "text": "2008 International Conference on Parallel Architectures and Compilation Techniques (PACT)"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696563"
                        ],
                        "name": "H. Esmaeilzadeh",
                        "slug": "H.-Esmaeilzadeh",
                        "structuredName": {
                            "firstName": "Hadi",
                            "lastName": "Esmaeilzadeh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Esmaeilzadeh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2314690"
                        ],
                        "name": "E. Blem",
                        "slug": "E.-Blem",
                        "structuredName": {
                            "firstName": "Emily",
                            "lastName": "Blem",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Blem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2677903"
                        ],
                        "name": "R. S. Amant",
                        "slug": "R.-S.-Amant",
                        "structuredName": {
                            "firstName": "Ren\u00e9e",
                            "lastName": "Amant",
                            "middleNames": [
                                "St."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. S. Amant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720300"
                        ],
                        "name": "K. Sankaralingam",
                        "slug": "K.-Sankaralingam",
                        "structuredName": {
                            "firstName": "Karthikeyan",
                            "lastName": "Sankaralingam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sankaralingam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144859824"
                        ],
                        "name": "D. Burger",
                        "slug": "D.-Burger",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Burger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 58
                            }
                        ],
                        "text": "Due to stringent energy constraints, such as Dark Silicon [10, 32], there is a growing consensus that future highperformance micro-architectures will take the form of heterogeneous multi-cores, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62736408,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ae04be7cd5430355ee380b283475e031b8dff4f",
            "isKey": false,
            "numCitedBy": 1714,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "A key question for the microprocessor research and design community is whether scaling multicores will provide the performance and value needed to scale down many more technology generations. To provide a quantitative answer to this question, a comprehensive study that projects the speedup potential of future multicores and examines the underutilization of integration capacity-dark silicon-is timely and crucial."
            },
            "slug": "Dark-Silicon-and-the-End-of-Multicore-Scaling-Esmaeilzadeh-Blem",
            "title": {
                "fragments": [],
                "text": "Dark Silicon and the End of Multicore Scaling"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A comprehensive study that projects the speedup potential of future multicores and examines the underutilization of integration capacity-dark silicon-is timely and crucial."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Micro"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1981539"
                        ],
                        "name": "Thomas Serre",
                        "slug": "Thomas-Serre",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Serre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Serre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128145"
                        ],
                        "name": "Lior Wolf",
                        "slug": "Lior-Wolf",
                        "structuredName": {
                            "firstName": "Lior",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lior Wolf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747918"
                        ],
                        "name": "S. Bileschi",
                        "slug": "S.-Bileschi",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Bileschi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bileschi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996960"
                        ],
                        "name": "M. Riesenhuber",
                        "slug": "M.-Riesenhuber",
                        "structuredName": {
                            "firstName": "Maximilian",
                            "lastName": "Riesenhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Riesenhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 229
                            }
                        ],
                        "text": "Since 2006, a subset of neural networks have emerged as achieving state-of-the-art machine-learning accuracy across a broad set of applications, partly inspired by progress in neuroscience models of computer vision, such as HMAX [37]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 224
                            }
                        ],
                        "text": "Since 2006, \na subset of neural networks have emerged as achieving state-of-the-art machine-learning accuracy across \na broad set of applica\u00adtions, partly inspired by progress in neuroscience models of computer vision, \nsuch as HMAX [37]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2179592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71e3d9fc53ba14c2feeb7390f0dc99076553b05a",
            "isKey": false,
            "numCitedBy": 1716,
            "numCiting": 82,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new general framework for the recognition of complex visual scenes, which is motivated by biology: We describe a hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation. We demonstrate the strength of the approach on a range of recognition tasks: From invariant single object recognition in clutter to multiclass categorization problems and complex scene understanding tasks that rely on the recognition of both shape-based as well as texture-based objects. Given the biological constraints that the system had to satisfy, the approach performs surprisingly well: It has the capability of learning from only a few training examples and competes with state-of-the-art systems. We also discuss the existence of a universal, redundant dictionary of features that could handle the recognition of most object categories. In addition to its relevance for computer vision, the success of this approach suggests a plausibility proof for a class of feedforward models of object recognition in cortex"
            },
            "slug": "Robust-Object-Recognition-with-Cortex-Like-Serre-Wolf",
            "title": {
                "fragments": [],
                "text": "Robust Object Recognition with Cortex-Like Mechanisms"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A hierarchical system that closely follows the organization of visual cortex and builds an increasingly complex and invariant feature representation by alternating between a template matching and a maximum pooling operation is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49618465"
                        ],
                        "name": "Daniel Larkin",
                        "slug": "Daniel-Larkin",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Larkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Larkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693451"
                        ],
                        "name": "Andrew Kinane",
                        "slug": "Andrew-Kinane",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Kinane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Kinane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35000458"
                        ],
                        "name": "Valentin Muresan",
                        "slug": "Valentin-Muresan",
                        "structuredName": {
                            "firstName": "Valentin",
                            "lastName": "Muresan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Valentin Muresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98536322"
                        ],
                        "name": "N. O'Connor",
                        "slug": "N.-O'Connor",
                        "structuredName": {
                            "firstName": "Noel",
                            "lastName": "O'Connor",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. O'Connor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 41
                            }
                        ],
                        "text": "As previously proposed in the literature [23, 38], the sigmoid of NFU-3 (for classifier and convolutional layers) can be efficiently implemented using piecewise linear interpolation (f(x) = ai \u00d7 x + bi, x \u2208 [xi;xi+1]) with negligible loss of accuracy (16 segments are sufficient) [24], see Figure 9."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12737006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb4b9078b6c4c4a1e80a1f31b40e18fc8b87ee82",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an efficient hardware architecture for a function generator suitable for an artificial neural network (ANN). A spline-based approximation function is designed that provides a good trade-off between accuracy and silicon area, whilst also being inherently scalable and adaptable for numerous activation functions. This has been achieved by using a minimax polynomial and through optimal placement of the approximating polynomials based on the results of a genetic algorithm. The approximation error of the proposed method compares favourably to all related research in this field. Efficient hardware multiplication circuitry is used in the implementation, which reduces the area overhead and increases the throughput."
            },
            "slug": "An-Efficient-Hardware-Architecture-for-a-Neural-Larkin-Kinane",
            "title": {
                "fragments": [],
                "text": "An Efficient Hardware Architecture for a Neural Network Activation Function Generator"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "An efficient hardware architecture for a function generator suitable for an artificial neural network (ANN) is proposed that provides a good trade-off between accuracy and silicon area, whilst also being inherently scalable and adaptable for numerous activation functions."
            },
            "venue": {
                "fragments": [],
                "text": "ISNN"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 107
                            }
                        ],
                        "text": ", initially hyped in the 1980s/1990s, then fading into oblivion with the advent of Support Vector Machines [6]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 218
                            }
                        ],
                        "text": "Primer on Recent Machine-Learning Techniques Even \nthough the role of neural networks in the machine\u00adlearning domain has been rocky, i.e., initially hyped \nin the 1980s/1990s, then fading into oblivion with the advent of Support Vector Machines [6]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 52874011,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52b7bf3ba59b31f362aa07f957f1543a29a4279e",
            "isKey": false,
            "numCitedBy": 33675,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The support-vector network is a new learning machine for two-group classification problems. The machine conceptually implements the following idea: input vectors are non-linearly mapped to a very high-dimension feature space. In this feature space a linear decision surface is constructed. Special properties of the decision surface ensures high generalization ability of the learning machine. The idea behind the support-vector network was previously implemented for the restricted case where the training data can be separated without errors. We here extend this result to non-separable training data.High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated. We also compare the performance of the support-vector network to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "slug": "Support-Vector-Networks-Cortes-Vapnik",
            "title": {
                "fragments": [],
                "text": "Support-Vector Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "High generalization ability of support-vector networks utilizing polynomial input transformations is demonstrated and the performance of the support- vector network is compared to various classical learning algorithms that all took part in a benchmark study of Optical Character Recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3255983"
                        ],
                        "name": "Volodymyr Mnih",
                        "slug": "Volodymyr-Mnih",
                        "structuredName": {
                            "firstName": "Volodymyr",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volodymyr Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9448393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdfdaccd946dc2fe4863aed048b12b5d2282f285",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "When training a system to label images, the amount of labeled training data tends to be a limiting factor. We consider the task of learning to label aerial images from existing maps. These provide abundant labels, but the labels are often incomplete and sometimes poorly registered. We propose two robust loss functions for dealing with these kinds of label noise and use the loss functions to train a deep neural network on two challenging aerial image datasets. The robust loss functions lead to big improvements in performance and our best system substantially outperforms the best published results on the task we consider."
            },
            "slug": "Learning-to-Label-Aerial-Images-from-Noisy-Data-Mnih-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning to Label Aerial Images from Noisy Data"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes two robust loss functions for dealing with incomplete and poorly registered label noise and uses the loss functions to train a deep neural network on two challenging aerial image datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2421691"
                        ],
                        "name": "Po-Sen Huang",
                        "slug": "Po-Sen-Huang",
                        "structuredName": {
                            "firstName": "Po-Sen",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Po-Sen Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144137069"
                        ],
                        "name": "Xiaodong He",
                        "slug": "Xiaodong-He",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800422"
                        ],
                        "name": "Jianfeng Gao",
                        "slug": "Jianfeng-Gao",
                        "structuredName": {
                            "firstName": "Jianfeng",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianfeng Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723644"
                        ],
                        "name": "A. Acero",
                        "slug": "A.-Acero",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Acero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Acero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46819684"
                        ],
                        "name": "Larry Heck",
                        "slug": "Larry-Heck",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Heck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Larry Heck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 275,
                                "start": 271
                            }
                        ],
                        "text": "Due to recent progress in machinelearning, certain types of neural networks, especially Deep Neural Networks [25] and Convolutional Neural Networks [27] have become state-of-the-art machine-learning techniques [26] across a broad range of applications such as web search [19], image analysis [31] or speech recognition [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8384258,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fdb813d8b927bdd21ae1858cafa6c34b66a36268",
            "isKey": false,
            "numCitedBy": 1482,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Latent semantic models, such as LSA, intend to map a query to its relevant documents at the semantic level where keyword-based matching often fails. In this study we strive to develop a series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them. The proposed deep structured semantic models are discriminatively trained by maximizing the conditional likelihood of the clicked documents given a query using the clickthrough data. To make our models applicable to large-scale Web search applications, we also use a technique called word hashing, which is shown to effectively scale up our semantic models to handle large vocabularies which are common in such tasks. The new models are evaluated on a Web document ranking task using a real-world data set. Results show that our best model significantly outperforms other latent semantic models, which were considered state-of-the-art in the performance prior to the work presented in this paper."
            },
            "slug": "Learning-deep-structured-semantic-models-for-web-Huang-He",
            "title": {
                "fragments": [],
                "text": "Learning deep structured semantic models for web search using clickthrough data"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A series of new latent semantic models with a deep structure that project queries and documents into a common low-dimensional space where the relevance of a document given a query is readily computed as the distance between them are developed."
            },
            "venue": {
                "fragments": [],
                "text": "CIKM"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71767925"
                        ],
                        "name": "E. J. King",
                        "slug": "E.-J.-King",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "King",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. J. King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715573"
                        ],
                        "name": "E. Swartzlander",
                        "slug": "E.-Swartzlander",
                        "structuredName": {
                            "firstName": "Earl",
                            "lastName": "Swartzlander",
                            "middleNames": [
                                "E."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Swartzlander"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": ", their output is 16 bits; we use a standard n-bit truncated multiplier with correction constant [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 40632114,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "3ebd27c34647d1d8dee75ca8bb7759715ebb8ed7",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The variable correction truncated multiplier is introduced. This is a method for minimizing the error of a truncated multiplier. The error is reduced by using information from the partial product bits of the column adjacent to the truncated LSB. This results in a complexity savings while introducing minimum distortion to the result."
            },
            "slug": "Data-dependent-truncation-scheme-for-parallel-King-Swartzlander",
            "title": {
                "fragments": [],
                "text": "Data-dependent truncation scheme for parallel multipliers"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "The variable correction truncated multiplier is introduced, a method for minimizing the error of a truncation multiplier by using information from the partial product bits of the column adjacent to the truncated LSB."
            },
            "venue": {
                "fragments": [],
                "text": "Conference Record of the Thirty-First Asilomar Conference on Signals, Systems and Computers (Cat. No.97CB36136)"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "CACTI 5.1. HP Labs"
            },
            "venue": {
                "fragments": [],
                "text": "CACTI 5.1. HP Labs"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Convolutional Neural Networks Applied to House Numbers Digit Classification. In Pattern Recognition (ICPR)"
            },
            "venue": {
                "fragments": [],
                "text": "Convolutional Neural Networks Applied to House Numbers Digit Classification. In Pattern Recognition (ICPR)"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Trading Dark Silicon for Scalable Energy Efficiency with Quasi - Specific Cores Categories and Subject Descriptors"
            },
            "venue": {
                "fragments": [],
                "text": "International Symposium on Microarchitecture"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 58
                            }
                        ],
                        "text": "Due to stringent energy constraints, such as Dark Silicon [10, 32], there is a growing consensus that future highperformance micro-architectures will take the form of heterogeneous multi-cores, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dark Silicon and the Internet"
            },
            "venue": {
                "fragments": [],
                "text": "In EE Times \u201dDesigning with ARM\u201d virtual conference,"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 232,
                                "start": 229
                            }
                        ],
                        "text": "1 \u306f\u3058\u3081\u306b \u7279\u5b9a\u306e\u51e6\u7406\u306b\u7279\u5316\u3057\u305f\u30b3\u30f3\u30d4\u30e5\u30fc\u30bf\u3067\u3042\u308b\u30a2\u30af\u30bb\u30e9\u30ec\u30fc \u30bf\u306f\u753b\u50cf\u51e6\u7406\u306b\u7279\u5316\u3057\u305f GPU\u3084\u6697\u53f7\u5316\u306b\u7279\u5316\u3057\u305f SSL \u30a2\u30af\u30bb\u30e9\u30ec\u30fc\u30bf\u306a\u3069\u69d8\u3005\u306a\u6240\u3067\u5fdc\u7528\u3055\u308c\u3066\u3044\u308b\u3002\u30a2\u30af\u30bb\u30e9 \u30ec\u30fc\u30bf\u3092\u4f7f\u3046\u3053\u3068\u306b\u3088\u3063\u3066\u9ad8\u901f\u306a\u51e6\u7406\u3092\u4f4e\u6d88\u8cbb\u96fb\u529b\u3067\u884c \u3046\u3053\u3068\u304c\u51fa\u6765\u308b\u3002\u307e\u305f\u3001\u4eba\u9593\u306e\u5b66\u7fd2\u80fd\u529b\u3068\u540c\u69d8\u306e\u6a5f\u80fd\u306e \u5b9f\u73fe\u3092\u76ee\u6307\u3057\u305f\u6a5f\u68b0\u5b66\u7fd2\u306f\u3001\u6700\u5148\u7aef\u306e\u30a2\u30eb\u30b4\u30ea\u30ba\u30e0\u3067\u3042 \u308b CNN(Convolutional Neural Network[1])\u3001DNN(Deep Neural Network[2])\u306a\u3069\u306e\u5927\u898f\u6a21\u306a\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc \u30af\u3092\u4f7f\u3046\u3053\u3068\u3067\u66f4\u306b\u6d3b\u8e8d\u306e\u5834\u3092\u5e83\u3052\u3066\u3044\u308b\u3002\u3057\u304b\u3057\u5927\u898f\u6a21 \u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u3092\u5229\u7528\u3059\u308b\u306b\u306f\u81a8\u5927\u306a\u30b3\u30f3\u30d4\u30e5\u30fc \u30bf\u8cc7\u6e90\u304c\u5fc5\u8981\u306a\u306e\u3067\u3001\u6a5f\u68b0\u5b66\u7fd2\u306b\u5229\u7528\u3067\u304d\u308b\u30a2\u30af\u30bb\u30e9\u30ec\u30fc \u30bf\u304c\u5fc5\u8981\u3068\u306a\u3063\u3066\u304f\u308b\u3002\u5f93\u6765\u306e\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u30a2 \u30af\u30bb\u30e9\u30ec\u30fc\u30bf\u7814\u7a76\u306f\u3001\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8a08\u7b97\u90e8 \u5206\u3092\u3088\u308a\u901f\u304f\u52b9\u7387\u7684\u306b\u5b9f\u884c\u3059\u308b\u3053\u3068\u306b\u7126\u70b9\u3092\u304a\u3044\u3066\u304d\u305f\u3002 \u5927\u898f\u6a21\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306e\u8a08\u7b97\u306b\u304a\u3044\u3066\u30dc\u30c8\u30eb\u30cd\u30c3 \u30af\u3068\u306a\u308b\u30e1\u30e2\u30ea\u8ee2\u9001\u306b\u3064\u3044\u3066\u306f\u3042\u307e\u308a\u8003\u616e\u3055\u308c\u3066\u3044\u306a\u3044\u3002 \u305d\u3053\u3067\u672c\u7814\u7a76\u3067\u306f\u3001\u30e1\u30e2\u30ea\u8ee2\u9001\u306e\u56de\u6570\u3092\u6e1b\u3089\u3059\u3053\u3068\u3001\u3067 \u304d\u308b\u3060\u3051\u52b9\u7387\u3088\u304f\u5b9f\u884c\u3057\u30a8\u30cd\u30eb\u30ae\u30fc\u3092\u6e1b\u3089\u3059\u3053\u3068\u306b\u7126\u70b9 \u3092\u304a\u3044\u305f\u5927\u898f\u6a21\u30cb\u30e5\u30fc\u30e9\u30eb\u30cd\u30c3\u30c8\u30ef\u30fc\u30af\u306b\u9069\u5fdc\u3067\u304d\u308b\u30a2\u30af \u30bb\u30e9\u30ec\u30fc\u30bf\u3092\u8a2d\u8a08\u3057\u3001\u6027\u80fd\u3092\u8a55\u4fa1\u3059\u308b\u3002"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving neural networks by peeventing co-adaptation of feature detectors. arXiv preprintarXiv:...,pages 1-18,2012"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 33,
            "methodology": 15,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 47,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/DianNao:-a-small-footprint-high-throughput-for-Chen-Du/22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd?sort=total-citations"
}