{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "7 times fewer MAdds than ResNet-101 [1] (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 247
                            }
                        ],
                        "text": "The MobileNetV2 architecture is based on an inverted residual structure where the input and output of the residual block are thin bottleneck layers opposite to traditional residual models which use expanded representations in the input and output [1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 67
                            }
                        ],
                        "text": "The importance of residual connection has been studied extensively [1, 28, 45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "VGGNet [6], GoogLeNet [7], and ResNet [1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 93
                            }
                        ],
                        "text": "When the expansion ratio is smaller than 1, this is a classical residual convolutional block [1, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 394,
                                "start": 388
                            }
                        ],
                        "text": "We have observed that: (a) the inference strategies, including multi-scale inputs and adding leftright flipped images, significantly increase the MAdds and thus are not suitable for on-device applications, (b) using output stride = 16 is more efficient than output stride = 8, (c) MobileNetV1 is already a powerful feature extractor and only requires about 4.9\u2212 5.7 times fewer MAdds than ResNet-101 [1] (e.g., mIOU: 78.56 vs 82.70, and MAdds: 941.9B vs 4870.6B), (d) it is more efficient to build DeepLabv3 heads on top of the second last feature map of MobileNetV2 than on the original last-layer feature map, since the second to last feature map contains 320 channels instead of 1280, and by doing so, we attain similar performance, but require about 2.5 times fewer operations than the MobileNetV1 counterparts, and (e) DeepLabv3 heads are computationally expensive and removing the ASPP module significantly reduces the MAdds with only a slight performance degradation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 149
                            }
                        ],
                        "text": "The bottleneck blocks appear similar to residual block where each block contains an input followed by several bottlenecks then followed by expansion [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 48
                            }
                        ],
                        "text": "Figure 3: The difference between residual block [1, 28] and inverted residual."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 49
                            }
                        ],
                        "text": "C V\n] 1\n6 Ja\nn 20\nVGGNet [6], GoogLeNet [7], and ResNet [1]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": true,
            "numCitedBy": 97653,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50875121"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiangyu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148927556"
                        ],
                        "name": "Xinyu Zhou",
                        "slug": "Xinyu-Zhou",
                        "structuredName": {
                            "firstName": "Xinyu",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyu Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3287035"
                        ],
                        "name": "Mengxiao Lin",
                        "slug": "Mengxiao-Lin",
                        "structuredName": {
                            "firstName": "Mengxiao",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mengxiao Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 55
                            }
                        ],
                        "text": "In this vein our approach is similar to those taken by [19, 21] and allows to further improve the performance, while providing a glimpse on its internal operation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 96
                            }
                        ],
                        "text": "In Table 3 we compare the needed sizes for each resolution between MobileNetV1, MobileNetV2 and ShuffleNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 106
                            }
                        ],
                        "text": "Depthwise Separable Convolutions are a key building block for many efficient neural network architectures [26, 27, 19] and we use them in the present work as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 230
                            }
                        ],
                        "text": "This has been successfully exploited by MobileNetV1 [26] to effectively trade off between computation and accuracy via a width multiplier parameter, and has been incorporated into efficient model designs of other networks as well [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 53
                            }
                        ],
                        "text": "Results We compare our networks against MobileNetV1, ShuffleNet and NASNet-A models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "(c) ShuffleNet [19] Conv 1x1, Relu6 Dwise 3x3, Relu6"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 151
                            }
                        ],
                        "text": "A substantial amount of work has also been dedicated to changing the connectivity structure of the internal convolutional blocks such as in ShuffleNet [19] or introducing sparsity [20] and others [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "ShuffleNet uses Group Convolutions [19] and shuffling, it also uses conventional residual approach where inner blocks are narrower than output."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 24982157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9da734397acd7ff7c557960c62fb1b400b27bd89",
            "isKey": true,
            "numCitedBy": 3251,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13\u00c3\u2014 actual speedup over AlexNet while maintaining comparable accuracy."
            },
            "slug": "ShuffleNet:-An-Extremely-Efficient-Convolutional-Zhang-Zhou",
            "title": {
                "fragments": [],
                "text": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An extremely computation-efficient CNN architecture named ShuffleNet is introduced, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs), to greatly reduce computation cost while maintaining accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46641573"
                        ],
                        "name": "W. Liu",
                        "slug": "W.-Liu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084646762"
                        ],
                        "name": "Cheng-Yang Fu",
                        "slug": "Cheng-Yang-Fu",
                        "structuredName": {
                            "firstName": "Cheng-Yang",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Yang Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "For MobileNetV2, the first layer of SSDLite is attached to the expansion of layer 15 (with output stride of 16)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 177
                            }
                        ],
                        "text": "We evaluate and compare the performance of MobileNetV2 and MobileNetV1 as feature extractors [32] for object detection with a modified version of the Single Shot Detector (SSD) [33] on COCO dataset [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "We also compare to YOLOv2 [34] and original SSD (with VGG-16 [6] as base network) as baselines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "We call this modified version SSDLite."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 110
                            }
                        ],
                        "text": "We replace all the regular convolutions with separable convolutions (depthwise followed by 1 \u00d7 1 projection) in SSD prediction layers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 27
                            }
                        ],
                        "text": "The\nsecond and the rest of SSDLite layers are attached on top of the last layer (with output stride of 32)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "SSDLite: In this paper, we introduce a mobile friendly variant of regular SSD."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "Notably, MobileNetV2 SSDLite is 20\u00d7 more efficient and 10\u00d7 smaller while still outperforms YOLOv2 on COCO dataset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Compared to regular SSD, SSDLite dramatically reduces both parameter count and computational cost as shown in Table 5."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 44
                            }
                        ],
                        "text": "Notably, our architecture combined with the SSDLite detection mod-\nule is 20\u00d7 less computation and 10\u00d7 less parameters than YOLOv2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "MobileNetV2 SSDLite is not only the most efficient model, but also the most accurate of the three."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2141740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
            "isKey": true,
            "numCitedBy": 15747,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL ."
            },
            "slug": "SSD:-Single-Shot-MultiBox-Detector-Liu-Anguelov",
            "title": {
                "fragments": [],
                "text": "SSD: Single Shot MultiBox Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "The approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location, which makes SSD easy to train and straightforward to integrate into systems that require a detection component."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3302320"
                        ],
                        "name": "Florian Schroff",
                        "slug": "Florian-Schroff",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Schroff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Schroff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595180"
                        ],
                        "name": "Hartwig Adam",
                        "slug": "Hartwig-Adam",
                        "structuredName": {
                            "firstName": "Hartwig",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hartwig Adam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "In this section, we compare MobileNetV1 and MobileNetV2 models used as feature extractors with DeepLabv3 [38] for the task of mobile semantic segmentation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 506,
                                "start": 497
                            }
                        ],
                        "text": "We have observed that: (a) the inference strategies, including multi-scale inputs and adding leftright flipped images, significantly increase the MAdds and thus are not suitable for on-device applications, (b) using output stride = 16 is more efficient than output stride = 8, (c) MobileNetV1 is already a powerful feature extractor and only requires about 4.9\u2212 5.7 times fewer MAdds than ResNet-101 [1] (e.g., mIOU: 78.56 vs 82.70, and MAdds: 941.9B vs 4870.6B), (d) it is more efficient to build DeepLabv3 heads on top of the second last feature map of MobileNetV2 than on the original last-layer feature map, since the second to last feature map contains 320 channels instead of 1280, and by doing so, we attain similar performance, but require about 2.5 times fewer operations than the MobileNetV1 counterparts, and (e) DeepLabv3 heads are computationally expensive and removing the ASPP module significantly reduces the MAdds with only a slight performance degradation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 125
                            }
                        ],
                        "text": "To build a mobile model, we experimented with three design variations: (1) different feature extractors, (2) simplifying the DeepLabv3 heads for faster computation, and (3) different inference strategies for boosting the performance."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 0
                            }
                        ],
                        "text": "DeepLabv3 adopts atrous convolution [39, 40, 41], a powerful tool to explicitly control the reso-\nlution of computed feature maps, and builds five parallel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) [42] containing three 3 \u00d7 3 convolutions with different atrous rates, (b) 1 \u00d7 1 convolution head, and (c) Image-level features [43]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 22655199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee4a012a4b12d11d7ab8c0e79c61e807927a163c",
            "isKey": false,
            "numCitedBy": 3983,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we revisit atrous convolution, a powerful tool to explicitly adjust filter's field-of-view as well as control the resolution of feature responses computed by Deep Convolutional Neural Networks, in the application of semantic image segmentation. To handle the problem of segmenting objects at multiple scales, we design modules which employ atrous convolution in cascade or in parallel to capture multi-scale context by adopting multiple atrous rates. Furthermore, we propose to augment our previously proposed Atrous Spatial Pyramid Pooling module, which probes convolutional features at multiple scales, with image-level features encoding global context and further boost performance. We also elaborate on implementation details and share our experience on training our system. The proposed `DeepLabv3' system significantly improves over our previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark."
            },
            "slug": "Rethinking-Atrous-Convolution-for-Semantic-Image-Chen-Papandreou",
            "title": {
                "fragments": [],
                "text": "Rethinking Atrous Convolution for Semantic Image Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed `DeepLabv3' system significantly improves over the previous DeepLab versions without DenseCRF post-processing and attains comparable performance with other state-of-art models on the PASCAL VOC 2012 semantic image segmentation benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144727050"
                        ],
                        "name": "Andrew G. Howard",
                        "slug": "Andrew-G.-Howard",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Howard",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew G. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2717876"
                        ],
                        "name": "Menglong Zhu",
                        "slug": "Menglong-Zhu",
                        "structuredName": {
                            "firstName": "Menglong",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Menglong Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Bo Chen",
                        "slug": "Bo-Chen",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2741985"
                        ],
                        "name": "Dmitry Kalenichenko",
                        "slug": "Dmitry-Kalenichenko",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Kalenichenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dmitry Kalenichenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108301072"
                        ],
                        "name": "Weijun Wang",
                        "slug": "Weijun-Wang",
                        "structuredName": {
                            "firstName": "Weijun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weijun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47447630"
                        ],
                        "name": "Tobias Weyand",
                        "slug": "Tobias-Weyand",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "Weyand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tobias Weyand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612392"
                        ],
                        "name": "M. Andreetto",
                        "slug": "M.-Andreetto",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Andreetto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Andreetto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595180"
                        ],
                        "name": "Hartwig Adam",
                        "slug": "Hartwig-Adam",
                        "structuredName": {
                            "firstName": "Hartwig",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hartwig Adam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Following MobileNetV1[26] setup we use initial learning rate of"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "We use ReLU6 as the non-linearity because of its robustness when used with low-precision computation [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 106
                            }
                        ],
                        "text": "Depthwise Separable Convolutions are a key building block for many efficient neural network architectures [26, 27, 19] and we use them in the present work as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "This has been successfully exploited by MobileNetV1 [26] to effectively trade off between computation and accuracy via a width multiplier parameter, and has been incorporated into efficient model designs of other networks as well [19]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "Trade-off hyper parameters As in [26] we tailor our architecture to different performance points, by using the input image resolution and width multiplier as tunable hyper parameters, that can be adjusted depending on desired accuracy/performance trade-offs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "Our network design is based on MobileNetV1 [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 186
                            }
                        ],
                        "text": "MobileNetV2 uses k = 3 (3\u00d7 3 depthwise separable convolutions) so the computational cost is 8 to 9 times smaller than that of standard convolutions at only a small reduction in accuracy [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "One minor implementation difference, with [26] is that for multipliers less than one, we apply width multi-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12670695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3647d6d0f151dc05626449ee09cc7bce55be497e",
            "isKey": false,
            "numCitedBy": 10323,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization."
            },
            "slug": "MobileNets:-Efficient-Convolutional-Neural-Networks-Howard-Zhu",
            "title": {
                "fragments": [],
                "text": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work introduces two simple global hyper-parameters that efficiently trade off between latency and accuracy and demonstrates the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010660"
                        ],
                        "name": "Iasonas Kokkinos",
                        "slug": "Iasonas-Kokkinos",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Kokkinos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iasonas Kokkinos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145081362"
                        ],
                        "name": "A. Yuille",
                        "slug": "A.-Yuille",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Yuille",
                            "middleNames": [
                                "Loddon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Yuille"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 890,
                                "start": 886
                            }
                        ],
                        "text": "We have observed that: (a) the inference strategies, including multi-scale inputs and adding leftright flipped images, significantly increase the MAdds and thus are not suitable for on-device applications, (b) using output stride = 16 is more efficient than output stride = 8, (c) MobileNetV1 is already a powerful feature extractor and only requires about 4.9\u2212 5.7 times fewer MAdds than ResNet-101 [1] (e.g., mIOU: 78.56 vs 82.70, and MAdds: 941.9B vs 4870.6B), (d) it is more efficient to build DeepLabv3 heads on top of the second last feature map of MobileNetV2 than on the original last-layer feature map, since the second to last feature map contains 320 channels instead of 1280, and by doing so, we attain similar performance, but require about 2.5 times fewer operations than the MobileNetV1 counterparts, and (e) DeepLabv3 heads are computationally expensive and removing the ASPP module significantly reduces the MAdds with only a slight performance degradation."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "lution of computed feature maps, and builds five parallel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) [42] containing three 3 \u00d7 3 convolutions with different atrous rates, (b) 1 \u00d7 1 convolution head, and (c) Image-level features [43]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 213
                            }
                        ],
                        "text": "DeepLabv3 adopts atrous convolution [39, 40, 41], a powerful tool to explicitly control the reso-\nlution of computed feature maps, and builds five parallel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) [42] containing three 3 \u00d7 3 convolutions with different atrous rates, (b) 1 \u00d7 1 convolution head, and (c) Image-level features [43]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3429309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cab372bc3824780cce20d9dd1c22d4df39ed081a",
            "isKey": false,
            "numCitedBy": 9615,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we address the task of semantic image segmentation with Deep Learning and make three main contributions that are experimentally shown to have substantial practical merit. First, we highlight convolution with upsampled filters, or \u2018atrous\u00a0convolution\u2019, as a powerful tool in dense prediction tasks. Atrous convolution allows us to explicitly control the resolution at which feature responses are computed within Deep Convolutional Neural Networks. It also allows us to effectively enlarge the field of view of filters to incorporate larger context without increasing the number of parameters or the amount of computation. Second, we propose atrous\u00a0spatial pyramid pooling (ASPP) to robustly segment objects at multiple scales. ASPP probes an incoming convolutional feature layer with filters at multiple sampling rates and effective fields-of-views, thus capturing objects as well as image context at multiple scales. Third, we improve the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models. The commonly deployed combination of max-pooling and downsampling in DCNNs achieves invariance but has a toll on localization accuracy. We overcome this by combining the responses at the final DCNN layer with a fully connected Conditional Random Field (CRF), which is shown both qualitatively and quantitatively to improve localization performance. Our proposed \u201cDeepLab\u201d system sets the new state-of-art at the PASCAL VOC-2012 semantic image segmentation task, reaching 79.7 percent mIOU in the test set, and advances the results on three other datasets: PASCAL-Context, PASCAL-Person-Part, and Cityscapes. All of our code is made publicly available online."
            },
            "slug": "DeepLab:-Semantic-Image-Segmentation-with-Deep-and-Chen-Papandreou",
            "title": {
                "fragments": [],
                "text": "DeepLab: Semantic Image Segmentation with Deep Convolutional Nets, Atrous Convolution, and Fully Connected CRFs"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work addresses the task of semantic image segmentation with Deep Learning and proposes atrous\u00a0spatial pyramid pooling (ASPP), which is proposed to robustly segment objects at multiple scales, and improves the localization of object boundaries by combining methods from DCNNs and probabilistic graphical models."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217430"
                        ],
                        "name": "M. Holschneider",
                        "slug": "M.-Holschneider",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Holschneider",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Holschneider"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398530308"
                        ],
                        "name": "R. Kronland-Martinet",
                        "slug": "R.-Kronland-Martinet",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Kronland-Martinet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kronland-Martinet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145515955"
                        ],
                        "name": "J. Morlet",
                        "slug": "J.-Morlet",
                        "structuredName": {
                            "firstName": "Jean",
                            "lastName": "Morlet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Morlet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830702"
                        ],
                        "name": "P. Tchamitchian",
                        "slug": "P.-Tchamitchian",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Tchamitchian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tchamitchian"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 36
                            }
                        ],
                        "text": "DeepLabv3 adopts atrous convolution [39, 40, 41], a powerful tool to explicitly control the reso-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60719890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef3ed634a28eee78a7e31fc525cf7083e6689345",
            "isKey": false,
            "numCitedBy": 766,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this paper is to present a real-time algorithm for the analysis of time-varying signals with the help of the wavelet transform. We shall briefly describe this transformation in the following. For more details, we refer to the literature [1]."
            },
            "slug": "A-real-time-algorithm-for-signal-analysis-with-the-Holschneider-Kronland-Martinet",
            "title": {
                "fragments": [],
                "text": "A real-time algorithm for signal analysis with the help of the wavelet transform"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The purpose of this paper is to present a real-time algorithm for the analysis of time-varying signals with the help of the wavelet transform."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368067"
                        ],
                        "name": "Barret Zoph",
                        "slug": "Barret-Zoph",
                        "structuredName": {
                            "firstName": "Barret",
                            "lastName": "Zoph",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barret Zoph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053781980"
                        ],
                        "name": "Vijay Vasudevan",
                        "slug": "Vijay-Vasudevan",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Vasudevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vijay Vasudevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397917613"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc V.",
                            "lastName": "Le",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 10
                            }
                        ],
                        "text": "Recently, [22, 23, 24, 25], opened up a new direction of bringing optimization methods including genetic algorithms and reinforcement learning to architectural search."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Our approach should be seen as complimentary to the one described in [22] and related work."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "(a) NasNet[22] input Dwise 3x3, stride=s, Relu6 conv 1x1, Relu6"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12227989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0611891b9e8a7c5731146097b6f201578f47b2f",
            "isKey": false,
            "numCitedBy": 3538,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the \"NASNet search space\") which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a \"NASNet architecture\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset."
            },
            "slug": "Learning-Transferable-Architectures-for-Scalable-Zoph-Vasudevan",
            "title": {
                "fragments": [],
                "text": "Learning Transferable Architectures for Scalable Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes to search for an architectural building block on a small dataset and then transfer the block to a larger dataset and introduces a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145329413"
                        ],
                        "name": "Karim Ahmed",
                        "slug": "Karim-Ahmed",
                        "structuredName": {
                            "firstName": "Karim",
                            "lastName": "Ahmed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karim Ahmed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732879"
                        ],
                        "name": "L. Torresani",
                        "slug": "L.-Torresani",
                        "structuredName": {
                            "firstName": "Lorenzo",
                            "lastName": "Torresani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Torresani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 227
                            }
                        ],
                        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [8, 9, 10] as well as various methods of network pruning [11, 12, 13, 14, 15, 16] and connectivity learning [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41803621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "610eaed09988471e49fc458a9dd828889dff23b0",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "While much of the work in the design of convolutional networks over the last five years has revolved around the empirical investigation of the importance of depth, filter sizes, and number of feature channels, recent studies have shown that branching, i.e., splitting the computation along parallel but distinct threads and then aggregating their outputs, represents a new promising dimension for significant improvements in performance. To combat the complexity of design choices in multi-branch architectures, prior work has adopted simple strategies, such as a fixed branching factor, the same input being fed to all parallel branches, and an additive combination of the outputs produced by all branches at aggregation points. \nIn this work we remove these predefined choices and propose an algorithm to learn the connections between branches in the network. Instead of being chosen a priori by the human designer, the multi-branch connectivity is learned simultaneously with the weights of the network by optimizing a single loss function defined with respect to the end task. We demonstrate our approach on the problem of multi-class image classification using three different datasets where it yields consistently higher accuracy compared to the state-of-the-art \"ResNeXt\" multi-branch network given the same learning capacity."
            },
            "slug": "Connectivity-Learning-in-Multi-Branch-Networks-Ahmed-Torresani",
            "title": {
                "fragments": [],
                "text": "Connectivity Learning in Multi-Branch Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes an algorithm to learn the connections between branches in the network, which is instead of being chosen a priori by the human designer, the multi-branch connectivity is learned simultaneously with the weights of the network by optimizing a single loss function defined with respect to the end task."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19317494"
                        ],
                        "name": "Tom V\u00e9niat",
                        "slug": "Tom-V\u00e9niat",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "V\u00e9niat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom V\u00e9niat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8905591"
                        ],
                        "name": "Ludovic Denoyer",
                        "slug": "Ludovic-Denoyer",
                        "structuredName": {
                            "firstName": "Ludovic",
                            "lastName": "Denoyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ludovic Denoyer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 227
                            }
                        ],
                        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [8, 9, 10] as well as various methods of network pruning [11, 12, 13, 14, 15, 16] and connectivity learning [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 195345798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da6d086617239f5668076515bf78790d4ec0af2a",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning neural network architectures is a way to discover new highly predictive models. We propose to focus on this problem from a different perspective where the goal is to discover architectures efficient in terms of both prediction quality and computation cost, e.g time in milliseconds, number of operations... For instance, our approach is able to solve the following task: find the best neural network architecture (in a very large set of possible architectures) able to predict well in less than 100 milliseconds on my mobile phone. Our contribution is based on a new family of models called Budgeted Super Networks that are learned using reinforcement-learning inspired techniques applied to a budgeted learning objective function which includes the computation cost during disk/memory operations at inference. We present a set of experiments on computer vision problems and show the ability of our method to discover efficient architectures in terms of both predictive quality and computation time."
            },
            "slug": "Learning-Time-Efficient-Deep-Architectures-with-V\u00e9niat-Denoyer",
            "title": {
                "fragments": [],
                "text": "Learning Time-Efficient Deep Architectures with Budgeted Super Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes a new family of models called Budgeted Super Networks that are learned using reinforcement-learning inspired techniques applied to a budgeted learning objective function which includes the computation cost during disk/memory operations at inference."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2892780"
                        ],
                        "name": "Esteban Real",
                        "slug": "Esteban-Real",
                        "structuredName": {
                            "firstName": "Esteban",
                            "lastName": "Real",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Esteban Real"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144375552"
                        ],
                        "name": "Sherry Moore",
                        "slug": "Sherry-Moore",
                        "structuredName": {
                            "firstName": "Sherry",
                            "lastName": "Moore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherry Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714992"
                        ],
                        "name": "Andrew Selle",
                        "slug": "Andrew-Selle",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Selle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Selle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054003577"
                        ],
                        "name": "Saurabh Saxena",
                        "slug": "Saurabh-Saxena",
                        "structuredName": {
                            "firstName": "Saurabh",
                            "lastName": "Saxena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saurabh Saxena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46901409"
                        ],
                        "name": "Y. Suematsu",
                        "slug": "Y.-Suematsu",
                        "structuredName": {
                            "firstName": "Yutaka",
                            "lastName": "Suematsu",
                            "middleNames": [
                                "Leon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Suematsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739176520"
                        ],
                        "name": "Jie Tan",
                        "slug": "Jie-Tan",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145714153"
                        ],
                        "name": "A. Kurakin",
                        "slug": "A.-Kurakin",
                        "structuredName": {
                            "firstName": "Alexey",
                            "lastName": "Kurakin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kurakin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 10
                            }
                        ],
                        "text": "Recently, [22, 23, 24, 25], opened up a new direction of bringing optimization methods including genetic algorithms and reinforcement learning to architectural search."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 743641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f108b65fe0003e387e1cd7e50f537af0531818e4",
            "isKey": false,
            "numCitedBy": 1118,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks have proven effective at solving difficult problems but designing their architectures can be challenging, even for image classification problems alone. Our goal is to minimize human participation, so we employ evolutionary algorithms to discover such networks automatically. Despite significant computational requirements, we show that it is now possible to evolve models with accuracies within the range of those published in the last year. Specifically, we employ simple evolutionary techniques at unprecedented scales to discover models for the CIFAR-10 and CIFAR-100 datasets, starting from trivial initial conditions and reaching accuracies of 94.6% (95.6% for ensemble) and 77.0%, respectively. To do this, we use novel and intuitive mutation operators that navigate large search spaces; we stress that no human participation is required once evolution starts and that the output is a fully-trained model. Throughout this work, we place special emphasis on the repeatability of results, the variability in the outcomes and the computational requirements."
            },
            "slug": "Large-Scale-Evolution-of-Image-Classifiers-Real-Moore",
            "title": {
                "fragments": [],
                "text": "Large-Scale Evolution of Image Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that it is now possible to evolve models with accuracies within the range of those published in the last year, starting from trivial initial conditions and reaching accuracies of 94.6% and 77.0%, respectively."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059199"
                        ],
                        "name": "Soravit Changpinyo",
                        "slug": "Soravit-Changpinyo",
                        "structuredName": {
                            "firstName": "Soravit",
                            "lastName": "Changpinyo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soravit Changpinyo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144882893"
                        ],
                        "name": "M. Sandler",
                        "slug": "M.-Sandler",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Sandler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sandler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422677"
                        ],
                        "name": "A. Zhmoginov",
                        "slug": "A.-Zhmoginov",
                        "structuredName": {
                            "firstName": "Andrey",
                            "lastName": "Zhmoginov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zhmoginov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 180
                            }
                        ],
                        "text": "A substantial amount of work has also been dedicated to changing the connectivity structure of the internal convolutional blocks such as in ShuffleNet [19] or introducing sparsity [20] and others [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8700217,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5710bce2dda5d01cb14644281731394ff719265",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep convolutional networks are well-known for their high computational and memory demands. Given limited resources, how does one design a network that balances its size, training time, and prediction accuracy? A surprisingly effective approach to trade accuracy for size and speed is to simply reduce the number of channels in each convolutional layer by a fixed fraction and retrain the network. In many cases this leads to significantly smaller networks with only minimal changes to accuracy. In this paper, we take a step further by empirically examining a strategy for deactivating connections between filters in convolutional layers in a way that allows us to harvest savings both in run-time and memory for many network architectures. More specifically, we generalize 2D convolution to use a channel-wise sparse connection structure and show that this leads to significantly better results than the baseline approach for large networks including VGG and Inception V3."
            },
            "slug": "The-Power-of-Sparsity-in-Convolutional-Neural-Changpinyo-Sandler",
            "title": {
                "fragments": [],
                "text": "The Power of Sparsity in Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "2D convolution is generalized to use a channel-wise sparse connection structure and it is shown that this leads to significantly better results than the baseline approach for large networks including VGG and Inception V3."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34901397"
                        ],
                        "name": "Min Wang",
                        "slug": "Min-Wang",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3169689"
                        ],
                        "name": "Baoyuan Liu",
                        "slug": "Baoyuan-Liu",
                        "structuredName": {
                            "firstName": "Baoyuan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baoyuan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691260"
                        ],
                        "name": "H. Foroosh",
                        "slug": "H.-Foroosh",
                        "structuredName": {
                            "firstName": "Hassan",
                            "lastName": "Foroosh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Foroosh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18430830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d20da75866f500ee9fbfa859e69556702e1f50a4",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep convolutional neural networks achieve remarkable visual recognition performance, at the cost of high computational complexity. In this paper, we have a new design of efficient convolutional layers based on three schemes. The 3D convolution operation in a convolutional layer can be considered as performing spatial convolution in each channel and linear projection across channels simultaneously. By unravelling them and arranging the spatial convolution sequentially, the proposed layer is composed of a single intra-channel convolution, of which the computation is negligible, and a linear channel projection. A topological subdivisioning is adopted to reduce the connection between the input channels and output channels. Additionally, we also introduce a spatial \"bottleneck\" structure that utilizes a convolution-projection-deconvolution pipeline to take advantage of the correlation between adjacent pixels in the input. Our experiments demonstrate that the proposed layers remarkably outperform the standard convolutional layers with regard to accuracy/complexity ratio. Our models achieve similar accuracy to VGG, ResNet-50, ResNet-101 while requiring 42, 4.5, 6.5 times less computation respectively."
            },
            "slug": "Design-of-Efficient-Convolutional-Layers-using-and-Wang-Liu",
            "title": {
                "fragments": [],
                "text": "Design of Efficient Convolutional Layers using Single Intra-channel Convolution, Topological Subdivisioning and Spatial \"Bottleneck\" Structure"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new design of efficient convolutional layers based on three schemes, including a spatial \"bottleneck\" structure that utilizes a convolution-projection-deconvolution pipeline to take advantage of the correlation between adjacent pixels in the input."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497777"
                        ],
                        "name": "Joseph Redmon",
                        "slug": "Joseph-Redmon",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Redmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Redmon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "We also compare to YOLOv2 [34] and original SSD (with VGG-16 [6] as base network) as baselines."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 120
                            }
                        ],
                        "text": "Notably, our architecture combined with the SSDLite detection mod-\nule is 20\u00d7 less computation and 10\u00d7 less parameters than YOLOv2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 89
                            }
                        ],
                        "text": "Notably, MobileNetV2 SSDLite is 20\u00d7 more efficient and 10\u00d7 smaller while still outperforms YOLOv2 on COCO dataset."
                    },
                    "intents": []
                }
            ],
            "corpusId": 786357,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d39d69b23424446f0400ef603b2e3e22d0309d6",
            "isKey": false,
            "numCitedBy": 8140,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories. First we propose various improvements to the YOLO detection method, both novel and drawn from prior work. The improved model, YOLOv2, is state-of-the-art on standard detection tasks like PASCAL VOC and COCO. Using a novel, multi-scale training method the same YOLOv2 model can run at varying sizes, offering an easy tradeoff between speed and accuracy. At 67 FPS, YOLOv2 gets 76.8 mAP on VOC 2007. At 40 FPS, YOLOv2 gets 78.6 mAP, outperforming state-of-the-art methods like Faster RCNN with ResNet and SSD while still running significantly faster. Finally we propose a method to jointly train on object detection and classification. Using this method we train YOLO9000 simultaneously on the COCO detection dataset and the ImageNet classification dataset. Our joint training allows YOLO9000 to predict detections for object classes that dont have labelled detection data. We validate our approach on the ImageNet detection task. YOLO9000 gets 19.7 mAP on the ImageNet detection validation set despite only having detection data for 44 of the 200 classes. On the 156 classes not in COCO, YOLO9000 gets 16.0 mAP. YOLO9000 predicts detections for more than 9000 different object categories, all in real-time."
            },
            "slug": "YOLO9000:-Better,-Faster,-Stronger-Redmon-Farhadi",
            "title": {
                "fragments": [],
                "text": "YOLO9000: Better, Faster, Stronger"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "YOLO9000, a state-of-the-art, real-time object detection system that can detect over 9000 object categories, is introduced and a method to jointly train on object detection and classification is proposed, both novel and drawn from prior work."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1817030"
                        ],
                        "name": "Saining Xie",
                        "slug": "Saining-Xie",
                        "structuredName": {
                            "firstName": "Saining",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saining Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144035504"
                        ],
                        "name": "Z. Tu",
                        "slug": "Z.-Tu",
                        "structuredName": {
                            "firstName": "Zhuowen",
                            "lastName": "Tu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Tu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 93
                            }
                        ],
                        "text": "When the expansion ratio is smaller than 1, this is a classical residual convolutional block [1, 28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 48
                            }
                        ],
                        "text": "Figure 3: The difference between residual block [1, 28] and inverted residual."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 67
                            }
                        ],
                        "text": "The importance of residual connection has been studied extensively [1, 28, 45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8485068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6e0856b4a9199fa968ac00da612a9407b5cb85c",
            "isKey": false,
            "numCitedBy": 5619,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple, highly modularized network architecture for image classification. Our network is constructed by repeating a building block that aggregates a set of transformations with the same topology. Our simple design results in a homogeneous, multi-branch architecture that has only a few hyper-parameters to set. This strategy exposes a new dimension, which we call cardinality (the size of the set of transformations), as an essential factor in addition to the dimensions of depth and width. On the ImageNet-1K dataset, we empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy. Moreover, increasing cardinality is more effective than going deeper or wider when we increase the capacity. Our models, named ResNeXt, are the foundations of our entry to the ILSVRC 2016 classification task in which we secured 2nd place. We further investigate ResNeXt on an ImageNet-5K set and the COCO detection set, also showing better results than its ResNet counterpart. The code and models are publicly available online."
            },
            "slug": "Aggregated-Residual-Transformations-for-Deep-Neural-Xie-Girshick",
            "title": {
                "fragments": [],
                "text": "Aggregated Residual Transformations for Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "On the ImageNet-1K dataset, it is empirically show that even under the restricted condition of maintaining complexity, increasing cardinality is able to improve classification accuracy and is more effective than going deeper or wider when the authors increase the capacity."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136435893"
                        ],
                        "name": "Jonathan Huang",
                        "slug": "Jonathan-Huang",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1382126732"
                        ],
                        "name": "V. Rathod",
                        "slug": "V.-Rathod",
                        "structuredName": {
                            "firstName": "Vivek",
                            "lastName": "Rathod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Rathod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1491624845"
                        ],
                        "name": "Chen Sun",
                        "slug": "Chen-Sun",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2717876"
                        ],
                        "name": "Menglong Zhu",
                        "slug": "Menglong-Zhu",
                        "structuredName": {
                            "firstName": "Menglong",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Menglong Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34786378"
                        ],
                        "name": "A. Balan",
                        "slug": "A.-Balan",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Balan",
                            "middleNames": [
                                "Korattikara"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Balan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50706340"
                        ],
                        "name": "A. Fathi",
                        "slug": "A.-Fathi",
                        "structuredName": {
                            "firstName": "Alireza",
                            "lastName": "Fathi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fathi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33091759"
                        ],
                        "name": "Ian S. Fischer",
                        "slug": "Ian-S.-Fischer",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Fischer",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian S. Fischer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3282833"
                        ],
                        "name": "Z. Wojna",
                        "slug": "Z.-Wojna",
                        "structuredName": {
                            "firstName": "Zbigniew",
                            "lastName": "Wojna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Wojna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157997231"
                        ],
                        "name": "Yang Song",
                        "slug": "Yang-Song",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702318"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "For MobileNetV1, we follow the setup in [32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "We evaluate and compare the performance of MobileNetV2 and MobileNetV1 as feature extractors [32] for object detection with a modified version of the Single Shot Detector (SSD) [33] on COCO dataset [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206595627,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a312a573ef81793d56401e932ef6c9498791a3d1",
            "isKey": false,
            "numCitedBy": 2017,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of this paper is to serve as a guide for selecting a detection architecture that achieves the right speed/memory/accuracy balance for a given application and platform. To this end, we investigate various ways to trade accuracy for speed and memory usage in modern convolutional object detection systems. A number of successful systems have been proposed in recent years, but apples-toapples comparisons are difficult due to different base feature extractors (e.g., VGG, Residual Networks), different default image resolutions, as well as different hardware and software platforms. We present a unified implementation of the Faster R-CNN [30], R-FCN [6] and SSD [25] systems, which we view as meta-architectures and trace out the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures. On one extreme end of this spectrum where speed and memory are critical, we present a detector that achieves real time speeds and can be deployed on a mobile device. On the opposite end in which accuracy is critical, we present a detector that achieves state-of-the-art performance measured on the COCO detection task."
            },
            "slug": "Speed/Accuracy-Trade-Offs-for-Modern-Convolutional-Huang-Rathod",
            "title": {
                "fragments": [],
                "text": "Speed/Accuracy Trade-Offs for Modern Convolutional Object Detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A unified implementation of the Faster R-CNN, R-FCN and SSD systems is presented and the speed/accuracy trade-off curve created by using alternative feature extractors and varying other critical parameters such as image size within each of these meta-architectures is traced out."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368067"
                        ],
                        "name": "Barret Zoph",
                        "slug": "Barret-Zoph",
                        "structuredName": {
                            "firstName": "Barret",
                            "lastName": "Zoph",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barret Zoph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 10
                            }
                        ],
                        "text": "Recently, [22, 23, 24, 25], opened up a new direction of bringing optimization methods including genetic algorithms and reinforcement learning to architectural search."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12713052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67d968c7450878190e45ac7886746de867bf673d",
            "isKey": false,
            "numCitedBy": 3482,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214."
            },
            "slug": "Neural-Architecture-Search-with-Reinforcement-Zoph-Le",
            "title": {
                "fragments": [],
                "text": "Neural Architecture Search with Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper uses a recurrent network to generate the model descriptions of neural networks and trains this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1565641737"
                        ],
                        "name": "Fran\u00e7ois Chollet",
                        "slug": "Fran\u00e7ois-Chollet",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Chollet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fran\u00e7ois Chollet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 106
                            }
                        ],
                        "text": "Depthwise Separable Convolutions are a key building block for many efficient neural network architectures [26, 27, 19] and we use them in the present work as well."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2375110,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b6ec746d309b165f9f9def873a2375b6fb40f3d",
            "isKey": false,
            "numCitedBy": 6758,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an interpretation of Inception modules in convolutional neural networks as being an intermediate step in-between regular convolution and the depthwise separable convolution operation (a depthwise convolution followed by a pointwise convolution). In this light, a depthwise separable convolution can be understood as an Inception module with a maximally large number of towers. This observation leads us to propose a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions. We show that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset (which Inception V3 was designed for), and significantly outperforms Inception V3 on a larger image classification dataset comprising 350 million images and 17,000 classes. Since the Xception architecture has the same number of parameters as Inception V3, the performance gains are not due to increased capacity but rather to a more efficient use of model parameters."
            },
            "slug": "Xception:-Deep-Learning-with-Depthwise-Separable-Chollet",
            "title": {
                "fragments": [],
                "text": "Xception: Deep Learning with Depthwise Separable Convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work proposes a novel deep convolutional neural network architecture inspired by Inception, where Inception modules have been replaced with depthwise separable convolutions, and shows that this architecture, dubbed Xception, slightly outperforms Inception V3 on the ImageNet dataset, and significantly outperforms it on a larger image classification dataset."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706574"
                        ],
                        "name": "Hao Li",
                        "slug": "Hao-Li",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2293919"
                        ],
                        "name": "Asim Kadav",
                        "slug": "Asim-Kadav",
                        "structuredName": {
                            "firstName": "Asim",
                            "lastName": "Kadav",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Asim Kadav"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844230"
                        ],
                        "name": "Igor Durdanovic",
                        "slug": "Igor-Durdanovic",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Durdanovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Igor Durdanovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719385"
                        ],
                        "name": "H. Samet",
                        "slug": "H.-Samet",
                        "structuredName": {
                            "firstName": "Hanan",
                            "lastName": "Samet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Samet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775043"
                        ],
                        "name": "H. Graf",
                        "slug": "H.-Graf",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Graf",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Graf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 176
                            }
                        ],
                        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [8, 9, 10] as well as various methods of network pruning [11, 12, 13, 14, 15, 16] and connectivity learning [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14089312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2a1cb1612ba21e067a5c3ba478a8d73b796b77a",
            "isKey": false,
            "numCitedBy": 2180,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "The success of CNNs in various applications is accompanied by a significant increase in the computation and parameter storage costs. Recent efforts toward reducing these overheads involve pruning and compressing the weights of various layers without hurting original accuracy. However, magnitude-based pruning of weights reduces a significant number of parameters from the fully connected layers and may not adequately reduce the computation costs in the convolutional layers due to irregular sparsity in the pruned networks. We present an acceleration method for CNNs, where we prune filters from CNNs that are identified as having a small effect on the output accuracy. By removing whole filters in the network together with their connecting feature maps, the computation costs are reduced significantly. In contrast to pruning weights, this approach does not result in sparse connectivity patterns. Hence, it does not need the support of sparse convolution libraries and can work with existing efficient BLAS libraries for dense matrix multiplications. We show that even simple filter pruning techniques can reduce inference costs for VGG-16 by up to 34% and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks."
            },
            "slug": "Pruning-Filters-for-Efficient-ConvNets-Li-Kadav",
            "title": {
                "fragments": [],
                "text": "Pruning Filters for Efficient ConvNets"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents an acceleration method for CNNs, where it is shown that even simple filter pruning techniques can reduce inference costs for VGG-16 and ResNet-110 by up to 38% on CIFAR10 while regaining close to the original accuracy by retraining the networks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2527106"
                        ],
                        "name": "Yiwen Guo",
                        "slug": "Yiwen-Guo",
                        "structuredName": {
                            "firstName": "Yiwen",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiwen Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2021251"
                        ],
                        "name": "Anbang Yao",
                        "slug": "Anbang-Yao",
                        "structuredName": {
                            "firstName": "Anbang",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anbang Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109184871"
                        ],
                        "name": "Yurong Chen",
                        "slug": "Yurong-Chen",
                        "structuredName": {
                            "firstName": "Yurong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yurong Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 176
                            }
                        ],
                        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [8, 9, 10] as well as various methods of network pruning [11, 12, 13, 14, 15, 16] and connectivity learning [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 744803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c220cdbcec6f92e4bc0f58c5fa6c1183105be1f9",
            "isKey": false,
            "numCitedBy": 737,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep learning has become a ubiquitous technology to improve machine intelligence. However, most of the existing deep models are structurally very complex, making them difficult to be deployed on the mobile platforms with limited computational power. In this paper, we propose a novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning. Unlike the previous methods which accomplish this task in a greedy way, we properly incorporate connection splicing into the whole process to avoid incorrect pruning and make it as a continual network maintenance. The effectiveness of our method is proved with experiments. Without any accuracy loss, our method can efficiently compress the number of parameters in LeNet-5 and AlexNet by a factor of $\\bm{108}\\times$ and $\\bm{17.7}\\times$ respectively, proving that it outperforms the recent pruning method by considerable margins. Code and some models are available at this https URL."
            },
            "slug": "Dynamic-Network-Surgery-for-Efficient-DNNs-Guo-Yao",
            "title": {
                "fragments": [],
                "text": "Dynamic Network Surgery for Efficient DNNs"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel network compression method called dynamic network surgery, which can remarkably reduce the network complexity by making on-the-fly connection pruning by proving that it outperforms the recent pruning method by considerable margins."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143840275"
                        ],
                        "name": "Song Han",
                        "slug": "Song-Han",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47325862"
                        ],
                        "name": "Jeff Pool",
                        "slug": "Jeff-Pool",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Pool",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Pool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46617804"
                        ],
                        "name": "Sharan Narang",
                        "slug": "Sharan-Narang",
                        "structuredName": {
                            "firstName": "Sharan",
                            "lastName": "Narang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sharan Narang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3123774"
                        ],
                        "name": "Huizi Mao",
                        "slug": "Huizi-Mao",
                        "structuredName": {
                            "firstName": "Huizi",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huizi Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2814338"
                        ],
                        "name": "Shijian Tang",
                        "slug": "Shijian-Tang",
                        "structuredName": {
                            "firstName": "Shijian",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijian Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152585800"
                        ],
                        "name": "Erich Elsen",
                        "slug": "Erich-Elsen",
                        "structuredName": {
                            "firstName": "Erich",
                            "lastName": "Elsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erich Elsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2301680"
                        ],
                        "name": "Bryan Catanzaro",
                        "slug": "Bryan-Catanzaro",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Catanzaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan Catanzaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066786849"
                        ],
                        "name": "J. Tran",
                        "slug": "J.-Tran",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80724002"
                        ],
                        "name": "W. Dally",
                        "slug": "W.-Dally",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dally",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dally"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 176
                            }
                        ],
                        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [8, 9, 10] as well as various methods of network pruning [11, 12, 13, 14, 15, 16] and connectivity learning [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1092465,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4690ff0ccfe8d160dd456d1d7900f8fd3b6ad782",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern deep neural networks have a large number of parameters, making them very powerful machine learning systems. A critical issue for training such large networks on large-scale data-sets is to prevent overfitting while at the same time providing enough model capacity. We propose DSD, a dense-sparse-dense training flow, for regularizing deep neural networks. In the first D step, we train a dense network to learn which connections are important. In the S step, we regularize the network by pruning the unimportant connections and retrain the network given the sparsity constraint. In the final D step, we increase the model capacity by freeing the sparsity constraint, re-initializing the pruned parameters, and retraining the whole dense network. Experiments show that DSD training can improve the performance of a wide range of CNN, RNN and LSTMs on the tasks of image classification, caption generation and speech recognition. On the Imagenet dataset, DSD improved the absolute accuracy of AlexNet, GoogleNet, VGG-16, ResNet50, ResNet-152 and SqueezeNet by a geo-mean of 2.1 points (Top-1) and 1.4 points (Top-5). On the WSJ\u201992 and WSJ\u201993 dataset, DSD improved DeepSpeech2 WER by 0.53 and 1.08 points. On the Flickr-8K dataset, DSD improved the NeuralTalk BLEU score by 2.0 points. DSD training flow produces the same model architecture and doesn\u2019t incur any inference overhead."
            },
            "slug": "DSD:-Regularizing-Deep-Neural-Networks-with-Flow-Han-Pool",
            "title": {
                "fragments": [],
                "text": "DSD: Regularizing Deep Neural Networks with Dense-Sparse-Dense Training Flow"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experiments show that DSD training can improve the performance of a wide range of CNN, RNN and LSTMs on the tasks of image classification, caption generation and speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304536"
                        ],
                        "name": "Jifeng Dai",
                        "slug": "Jifeng-Dai",
                        "structuredName": {
                            "firstName": "Jifeng",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jifeng Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2153682629"
                        ],
                        "name": "Yi Li",
                        "slug": "Yi-Li",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "We do not compare performance with other architectures such as Faster-RCNN [35] and RFCN [36] since our focus is on mobile/real-time models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7428689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b724c3f7ff395235b62537203ddeb710f0eb27bb",
            "isKey": false,
            "numCitedBy": 3931,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "We present region-based, fully convolutional networks for accurate and efficient object detection. In contrast to previous region-based detectors such as Fast/Faster R-CNN that apply a costly per-region subnetwork hundreds of times, our region-based detector is fully convolutional with almost all computation shared on the entire image. To achieve this goal, we propose position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection. Our method can thus naturally adopt fully convolutional image classifier backbones, such as the latest Residual Networks (ResNets), for object detection. We show competitive results on the PASCAL VOC datasets (e.g., 83.6% mAP on the 2007 set) with the 101-layer ResNet. Meanwhile, our result is achieved at a test-time speed of 170ms per image, 2.5-20 times faster than the Faster R-CNN counterpart. Code is made publicly available at: https://github.com/daijifeng001/r-fcn."
            },
            "slug": "R-FCN:-Object-Detection-via-Region-based-Fully-Dai-Li",
            "title": {
                "fragments": [],
                "text": "R-FCN: Object Detection via Region-based Fully Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This work presents region-based, fully convolutional networks for accurate and efficient object detection, and proposes position-sensitive score maps to address a dilemma between translation-invariance in image classification and translation-variance in object detection."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799898"
                        ],
                        "name": "Andreas Veit",
                        "slug": "Andreas-Veit",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Veit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Veit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3035230"
                        ],
                        "name": "Michael J. Wilber",
                        "slug": "Michael-J.-Wilber",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wilber",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Wilber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "Similar experimental results were reported in [29]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18359848,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec6bb39f18f20dbb504790a5f7089e40de33b169",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we introduce a novel interpretation of residual networks showing they are exponential ensembles. This observation is supported by a large-scale lesion study that demonstrates they behave just like ensembles at test time. Subsequently, we perform an analysis showing these ensembles mostly consist of networks that are each relatively shallow. For example, contrary to our expectations, most of the gradient in a residual network with 110 layers comes from an ensemble of very short networks, i.e., only 10-34 layers deep. This suggests that in addition to describing neural networks in terms of width and depth, there is a third dimension: multiplicity, the size of the implicit ensemble. Ultimately, residual networks do not resolve the vanishing gradient problem by preserving gradient flow throughout the entire depth of the network - rather, they avoid the problem simply by ensembling many short networks together. This insight reveals that depth is still an open research question and invites the exploration of the related notion of multiplicity."
            },
            "slug": "Residual-Networks-are-Exponential-Ensembles-of-Veit-Wilber",
            "title": {
                "fragments": [],
                "text": "Residual Networks are Exponential Ensembles of Relatively Shallow Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This work introduces a novel interpretation of residual networks showing they are exponential ensembles, and suggests that in addition to describing neural networks in terms of width and depth, there is a third dimension: multiplicity, the size of the implicit ensemble."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122113652"
                        ],
                        "name": "Alexander A. Alemi",
                        "slug": "Alexander-A.-Alemi",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Alemi",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander A. Alemi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 67
                            }
                        ],
                        "text": "The importance of residual connection has been studied extensively [1, 28, 45]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1023605,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5c26ab8767d046cb6e32d959fdf726aee89bb62",
            "isKey": false,
            "numCitedBy": 8225,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "\n \n Very deep convolutional networks have been central to the largest advances in image recognition performance in recent years. One example is the Inception architecture that has been shown to achieve very good performance at relatively low computational cost. Recently, the introduction of residual connections in conjunction with a more traditional architecture has yielded state-of-the-art performance in the 2015 ILSVRC challenge; its performance was similar to the latest generation Inception-v3 network. This raises the question: Are there any benefits to combining Inception architectures with residual connections? Here we give clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly. There is also some evidence of residual Inception networks outperforming similarly expensive Inception networks without residual connections by a thin margin. We also present several new streamlined architectures for both residual and non-residual Inception networks. These variations improve the single-frame recognition performance on the ILSVRC 2012 classification task significantly. We further demonstrate how proper activation scaling stabilizes the training of very wide residual Inception networks. With an ensemble of three residual and one Inception-v4 networks, we achieve 3.08% top-5 error on the test set of the ImageNet classification (CLS) challenge.\n \n"
            },
            "slug": "Inception-v4,-Inception-ResNet-and-the-Impact-of-on-Szegedy-Ioffe",
            "title": {
                "fragments": [],
                "text": "Inception-v4, Inception-ResNet and the Impact of Residual Connections on Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Clear empirical evidence that training with residual connections accelerates the training of Inception networks significantly is given and several new streamlined architectures for both residual and non-residual Inception Networks are presented."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057642721"
                        ],
                        "name": "Mart\u00edn Abadi",
                        "slug": "Mart\u00edn-Abadi",
                        "structuredName": {
                            "firstName": "Mart\u00edn",
                            "lastName": "Abadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mart\u00edn Abadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078528337"
                        ],
                        "name": "Ashish Agarwal",
                        "slug": "Ashish-Agarwal",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144758007"
                        ],
                        "name": "P. Barham",
                        "slug": "P.-Barham",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Barham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Barham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445241"
                        ],
                        "name": "E. Brevdo",
                        "slug": "E.-Brevdo",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Brevdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brevdo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545358"
                        ],
                        "name": "Z. Chen",
                        "slug": "Z.-Chen",
                        "structuredName": {
                            "firstName": "Z.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48738717"
                        ],
                        "name": "C. Citro",
                        "slug": "C.-Citro",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Citro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Citro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36347083"
                        ],
                        "name": "Andy Davis",
                        "slug": "Andy-Davis",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andy Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145139947"
                        ],
                        "name": "Matthieu Devin",
                        "slug": "Matthieu-Devin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Devin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Devin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780892"
                        ],
                        "name": "S. Ghemawat",
                        "slug": "S.-Ghemawat",
                        "structuredName": {
                            "firstName": "Sanjay",
                            "lastName": "Ghemawat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ghemawat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064102917"
                        ],
                        "name": "A. Harp",
                        "slug": "A.-Harp",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Harp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Harp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060655766"
                        ],
                        "name": "Geoffrey Irving",
                        "slug": "Geoffrey-Irving",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Irving",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey Irving"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944541"
                        ],
                        "name": "R. J\u00f3zefowicz",
                        "slug": "R.-J\u00f3zefowicz",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "J\u00f3zefowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J\u00f3zefowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1942300"
                        ],
                        "name": "M. Kudlur",
                        "slug": "M.-Kudlur",
                        "structuredName": {
                            "firstName": "Manjunath",
                            "lastName": "Kudlur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kudlur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3369421"
                        ],
                        "name": "J. Levenberg",
                        "slug": "J.-Levenberg",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "Levenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Levenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30415265"
                        ],
                        "name": "Dandelion Man\u00e9",
                        "slug": "Dandelion-Man\u00e9",
                        "structuredName": {
                            "firstName": "Dandelion",
                            "lastName": "Man\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dandelion Man\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089272"
                        ],
                        "name": "R. Monga",
                        "slug": "R.-Monga",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Monga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Monga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144375552"
                        ],
                        "name": "Sherry Moore",
                        "slug": "Sherry-Moore",
                        "structuredName": {
                            "firstName": "Sherry",
                            "lastName": "Moore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherry Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20154699"
                        ],
                        "name": "D. Murray",
                        "slug": "D.-Murray",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Murray",
                            "middleNames": [
                                "Gordon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37232298"
                        ],
                        "name": "C. Olah",
                        "slug": "C.-Olah",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Olah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Olah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32163737"
                        ],
                        "name": "Benoit Steiner",
                        "slug": "Benoit-Steiner",
                        "structuredName": {
                            "firstName": "Benoit",
                            "lastName": "Steiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benoit Steiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35210462"
                        ],
                        "name": "Kunal Talwar",
                        "slug": "Kunal-Talwar",
                        "structuredName": {
                            "firstName": "Kunal",
                            "lastName": "Talwar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kunal Talwar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080690"
                        ],
                        "name": "P. Tucker",
                        "slug": "P.-Tucker",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Tucker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053781980"
                        ],
                        "name": "Vijay Vasudevan",
                        "slug": "Vijay-Vasudevan",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Vasudevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vijay Vasudevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765169"
                        ],
                        "name": "F. Vi\u00e9gas",
                        "slug": "F.-Vi\u00e9gas",
                        "structuredName": {
                            "firstName": "Fernanda",
                            "lastName": "Vi\u00e9gas",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Vi\u00e9gas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47941411"
                        ],
                        "name": "P. Warden",
                        "slug": "P.-Warden",
                        "structuredName": {
                            "firstName": "Pete",
                            "lastName": "Warden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Warden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145233583"
                        ],
                        "name": "M. Wattenberg",
                        "slug": "M.-Wattenberg",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wattenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wattenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35078078"
                        ],
                        "name": "M. Wicke",
                        "slug": "M.-Wicke",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wicke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117163698"
                        ],
                        "name": "Yuan Yu",
                        "slug": "Yuan-Yu",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152198093"
                        ],
                        "name": "Xiaoqiang Zheng",
                        "slug": "Xiaoqiang-Zheng",
                        "structuredName": {
                            "firstName": "Xiaoqiang",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoqiang Zheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Training setup We train our models using TensorFlow[30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 65
                            }
                        ],
                        "text": "Both MobileNet models are trained and evaluated with Open Source TensorFlow Object Detection API [37]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "A standard efficient implementation of inference that uses for instance TensorFlow[30] or Caffe [31], builds a directed acyclic compute hypergraph G, consisting of edges representing the operations and nodes representing tensors of intermediate computation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5707386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d",
            "isKey": false,
            "numCitedBy": 9255,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
            },
            "slug": "TensorFlow:-Large-Scale-Machine-Learning-on-Systems-Abadi-Agarwal",
            "title": {
                "fragments": [],
                "text": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The TensorFlow interface and an implementation of that interface that is built at Google are described, which has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776496"
                        ],
                        "name": "G. Papandreou",
                        "slug": "G.-Papandreou",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Papandreou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Papandreou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010660"
                        ],
                        "name": "Iasonas Kokkinos",
                        "slug": "Iasonas-Kokkinos",
                        "structuredName": {
                            "firstName": "Iasonas",
                            "lastName": "Kokkinos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Iasonas Kokkinos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3001138"
                        ],
                        "name": "Pierre-Andr\u00e9 Savalle",
                        "slug": "Pierre-Andr\u00e9-Savalle",
                        "structuredName": {
                            "firstName": "Pierre-Andr\u00e9",
                            "lastName": "Savalle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre-Andr\u00e9 Savalle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 36
                            }
                        ],
                        "text": "DeepLabv3 adopts atrous convolution [39, 40, 41], a powerful tool to explicitly control the reso-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 377167,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91768b21a2f8a65f1dede87adb0cec2b621ce1f9",
            "isKey": false,
            "numCitedBy": 162,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Convolutional Neural Networks (DCNNs) achieve invariance to domain transformations (deformations) by using multiple `max-pooling' (MP) layers. In this work we show that alternative methods of modeling deformations can improve the accuracy and efficiency of DCNNs. First, we introduce epitomic convolution as an alternative to the common convolution-MP cascade of DCNNs, that comes with the same computational cost but favorable learning properties. Second, we introduce a Multiple Instance Learning algorithm to accommodate global translation and scaling in image classification, yielding an efficient algorithm that trains and tests a DCNN in a consistent manner. Third we develop a DCNN sliding window detector that explicitly, but efficiently, searches over the object's position, scale, and aspect ratio. We provide competitive image classification and localization results on the ImageNet dataset and object detection results on Pascal VOC2007."
            },
            "slug": "Modeling-local-and-global-deformations-in-Deep-and-Papandreou-Kokkinos",
            "title": {
                "fragments": [],
                "text": "Modeling local and global deformations in Deep Learning: Epitomic convolution, Multiple Instance Learning, and sliding window detection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces epitomic convolution as an alternative to the common convolution-MP cascade of DCNNs, that comes with the same computational cost but favorable learning properties, and introduces a Multiple Instance Learning algorithm to accommodate global translation and scaling in image classification."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144108062"
                        ],
                        "name": "Jasper Snoek",
                        "slug": "Jasper-Snoek",
                        "structuredName": {
                            "firstName": "Jasper",
                            "lastName": "Snoek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jasper Snoek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757324"
                        ],
                        "name": "Oren Rippel",
                        "slug": "Oren-Rippel",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Rippel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Rippel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754860"
                        ],
                        "name": "Kevin Swersky",
                        "slug": "Kevin-Swersky",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Swersky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Swersky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450996"
                        ],
                        "name": "Ryan Kiros",
                        "slug": "Ryan-Kiros",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Kiros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Kiros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143758120"
                        ],
                        "name": "N. Satish",
                        "slug": "N.-Satish",
                        "structuredName": {
                            "firstName": "Nadathur",
                            "lastName": "Satish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Satish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789372"
                        ],
                        "name": "N. Sundaram",
                        "slug": "N.-Sundaram",
                        "structuredName": {
                            "firstName": "Narayanan",
                            "lastName": "Sundaram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sundaram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8176660"
                        ],
                        "name": "Md. Mostofa Ali Patwary",
                        "slug": "Md.-Mostofa-Ali-Patwary",
                        "structuredName": {
                            "firstName": "Md.",
                            "lastName": "Patwary",
                            "middleNames": [
                                "Mostofa",
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Md. Mostofa Ali Patwary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764912"
                        ],
                        "name": "Prabhat",
                        "slug": "Prabhat",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Prabhat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prabhat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722180"
                        ],
                        "name": "Ryan P. Adams",
                        "slug": "Ryan-P.-Adams",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Adams",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan P. Adams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 119
                            }
                        ],
                        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [8, 9, 10] as well as various methods of network pruning [11, 12, 13, 14, 15, 16] and connectivity learning [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12604141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93bc65d2842b8cc5f3cf72ebc5b8f75daeacea35",
            "isKey": false,
            "numCitedBy": 711,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian optimization is an effective methodology for the global optimization of functions with expensive evaluations. It relies on querying a distribution over functions defined by a relatively cheap surrogate model. An accurate model for this distribution over functions is critical to the effectiveness of the approach, and is typically fit using Gaussian processes (GPs). However, since GPs scale cubically with the number of observations, it has been challenging to handle objectives whose optimization requires many evaluations, and as such, massively parallelizing the optimization. \n \nIn this work, we explore the use of neural networks as an alternative to GPs to model distributions over functions. We show that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically. This allows us to achieve a previously intractable degree of parallelism, which we apply to large scale hyperparameter optimization, rapidly finding competitive models on benchmark object recognition tasks using convolutional networks, and image caption generation using neural language models."
            },
            "slug": "Scalable-Bayesian-Optimization-Using-Deep-Neural-Snoek-Rippel",
            "title": {
                "fragments": [],
                "text": "Scalable Bayesian Optimization Using Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that performing adaptive basis function regression with a neural network as the parametric form performs competitively with state-of-the-art GP-based approaches, but scales linearly with the number of data rather than cubically, which allows for a previously intractable degree of parallelism."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157222093"
                        ],
                        "name": "Wei Liu",
                        "slug": "Wei-Liu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39863668"
                        ],
                        "name": "Andrew Rabinovich",
                        "slug": "Andrew-Rabinovich",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rabinovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 250
                            }
                        ],
                        "text": "lution of computed feature maps, and builds five parallel heads including (a) Atrous Spatial Pyramid Pooling module (ASPP) [42] containing three 3 \u00d7 3 convolutions with different atrous rates, (b) 1 \u00d7 1 convolution head, and (c) Image-level features [43]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10875471,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2173f1b8172c9b5bed4650c81b69988717bd6df6",
            "isKey": false,
            "numCitedBy": 903,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a technique for adding global context to deep convolutional networks for semantic segmentation. The approach is simple, using the average feature for a layer to augment the features at each location. In addition, we study several idiosyncrasies of training, significantly increasing the performance of baseline networks (e.g. from FCN). When we add our proposed global feature, and a technique for learning normalization parameters, accuracy increases consistently even over our improved versions of the baselines. Our proposed approach, ParseNet, achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines, and near current state-of-the-art performance on PASCAL VOC 2012 semantic segmentation with a simple approach. Code is available at this https URL ."
            },
            "slug": "ParseNet:-Looking-Wider-to-See-Better-Liu-Rabinovich",
            "title": {
                "fragments": [],
                "text": "ParseNet: Looking Wider to See Better"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "This work presents a technique for adding global context to deep convolutional networks for semantic segmentation, and achieves state-of-the-art performance on SiftFlow and PASCAL-Context with small additional computational cost over baselines."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143840275"
                        ],
                        "name": "Song Han",
                        "slug": "Song-Han",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47325862"
                        ],
                        "name": "Jeff Pool",
                        "slug": "Jeff-Pool",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Pool",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Pool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066786849"
                        ],
                        "name": "J. Tran",
                        "slug": "J.-Tran",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80724002"
                        ],
                        "name": "W. Dally",
                        "slug": "W.-Dally",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dally",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dally"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 176
                            }
                        ],
                        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [8, 9, 10] as well as various methods of network pruning [11, 12, 13, 14, 15, 16] and connectivity learning [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2238772,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ff9a37d766e3a4f39757f5e1b235a42dacf18ff",
            "isKey": false,
            "numCitedBy": 4076,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems. Also, conventional networks fix the architecture before training starts; as a result, training cannot improve the architecture. To address these limitations, we describe a method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections. Our method prunes redundant connections using a three-step method. First, we train the network to learn which connections are important. Next, we prune the unimportant connections. Finally, we retrain the network to fine tune the weights of the remaining connections. On the ImageNet dataset, our method reduced the number of parameters of AlexNet by a factor of 9x, from 61 million to 6.7 million, without incurring accuracy loss. Similar experiments with VGG-16 found that the number of parameters can be reduced by 13x, from 138 million to 10.3 million, again with no loss of accuracy."
            },
            "slug": "Learning-both-Weights-and-Connections-for-Efficient-Han-Pool",
            "title": {
                "fragments": [],
                "text": "Learning both Weights and Connections for Efficient Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method to reduce the storage and computation required by neural networks by an order of magnitude without affecting their accuracy by learning only the important connections, and prunes redundant connections using a three-step method."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032184078"
                        ],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "We do not compare performance with other architectures such as Faster-RCNN [35] and RFCN [36] since our focus is on mobile/real-time models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10328909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "424561d8585ff8ebce7d5d07de8dbf7aae5e7270",
            "isKey": false,
            "numCitedBy": 33308,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art object detection networks depend on region proposal algorithms to hypothesize object locations. Advances like SPPnet and Fast R-CNN have reduced the running time of these detection networks, exposing region proposal computation as a bottleneck. In this work, we introduce a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals. An RPN is a fully convolutional network that simultaneously predicts object bounds and objectness scores at each position. The RPN is trained end-to-end to generate high-quality region proposals, which are used by Fast R-CNN for detection. We further merge RPN and Fast R-CNN into a single network by sharing their convolutional features---using the recently popular terminology of neural networks with 'attention' mechanisms, the RPN component tells the unified network where to look. For the very deep VGG-16 model, our detection system has a frame rate of 5fps (including all steps) on a GPU, while achieving state-of-the-art object detection accuracy on PASCAL VOC 2007, 2012, and MS COCO datasets with only 300 proposals per image. In ILSVRC and COCO 2015 competitions, Faster R-CNN and RPN are the foundations of the 1st-place winning entries in several tracks. Code has been made publicly available"
            },
            "slug": "Faster-R-CNN:-Towards-Real-Time-Object-Detection-Ren-He",
            "title": {
                "fragments": [],
                "text": "Faster R-CNN: Towards Real-Time Object Detection with Region Proposal Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work introduces a Region Proposal Network (RPN) that shares full-image convolutional features with the detection network, thus enabling nearly cost-free region proposals and further merge RPN and Fast R-CNN into a single network by sharing their convolutionAL features."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "We also compare to YOLOv2 [34] and original SSD (with VGG-16 [6] as base network) as baselines."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "VGGNet [6], GoogLeNet [7], and ResNet [1]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14124313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42cf88027de515750f230b23b1a057dc782108",
            "isKey": false,
            "numCitedBy": 63195,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            "slug": "Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056091"
                        ],
                        "name": "M. Everingham",
                        "slug": "M.-Everingham",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Everingham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Everingham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143648071"
                        ],
                        "name": "S. Eslami",
                        "slug": "S.-Eslami",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Eslami",
                            "middleNames": [
                                "M.",
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eslami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33652486"
                        ],
                        "name": "J. Winn",
                        "slug": "J.-Winn",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Winn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Winn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 109
                            }
                        ],
                        "text": "We measure our performance on ImageNet [2] classification, COCO object detection [3], VOC image segmentation [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "We conduct the experiments on the PASCAL VOC 2012 dataset [4], with extra annotated images from [44] and evaluation metric mIOU."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207252270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "616b246e332573af1f4859aa91440280774c183a",
            "isKey": false,
            "numCitedBy": 3823,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes (VOC) challenge consists of two components: (i)\u00a0a publicly available dataset of images together with ground truth annotation and standardised evaluation software; and (ii)\u00a0an annual competition and workshop. There are five challenges: classification, detection, segmentation, action classification, and person layout. In this paper we provide a review of the challenge from 2008\u20132012. The paper is intended for two audiences: algorithm designers, researchers who want to see what the state of the art is, as measured by performance on the VOC datasets, along with the limitations and weak points of the current generation of algorithms; and, challenge designers, who want to see what we as organisers have learnt from the process and our recommendations for the organisation of future challenges. To analyse the performance of submitted algorithms on the VOC datasets we introduce a number of novel evaluation methods: a bootstrapping method for determining whether differences in the performance of two algorithms are significant or not; a normalised average precision so that performance can be compared across classes with different proportions of positive instances; a clustering method for visualising the performance across multiple algorithms so that the hard and easy images can be identified; and the use of a joint classifier over the submitted algorithms in order to measure their complementarity and combined performance. We also analyse the community\u2019s progress through time using the methods of Hoiem et al. (Proceedings of European Conference on Computer Vision, 2012) to identify the types of occurring errors. We conclude the paper with an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges."
            },
            "slug": "The-Pascal-Visual-Object-Classes-Challenge:-A-Everingham-Eslami",
            "title": {
                "fragments": [],
                "text": "The Pascal Visual Object Classes Challenge: A Retrospective"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A review of the Pascal Visual Object Classes challenge from 2008-2012 and an appraisal of the aspects of the challenge that worked well, and those that could be improved in future challenges."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33493200"
                        ],
                        "name": "Tsung-Yi Lin",
                        "slug": "Tsung-Yi-Lin",
                        "structuredName": {
                            "firstName": "Tsung-Yi",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsung-Yi Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145854440"
                        ],
                        "name": "M. Maire",
                        "slug": "M.-Maire",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Maire",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Maire"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48966748"
                        ],
                        "name": "James Hays",
                        "slug": "James-Hays",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hays",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Hays"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690922"
                        ],
                        "name": "P. Perona",
                        "slug": "P.-Perona",
                        "structuredName": {
                            "firstName": "Pietro",
                            "lastName": "Perona",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Perona"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770537"
                        ],
                        "name": "D. Ramanan",
                        "slug": "D.-Ramanan",
                        "structuredName": {
                            "firstName": "Deva",
                            "lastName": "Ramanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ramanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127283"
                        ],
                        "name": "Piotr Doll\u00e1r",
                        "slug": "Piotr-Doll\u00e1r",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Doll\u00e1r",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piotr Doll\u00e1r"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699161"
                        ],
                        "name": "C. L. Zitnick",
                        "slug": "C.-L.-Zitnick",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Zitnick",
                            "middleNames": [
                                "Lawrence"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Zitnick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 198
                            }
                        ],
                        "text": "We evaluate and compare the performance of MobileNetV2 and MobileNetV1 as feature extractors [32] for object detection with a modified version of the Single Shot Detector (SSD) [33] on COCO dataset [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "We measure our performance on ImageNet [2] classification, COCO object detection [3], VOC image segmentation [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14113767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71b7178df5d2b112d07e45038cb5637208659ff7",
            "isKey": false,
            "numCitedBy": 20273,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding. This is achieved by gathering images of complex everyday scenes containing common objects in their natural context. Objects are labeled using per-instance segmentations to aid in precise object localization. Our dataset contains photos of 91 objects types that would be easily recognizable by a 4 year old. With a total of 2.5 million labeled instances in 328k images, the creation of our dataset drew upon extensive crowd worker involvement via novel user interfaces for category detection, instance spotting and instance segmentation. We present a detailed statistical analysis of the dataset in comparison to PASCAL, ImageNet, and SUN. Finally, we provide baseline performance analysis for bounding box and segmentation detection results using a Deformable Parts Model."
            },
            "slug": "Microsoft-COCO:-Common-Objects-in-Context-Lin-Maire",
            "title": {
                "fragments": [],
                "text": "Microsoft COCO: Common Objects in Context"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A new dataset with the goal of advancing the state-of-the-art in object recognition by placing the question of object recognition in the context of the broader question of scene understanding by gathering images of complex everyday scenes containing common objects in their natural context."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2192178"
                        ],
                        "name": "Olga Russakovsky",
                        "slug": "Olga-Russakovsky",
                        "structuredName": {
                            "firstName": "Olga",
                            "lastName": "Russakovsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Olga Russakovsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153302678"
                        ],
                        "name": "Jia Deng",
                        "slug": "Jia-Deng",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144914140"
                        ],
                        "name": "Hao Su",
                        "slug": "Hao-Su",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Su"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2285165"
                        ],
                        "name": "J. Krause",
                        "slug": "J.-Krause",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Krause",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145031342"
                        ],
                        "name": "S. Satheesh",
                        "slug": "S.-Satheesh",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Satheesh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Satheesh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145423516"
                        ],
                        "name": "S. Ma",
                        "slug": "S.-Ma",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3109481"
                        ],
                        "name": "Zhiheng Huang",
                        "slug": "Zhiheng-Huang",
                        "structuredName": {
                            "firstName": "Zhiheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2556428"
                        ],
                        "name": "A. Khosla",
                        "slug": "A.-Khosla",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Khosla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Khosla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879842"
                        ],
                        "name": "Michael S. Bernstein",
                        "slug": "Michael-S.-Bernstein",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bernstein",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael S. Bernstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "We measure our performance on ImageNet [2] classification, COCO object detection [3], VOC image segmentation [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 8
                            }
                        ],
                        "text": "For the ImageNet dataset, our architecture works for models ranging from 10 Million Mult-Adds to 580 Million Mult-Adds improving on the state of the art."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2930547,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e74f9b7f8eec6ba4704c206b93bc8079af3da4bd",
            "isKey": false,
            "numCitedBy": 25826,
            "numCiting": 138,
            "paperAbstract": {
                "fragments": [],
                "text": "The ImageNet Large Scale Visual Recognition Challenge is a benchmark in object category classification and detection on hundreds of object categories and millions of images. The challenge has been run annually from 2010 to present, attracting participation from more than fifty institutions. This paper describes the creation of this benchmark dataset and the advances in object recognition that have been possible as a result. We discuss the challenges of collecting large-scale ground truth annotation, highlight key breakthroughs in categorical object recognition, provide a detailed analysis of the current state of the field of large-scale image classification and object detection, and compare the state-of-the-art computer vision accuracy with human accuracy. We conclude with lessons learned in the 5\u00a0years of the challenge, and propose future directions and improvements."
            },
            "slug": "ImageNet-Large-Scale-Visual-Recognition-Challenge-Russakovsky-Deng",
            "title": {
                "fragments": [],
                "text": "ImageNet Large Scale Visual Recognition Challenge"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The creation of this benchmark dataset and the advances in object recognition that have been possible as a result are described, and the state-of-the-art computer vision accuracy with human accuracy is compared."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 176
                            }
                        ],
                        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [8, 9, 10] as well as various methods of network pruning [11, 12, 13, 14, 15, 16] and connectivity learning [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7785881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "isKey": false,
            "numCitedBy": 3518,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application."
            },
            "slug": "Optimal-Brain-Damage-LeCun-Denker",
            "title": {
                "fragments": [],
                "text": "Optimal Brain Damage"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A class of practical and nearly optimal schemes for adapting the size of a neural network by using second-derivative information to make a tradeoff between network complexity and training set error is derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736279"
                        ],
                        "name": "B. Hassibi",
                        "slug": "B.-Hassibi",
                        "structuredName": {
                            "firstName": "Babak",
                            "lastName": "Hassibi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hassibi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 176
                            }
                        ],
                        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [8, 9, 10] as well as various methods of network pruning [11, 12, 13, 14, 15, 16] and connectivity learning [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7057040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a42954d4b9d0ccdf1036e0af46d87a01b94c3516",
            "isKey": false,
            "numCitedBy": 1594,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of information from all second order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Solla, 1990], which often remove the wrong weights. OBS permits the pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-1 from training data and structural information of the net. OBS permits a 90%, a 76%, and a 62% reduction in weights over backpropagation with weight decay on three benchmark MONK's problems [Thrun et al., 1991]. Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg [1987] used 18,000 weights in their NETtalk network, we used OBS to prune a network to just 1560 weights, yielding better generalization."
            },
            "slug": "Second-Order-Derivatives-for-Network-Pruning:-Brain-Hassibi-Stork",
            "title": {
                "fragments": [],
                "text": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case, and thus yields better generalization on test data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1778133"
                        ],
                        "name": "Pablo Arbel\u00e1ez",
                        "slug": "Pablo-Arbel\u00e1ez",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Arbel\u00e1ez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pablo Arbel\u00e1ez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769383"
                        ],
                        "name": "Lubomir D. Bourdev",
                        "slug": "Lubomir-D.-Bourdev",
                        "structuredName": {
                            "firstName": "Lubomir",
                            "lastName": "Bourdev",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lubomir D. Bourdev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35208858"
                        ],
                        "name": "Subhransu Maji",
                        "slug": "Subhransu-Maji",
                        "structuredName": {
                            "firstName": "Subhransu",
                            "lastName": "Maji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subhransu Maji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143751119"
                        ],
                        "name": "Jitendra Malik",
                        "slug": "Jitendra-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jitendra Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "We conduct the experiments on the PASCAL VOC 2012 dataset [4], with extra annotated images from [44] and evaluation metric mIOU."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6683607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82fae97673a353271b1d4c001afda1af6ef6dc23",
            "isKey": false,
            "numCitedBy": 1078,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the challenging problem of localizing and classifying category-specific object contours in real world images. For this purpose, we present a simple yet effective method for combining generic object detectors with bottom-up contours to identify object contours. We also provide a principled way of combining information from different part detectors and across categories. In order to study the problem and evaluate quantitatively our approach, we present a dataset of semantic exterior boundaries on more than 20, 000 object instances belonging to 20 categories, using the images from the VOC2011 PASCAL challenge [7]."
            },
            "slug": "Semantic-contours-from-inverse-detectors-Hariharan-Arbel\u00e1ez",
            "title": {
                "fragments": [],
                "text": "Semantic contours from inverse detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple yet effective method for combining generic object detectors with bottom-up contours to identify object contours is presented and a principled way of combining information from different part detectors and across categories is provided."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Computer Vision"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32837403"
                        ],
                        "name": "J. Bergstra",
                        "slug": "J.-Bergstra",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 119
                            }
                        ],
                        "text": "Recently there has been lots of progress in algorithmic architecture exploration included hyper-parameter optimization [8, 9, 10] as well as various methods of network pruning [11, 12, 13, 14, 15, 16] and connectivity learning [17, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15700257,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "188e247506ad992b8bc62d6c74789e89891a984f",
            "isKey": false,
            "numCitedBy": 5726,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Grid search and manual search are the most widely used strategies for hyper-parameter optimization. This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid. Empirical evidence comes from a comparison with a large previous study that used grid search and manual search to configure neural networks and deep belief networks. Compared with neural networks configured by a pure grid search, we find that random search over the same domain is able to find models that are as good or better within a small fraction of the computation time. Granting random search the same computational budget, random search finds better models by effectively searching a larger, less promising configuration space. Compared with deep belief networks configured by a thoughtful combination of manual search and grid search, purely random search over the same 32-dimensional configuration space found statistically equal performance on four of seven data sets, and superior performance on one of seven. A Gaussian process analysis of the function from hyper-parameters to validation set performance reveals that for most data sets only a few of the hyper-parameters really matter, but that different hyper-parameters are important on different data sets. This phenomenon makes grid search a poor choice for configuring algorithms for new data sets. Our analysis casts some light on why recent \"High Throughput\" methods achieve surprising success--they appear to search through a large number of hyper-parameters because most hyper-parameters do not matter much. We anticipate that growing interest in large hierarchical models will place an increasing burden on techniques for hyper-parameter optimization; this work shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper-parameter optimization algorithms."
            },
            "slug": "Random-Search-for-Hyper-Parameter-Optimization-Bergstra-Bengio",
            "title": {
                "fragments": [],
                "text": "Random Search for Hyper-Parameter Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This paper shows empirically and theoretically that randomly chosen trials are more efficient for hyper-parameter optimization than trials on a grid, and shows that random search is a natural baseline against which to judge progress in the development of adaptive (sequential) hyper- parameter optimization algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157222093"
                        ],
                        "name": "Wei Liu",
                        "slug": "Wei-Liu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39863668"
                        ],
                        "name": "Andrew Rabinovich",
                        "slug": "Andrew-Rabinovich",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rabinovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 30
                            }
                        ],
                        "text": "C V\n] 1\n6 Ja\nn 20\nVGGNet [6], GoogLeNet [7], and ResNet [1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "VGGNet [6], GoogLeNet [7], and ResNet [1]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206592484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "isKey": false,
            "numCitedBy": 29917,
            "numCiting": 278,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
            },
            "slug": "Going-deeper-with-convolutions-Szegedy-Liu",
            "title": {
                "fragments": [],
                "text": "Going deeper with convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A deep convolutional neural network architecture codenamed Inception is proposed that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049736"
                        ],
                        "name": "Sergey Karayev",
                        "slug": "Sergey-Karayev",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Karayev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Karayev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "A standard efficient implementation of inference that uses for instance TensorFlow[30] or Caffe [31], builds a directed acyclic compute hypergraph G, consisting of edges representing the operations and nodes representing tensors of intermediate computation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1799558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "isKey": false,
            "numCitedBy": 13814,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia."
            },
            "slug": "Caffe:-Convolutional-Architecture-for-Fast-Feature-Jia-Shelhamer",
            "title": {
                "fragments": [],
                "text": "Caffe: Convolutional Architecture for Fast Feature Embedding"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784667"
                        ],
                        "name": "Guido Mont\u00fafar",
                        "slug": "Guido-Mont\u00fafar",
                        "structuredName": {
                            "firstName": "Guido",
                            "lastName": "Mont\u00fafar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guido Mont\u00fafar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996134"
                        ],
                        "name": "Razvan Pascanu",
                        "slug": "Razvan-Pascanu",
                        "structuredName": {
                            "firstName": "Razvan",
                            "lastName": "Pascanu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Razvan Pascanu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5941770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b034b5769ab94acf9fb8ae48c7edb560a300bb63",
            "isKey": false,
            "numCitedBy": 927,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have. Deep networks are able to sequentially map portions of each layer's input-space to the same output. In this way, deep models compute functions that react equally to complicated patterns of different inputs. The compositional structure of these functions enables them to re-use pieces of computation exponentially often in terms of the network's depth. This paper investigates the complexity of such compositional maps and contributes new theoretical results regarding the advantage of depth for neural networks with piecewise linear activation functions. In particular, our analysis is not specific to a single family of models, and as an example, we employ it for rectifier and maxout networks. We improve complexity bounds from pre-existing work and investigate the behavior of units in higher layers."
            },
            "slug": "On-the-Number-of-Linear-Regions-of-Deep-Neural-Mont\u00fafar-Pascanu",
            "title": {
                "fragments": [],
                "text": "On the Number of Linear Regions of Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The complexity of functions computable by deep feedforward neural networks with piecewise linear activations in terms of the symmetries and the number of linear regions that they have is investigated."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060028"
                        ],
                        "name": "D. Eigen",
                        "slug": "D.-Eigen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eigen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eigen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 36
                            }
                        ],
                        "text": "DeepLabv3 adopts atrous convolution [39, 40, 41], a powerful tool to explicitly control the reso-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4071727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5507caf1210b6bfbd04fe02b2669bc14292e23a1",
            "isKey": false,
            "numCitedBy": 4388,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
            },
            "slug": "OverFeat:-Integrated-Recognition,-Localization-and-Sermanet-Eigen",
            "title": {
                "fragments": [],
                "text": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This integrated framework for using Convolutional Networks for classification, localization and detection is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 and obtained very competitive results for the detection and classifications tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "Both MobileNet models are trained and evaluated with Open Source TensorFlow Object Detection API [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tensorflow object detection api, 2017"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "mantic contours from inverse detectors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 31,
            "result": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 44,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Inverted-Residuals-and-Linear-Bottlenecks:-Mobile-Sandler-Howard/16b42f570873fc03d503090adb0a75a467c5f30c?sort=total-citations"
}