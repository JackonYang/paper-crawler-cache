{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549369"
                        ],
                        "name": "Kyuyeon Hwang",
                        "slug": "Kyuyeon-Hwang",
                        "structuredName": {
                            "firstName": "Kyuyeon",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyuyeon Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66936521"
                        ],
                        "name": "W. Sung",
                        "slug": "W.-Sung",
                        "structuredName": {
                            "firstName": "Wonyong",
                            "lastName": "Sung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Sung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 105
                            }
                        ],
                        "text": "A special part of this network is that we constrain the weights to have values between \u22121 and +1 as well by wrapping them with tanh."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 178
                            }
                        ],
                        "text": "Most of the effort is focused on training networks whose weights can be transformed into some quantized representations with a minimal loss of performance (Fiesler et al., 1990; Hwang & Sung, 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 92
                            }
                        ],
                        "text": "The training procedure is similar to the ones with quantized weights (Fiesler et al., 1990; Hwang & Sung, 2014), except that the values we deal with are all bits, and the operations on them are bitwise."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16104422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4db2d26b5d169de6b64de361dc7d4fd5b1f61a3",
            "isKey": false,
            "numCitedBy": 221,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward deep neural networks that employ multiple hidden layers show high performance in many applications, but they demand complex hardware for implementation. The hardware complexity can be much lowered by minimizing the word-length of weights and signals, but direct quantization for fixed-point network design does not yield good results. We optimize the fixed-point design by employing backpropagation based retraining. The designed fixed-point networks with ternary weights (+1, 0, and -1) and 3-bit signal show only negligible performance loss when compared to the floating-point coun-terparts. The backpropagation for retraining uses quantized weights and fixed-point signal to compute the output, but utilizes high precision values for adapting the networks. A character recognition and a phoneme recognition examples are presented."
            },
            "slug": "Fixed-point-feedforward-deep-neural-network-design-Hwang-Sung",
            "title": {
                "fragments": [],
                "text": "Fixed-point feedforward deep neural network design using weights +1, 0, and \u22121"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The designed fixed-point networks with ternary weights (+1, 0, and -1) and 3-bit signal show only negligible performance loss when compared to the floating-point coun-terparts."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Workshop on Signal Processing Systems (SiPS)"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388466"
                        ],
                        "name": "Matthieu Courbariaux",
                        "slug": "Matthieu-Courbariaux",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Courbariaux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Courbariaux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145719986"
                        ],
                        "name": "J. David",
                        "slug": "J.-David",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. David"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 181
                            }
                        ],
                        "text": "It was also shown that 10 bits and 12 bits are enough to represent gradients and storing weights for implementing the stateof-the-art maxout networks even for training the network (Courbariaux et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16349374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b82d54e9a3b06c603d7987ba3ecf437425f6330",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications."
            },
            "slug": "Training-deep-neural-networks-with-low-precision-Courbariaux-Bengio",
            "title": {
                "fragments": [],
                "text": "Training deep neural networks with low precision multiplications"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is found that very low precision is sufficient not just for running trained networks but also for training them, and it is possible to train Maxout networks with 10 bits multiplications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388466"
                        ],
                        "name": "Matthieu Courbariaux",
                        "slug": "Matthieu-Courbariaux",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Courbariaux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Courbariaux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145719986"
                        ],
                        "name": "J. David",
                        "slug": "J.-David",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. David"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 181
                            }
                        ],
                        "text": "It was also shown that 10 bits and 12 bits are enough to represent gradients and storing weights for implementing the stateof-the-art maxout networks even for training the network (Courbariaux et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26450018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8c35c2c39fdd2ec6af37ddc8c51deb396aefef8",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those arithmetics, we assess the impact of the precision of the computations on the final error of the training. We find that very low precision computation is sufficient not just for running trained networks but also for training them. For example, almost state-of-the-art results were obtained on most datasets with 10 bits for computing activations and gradients, and 12 bits for storing updated parameters."
            },
            "slug": "Low-precision-arithmetic-for-deep-learning-Courbariaux-Bengio",
            "title": {
                "fragments": [],
                "text": "Low precision arithmetic for deep learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that very low precision computation is sufficient not just for running trained networks but also for training them."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 102
                            }
                        ],
                        "text": "From the first round of training, we get a regular dropout network with the same setting suggested in (Srivastava et al., 2014), except the fact that we used the hyperbolic tangent for both weight compression and activation to make Table 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 103
                            }
                        ],
                        "text": "From the first round of training, we get a regular dropout network with the same setting suggested in (Srivastava et al., 2014), except the fact that we used the hyperbolic tangent for both weight compression and activation to make\nthe network suitable for initializing the following bipolar bitwise\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6844431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "isKey": false,
            "numCitedBy": 28464,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
            },
            "slug": "Dropout:-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton",
            "title": {
                "fragments": [],
                "text": "Dropout: a simple way to prevent neural networks from overfitting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3291954"
                        ],
                        "name": "M. Golea",
                        "slug": "M.-Golea",
                        "structuredName": {
                            "firstName": "Mostefa",
                            "lastName": "Golea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Golea"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143858557"
                        ],
                        "name": "M. Marchand",
                        "slug": "M.-Marchand",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Marchand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marchand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3175302"
                        ],
                        "name": "T. Hancock",
                        "slug": "T.-Hancock",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hancock",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Hancock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Although learning those bitwise weights as a Boolean concept is an NPcomplete problem (Pitt & Valiant, 1988), the bitwise networks have been studied in the limited setting, such as \u03bcperceptron networks where an input node is allowed to be connected to one and only one hidden node and its final layer is a union of those hidden nodes (Golea et al., 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 274
                            }
                        ],
                        "text": "\u2026is an NPcomplete problem (Pitt & Valiant, 1988), the bitwise networks have been studied in the limited setting, such as \u00b5perceptron networks where an input node is allowed to be connected to one and only one hidden node and its final layer is a union of those hidden nodes (Golea et al., 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18570109,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9eeef14481d946db39f675bdd3196222071684b5",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks with binary weights are very important from both the theoretical and practical points of view. In this paper, we investigate the learnability of single binary perceptrons and unions of \u00b5-binary-perceptron networks, i.e. an \"OR\" of binary perceptrons where each input unit is connected to one and only one perceptron. We give a polynomial time algorithm that PAC learns these networks under the uniform distribution. The algorithm is able to identify both the network connectivity and the weight values necessary to represent the target function. These results suggest that, under reasonable distributions, \u00b5-perceptron networks may be easier to learn than fully connected networks."
            },
            "slug": "On-Learning-\u00b5-Perceptron-Networks-with-Binary-Golea-Marchand",
            "title": {
                "fragments": [],
                "text": "On Learning \u00b5-Perceptron Networks with Binary Weights"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper gives a polynomial time algorithm that PAC learns these networks under the uniform distribution and suggests that, under reasonable distributions, \u00b5-perceptron networks may be easier to learn than fully connected networks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725303"
                        ],
                        "name": "Y. Teh",
                        "slug": "Y.-Teh",
                        "structuredName": {
                            "firstName": "Yee",
                            "lastName": "Teh",
                            "middleNames": [
                                "Whye"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Teh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 121
                            }
                        ],
                        "text": "cated function, Deep Neural Networks (DNN) achieve the goal by learning a hierarchy of features in their multiple layers (Hinton et al., 2006; Bengio, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 155
                            }
                        ],
                        "text": "Copyright 2015 by the author(s).\ncated function, Deep Neural Networks (DNN) achieve the goal by learning a hierarchy of features in their multiple layers (Hinton et al., 2006; Bengio, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2309950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8978cf7574ceb35f4c3096be768c7547b28a35d0",
            "isKey": false,
            "numCitedBy": 13502,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how to use complementary priors to eliminate the explaining-away effects that make inference difficult in densely connected belief nets that have many hidden layers. Using complementary priors, we derive a fast, greedy algorithm that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory. The fast, greedy algorithm is used to initialize a slower learning procedure that fine-tunes the weights using a contrastive version of the wake-sleep algorithm. After fine-tuning, a network with three hidden layers forms a very good generative model of the joint distribution of handwritten digit images and their labels. This generative model gives better digit classification than the best discriminative learning algorithms. The low-dimensional manifolds on which the digits lie are modeled by long ravines in the free-energy landscape of the top-level associative memory, and it is easy to explore these ravines by using the directed connections to display what the associative memory has in mind."
            },
            "slug": "A-Fast-Learning-Algorithm-for-Deep-Belief-Nets-Hinton-Osindero",
            "title": {
                "fragments": [],
                "text": "A Fast Learning Algorithm for Deep Belief Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A fast, greedy algorithm is derived that can learn deep, directed belief networks one layer at a time, provided the top two layers form an undirected associative memory."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694181"
                        ],
                        "name": "E. Fiesler",
                        "slug": "E.-Fiesler",
                        "structuredName": {
                            "firstName": "Emile",
                            "lastName": "Fiesler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Fiesler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29288011"
                        ],
                        "name": "A. Choudry",
                        "slug": "A.-Choudry",
                        "structuredName": {
                            "firstName": "Amar",
                            "lastName": "Choudry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Choudry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49427181"
                        ],
                        "name": "H. Caulfield",
                        "slug": "H.-Caulfield",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Caulfield",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Caulfield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 156
                            }
                        ],
                        "text": "Most of the effort is focused on training networks whose weights can be transformed into some quantized representations with a minimal loss of performance (Fiesler et al., 1990; Hwang & Sung, 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 70
                            }
                        ],
                        "text": "The training procedure is similar to the ones with quantized weights (Fiesler et al., 1990; Hwang & Sung, 2014), except that the values we deal with are all bits, and the operations on them are bitwise."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62160723,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "843d0f89223e18f20c6d7b1e9e4cc87db1735f0a",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks are a primary candidate architecture for optical computing. One of the major problems in using neural networks for optical computers is that the information holders: the interconnection strengths (or weights) are normally real valued (continuous), whereas optics (light) is only capable of representing a few distinguishable intensity levels (discrete). In this paper a weight discretization paradigm is presented for back(ward error) propagation neural networks which can work with a very limited number of discretization levels. The number of interconnections in a (fully connected) neural network grows quadratically with the number of neurons of the network. Optics can handle a large number of interconnections because of the fact that light beams do not interfere with each other. A vast amount of light beams can therefore be used per unit of area. However the number of different values one can represent in a light beam is very limited. A flexible, portable (machine independent) neural network software package which is capable of weight discretization, is presented. The development of the software and some experiments have been done on personal computers. The major part of the testing, which requires a lot of computation, has been done using a CRAY X-MP/24 super computer."
            },
            "slug": "Weight-discretization-paradigm-for-optical-neural-Fiesler-Choudry",
            "title": {
                "fragments": [],
                "text": "Weight discretization paradigm for optical neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "In this paper a weight discretization paradigm is presented for back(ward error) propagation neural networks which can work with a very limited number of discretized levels."
            },
            "venue": {
                "fragments": [],
                "text": "Other Conferences"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64294544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f42b865e20e61a954239f421b42007236e671f19",
            "isKey": false,
            "numCitedBy": 3561,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique. Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called Graph Transformer Networks (GTN), allows such multi-module systems to be trained globally using Gradient-Based methods so as to minimize an overall performance measure. Two systems for on-line handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of Graph Transformer Networks. A Graph Transformer Network for reading bank check is also described. It uses Convolutional Neural Network character recognizers combined with global training techniques to provides record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day."
            },
            "slug": "GradientBased-Learning-Applied-to-Document-Haykin-Kosko",
            "title": {
                "fragments": [],
                "text": "GradientBased Learning Applied to Document Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Various methods applied to handwritten character recognition are reviewed and compared and Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 117
                            }
                        ],
                        "text": "In this section we go over the details and results of the hand-written digit recognition task on the MNIST data set (LeCun et al., 1998) using the proposed BNN system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35624,
            "numCiting": 246,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 121
                            }
                        ],
                        "text": "cated function, Deep Neural Networks (DNN) achieve the goal by learning a hierarchy of features in their multiple layers (Hinton et al., 2006; Bengio, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 176
                            }
                        ],
                        "text": "Copyright 2015 by the author(s).\ncated function, Deep Neural Networks (DNN) achieve the goal by learning a hierarchy of features in their multiple layers (Hinton et al., 2006; Bengio, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207178999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e60ff004dde5c13ec53087872cfcdd12e85beb57",
            "isKey": false,
            "numCitedBy": 7604,
            "numCiting": 346,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical results strongly suggest that in order to learn the kind of complicated functions that can represent high-level abstractions (e.g. in vision, language, and other AI-level tasks), one needs deep architectures. Deep architectures are composed of multiple levels of non-linear operations, such as in neural nets with many hidden layers or in complicated propositional formulae re-using many sub-formulae. Searching the parameter space of deep architectures is a difficult optimization task, but learning algorithms such as those for Deep Belief Networks have recently been proposed to tackle this problem with notable success, beating the state-of-the-art in certain areas. This paper discusses the motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer models such as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks."
            },
            "slug": "Learning-Deep-Architectures-for-AI-Bengio",
            "title": {
                "fragments": [],
                "text": "Learning Deep Architectures for AI"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The motivations and principles regarding learning algorithms for deep architectures, in particular those exploiting as building blocks unsupervised learning of single-layer modelssuch as Restricted Boltzmann Machines, used to construct deeper models such as Deep Belief Networks are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "Found. Trends Mach. Learn."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1912398"
                        ],
                        "name": "Daniel Soudry",
                        "slug": "Daniel-Soudry",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Soudry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Soudry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2477463"
                        ],
                        "name": "Itay Hubara",
                        "slug": "Itay-Hubara",
                        "structuredName": {
                            "firstName": "Itay",
                            "lastName": "Hubara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Itay Hubara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766683"
                        ],
                        "name": "R. Meir",
                        "slug": "R.-Meir",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Meir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Meir"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 42
                            }
                        ],
                        "text": "A more practical network was proposed in (Soudry et al., 2014) recently, where the posterior probabilities of the binary weights were sought using the Expectation Back Propagation (EBP) scheme, which is similar to backpropagation in its form, but has some advantages, such as parameterfree learning\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 41
                            }
                        ],
                        "text": "A more practical network was proposed in (Soudry et al., 2014) recently, where the posterior probabilities of the binary weights were sought using the Expectation Back Propagation (EBP) scheme, which is similar to backpropagation in its form, but has some advantages, such as parameterfree learning and a straightforward discretization of the weights."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14245558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "feaa7e295c7a43ec52091ed9ade1a9a1e5d9bed2",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer Neural Networks (MNNs) are commonly trained using gradient descent-based methods, such as BackPropagation (BP). Inference in probabilistic graphical models is often done using variational Bayes methods, such as Expectation Propagation (EP). We show how an EP based approach can also be used to train deterministic MNNs. Specifically, we approximate the posterior of the weights given the data using a \"mean-field\" factorized distribution, in an online setting. Using online EP and the central limit theorem we find an analytical approximation to the Bayes update of this posterior, as well as the resulting Bayes estimates of the weights and outputs. \n \nDespite a different origin, the resulting algorithm, Expectation BackPropagation (EBP), is very similar to BP in form and efficiency. However, it has several additional advantages: (1) Training is parameter-free, given initial conditions (prior) and the MNN architecture. This is useful for large-scale problems, where parameter tuning is a major challenge. (2) The weights can be restricted to have discrete values. This is especially useful for implementing trained MNNs in precision limited hardware chips, thus improving their speed and energy efficiency by several orders of magnitude. \n \nWe test the EBP algorithm numerically in eight binary text classification tasks. In all tasks, EBP outperforms: (1) standard BP with the optimal constant learning rate (2) previously reported state of the art. Interestingly, EBP-trained MNNs with binary weights usually perform better than MNNs with continuous (real) weights - if we average the MNN output using the inferred posterior."
            },
            "slug": "Expectation-Backpropagation:-Parameter-Free-of-with-Soudry-Hubara",
            "title": {
                "fragments": [],
                "text": "Expectation Backpropagation: Parameter-Free Training of Multilayer Neural Networks with Continuous or Discrete Weights"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown how an EP based approach can also be used to train deterministic MNNs, and an analytical approximation to the Bayes update of this posterior is found, as well as the resulting Bayes estimates of the weights and outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764952"
                        ],
                        "name": "K. Hornik",
                        "slug": "K.-Hornik",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Hornik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hornik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 183
                            }
                        ],
                        "text": "According to the universal approximation theorem, a single hidden layer with a finite number of units can approximate a continuous function with some mild assumptions (Cybenko, 1989; Hornik, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7343126,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d35f1e533b72370683d8fa2dabff5f0fc16490cc",
            "isKey": false,
            "numCitedBy": 4691,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Approximation-capabilities-of-multilayer-networks-Hornik",
            "title": {
                "fragments": [],
                "text": "Approximation capabilities of multilayer feedforward networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50126200"
                        ],
                        "name": "Yong Xu",
                        "slug": "Yong-Xu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419855"
                        ],
                        "name": "Jun Du",
                        "slug": "Jun-Du",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1860774"
                        ],
                        "name": "Lirong Dai",
                        "slug": "Lirong-Dai",
                        "structuredName": {
                            "firstName": "Lirong",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lirong Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9391905"
                        ],
                        "name": "Chin-Hui Lee",
                        "slug": "Chin-Hui-Lee",
                        "structuredName": {
                            "firstName": "Chin-Hui",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chin-Hui Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 28
                            }
                        ],
                        "text": ", 2012), speech enhancement (Xu et al., 2014), etc, it is also the case that the relatively bigger networks with more parameters than before call for more resources (processing power, memory, battery time, etc), which are sometimes critically constrained in applications running on embedded devices."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 150
                            }
                        ],
                        "text": "\u2026art results for various tasks, such as image classification (Goodfellow et al., 2013), speech recognition (Hinton et al., 2012), speech enhancement (Xu et al., 2014), etc, it is also the case that the relatively bigger networks with more parameters than before call for more resources (processing\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4228271,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2978880d0c3a469a1420411d4b0b30a7b3fe56e9",
            "isKey": false,
            "numCitedBy": 682,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This letter presents a regression-based speech enhancement framework using deep neural networks (DNNs) with a multiple-layer deep architecture. In the DNN learning process, a large training set ensures a powerful modeling capability to estimate the complicated nonlinear mapping from observed noisy speech to desired clean signals. Acoustic context was found to improve the continuity of speech to be separated from the background noises successfully without the annoying musical artifact commonly observed in conventional speech enhancement algorithms. A series of pilot experiments were conducted under multi-condition training with more than 100 hours of simulated speech data, resulting in a good generalization capability even in mismatched testing conditions. When compared with the logarithmic minimum mean square error approach, the proposed DNN-based algorithm tends to achieve significant improvements in terms of various objective quality measures. Furthermore, in a subjective preference evaluation with 10 listeners, 76.35% of the subjects were found to prefer DNN-based enhanced speech to that obtained with other conventional technique."
            },
            "slug": "An-Experimental-Study-on-Speech-Enhancement-Based-Xu-Du",
            "title": {
                "fragments": [],
                "text": "An Experimental Study on Speech Enhancement Based on Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This letter presents a regression-based speech enhancement framework using deep neural networks (DNNs) with a multiple-layer deep architecture that tends to achieve significant improvements in terms of various objective quality measures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Letters"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144718788"
                        ],
                        "name": "L. Deng",
                        "slug": "L.-Deng",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Deng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35188630"
                        ],
                        "name": "George E. Dahl",
                        "slug": "George-E.-Dahl",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Dahl",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George E. Dahl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3111912"
                        ],
                        "name": "Navdeep Jaitly",
                        "slug": "Navdeep-Jaitly",
                        "structuredName": {
                            "firstName": "Navdeep",
                            "lastName": "Jaitly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Navdeep Jaitly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14902530"
                        ],
                        "name": "P. Nguyen",
                        "slug": "P.-Nguyen",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 144
                            }
                        ],
                        "text": "\u2026DNNs are extending the state of the art results for various tasks, such as image classification (Goodfellow et al., 2013), speech recognition (Hinton et al., 2012), speech enhancement (Xu et al., 2014), etc, it is also the case that the relatively bigger networks with more parameters than\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 143
                            }
                        ],
                        "text": "It has long been known that any Boolean function, which takes binary values as input and produces binary outputs as well, can be represented as a bitwise network with one hidden layer (McCulloch & Pitts, 1943), for example, by merely memorizing all the possible mappings between input and output\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206485943,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31868290adf1c000c611dfc966b514d5a34e8d23",
            "isKey": false,
            "numCitedBy": 7484,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Most current speech recognition systems use hidden Markov models (HMMs) to deal with the temporal variability of speech and Gaussian mixture models (GMMs) to determine how well each state of each HMM fits a frame or a short window of frames of coefficients that represents the acoustic input. An alternative way to evaluate the fit is to use a feed-forward neural network that takes several frames of coefficients as input and produces posterior probabilities over HMM states as output. Deep neural networks (DNNs) that have many hidden layers and are trained using new methods have been shown to outperform GMMs on a variety of speech recognition benchmarks, sometimes by a large margin. This article provides an overview of this progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition."
            },
            "slug": "Deep-Neural-Networks-for-Acoustic-Modeling-in-The-Hinton-Deng",
            "title": {
                "fragments": [],
                "text": "Deep Neural Networks for Acoustic Modeling in Speech Recognition: The Shared Views of Four Research Groups"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "This article provides an overview of progress and represents the shared views of four research groups that have had recent successes in using DNNs for acoustic modeling in speech recognition."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393680089"
                        ],
                        "name": "David Warde-Farley",
                        "slug": "David-Warde-Farley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Warde-Farley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Warde-Farley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153583218"
                        ],
                        "name": "Mehdi Mirza",
                        "slug": "Mehdi-Mirza",
                        "structuredName": {
                            "firstName": "Mehdi",
                            "lastName": "Mirza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mehdi Mirza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 106
                            }
                        ],
                        "text": "Although DNNs are extending the state of the art results for various tasks, such as image classification (Goodfellow et al., 2013), speech recognition (Hinton et al., 2012), speech enhancement (Xu et al., 2014), etc, it is also the case that the relatively bigger networks with more parameters than\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10600578,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7b915d508987b73b61eccd2b237e7ed099a2d29",
            "isKey": false,
            "numCitedBy": 1834,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of designing models to leverage a recently introduced approximate model averaging technique called dropout. We define a simple new model called maxout (so named because its output is the max of a set of inputs, and because it is a natural companion to dropout) designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique. We empirically verify that the model successfully accomplishes both of these tasks. We use maxout and dropout to demonstrate state of the art classification performance on four benchmark datasets: MNIST, CIFAR-10, CIFAR-100, and SVHN."
            },
            "slug": "Maxout-Networks-Goodfellow-Warde-Farley",
            "title": {
                "fragments": [],
                "text": "Maxout Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A simple new model called maxout is defined designed to both facilitate optimization by dropout and improve the accuracy of dropout's fast approximate model averaging technique."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 150
                            }
                        ],
                        "text": "\u2026can check the prediction error E by measuring the bitwise agreement of target vector t and the output units of L-th layer using XNOR as a multiplication operator,\nE = KL+1\u2211\ni\n( 1\u2212 ti \u2297 zL+1i ) /2, (4)\nbut this error function can be tentatively replaced by involving a softmax layer during the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 194
                            }
                        ],
                        "text": "Additionally, even in an environment with the necessary resources being available, speeding up a DNN can greatly improve the user experience when it comes to tasks like searching big databases (Salakhutdinov & Hinton, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1501682,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd5af41a81e7fc9588dc74f3831fb14daf2f8e2a",
            "isKey": false,
            "numCitedBy": 1264,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Semantic-hashing-Salakhutdinov-Hinton",
            "title": {
                "fragments": [],
                "text": "Semantic hashing"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Approx. Reason."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4605464"
                        ],
                        "name": "W. McCulloch",
                        "slug": "W.-McCulloch",
                        "structuredName": {
                            "firstName": "Warren",
                            "lastName": "McCulloch",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. McCulloch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50314979"
                        ],
                        "name": "W. Pitts",
                        "slug": "W.-Pitts",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Pitts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Pitts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We first train some compressed network parameters, and then retrain them using noisy backpropagation for BNNs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 179
                            }
                        ],
                        "text": "\u2026long been known that any Boolean function, which takes binary values as input and produces binary outputs as well, can be represented as a bitwise network with one hidden layer (McCulloch & Pitts, 1943), for example, by merely memorizing all the possible mappings between input and output patterns."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15619658,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "090c5a5df345ab60c41d6de02b3e366e1a27cf43",
            "isKey": false,
            "numCitedBy": 6259,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of the \u201call-or-none\u201d character of nervous activity, neural events and the relations among them can be treated by means of propositional logic. It is found that the behavior of every net can be described in these terms, with the addition of more complicated logical means for nets containing circles; and that for any logical expression satisfying certain conditions, one can find a net behaving in the fashion it describes. It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time. Various applications of the calculus are discussed."
            },
            "slug": "A-logical-calculus-of-the-ideas-immanent-in-nervous-McCulloch-Pitts",
            "title": {
                "fragments": [],
                "text": "A logical calculus of the ideas immanent in nervous activity"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that many particular choices among possible neurophysiological assumptions are equivalent, in the sense that for every net behaving under one assumption, there exists another net which behaves under the other and gives the same results, although perhaps not in the same time."
            },
            "venue": {
                "fragments": [],
                "text": "The Philosophy of Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47494927"
                        ],
                        "name": "L. Pitt",
                        "slug": "L.-Pitt",
                        "structuredName": {
                            "firstName": "Leonard",
                            "lastName": "Pitt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pitt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 87
                            }
                        ],
                        "text": "Although learning those bitwise weights as a Boolean concept is an NPcomplete problem (Pitt & Valiant, 1988), the bitwise networks have been studied in the limited setting, such as \u00b5perceptron networks where an input node is allowed to be connected to one and only one hidden node and its final\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18940285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93849122caf1b10c9611eddb707e0720441c73f6",
            "isKey": false,
            "numCitedBy": 543,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The computational complexity of learning Boolean concepts from examples is investigated. It is shown for various classes of concept representations that these cannot be learned feasibly in a distribution-free sense unless R = NP. These classes include (a) disjunctions of two monomials, (b) Boolean threshold functions, and (c) Boolean formulas in which each variable occurs at most once. Relationships between learning of heuristics and finding approximate solutions to NP-hard optimization problems are given."
            },
            "slug": "Computational-limitations-on-learning-from-examples-Pitt-Valiant",
            "title": {
                "fragments": [],
                "text": "Computational limitations on learning from examples"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "It is shown for various classes of concept representations that these cannot be learned feasibly in a distribution-free sense unless R = NP, and relationships between learning of heuristics and finding approximate solutions to NP-hard optimization problems are given."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2310595"
                        ],
                        "name": "Matthias Baldauf",
                        "slug": "Matthias-Baldauf",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Baldauf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthias Baldauf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691109"
                        ],
                        "name": "S. Dustdar",
                        "slug": "S.-Dustdar",
                        "structuredName": {
                            "firstName": "Schahram",
                            "lastName": "Dustdar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dustdar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144687957"
                        ],
                        "name": "Florian Rosenberg",
                        "slug": "Florian-Rosenberg",
                        "structuredName": {
                            "firstName": "Florian",
                            "lastName": "Rosenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Florian Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "Examples of those applications span from context-aware computing, collecting and analysing a variety of sensor signals on the device (Baldauf et al., 2007), to always-on computer vision applications (e.g. Google glasses), to speechdriven personal assistant services, such as \u201cHey, Siri.\u201d"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 133
                            }
                        ],
                        "text": "Examples of those applications span from context-aware computing, collecting and analysing a variety of sensor signals on the device (Baldauf et al., 2007), to always-on computer vision applications (e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1113420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7fe668f495eb491a52ca1aee37262787ca71a75",
            "isKey": false,
            "numCitedBy": 2112,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Context-aware systems offer entirely new opportunities for application developers and for end users by gathering context data and adapting systems behaviour accordingly. Especially in combination with mobile devices, these mechanisms are of high value and are used to increase usability tremendously. In this paper, we present common architecture principles of context-aware systems and derive a layered conceptual design framework to explain the different elements common to most context-aware architectures. Based on these design principles, we introduce various existing context-aware systems focusing on context-aware middleware and frameworks, which ease the development of context-aware applications. We discuss various approaches and analyse important aspects in context-aware computing on the basis of the presented systems."
            },
            "slug": "A-survey-on-context-aware-systems-Baldauf-Dustdar",
            "title": {
                "fragments": [],
                "text": "A survey on context-aware systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Common architecture principles of context-aware systems are presented and a layered conceptual design framework is derived to explain the different elements common to mostcontext-aware architectures."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Ad Hoc Ubiquitous Comput."
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 106
                            }
                        ],
                        "text": "Although DNNs are extending the state of the art results for various tasks, such as image classification (Goodfellow et al., 2013), speech recognition (Hinton et al., 2012), speech enhancement (Xu et al., 2014), etc, it is also the case that the relatively bigger networks with more parameters than\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Maxout networks. In Proceedings of the International Conference on Machine Learning (ICML)"
            },
            "venue": {
                "fragments": [],
                "text": "Maxout networks. In Proceedings of the International Conference on Machine Learning (ICML)"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 176
                            }
                        ],
                        "text": "Copyright 2015 by the author(s).\ncated function, Deep Neural Networks (DNN) achieve the goal by learning a hierarchy of features in their multiple layers (Hinton et al., 2006; Bengio, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning deep architectures for AI. Foundations and Trends in Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Learning deep architectures for AI. Foundations and Trends in Machine Learning"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 168
                            }
                        ],
                        "text": "According to the universal approximation theorem, a single hidden layer with a finite number of units can approximate a continuous function with some mild assumptions (Cybenko, 1989; Hornik, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Approximations by superpositions of sigmoidal functions"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematics of Control, Signals, and Systems,"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 18,
            "methodology": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Bitwise-Neural-Networks-Kim-Smaragdis/8fcfd67f21738eff12d853fdf2b31ee192e2312a?sort=total-citations"
}