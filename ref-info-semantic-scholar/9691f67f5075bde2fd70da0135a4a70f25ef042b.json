{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754328"
                        ],
                        "name": "D. Hush",
                        "slug": "D.-Hush",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Hush",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30731558"
                        ],
                        "name": "P. Kelly",
                        "slug": "P.-Kelly",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Kelly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kelly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143790221"
                        ],
                        "name": "C. Scovel",
                        "slug": "C.-Scovel",
                        "structuredName": {
                            "firstName": "Clint",
                            "lastName": "Scovel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Scovel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782193"
                        ],
                        "name": "Ingo Steinwart",
                        "slug": "Ingo-Steinwart",
                        "structuredName": {
                            "firstName": "Ingo",
                            "lastName": "Steinwart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ingo Steinwart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 255
                            }
                        ],
                        "text": "Moreover, since decomposition methods .nd a feasible dual solution and their goal is to maximize \nthe dual objective function, they often result in a rather slow conver\u00adgence rate to the optimum of the \nprimal objective function (See also the discussion in (Hush et al., 2006))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 255
                            }
                        ],
                        "text": "Moreover, since decomposition methods find a feasible dual solution and their goal is to maximize the dual objective function, they often result in a rather slow convergence rate to the optimum of the primal objective function (See also the discussion in (Hush et al., 2006))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18644544,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "75a4608c9317cbdb7c4c5e0685fc1818533798cd",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe polynomial--time algorithms that produce approximate solutions with guaranteed accuracy for a class of QP problems that are used in the design of support vector machine classifiers. These algorithms employ a two--stage process where the first stage produces an approximate solution to a dual QP problem and the second stage maps this approximate dual solution to an approximate primal solution. For the second stage we describe an O(n log n) algorithm that maps an approximate dual solution with accuracy (2(2Km)1/2+8(\u03bb)1/2)-2 \u03bb ep2 to an approximate primal solution with accuracy ep where n is the number of data samples, Kn is the maximum kernel value over the data and \u03bb > 0 is the SVM regularization parameter. For the first stage we present new results for decomposition algorithms and describe new decomposition algorithms with guaranteed accuracy and run time. In particular, for \u03c4-rate certifying decomposition algorithms we establish the optimality of \u03c4 = 1/(n-1). In addition we extend the recent \u03c4 = 1/(n-1) algorithm of Simon (2004) to form two new composite algorithms that also achieve the \u03c4 = 1/(n-1) iteration bound of List and Simon (2005), but yield faster run times in practice. We also exploit the \u03c4-rate certifying property of these algorithms to produce new stopping rules that are computationally efficient and that guarantee a specified accuracy for the approximate dual solution. Furthermore, for the dual QP problem corresponding to the standard classification problem we describe operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations. For this same problem we also describe general conditions for which a matching lower bound exists for any decomposition algorithm that uses working sets of size 2. For the Simon and composite algorithms we also establish an O(n2) bound on the overall run time for the first stage. Combining the first and second stages gives an overall run time of O(n2(ck + 1)) where ck is an upper bound on the computation to perform a kernel evaluation. Pseudocode is presented for a complete algorithm that inputs an accuracy ep and produces an approximate solution that satisfies this accuracy in low order polynomial time. Experiments are included to illustrate the new stopping rules and to compare the Simon and composite decomposition algorithms."
            },
            "slug": "QP-Algorithms-with-Guaranteed-Accuracy-and-Run-Time-Hush-Kelly",
            "title": {
                "fragments": [],
                "text": "QP Algorithms with Guaranteed Accuracy and Run Time for Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Operational conditions for which the Simon and composite algorithms possess an upper bound of O(n) on the number of iterations are described and general conditions forwhich a matching lower bound exists for any decomposition algorithm that uses working sets of size 2 are described."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042259"
                        ],
                        "name": "Chuong B. Do",
                        "slug": "Chuong-B.-Do",
                        "structuredName": {
                            "firstName": "Chuong",
                            "lastName": "Do",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuong B. Do"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121484"
                        ],
                        "name": "Chuan-Sheng Foo",
                        "slug": "Chuan-Sheng-Foo",
                        "structuredName": {
                            "firstName": "Chuan-Sheng",
                            "lastName": "Foo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuan-Sheng Foo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "This phenomenon has been observed before and there have been rather successful a ttempts to improve Pegasos when\u03bb is small (see for example [13])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12115491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5aaee35181928fa35c68ca0b7598829f8afb1bfe",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Many learning algorithms rely on the curvature (in particular, strong convexity) of regularized objective functions to provide good theoretical performance guarantees. In practice, the choice of regularization penalty that gives the best testing set performance may result in objective functions with little or even no curvature. In these cases, algorithms designed specifically for regularized objectives often either fail completely or require some modification that involves a substantial compromise in performance.\n We present new online and batch algorithms for training a variety of supervised learning models (such as SVMs, logistic regression, structured prediction models, and CRFs) under conditions where the optimal choice of regularization parameter results in functions with low curvature. We employ a technique called proximal regularization, in which we solve the original learning problem via a sequence of modified optimization tasks whose objectives are chosen to have greater curvature than the original problem. Theoretically, our algorithms achieve low regret bounds in the online setting and fast convergence in the batch setting. Experimentally, our algorithms improve upon state-of-the-art techniques, including Pegasos and bundle methods, on medium and large-scale SVM and structured learning tasks."
            },
            "slug": "Proximal-regularization-for-online-and-batch-Do-Le",
            "title": {
                "fragments": [],
                "text": "Proximal regularization for online and batch learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "P proximal regularization is employed, in which the original learning problem is solved via a sequence of modified optimization tasks whose objectives are chosen to have greater curvature than the original problem."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189092"
                        ],
                        "name": "John C. Platt",
                        "slug": "John-C.-Platt",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Platt",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John C. Platt"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 144
                            }
                        ],
                        "text": "\u2026for multiple choices of .. Decomposition methods: To overcome the quadratic memory requirement \nof IP methods, decomposition meth\u00adods such as SMO (Platt, 1998) and SVM-Light (Joachims, 1998) switch \nto the dual representation of the SVM opti\u00admization problem, and employ an active set of\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1099857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4de39c94e340a108fff01a90a67b0c17c86fb981",
            "isKey": false,
            "numCitedBy": 5923,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter describes a new algorithm for training Support Vector Machines: Sequential Minimal Optimization, or SMO. Training a Support Vector Machine (SVM) requires the solution of a very large quadratic programming (QP) optimization problem. SMO breaks this large QP problem into a series of smallest possible QP problems. These small QP problems are solved analytically, which avoids using a time-consuming numerical QP optimization as an inner loop. The amount of memory required for SMO is linear in the training set size, which allows SMO to handle very large training sets. Because large matrix computation is avoided, SMO scales somewhere between linear and quadratic in the training set size for various test problems, while a standard projected conjugate gradient (PCG) chunking algorithm scales somewhere between linear and cubic in the training set size. SMO's computation time is dominated by SVM evaluation, hence SMO is fastest for linear SVMs and sparse data sets. For the MNIST database, SMO is as fast as PCG chunking; while for the UCI Adult database and linear SVMs, SMO can be more than 1000 times faster than the PCG chunking algorithm."
            },
            "slug": "Fast-training-of-support-vector-machines-using-in-Platt",
            "title": {
                "fragments": [],
                "text": "Fast training of support vector machines using sequential minimal optimization, advances in kernel methods"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "SMO breaks this large quadratic programming problem into a series of smallest possible QP problems, which avoids using a time-consuming numerical QP optimization as an inner loop and hence SMO is fastest for linear SVMs and sparse data sets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34840427"
                        ],
                        "name": "Elad Hazan",
                        "slug": "Elad-Hazan",
                        "structuredName": {
                            "firstName": "Elad",
                            "lastName": "Hazan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elad Hazan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078528382"
                        ],
                        "name": "A. Agarwal",
                        "slug": "A.-Agarwal",
                        "structuredName": {
                            "firstName": "Amit",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144055676"
                        ],
                        "name": "Satyen Kale",
                        "slug": "Satyen-Kale",
                        "structuredName": {
                            "firstName": "Satyen",
                            "lastName": "Kale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satyen Kale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 66
                            }
                        ],
                        "text": "We .rst need the following lemma which generalizes a result from (Hazan \net al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 66
                            }
                        ],
                        "text": "We first need the following lemma which generalizes a result from (Hazan et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11569359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c883f38d202548c1d89ef5de8892d53227842092",
            "isKey": false,
            "numCitedBy": 947,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract\nIn an online convex optimization problem a decision-maker makes a sequence of decisions, i.e., chooses a sequence of points in Euclidean space, from a fixed feasible set. After each point is chosen, it encounters a sequence of (possibly unrelated) convex cost functions. Zinkevich (ICML 2003) introduced this framework, which models many natural repeated decision-making problems and generalizes many existing problems such as Prediction from Expert Advice and Cover\u2019s Universal Portfolios. Zinkevich showed that a simple online gradient descent algorithm achieves additive regret$O(\\sqrt{T})$\n, for an arbitrary sequence of T convex cost functions (of bounded gradients), with respect to the best single decision in hindsight.\n\nIn this paper, we give algorithms that achieve regret O(log\u2009(T)) for an arbitrary sequence of strictly convex functions (with bounded first and second derivatives). This mirrors what has been done for the special cases of prediction from expert advice by Kivinen and Warmuth (EuroCOLT 1999), and Universal Portfolios by Cover (Math. Finance 1:1\u201319, 1991). We propose several algorithms achieving logarithmic regret, which besides being more general are also much more efficient to implement.\n\nThe main new ideas give rise to an efficient algorithm based on the Newton method for optimization, a new tool in the field. Our analysis shows a surprising connection between the natural follow-the-leader approach and the Newton method. We also analyze other algorithms, which tie together several different previous approaches including follow-the-leader, exponential weighting, Cover\u2019s algorithm and gradient descent.\n"
            },
            "slug": "Logarithmic-regret-algorithms-for-online-convex-Hazan-Agarwal",
            "title": {
                "fragments": [],
                "text": "Logarithmic regret algorithms for online convex optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Several algorithms achieving logarithmic regret are proposed, which besides being more general are also much more efficient to implement, and give rise to an efficient algorithm based on the Newton method for optimization, a new tool in the field."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700597"
                        ],
                        "name": "Jyrki Kivinen",
                        "slug": "Jyrki-Kivinen",
                        "structuredName": {
                            "firstName": "Jyrki",
                            "lastName": "Kivinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jyrki Kivinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143957317"
                        ],
                        "name": "R. C. Williamson",
                        "slug": "R.-C.-Williamson",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Williamson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Williamson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2959806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12fa4a3ee546ba8eeb0b88b06bcb571d65d91cc4",
            "isKey": false,
            "numCitedBy": 1045,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Kernel-based algorithms such as support vector machines have achieved considerable success in various problems in batch setting, where all of the training data is available in advance. Support vector machines combine the so-called kernel trick with the large margin idea. There has been little use of these methods in an online setting suitable for real-time applications. In this paper, we consider online learning in a reproducing kernel Hilbert space. By considering classical stochastic gradient descent within a feature space and the use of some straightforward tricks, we develop simple and computationally efficient algorithms for a wide range of problems such as classification, regression, and novelty detection. In addition to allowing the exploitation of the kernel trick in an online setting, we examine the value of large margins for classification in the online setting with a drifting target. We derive worst-case loss bounds, and moreover, we show the convergence of the hypothesis to the minimizer of the regularized risk functional. We present some experimental results that support the theory as well as illustrating the power of the new algorithms for online novelty detection."
            },
            "slug": "Online-learning-with-kernels-Kivinen-Smola",
            "title": {
                "fragments": [],
                "text": "Online learning with kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper considers online learning in a reproducing kernel Hilbert space, and allows the exploitation of the kernel trick in an online setting, and examines the value of large margins for classification in the online setting with a drifting target."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143676697"
                        ],
                        "name": "Y. Nesterov",
                        "slug": "Y.-Nesterov",
                        "structuredName": {
                            "firstName": "Yurii",
                            "lastName": "Nesterov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Nesterov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Intuitively, the ideas presented in [28] ca n be combined with the stochastic regime of Pegasos."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "Another related recent work is Nesterov\u2019s general primal-dual subgradient method for t he minimization of non-smooth functions [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14935076,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "73f583aad5195324ee75eb981b8b5f1fed6f9d38",
            "isKey": false,
            "numCitedBy": 800,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a new approach for constructing subgradient schemes for different types of nonsmooth problems with convex structure. Our methods are primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem. Besides other advantages, this useful feature provides the methods with a reliable stopping criterion. The proposed schemes differ from the classical approaches (divergent series methods, mirror descent methods) by presence of two control sequences. The first sequence is responsible for aggregating the support functions in the dual space, and the second one establishes a dynamically updated scale between the primal and dual spaces. This additional flexibility allows to guarantee a boundedness of the sequence of primal test points even in the case of unbounded feasible set (however, we always assume the uniform boundedness of subgradients). We present the variants of subgradient schemes for nonsmooth convex minimization, minimax problems, saddle point problems, variational inequalities, and stochastic optimization. In all situations our methods are proved to be optimal from the view point of worst-case black-box lower complexity bounds."
            },
            "slug": "Primal-dual-subgradient-methods-for-convex-problems-Nesterov",
            "title": {
                "fragments": [],
                "text": "Primal-dual subgradient methods for convex problems"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A new approach for constructing subgradient schemes for different types of nonsmooth problems with convex structure that is primal-dual since they are always able to generate a feasible approximation to the optimum of an appropriately formulated dual problem."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445783"
                        ],
                        "name": "A. Conconi",
                        "slug": "A.-Conconi",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Conconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Conconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895207"
                        ],
                        "name": "C. Gentile",
                        "slug": "C.-Gentile",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gentile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 125
                            }
                        ],
                        "text": "Such algo rithms can be used to obtain a predictor with low generalization error using an online-t o-batch conversion scheme [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 124
                            }
                        ],
                        "text": "Such algorithms \ncan be used to obtain a predictor with low generalization error using an online-to-batch conversion scheme \n(Cesa-Bianchi et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 110
                            }
                        ],
                        "text": "In contrast, by applying previ\u00ad . d o ously studied conversions of online algorithms in the PAC setting \n(e.g. (Cesa-Bianchi et al., 2004; Cesa-Bianchi &#38; Gentile, 2006)) one can obtain accuracy of o with \ncon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 437093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "78396e535101308d4431c08f0e85b18c920ee44f",
            "isKey": false,
            "numCitedBy": 523,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, it is shown how to extract a hypothesis with small risk from the ensemble of hypotheses generated by an arbitrary on-line learning algorithm run on an independent and identically distributed (i.i.d.) sample of data. Using a simple large deviation argument, we prove tight data-dependent bounds for the risk of this hypothesis in terms of an easily computable statistic M/sub n/ associated with the on-line performance of the ensemble. Via sharp pointwise bounds on M/sub n/, we then obtain risk tail bounds for kernel perceptron algorithms in terms of the spectrum of the empirical kernel matrix. These bounds reveal that the linear hypotheses found via our approach achieve optimal tradeoffs between hinge loss and margin size over the class of all linear functions, an issue that was left open by previous results. A distinctive feature of our approach is that the key tools for our analysis come from the model of prediction of individual sequences; i.e., a model making no probabilistic assumptions on the source generating the data. In fact, these tools turn out to be so powerful that we only need very elementary statistical facts to obtain our final risk bounds."
            },
            "slug": "On-the-generalization-ability-of-on-line-learning-Cesa-Bianchi-Conconi",
            "title": {
                "fragments": [],
                "text": "On the generalization ability of on-line learning algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proves tight data-dependent bounds for the risk of this hypothesis in terms of an easily computable statistic M/sub n/ associated with the on-line performance of the ensemble, and obtains risk tail bounds for kernel perceptron algorithms interms of the spectrum of the empirical kernel matrix."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684722"
                        ],
                        "name": "S. Fine",
                        "slug": "S.-Fine",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Fine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2005127"
                        ],
                        "name": "K. Scheinberg",
                        "slug": "K.-Scheinberg",
                        "structuredName": {
                            "firstName": "Katya",
                            "lastName": "Scheinberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Scheinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 125
                            }
                        ],
                        "text": "It should be noted that there have been several attempts to reduce the complexity based on \nadditional assumptions (see e.g. (Fine &#38; Scheinberg, 2001))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13899309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0db7af02be7cbadc029f9104a8c784d02de42df7",
            "isKey": false,
            "numCitedBy": 665,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "SVM training is a convex optimization problem which scales with the training set size rather than the feature space dimension. While this is usually considered to be a desired quality, in large scale problems it may cause training to be impractical. The common techniques to handle this difficulty basically build a solution by solving a sequence of small scale subproblems. Our current effort is concentrated on the rank of the kernel matrix as a source for further enhancement of the training procedure. We first show that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity. We then suggest an efficient use of a known factorization technique to approximate a given kernel matrix by a low rank matrix, which in turn will be used to feed the optimizer. Finally, we derive an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors). This bound is general in the sense that it holds regardless of the approximation method."
            },
            "slug": "Efficient-SVM-Training-Using-Low-Rank-Kernel-Fine-Scheinberg",
            "title": {
                "fragments": [],
                "text": "Efficient SVM Training Using Low-Rank Kernel Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work shows that for a low rank kernel matrix it is possible to design a better interior point method (IPM) in terms of storage requirements as well as computational complexity and derives an upper bound on the change in the objective function value based on the approximation error and the number of active constraints (support vectors)."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793529"
                        ],
                        "name": "Cho-Jui Hsieh",
                        "slug": "Cho-Jui-Hsieh",
                        "structuredName": {
                            "firstName": "Cho-Jui",
                            "lastName": "Hsieh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cho-Jui Hsieh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2782886"
                        ],
                        "name": "Kai-Wei Chang",
                        "slug": "Kai-Wei-Chang",
                        "structuredName": {
                            "firstName": "Kai-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144106136"
                        ],
                        "name": "S. Keerthi",
                        "slug": "S.-Keerthi",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Keerthi",
                            "middleNames": [
                                "Sathiya"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keerthi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144833733"
                        ],
                        "name": "S. Sundararajan",
                        "slug": "S.-Sundararajan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Sundararajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sundararajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "Last, we would like to point to the SVM-Perf algorithm \nrecently proposed by Joachims (2006) for linear SVMs. SVM-Perf uses cutting planes to .nd a solution \nwith accu\u00adracy oin time O(md/(.o2))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 124
                            }
                        ],
                        "text": "Following Pegasos\u2019s initial presentation [31] , stochastic DCA was suggested as an alternative optimization method for SVMs [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Introduction Support Vector Machines (SVMs) are effective and popu\u00adlar \nclassi.cation learning tool (Vapnik, 1998; Cristianini &#38; Shawe-Taylor, 2000)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 52
                            }
                        ],
                        "text": "Furthermore, most methods we compare t o, including [21,24,37,18], do not incorporate a bias term either."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7880266,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0389a414c5d0ef50e06fe0c15f6102f374ce1b04",
            "isKey": true,
            "numCitedBy": 934,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications, data appear with a huge number of instances as well as features. Linear Support Vector Machines (SVM) is one of the most popular tools to deal with such large-scale sparse data. This paper presents a novel dual coordinate descent method for linear SVM with L1-and L2-loss functions. The proposed method is simple and reaches an \u03b5-accurate solution in O(log(1/\u03b5)) iterations. Experiments indicate that our method is much faster than state of the art solvers such as Pegasos, TRON, SVMperf, and a recent primal coordinate descent implementation."
            },
            "slug": "A-dual-coordinate-descent-method-for-large-scale-Hsieh-Chang",
            "title": {
                "fragments": [],
                "text": "A dual coordinate descent method for large-scale linear SVM"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel dual coordinate descent method for linear SVM with L1-and L2-loss functions that reaches an \u03b5-accurate solution in O(log(1/\u03b5)) iterations is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37564609"
                        ],
                        "name": "Karthik Sridharan",
                        "slug": "Karthik-Sridharan",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Sridharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karthik Sridharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "1 To do so, we can simply calculate the objective on the entire data set or estimate it according to a sample of size O(1/(\u03bb \ufffd)) ,w here\ufffd is the desired accuracy (see [ 35 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7065301,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "a4cf35b6772b57aa838f7d6d0daad83d3e36cbb6",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We study convergence properties of empirical minimization of a stochastic strongly convex objective, where the stochastic component is linear. We show that the value attained by the empirical minimizer converges to the optimal value with rate 1/n. The result applies, in particular, to the SVM objective. Thus, we obtain a rate of 1/n on the convergence of the SVM objective (with fixed regularization parameter) to its infinite data limit. We demonstrate how this is essential for obtaining certain type of oracle inequalities for SVMs. The results extend also to approximate minimization as well as to strong convexity with respect to an arbitrary norm, and so also to objectives regularized using other lp norms."
            },
            "slug": "Fast-Rates-for-Regularized-Objectives-Sridharan-Shalev-Shwartz",
            "title": {
                "fragments": [],
                "text": "Fast Rates for Regularized Objectives"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the value attained by the empirical minimizer converges to the optimal value with rate 1/n, which is essential for obtaining certain type of oracle inequalities for SVMs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 59
                            }
                        ],
                        "text": "A detailed proof and further \nexplanations can be found in (Shalev-Shwartz &#38; Singer, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 34
                            }
                        ],
                        "text": "-strongly \ncon\u00advex (see Lemma 1 in (Shalev-Shwartz &#38; Singer, 2007))."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10661811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e9dd1c197cbc73434f12e9d62d10ed2a79190d7",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Many problems arising in machine learning can be cast as a convex optimization problem, in which a sum of a loss term and a regularization term is minimized. For example, in Support Vector Machines the loss term is the average hinge-loss of a vector over a training set of examples and the regularization term is the squared Euclidean norm of this vector. In this paper we study an algorithmic framework for strongly convex repeated games and apply it for solving regularized loss minimization problems. In a convex repeated game, a predictor chooses a sequence of vectors from a convex set. After each vector is chosen, the opponent responds with a convex loss function and the predictor pays for applying the loss function to the vector she chose. The regret of the predictor is the difference between her cumulative loss and the minimal cumulative loss achievable by a fixed vector, even one that is chosen in hindsight. In strongly convex repeated games, the opponent is forced to choose loss functions that are strongly convex. We describe a family of prediction algorithms for strongly convex repeated games that attain logarithmic regret."
            },
            "slug": "Logarithmic-Regret-Algorithms-for-Strongly-Convex-Shalev-Shwartz-Singer",
            "title": {
                "fragments": [],
                "text": "Logarithmic Regret Algorithms for Strongly Convex Repeated Games"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper describes a family of prediction algorithms for strongly convex repeated games that attain logarithmic regret and applies it for solving regularized loss minimization problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145713876"
                        ],
                        "name": "S. Vishwanathan",
                        "slug": "S.-Vishwanathan",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Vishwanathan",
                            "middleNames": [
                                "V.",
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vishwanathan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "This bound was later improved by Smola et al [33] to O(md/(\u03bb ))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5789659,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a67f1dcf735a1c52708a9a5392ff585658dbc8ce",
            "isKey": false,
            "numCitedBy": 145,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a globally convergent method for regularized risk minimization problems. Our method applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem. SVMPerf can be shown to be a special case of our approach. In addition to the unified framework we present tight convergence bounds, which show that our algorithm converges in O(1/\u220a) steps to \u220a precision for general convex problems and in O(log(1/\u220a)) steps for continuously differentiable problems. We demonstrate in experiments the performance of our approach."
            },
            "slug": "Bundle-Methods-for-Machine-Learning-Smola-Vishwanathan",
            "title": {
                "fragments": [],
                "text": "Bundle Methods for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This work presents a globally convergent method that applies to Support Vector estimation, regression, Gaussian Processes, and any other regularized risk minimization setting which leads to a convex optimization problem and presents tight convergence bounds, which show that the algorithm converges in O(1/\u220a) steps to \u220a precision for general convex problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50263663"
                        ],
                        "name": "D. Boswell",
                        "slug": "D.-Boswell",
                        "structuredName": {
                            "firstName": "Dustin",
                            "lastName": "Boswell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Boswell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 84
                            }
                        ],
                        "text": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM Shai Shalev-Shwartz School of Computer Science \nand Engineering, The Hebrew University, Jerusalem, Israel Yoram Singer Google inc., Mountain View, \nUSA and The Hebrew University, Jerusalem, Israel Nathan Srebro Toyota Technological Institute, Chicago, \nUSA  Abstract We describe and analyze a simple and effec\u00adtive iterative algorithm for solving the optimiza\u00adtion \nproblem cast by Support Vector Machines (SVM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 101
                            }
                        ],
                        "text": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM Shai Shalev-Shwartz School of Computer Science \nand Engineering, The Hebrew University, Jerusalem, Israel Yoram Singer Google inc., Mountain View, \nUSA and The Hebrew University, Jerusalem, Israel Nathan Srebro Toyota Technological Institute,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 113
                            }
                        ],
                        "text": "Introduction Support Vector Machines (SVMs) are effective and popu\u00adlar \nclassi.cation learning tool (Vapnik, 1998; Cristianini &#38; Shawe-Taylor, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18986102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea2ea7c6e280c1cfb67ee38ea63a327b1ba3ca36",
            "isKey": false,
            "numCitedBy": 2135,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines (SVM\u2019s) are a relatively new learning method used for binary classification. The basic idea is to find a hyperplane which separates the d-dimensional data perfectly into its two classes. However, since example data is often not linearly separable, SVM\u2019s introduce the notion of a \u201ckernel induced feature space\u201d which casts the data into a higher dimensional space where the data is separable. Typically, casting into such a space would cause problems computationally, and with overfitting. The key insight used in SVM\u2019s is that the higher-dimensional space doesn\u2019t need to be dealt with directly (as it turns out, only the formula for the dot-product in that space is needed), which eliminates the above concerns. Furthermore, the VC-dimension (a measure of a system\u2019s likelihood to perform well on unseen data) of SVM\u2019s can be explicitly calculated, unlike other learning methods like neural networks, for which there is no measure. Overall, SVM\u2019s are intuitive, theoretically wellfounded, and have shown to be practically successful. SVM\u2019s have also been extended to solve regression tasks (where the system is trained to output a numerical value, rather than \u201cyes/no\u201d classification)."
            },
            "slug": "Introduction-to-Support-Vector-Machines-Boswell",
            "title": {
                "fragments": [],
                "text": "Introduction to Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "Support Vector Machines (SVM\u2019s) are intuitive, theoretically wellfounded, and have shown to be practically successful."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2676309"
                        ],
                        "name": "C. Burges",
                        "slug": "C.-Burges",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Burges",
                            "middleNames": [
                                "J.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Burges"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60502900,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c4da62e9e89e65ac78ee271e424e8b498053e8c",
            "isKey": false,
            "numCitedBy": 5545,
            "numCiting": 260,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to support vector learning roadmap. Part 1 Theory: three remarks on the support vector method of function estimation, Vladimir Vapnik generalization performance of support vector machines and other pattern classifiers, Peter Bartlett and John Shawe-Taylor Bayesian voting schemes and large margin classifiers, Nello Cristianini and John Shawe-Taylor support vector machines, reproducing kernel Hilbert spaces, and randomized GACV, Grace Wahba geometry and invariance in kernel based methods, Christopher J.C. Burges on the annealed VC entropy for margin classifiers - a statistical mechanics study, Manfred Opper entropy numbers, operators and support vector kernels, Robert C. Williamson et al. Part 2 Implementations: solving the quadratic programming problem arising in support vector classification, Linda Kaufman making large-scale support vector machine learning practical, Thorsten Joachims fast training of support vector machines using sequential minimal optimization, John C. Platt. Part 3 Applications: support vector machines for dynamic reconstruction of a chaotic system, Davide Mattera and Simon Haykin using support vector machines for time series prediction, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel. Part 4 Extensions of the algorithm: reducing the run-time complexity in support vector machines, Edgar E. Osuna and Federico Girosi support vector regression with ANOVA decomposition kernels, Mark O. Stitson et al support vector density estimation, Jason Weston et al combining support vector and mathematical programming methods for classification, Bernhard Scholkopf et al."
            },
            "slug": "Advances-in-kernel-methods:-support-vector-learning-Sch\u00f6lkopf-Burges",
            "title": {
                "fragments": [],
                "text": "Advances in kernel methods: support vector learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Support vector machines for dynamic reconstruction of a chaotic system, Klaus-Robert Muller et al pairwise classification and support vector machines, Ulrich Kressel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 46
                            }
                        ],
                        "text": "Training time in CPU-seconds Pegasos SVM-Perf \nSVM-Light CCAT 2 77 20,075 Covertype 6 85 25,514 astro-ph 2 5 80 The above adaptation indeed work for \nthe case At = S and we obtain the same rate of convergence as in the no-bias case."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 55
                            }
                        ],
                        "text": "For completeness, \nwe added to the table the runtime of SVM-Light as reported in (Joachims, 2006)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 284
                            }
                        ],
                        "text": "In particul ar, we compare its runtime on three large datasets to the runtimes of the state-of-the-art solv er SVM-Perf [21], a cutting plane algorithm designed specifically for use with sparse feature vectors, as well as of two more conventional SVM solvers: LASVM [2] and SVM-Light [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 316,
                                "start": 309
                            }
                        ],
                        "text": "The time spent by IP methods \nfor .nding a single accurate solution may, for instance, be better utilized for .nding numerous approximate \nsolutions for multiple choices of .. Decomposition methods: To overcome the quadratic memory requirement \nof IP methods, decomposition meth\u00adods such as SMO (Platt, 1998) and SVM-Light (Joachims, 1998) switch \nto the dual representation of the SVM opti\u00admization problem, and employ an active set of constraints \nthus working on a subset of dual variables."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "Decomposition methods:To overcome the quadratic memory requirement of IP methods, decomposition methods such as SMO [29] and SVM-Light [20] ta ckle the dual representation of the SVM optimization problem, and employ an active se t of constraints thus working on a subset of dual variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 148
                            }
                        ],
                        "text": "\u2026.. Decomposition methods: To overcome the quadratic memory requirement \nof IP methods, decomposition meth\u00adods such as SMO (Platt, 1998) and SVM-Light (Joachims, 1998) switch \nto the dual representation of the SVM opti\u00admization problem, and employ an active set of constraints \nthus working on a\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 76
                            }
                        ],
                        "text": "It was shown in (Joachims, 2006) that SVM-Perf is \nsubstantially faster than SVM-Light, achieving a speedup of several orders of magnitude on most datasets."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60502770,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb99668d4df98a3f6ff0b9fa3402e09008f22e2c",
            "isKey": true,
            "numCitedBy": 1844,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Training a support vector machine (SVM) leads to a quadratic optimization problem with bound constraints and one linear equality constraint. Despite the fact that this type of problem is well understood, there are many issues to be considered in designing an SVM learner. In particular, for large learning tasks with many training examples, oo-the-shelf optimization techniques for general quadratic programs quickly become intractable in their memory and time requirements. SV M light1 is an implementation of an SVM learner which addresses the problem of large tasks. This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical. The results give guidelines for the application of SVMs to large domains."
            },
            "slug": "Making-large-scale-support-vector-machine-learning-Joachims",
            "title": {
                "fragments": [],
                "text": "Making large-scale support vector machine learning practical"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter presents algorithmic and computational results developed for SV M light V2.0, which make large-scale SVM training more practical and give guidelines for the application of SVMs to large domains."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117881943"
                        ],
                        "name": "Tong Zhang",
                        "slug": "Tong-Zhang",
                        "structuredName": {
                            "firstName": "Tong",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tong Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 32
                            }
                        ],
                        "text": "Based on the analysis given in \n(Zhang, 2004) we started by setting . to be 10-5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 180
                            }
                        ],
                        "text": "Finally, we compare Pegasos to two pre viously proposed methods that are based on stochastic gradient descent: Norma [24] by Kivi nen, Smola, Williamson and to the method by Zhang [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 174
                            }
                        ],
                        "text": "Next, we compare Pegasos to two \npreviously proposed methods that are based on stochastic gradient descent, namely to Norma (Kivinen et \nal., 2002) and to the method given in (Zhang, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 138
                            }
                        ],
                        "text": "In our next experiment, we compared \nPegasos to Norma (Kivinen et al., 2002) and to a variant of stochastic gradient descent described in \n(Zhang, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 106
                            }
                        ],
                        "text": "In all of the experiments we did not incoprorate a bias term \nsince (Joachims, 2006; Kivinen et al., 2002; Zhang, 2004) do not incorporate that term either."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "In our last set of experiments, we compared Pegasos to Norma [ 24] and to a variant of stochastic gradient descent due to Zhang [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 52
                            }
                        ],
                        "text": "Furthermore, most methods we compare t o, including [21,24,37,18], do not incorporate a bias term either."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 56
                            }
                        ],
                        "text": "We now turn to comparing Pegasos to the algo\u00adrithm from (Zhang, 2004) which simply \nsets .t = ., where . is a (.xed) small number."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 195
                            }
                        ],
                        "text": "Two concrete \nalgorithms that are closely related to the Pegasos algorithm that are based on gradient methods are the \nNORMA algorithm (Kivinen et al., 2002) and a stochastic gradient algorithm by Zhang (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "The analysis in [37] for the case of regularized loss s how that the squared Euclidean distance to the optimal solution converges to zero but the ra te of convergence depends on"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 210
                            }
                        ],
                        "text": "Two concrete algorithms that are closely related to the Pega sos lgorithm and are also variants of stochastic sub-gradient methods are the NORMA a lgorithm [24] and a stochastic gradient algorithm due to Zhang [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "Based on the analysis given in [37] we started by setting\u03b7 to be10\u22125."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5306879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ef7d9e618cbb507d69f8ebcdc60b8a1f3135bff",
            "isKey": true,
            "numCitedBy": 1016,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear prediction methods, such as least squares for regression, logistic regression and support vector machines for classification, have been extensively used in statistics and machine learning. In this paper, we study stochastic gradient descent (SGD) algorithms on regularized forms of linear prediction methods. This class of methods, related to online algorithms such as perceptron, are both efficient and very simple to implement. We obtain numerical rate of convergence for such algorithms, and discuss its implications. Experiments on text data will be provided to demonstrate numerical and statistical consequences of our theoretical findings."
            },
            "slug": "Solving-large-scale-linear-prediction-problems-Zhang",
            "title": {
                "fragments": [],
                "text": "Solving large scale linear prediction problems using stochastic gradient descent algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Stochastic gradient descent algorithms on regularized forms of linear prediction methods, related to online algorithms such as perceptron, are studied, and numerical rate of convergence for such algorithms is obtained."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3064914"
                        ],
                        "name": "Ambuj Tewari",
                        "slug": "Ambuj-Tewari",
                        "structuredName": {
                            "firstName": "Ambuj",
                            "lastName": "Tewari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ambuj Tewari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2891410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "825f3932a53fd25628ab74c9faac19810dc27545",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper examines the generalization properties of online convex programming algorithms when the loss function is Lipschitz and strongly convex. Our main result is a sharp bound, that holds with high probability, on the excess risk of the output of an online algorithm in terms of the average regret. This allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excess risk with high probability. As a corollary, we characterize the convergence rate of PEGASOS (with high probability), a recently proposed method for solving the SVM optimization problem."
            },
            "slug": "On-the-Generalization-Ability-of-Online-Strongly-Kakade-Tewari",
            "title": {
                "fragments": [],
                "text": "On the Generalization Ability of Online Strongly Convex Programming Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A sharp bound is held on the excess risk of the output of an online algorithm in terms of the average regret, that allows one to use recent algorithms with logarithmic cumulative regret guarantees to achieve fast convergence rates for the excessrisk with high probability."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680188"
                        ],
                        "name": "T. Joachims",
                        "slug": "T.-Joachims",
                        "structuredName": {
                            "firstName": "Thorsten",
                            "lastName": "Joachims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Joachims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 7
                            }
                        ],
                        "text": "As in (Joachims, 2006), we set ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 70
                            }
                        ],
                        "text": "In our .rst experiment we compared Pegasos to the SVM-Perf algorithm \n(Joachims, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 81
                            }
                        ],
                        "text": "For completeness, \nwe added to the table the runtime of SVM-Light as reported in (Joachims, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 76
                            }
                        ],
                        "text": "Last, we would like to point to the SVM-Perf algorithm \nrecently proposed by Joachims (2006) for linear SVMs. SVM-Perf uses cutting planes to .nd a solution \nwith accu\u00adracy oin time O(md/(.o2))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 9
                            }
                        ],
                        "text": "SVM-Perf (Joachims, 2006) is a cutting plane algorithm for solving SVM that is based on a reformulation of the SVM problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 16
                            }
                        ],
                        "text": "It was shown in (Joachims, 2006) that SVM-Perf is substantially faster than SVM-Light, achieving a speedup of several orders of magnitude on most datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 68
                            }
                        ],
                        "text": "In all of the experiments we did not incoprorate a bias term \nsince (Joachims, 2006; Kivinen et al., 2002; Zhang, 2004) do not incorporate that term either."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 69
                            }
                        ],
                        "text": "In our first experiment we compared Pegasos to the SVMPerf algorithm (Joachims, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 84
                            }
                        ],
                        "text": "We run both Pegasos and SVM-Perf on the three datasets with values of \u03bb as given in (Joachims, 2006), namely, \u03bb = 10\u22124 for CCAT, \u03bb = 2 \u00b7 10\u22124 for Astro-physics, and\u03bb = 10\u22126 for Covertype."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 71
                            }
                        ],
                        "text": "In particular, we compare its runtime to a \nnew state-of-the\u00adart solver (Joachims, 2006) on three large datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 70
                            }
                        ],
                        "text": "In particular, we compare its runtime to a new state-of-theart solver (Joachims, 2006) on three large datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 17
                            }
                        ],
                        "text": "It was shown in (Joachims, 2006) that SVM-Perf is \nsubstantially faster than SVM-Light, achieving a speedup of several orders of magnitude on most datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 10
                            }
                        ],
                        "text": "SVM-Perf (Joachims, 2006) is a cutting plane algorithm for solving SVM \nthat is based on a reformulation of the SVM problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 67
                            }
                        ],
                        "text": "In all of the experiments we did not incoprorate a bias term since (Joachims, 2006; Kivinen et al., 2002; Zhang, 2004) do not incorporate that term either."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 85
                            }
                        ],
                        "text": "We run both Pegasos and SVM-Perf on the three datasets with values of . as given in (Joachims, 2006), \nnamely, ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 7
                            }
                        ],
                        "text": "T T is Joachims (2006) \nrecently suggested a t=1 t=1 method, called SVM-Perf, which requires O(R2/(. o2)) it\u00aderations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 79
                            }
                        ],
                        "text": "For completeness, we added to the table the runtime of SVMLight as reported in (Joachims, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5155714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "427b168f490b56716f22b129ac93aba5425ea08f",
            "isKey": true,
            "numCitedBy": 2122,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Linear Support Vector Machines (SVMs) have become one of the most prominent machine learning techniques for high-dimensional sparse data commonly encountered in applications like text classification, word-sense disambiguation, and drug design. These applications involve a large number of examples n as well as a large number of features N, while each example has only s << N non-zero features. This paper presents a Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n))for ordinal regression problems. The algorithm is based on an alternative, but equivalent formulation of the SVM optimization problem. Empirically, the Cutting-Plane Algorithm is several orders of magnitude faster than decomposition methods like svm light for large datasets."
            },
            "slug": "Training-linear-SVMs-in-linear-time-Joachims",
            "title": {
                "fragments": [],
                "text": "Training linear SVMs in linear time"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A Cutting Plane Algorithm for training linear SVMs that provably has training time 0(s,n) for classification problems and o(sn log (n)) for ordinal regression problems and several orders of magnitude faster than decomposition methods like svm light for large datasets."
            },
            "venue": {
                "fragments": [],
                "text": "KDD '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703537"
                        ],
                        "name": "Y. Freund",
                        "slug": "Y.-Freund",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Freund",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Freund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 10
                            }
                        ],
                        "text": "Following [16,24 ,10], the approach we take here is to directly minimize the primal problem while still using kernels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 81
                            }
                        ],
                        "text": "Online learn\u00ading algorithms \nwere also suggested as fast alternatives to SVM (see (Freund &#38; Schapire, 1999))."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 11
                            }
                        ],
                        "text": "Following (Freund \n&#38; Schapire, 1999; Kivinen et al., 2002), we outline a different approach and directly minimize the \nprimal problem while still using kernels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5885617,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2479a5cf6cefefb83166c612564787414e47131f",
            "isKey": false,
            "numCitedBy": 850,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce and analyze a new algorithm for linear classification which combines Rosenblatt's perceptron algorithm with Helmbold and Warmuth's leave-one-out method. Like Vapnik's maximal-margin classifier, our algorithm takes advantage of data that are linearly separable with large margins. Compared to Vapnik's algorithm, however, ours is much simpler to implement, and much more efficient in terms of computation time. We also show that our algorithm can be efficiently used in very high dimensional spaces using kernel functions. We performed some experiments using our algorithm, and some variants of it, for classifying images of handwritten digits. The performance of our algorithm is close to, but not as good as, the performance of maximal-margin classifiers on the same problem, while saving significantly on computation time and programming effort."
            },
            "slug": "Large-Margin-Classification-Using-the-Perceptron-Freund-Schapire",
            "title": {
                "fragments": [],
                "text": "Large Margin Classification Using the Perceptron Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A new algorithm for linear classification which combines Rosenblatt's perceptron algorithm with Helmbold and Warmuth's leave-one-out method is introduced, which is much simpler to implement, and much more efficient in terms of computation time."
            },
            "venue": {
                "fragments": [],
                "text": "COLT' 98"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713934"
                        ],
                        "name": "Antoine Bordes",
                        "slug": "Antoine-Bordes",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Bordes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Bordes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769472"
                        ],
                        "name": "S. Ertekin",
                        "slug": "S.-Ertekin",
                        "structuredName": {
                            "firstName": "Seyda",
                            "lastName": "Ertekin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ertekin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The parameters for the Reuters dataset are taken from [ 2 ], while those for the Adult dataset are from [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, we compare its runtime on three large datasets to the runtimes of the state-of-the-art solver SVM-Perf [21], a cutting plane algorithm designed specifically for use with sparse feature vectors, as well as of two more conventional SVM solvers: LASVM [ 2 ] and SVM-Light [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The parameters for the USPS and MNIST datasets are based on those in [ 2 ], but we increased the regularization parameters by a factor of 1,000."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14227081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07b54bac0159028aed116dbdbc2b747f723e585e",
            "isKey": true,
            "numCitedBy": 693,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Very high dimensional learning systems become theoretically possible when training examples are abundant. The computing cost then becomes the limiting factor. Any efficient learning algorithm should at least take a brief look at each example. But should all examples be given equal attention?This contribution proposes an empirical answer. We first present an online SVM algorithm based on this premise. LASVM yields competitive misclassification rates after a single pass over the training examples, outspeeding state-of-the-art SVM solvers. Then we show how active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels."
            },
            "slug": "Fast-Kernel-Classifiers-with-Online-and-Active-Bordes-Ertekin",
            "title": {
                "fragments": [],
                "text": "Fast Kernel Classifiers with Online and Active Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This contribution presents an online SVM algorithm based on the premise that active example selection can yield faster training, higher accuracies, and simpler models, using only a fraction of the training example labels."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 113
                            }
                        ],
                        "text": "Introduction Support Vector Machines (SVMs) are effective and popu\u00adlar \nclassi.cation learning tool (Vapnik, 1998; Cristianini &#38; Shawe-Taylor, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14727192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c04f8002e24a8c09bfbfedca3c6c346fe1e5d53",
            "isKey": false,
            "numCitedBy": 13495,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "From the publisher: This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory. SVMs deliver state-of-the-art performance in real-world applications such as text categorisation, hand-written character recognition, image classification, biosequences analysis, etc., and are now established as one of the standard tools for machine learning and data mining. Students will find the book both stimulating and accessible, while practitioners will be guided smoothly through the material required for a good grasp of the theory and its applications. The concepts are introduced gradually in accessible and self-contained stages, while the presentation is rigorous and thorough. Pointers to relevant literature and web sites containing software ensure that it forms an ideal starting point for further study. Equally, the book and its associated web site will guide practitioners to updated literature, new applications, and on-line software."
            },
            "slug": "An-Introduction-to-Support-Vector-Machines-and-Cristianini-Shawe-Taylor",
            "title": {
                "fragments": [],
                "text": "An Introduction to Support Vector Machines and Other Kernel-based Learning Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "This is the first comprehensive introduction to Support Vector Machines (SVMs), a new generation learning system based on recent advances in statistical learning theory, and will guide practitioners to updated literature, new applications, and on-line software."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 10
                            }
                        ],
                        "text": "Following [16,24 ,10], the approach we take here is to directly minimize the primal problem while still using kernels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": ", by Chapelle [10], is to rewrite the primal problem as a function of\u03b1 and then taking gradients with respect to \u03b1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "Tackling the primal objective directly was studied, for example, by Chap elle [10], who considered using smooth loss functions instead of the hinge loss, in which case the optimization problem becomes a smooth unconstrained optimization problem."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12601634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "835c1fa10bbe06730b55ccca95be239f9421e52c",
            "isKey": false,
            "numCitedBy": 820,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Most literature on support vector machines (SVMs) concentrates on the dual optimization problem. In this letter, we point out that the primal problem can also be solved efficiently for both linear and nonlinear SVMs and that there is no reason for ignoring this possibility. On the contrary, from the primal point of view, new families of algorithms for large-scale SVM training can be investigated."
            },
            "slug": "Training-a-Support-Vector-Machine-in-the-Primal-Chapelle",
            "title": {
                "fragments": [],
                "text": "Training a Support Vector Machine in the Primal"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is pointed out that the primal problem can also be solved efficiently for both linear and nonlinear SVMs and that there is no reason for ignoring this possibility."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the context of machine learning problems, the efficiency of the stochastic gradient approach has been studied in [ 1 ,3,5,6,26,27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207585383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5a767a341364de1f75bea85e0b12ba7d3586a461",
            "isKey": false,
            "numCitedBy": 2738,
            "numCiting": 78,
            "paperAbstract": {
                "fragments": [],
                "text": "When a parameter space has a certain underlying structure, the ordinary gradient of a function does not represent its steepest direction, but the natural gradient does. Information geometry is used for calculating the natural gradients in the parameter space of perceptrons, the space of matrices (for blind source separation), and the space of linear dynamical systems (for blind source deconvolution). The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters. This suggests that the plateau phenomenon, which appears in the backpropagation learning algorithm of multilayer perceptrons, might disappear or might not be so serious when the natural gradient is used. An adaptive method of updating the learning rate is proposed and analyzed."
            },
            "slug": "Natural-Gradient-Works-Efficiently-in-Learning-Amari",
            "title": {
                "fragments": [],
                "text": "Natural Gradient Works Efficiently in Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The dynamical behavior of natural gradient online learning is analyzed and is proved to be Fisher efficient, implying that it has asymptotically the same performance as the optimal batch estimation of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843103"
                        ],
                        "name": "Stephen P. Boyd",
                        "slug": "Stephen-P.-Boyd",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Boyd",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen P. Boyd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2014414"
                        ],
                        "name": "L. Vandenberghe",
                        "slug": "L.-Vandenberghe",
                        "structuredName": {
                            "firstName": "Lieven",
                            "lastName": "Vandenberghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vandenberghe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 37925315,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f607f03272e4d62708f5b2441355f9e005cb452",
            "isKey": false,
            "numCitedBy": 38849,
            "numCiting": 277,
            "paperAbstract": {
                "fragments": [],
                "text": "Convex optimization problems arise frequently in many different fields. A comprehensive introduction to the subject, this book shows in detail how such problems can be solved numerically with great efficiency. The focus is on recognizing convex optimization problems and then finding the most appropriate technique for solving them. The text contains many worked examples and homework exercises and will appeal to students, researchers and practitioners in fields such as engineering, computer science, mathematics, statistics, finance, and economics."
            },
            "slug": "Convex-Optimization-Boyd-Vandenberghe",
            "title": {
                "fragments": [],
                "text": "Convex Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A comprehensive introduction to the subject of convex optimization shows in detail how such problems can be solved numerically with great efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2653061"
                        ],
                        "name": "N. Murata",
                        "slug": "N.-Murata",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Murata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Murata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144362425"
                        ],
                        "name": "S. Amari",
                        "slug": "S.-Amari",
                        "structuredName": {
                            "firstName": "Shun\u2010ichi",
                            "lastName": "Amari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Amari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the context of machine learning problems, the efficiency of the stochastic gradient approach has been studied in [1,3,5,6,26, 27 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5624022,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3380f30e85577f67f7e178b70bf9f120ec16a3bc",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Statistical-analysis-of-learning-dynamics-Murata-Amari",
            "title": {
                "fragments": [],
                "text": "Statistical analysis of learning dynamics"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process."
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693407"
                        ],
                        "name": "K. Crammer",
                        "slug": "K.-Crammer",
                        "structuredName": {
                            "firstName": "Koby",
                            "lastName": "Crammer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Crammer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716876"
                        ],
                        "name": "O. Dekel",
                        "slug": "O.-Dekel",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Dekel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Dekel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771345"
                        ],
                        "name": "Joseph Keshet",
                        "slug": "Joseph-Keshet",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Keshet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Keshet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740765"
                        ],
                        "name": "Y. Singer",
                        "slug": "Y.-Singer",
                        "structuredName": {
                            "firstName": "Yoram",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Singer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5919882,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ca4d4229b8a843c0847fc70531790df6bd017ec",
            "isKey": false,
            "numCitedBy": 1781,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a unified view for online classification, regression, and uni-class problems. This view leads to a single algorithmic framework for the three problems. We prove worst case loss bounds for various algorithms for both the realizable case and the non-realizable case. A conversion of our main online algorithm to the setting of batch learning is also discussed. The end result is new algorithms and accompanying loss bounds for the hinge-loss."
            },
            "slug": "Online-Passive-Aggressive-Algorithms-Crammer-Dekel",
            "title": {
                "fragments": [],
                "text": "Online Passive-Aggressive Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This work presents a unified view for online classification, regression, and uni-class problems, and proves worst case loss bounds for various algorithms for both the realizable case and the non-realizable case."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "101750654"
                        ],
                        "name": "G. Kimeldorf",
                        "slug": "G.-Kimeldorf",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kimeldorf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kimeldorf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 121062339,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "3065c5e37a0c1f1be365e88ddf2d5cd02faa5db1",
            "isKey": false,
            "numCitedBy": 1334,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Some-results-on-Tchebycheffian-spline-functions-Kimeldorf-Wahba",
            "title": {
                "fragments": [],
                "text": "Some results on Tchebycheffian spline functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1971
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 53
                            }
                        ],
                        "text": "This claim has recently received formal treatment in [4,32] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "In a su bsequent paper [32], we analyze Pegasos and other SVM training methods from a machine learni ng perspective, and showed that Pegasos is more efficient than other methods when measur ing the runtime required to guarantee good predictive performance (test error)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5771157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c103834c918d18b79a1794f26c3422435768fcd",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss how the runtime of SVM optimization should decrease as the size of the training data increases. We present theoretical and empirical results demonstrating how a simple subgradient descent approach indeed displays such behavior, at least for linear kernels."
            },
            "slug": "SVM-optimization:-inverse-dependence-on-training-Shalev-Shwartz-Srebro",
            "title": {
                "fragments": [],
                "text": "SVM optimization: inverse dependence on training set size"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Tests are presented demonstrating how a simple subgradient descent approach indeed displays such behavior, at least for linear kernels, as the runtime of SVM optimization should decrease as the size of the training data increases."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '08"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770186"
                        ],
                        "name": "A. Benveniste",
                        "slug": "A.-Benveniste",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Benveniste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Benveniste"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47085520"
                        ],
                        "name": "M. M\u00e9tivier",
                        "slug": "M.-M\u00e9tivier",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "M\u00e9tivier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00e9tivier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2174182"
                        ],
                        "name": "P. Priouret",
                        "slug": "P.-Priouret",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Priouret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Priouret"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60753831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "990b10ce4ef643e148b6c719e99dbf2430671a74",
            "isKey": false,
            "numCitedBy": 1949,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive systems are widely encountered in many applications ranging through adaptive filtering and more generally adaptive signal processing, systems identification and adaptive control, to pattern recognition and machine intelligence: adaptation is now recognised as keystone of \"intelligence\" within computerised systems. These diverse areas echo the classes of models which conveniently describe each corresponding system. Thus although there can hardly be a \"general theory of adaptive systems\" encompassing both the modelling task and the design of the adaptation procedure, nevertheless, these diverse issues have a major common component: namely the use of adaptive algorithms, also known as stochastic approximations in the mathematical statistics literature, that is to say the adaptation procedure (once all modelling problems have been resolved). The juxtaposition of these two expressions in the title reflects the ambition of the authors to produce a reference work, both for engineers who use these adaptive algorithms and for probabilists or statisticians who would like to study stochastic approximations in terms of problems arising from real applications. Hence the book is organised in two parts, the first one user-oriented, and the second providing the mathematical foundations to support the practice described in the first part. The book covers the topcis of convergence, convergence rate, permanent adaptation and tracking, change detection, and is illustrated by various realistic applications originating from these areas of applications."
            },
            "slug": "Adaptive-Algorithms-and-Stochastic-Approximations-Benveniste-M\u00e9tivier",
            "title": {
                "fragments": [],
                "text": "Adaptive Algorithms and Stochastic Approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The juxtaposition of these two expressions in the title reflects the ambition of the authors to produce a reference work, both for engineers who use adaptive algorithms and for probabilists or statisticians who would like to study stochastic approximations in terms of problems arising from real applications."
            },
            "venue": {
                "fragments": [],
                "text": "Applications of Mathematics"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895207"
                        ],
                        "name": "C. Gentile",
                        "slug": "C.-Gentile",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Gentile",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gentile"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 137
                            }
                        ],
                        "text": "In contrast, by applying previ\u00ad . d o ously studied conversions of online algorithms in the PAC setting \n(e.g. (Cesa-Bianchi et al., 2004; Cesa-Bianchi &#38; Gentile, 2006)) one can obtain accuracy of o with \ncon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5386078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e615f619b842b603c533dac75b98880c2969edc",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Tight bounds are derived on the risk of models in the ensemble generated by incremental training of an arbitrary learning algorithm. The result is based on proof techniques that are remarkably different from the standard risk analysis based on uniform convergence arguments, and improves on previous bounds published by the same authors."
            },
            "slug": "Improved-Risk-Tail-Bounds-for-On-Line-Algorithms-Cesa-Bianchi-Gentile",
            "title": {
                "fragments": [],
                "text": "Improved Risk Tail Bounds for On-Line Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Tight bounds are derived on the risk of models in the ensemble generated by incremental training of an arbitrary learning algorithm based on uniform convergence arguments, and improves on previous bounds published by the same authors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 53
                            }
                        ],
                        "text": "This claim has recently received formal treatment in [4,32] ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "Recently, Bottou and Bousquet [4] proposed to analyse optim ization algorithms from the perspective of the underlying machine learning task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7431525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5936754b5762260bf102ac95d7b26cfc9d31956a",
            "isKey": false,
            "numCitedBy": 1493,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation-estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways."
            },
            "slug": "The-Tradeoffs-of-Large-Scale-Learning-Bottou-Bousquet",
            "title": {
                "fragments": [],
                "text": "The Tradeoffs of Large Scale Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms and shows distinct tradeoffs for the case of small-scale and large-scale learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 37
                            }
                        ],
                        "text": ") + [f(wt)] [f(wt)] 1 1 1 1 At-1 EAT Combining the above with Eq."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 186
                            }
                        ],
                        "text": "It is interesting to note that the performance \nof Pegasos does not depend on 1102 0.9 0.8 0.7 101 0.6 0.5 0.4 100 0.3 0.2 10-1 0.1 0 2 3 4 5 6 102 \n103 104 105 106 1010101010T T Figure 2.Comparisons of Pegasos to Norma (left) and Pegasos to stochastic \ngradient descent with a .xed learning rate (right) on the Astro-Physics datset."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "810 Combining \nthe above with Eq."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM Shai Shalev-Shwartz School of Computer Science \nand Engineering, The Hebrew University, Jerusalem, Israel Yoram Singer Google inc., Mountain View, \nUSA and The Hebrew University, Jerusalem, Israel Nathan Srebro Toyota Technological Institute, Chicago, \nUSA  Abstract We describe and analyze a simple and effec\u00adtive iterative algorithm for solving the optimiza\u00adtion \nproblem cast by Support Vector Machines (SVM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 99
                            }
                        ],
                        "text": "Introduction Support Vector Machines (SVMs) are effective and popu\u00adlar \nclassi.cation learning tool (Vapnik, 1998; Cristianini &#38; Shawe-Taylor, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 87
                            }
                        ],
                        "text": "Pegasos: Primal Estimated sub-GrAdient SOlver for SVM Shai Shalev-Shwartz School of Computer Science \nand Engineering, The Hebrew University, Jerusalem, Israel Yoram Singer Google inc., Mountain View, \nUSA and The Hebrew University, Jerusalem, Israel Nathan Srebro Toyota Technological Institute,\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28637672,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "385197d4c02593e2823c71e4f90a0993b703620e",
            "isKey": true,
            "numCitedBy": 26393,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "A comprehensive look at learning and generalization theory. The statistical theory of learning and generalization concerns the problem of choosing desired functions on the basis of empirical data. Highly applicable to a variety of computer science and robotics fields, this book offers lucid coverage of the theory as a whole. Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "slug": "Statistical-learning-theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "Statistical learning theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Presenting a method for determining the necessary and sufficient conditions for consistency of learning process, the author covers function estimates from small data pools, applying these estimations to real-life problems, and much more."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2700298"
                        ],
                        "name": "J. Spall",
                        "slug": "J.-Spall",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Spall",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Spall"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 121
                            }
                        ],
                        "text": "Stochastic gradient descent: The Pegasos algorithm is an application of a stochastic subgradient method (see for example [25,34])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59958747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "115e9ae70393b0d26463cd1ebc4109e7a2f5eeb9",
            "isKey": false,
            "numCitedBy": 1278,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \n* Unique in its survey of the range of topics. \n* Contains a strong, interdisciplinary format that will appeal to both students and researchers. \n* Features exercises and web links to software and data sets."
            },
            "slug": "Introduction-to-Stochastic-Search-and-Optimization-Spall",
            "title": {
                "fragments": [],
                "text": "Introduction to Stochastic Search and Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A survey of the range of topics, in a strong, interdisciplinary format that will appeal to both students and researchers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3319973"
                        ],
                        "name": "A. Stachurski",
                        "slug": "A.-Stachurski",
                        "structuredName": {
                            "firstName": "Andrzej",
                            "lastName": "Stachurski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stachurski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19584334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da45065be36213069eec538159138b7bc49cd208",
            "isKey": false,
            "numCitedBy": 573,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Yair Censor and Stavros A. Zenios, Oxford University Press, New York, 1997, 539 pp., ISBN 0-19-510062-X, $85.00"
            },
            "slug": "Parallel-Optimization:-Theory,-Algorithms-and-Stachurski",
            "title": {
                "fragments": [],
                "text": "Parallel Optimization: Theory, Algorithms and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Yair Censor and Stavros A. Zenios, Oxford University Press, New York, 1997, 539 pp."
            },
            "venue": {
                "fragments": [],
                "text": "Scalable Comput. Pract. Exp."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3085659"
                        ],
                        "name": "H. Kushner",
                        "slug": "H.-Kushner",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Kushner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kushner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145050324"
                        ],
                        "name": "G. Yin",
                        "slug": "G.-Yin",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Yin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Stochastic gradient descent The Pegasos algorithm is an application of a stochastic sub-gradient method (see for example [ 25 ,34])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 125672439,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "3d0568a6b073cf7f19b98f7f31fda254d93d70a0",
            "isKey": false,
            "numCitedBy": 755,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Applications and issues application to learning, state dependent noise and queueing applications to signal processing and adaptive control mathematical background convergence with probability one - Martingale difference noise convergence with probability one - correlated noise weak convergence - introduction weak convergence methods for general algorithms applications - proofs of convergence rate of convergence averaging of the iterates distributed/decentralized and asynchronous algorithms."
            },
            "slug": "Stochastic-Approximation-Algorithms-and-Kushner-Yin",
            "title": {
                "fragments": [],
                "text": "Stochastic Approximation Algorithms and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "Applications and issues application to learning, state dependent noise and queueing applications to signal processing and adaptive control mathematical background convergence with probability one, introduction weak convergence methods for general algorithms applications, proofs of convergence rate of convergence averaging of the iterates distributed/decentralized and asynchronous algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Applications of Mathematics"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the context of machine learning problems, the efficiency of the stochastic gradient approach has been studied in [1,3, 5 ,6,26,27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7247765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "133809cf62bf67f0a63b35e5ef5180d20c9aec19",
            "isKey": false,
            "numCitedBy": 402,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider situations where training data is abundant and computing resources are comparatively scarce. We argue that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm. Both theoretical and experimental evidences are presented."
            },
            "slug": "Large-Scale-Online-Learning-Bottou-LeCun",
            "title": {
                "fragments": [],
                "text": "Large Scale Online Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "It is argued that suitably designed online learning algorithms asymptotically outperform any batch learning algorithm in situations where training data is abundant and computing resources are comparatively scarce."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35106875"
                        ],
                        "name": "R. Duda",
                        "slug": "R.-Duda",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Duda",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Duda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3108177"
                        ],
                        "name": "P. Hart",
                        "slug": "P.-Hart",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12946615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b07ce649d6f6eb636872527104b0209d3edc8188",
            "isKey": false,
            "numCitedBy": 16989,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a unified, comprehensive and up-to-date treatment of both statistical and descriptive methods for pattern recognition. The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "slug": "Pattern-classification-and-scene-analysis-Duda-Hart",
            "title": {
                "fragments": [],
                "text": "Pattern classification and scene analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The topics treated include Bayesian decision theory, supervised and unsupervised learning, nonparametric techniques, discriminant analysis, clustering, preprosessing of pictorial data, spatial filtering, shape description techniques, perspective transformations, projective invariants, linguistic procedures, and artificial intelligence techniques for scene analysis."
            },
            "venue": {
                "fragments": [],
                "text": "A Wiley-Interscience publication"
            },
            "year": 1973
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 39
                            }
                        ],
                        "text": "Based on the bound given in Thm. 4 of (Kivinen \net al., 2002), the optimal choice of p is 0.5(2 + 0.5T-1/2 )1/2, which for t = 100 is in the range [0.7,0.716]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "Finally, we compare Pegasos to two pre viously proposed methods that are based on stochastic gradient descent: Norma [24] by Kivi nen, Smola, Williamson and to the method by Zhang [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 124
                            }
                        ],
                        "text": "Next, we compare Pegasos to two \npreviously proposed methods that are based on stochastic gradient descent, namely to Norma (Kivinen et \nal., 2002) and to the method given in (Zhang, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "The convergence rate given in [24] implies that the number of iterations required to achieve -accurate solution is O(1/(\u03bb )(2))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "4 from [24], suggests to set \u03b7t = p/(\u03bb \u221a t), wherep \u2208 (0, 1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 10
                            }
                        ],
                        "text": "Following [16,24 ,10], the approach we take here is to directly minimize the primal problem while still using kernels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 49
                            }
                        ],
                        "text": "Plugging the optimal value of pinto Thm. 4 in v (Kivinen et al., 2002) yields the bound O(1/(."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 54
                            }
                        ],
                        "text": "In our next experiment, we compared \nPegasos to Norma (Kivinen et al., 2002) and to a variant of stochastic gradient descent described in \n(Zhang, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 84
                            }
                        ],
                        "text": "In all of the experiments we did not incoprorate a bias term \nsince (Joachims, 2006; Kivinen et al., 2002; Zhang, 2004) do not incorporate that term either."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 52
                            }
                        ],
                        "text": "Furthermore, most methods we compare t o, including [21,24,37,18], do not incorporate a bias term either."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 36
                            }
                        ],
                        "text": "Following (Freund \n&#38; Schapire, 1999; Kivinen et al., 2002), we outline a different approach and directly minimize the \nprimal problem while still using kernels."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 134
                            }
                        ],
                        "text": "Two concrete \nalgorithms that are closely related to the Pegasos algorithm that are based on gradient methods are the \nNORMA algorithm (Kivinen et al., 2002) and a stochastic gradient algorithm by Zhang (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "4 in [24] yields the boundO(1/(\u03bb \u221a T ))."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "Two concrete algorithms that are closely related to the Pega sos lgorithm and are also variants of stochastic sub-gradient methods are the NORMA a lgorithm [24] and a stochastic gradient algorithm due to Zhang [37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Online lear  ning with kernels"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Signal Processing52(8), 2165\u20132176"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 32
                            }
                        ],
                        "text": "Based on the analysis given in \n(Zhang, 2004) we started by setting . to be 10-5."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 31
                            }
                        ],
                        "text": "Based on the analysis given in (Zhang, 2004) we started by setting \u03b7 to be10."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 174
                            }
                        ],
                        "text": "Next, we compare Pegasos to two \npreviously proposed methods that are based on stochastic gradient descent, namely to Norma (Kivinen et \nal., 2002) and to the method given in (Zhang, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 55
                            }
                        ],
                        "text": "We now turn to comparing Pegasos to the algorithm from (Zhang, 2004) which simply sets \u03b7t = \u03b7, where \u03b7 is a (fixed) small number."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 138
                            }
                        ],
                        "text": "In our next experiment, we compared \nPegasos to Norma (Kivinen et al., 2002) and to a variant of stochastic gradient descent described in \n(Zhang, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 35
                            }
                        ],
                        "text": ", 2002) and to the method given in (Zhang, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 106
                            }
                        ],
                        "text": "In all of the experiments we did not incoprorate a bias term \nsince (Joachims, 2006; Kivinen et al., 2002; Zhang, 2004) do not incorporate that term either."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 69
                            }
                        ],
                        "text": ", 2002) and to a variant of stochastic gradient descent described in (Zhang, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 56
                            }
                        ],
                        "text": "We now turn to comparing Pegasos to the algo\u00adrithm from (Zhang, 2004) which simply \nsets .t = ., where . is a (.xed) small number."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 195
                            }
                        ],
                        "text": "Two concrete \nalgorithms that are closely related to the Pegasos algorithm that are based on gradient methods are the \nNORMA algorithm (Kivinen et al., 2002) and a stochastic gradient algorithm by Zhang (2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 67
                            }
                        ],
                        "text": "In all of the experiments we did not incoprorate a bias term since (Joachims, 2006; Kivinen et al., 2002; Zhang, 2004) do not incorporate that term either."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Solving large scale linear prediction problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "67\u00d7 10\u22125 The parameters for the Reuters dataset are taken from [2], wh ile those for the Adult dataset are from [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 144
                            }
                        ],
                        "text": "\u2026for multiple choices of .. Decomposition methods: To overcome the quadratic memory requirement \nof IP methods, decomposition meth\u00adods such as SMO (Platt, 1998) and SVM-Light (Joachims, 1998) switch \nto the dual representation of the SVM opti\u00admization problem, and employ an active set of\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "Decomposition methods:To overcome the quadratic memory requirement of IP methods, decomposition methods such as SMO [29] and SVM-Light [20] ta ckle the dual representation of the SVM optimization problem, and employ an active se t of constraints thus working on a subset of dual variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 287
                            }
                        ],
                        "text": "The time spent by IP methods \nfor .nding a single accurate solution may, for instance, be better utilized for .nding numerous approximate \nsolutions for multiple choices of .. Decomposition methods: To overcome the quadratic memory requirement \nof IP methods, decomposition meth\u00adods such as SMO (Platt, 1998) and SVM-Light (Joachims, 1998) switch \nto the dual representation of the SVM opti\u00admization problem, and employ an active set of constraints \nthus working on a subset of dual variables."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast training of Support Vector Machines us  ing sequential minimal optimization"
            },
            "venue": {
                "fragments": [],
                "text": "B. Sch\u00f6lkopf, C. Burges, A. Smola (eds.) Advances in Kernel  M thods - Support Vector Learning. MIT Press"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2653061"
                        ],
                        "name": "N. Murata",
                        "slug": "N.-Murata",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Murata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Murata"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the context of machine learning problems, the efficiency of the stochastic gradient approach has been studied in [26,1,3,27,6,5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 116743726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebb23ce6440db11bfacf75e5e148b8472eb378ab",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Statistical-Study-on-On-line-Learning-Murata",
            "title": {
                "fragments": [],
                "text": "A Statistical Study on On-line Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66418228"
                        ],
                        "name": "\u4e38\u5c71 \u5fb9",
                        "slug": "\u4e38\u5c71-\u5fb9",
                        "structuredName": {
                            "firstName": "\u4e38\u5c71",
                            "lastName": "\u5fb9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u4e38\u5c71 \u5fb9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Proof Since ft is strongly convex and \u2207t is in the sub-gradient set of ft at wt we have that (see [30])"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "The lemma relies on the notion of strongly convex functions (see for example [30])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117573922,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b272701e77ddb860741a193ac1701ca382853680",
            "isKey": false,
            "numCitedBy": 8042,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Convex-Analysis\u306e\u4e8c,\u4e09\u306e\u9032\u5c55\u306b\u3064\u3044\u3066-\u4e38\u5c71",
            "title": {
                "fragments": [],
                "text": "Convex Analysis\u306e\u4e8c,\u4e09\u306e\u9032\u5c55\u306b\u3064\u3044\u3066"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "This bound was later improved by Smola et al [33] to O(md/(\u03bb ))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Bundle methods for m  achine learning"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 21"
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Solving large scale linear prediction problems using stochastic gradient descent algorithms . ICML ."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 116
                            }
                        ],
                        "text": "In the context of machine learning problems, the efficiency of the stochastic gradient approach has been studied in [26,1,3,27,6,5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Online algorithms and stochastic approximations"
            },
            "venue": {
                "fragments": [],
                "text": "D. Saad (ed.) Online Learning and Neural Networks. Cambridge University Press, Cambridge, UK"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 117
                            }
                        ],
                        "text": "In the context of machine learning problems, the efficiency of the stochastic gradient approach has been s tudied in [26,1,3,27,6,5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Online algorithms and stochastic approximat  ons"
            },
            "venue": {
                "fragments": [],
                "text": "D. Saad (ed.) Online Learning and Neural Networks. Cambridge University Press, Cambridge, U  K"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 121
                            }
                        ],
                        "text": "Stochastic gradient descent: The Pegasos algorithm is an application of a stochastic subgradient method (see for example [25,34])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic approximation algorit  hms and applications"
            },
            "venue": {
                "fragments": [],
                "text": "New-York: Springer-Verlag"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 164
                            }
                        ],
                        "text": "1 To do so, we can simply calculate the objective on the entire d ata set or estimate it according to a sample of sizeO(1/(\u03bb )), where is the desired accuracy (see [35])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast rat es for regularized objectives"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 22"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "Support Vector Machines (SVMs) are effective and popular cl ssification learning tool [36, 12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 113
                            }
                        ],
                        "text": "Introduction Support Vector Machines (SVMs) are effective and popu\u00adlar \nclassi.cation learning tool (Vapnik, 1998; Cristianini &#38; Shawe-Taylor, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Introduction to S  upport Vector Machines"
            },
            "venue": {
                "fragments": [],
                "text": "Cambridge University Press"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 266
                            }
                        ],
                        "text": "In particul ar, we compare its runtime on three large datasets to the runtimes of the state-of-the-art solv er SVM-Perf [21], a cutting plane algorithm designed specifically for use with sparse feature vectors, as well as of two more conventional SVM solvers: LASVM [2] and SVM-Light [20]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "67\u00d7 10\u22125 The parameters for the Reuters dataset are taken from [2], wh ile those for the Adult dataset are from [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "The parameters for the USPS and MNIST d atasets are based on those in [2], but we increased the regularization parameters by a f actor of 1000."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast kern  el classifiers with online and active learning"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Machine Learning Research  6, 1579\u20131619"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 116
                            }
                        ],
                        "text": "In the context of machine learning problems, the efficiency of the stochastic gradient approach has been studied in [26,1,3,27,6,5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic approximations and efficient learning"
            },
            "venue": {
                "fragments": [],
                "text": "M.A. Arbib (ed.) The Handbook of Brain Theory and Neural Networks, Second edition,. The MIT Press, Cambridge, MA"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 255
                            }
                        ],
                        "text": "Moreover, since decomposition methods .nd a feasible dual solution and their goal is to maximize \nthe dual objective function, they often result in a rather slow conver\u00adgence rate to the optimum of the \nprimal objective function (See also the discussion in (Hush et al., 2006))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Qp algori thms with guaranteed accuracy and run time for support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Machine Learning Resea  rch"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 117
                            }
                        ],
                        "text": "In the context of machine learning problems, the efficiency of the stochastic gradient approach has been s tudied in [26,1,3,27,6,5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Stochastic approximations and ef fici nt learning"
            },
            "venue": {
                "fragments": [],
                "text": "M.A. Arbib (ed.) The Handbook of Brain Theory and Neural Networks, Second editio n,. The MIT Press, Cambridge, MA"
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 28,
            "methodology": 22,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 53,
        "totalPages": 6
    },
    "page_url": "https://www.semanticscholar.org/paper/Pegasos:-primal-estimated-sub-gradient-solver-for-Shalev-Shwartz-Singer/9691f67f5075bde2fd70da0135a4a70f25ef042b?sort=total-citations"
}