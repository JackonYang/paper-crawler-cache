{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1597383891"
                        ],
                        "name": "Manjeet Dahiya",
                        "slug": "Manjeet-Dahiya",
                        "structuredName": {
                            "firstName": "Manjeet",
                            "lastName": "Dahiya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manjeet Dahiya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145083365"
                        ],
                        "name": "Sorav Bansal",
                        "slug": "Sorav-Bansal",
                        "structuredName": {
                            "firstName": "Sorav",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorav Bansal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28654424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5da199a2b340da3bec5aced418d7e52ccabda182",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Equivalence checking is an important building block for program synthesis and verification. For a synthesis tool to compete with modern compilers, its equivalence checker should be able to verify the transformations produced by these compilers. We find that the transformations produced by compilers are much varied and the presence of undefined behaviour allows them to produce even more aggressive optimizations. Previous work on equivalence checking has been done in the context of translation validation, where either a pass-by-pass based approach was employed or a set of handpicked optimizations were proven. These settings are not suitable for a synthesis tool where a black-box approach is required."
            },
            "slug": "Black-Box-Equivalence-Checking-Across-Compiler-Dahiya-Bansal",
            "title": {
                "fragments": [],
                "text": "Black-Box Equivalence Checking Across Compiler Optimizations"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is found that the transformations produced by compilers are much varied and the presence of undefined behaviour allows them to produce even more aggressive optimizations."
            },
            "venue": {
                "fragments": [],
                "text": "APLAS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1597383891"
                        ],
                        "name": "Manjeet Dahiya",
                        "slug": "Manjeet-Dahiya",
                        "structuredName": {
                            "firstName": "Manjeet",
                            "lastName": "Dahiya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manjeet Dahiya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145083365"
                        ],
                        "name": "Sorav Bansal",
                        "slug": "Sorav-Bansal",
                        "structuredName": {
                            "firstName": "Sorav",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorav Bansal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 94
                            }
                        ],
                        "text": "Adding assumptions to avoid undefined behavior is generally required for equivalence checking [8, 33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 28105062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a97b91dbbdae39cb3d9f0dd0d981a5b3d664d94a",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on equivalence checking for synthesis and translation validation has usually verified programs across selected optimizations, disabling the ones that exploit undefined behaviour. On the other hand, modern compilers extensively exploit language level undefined behaviour for optimization. Previous work on equivalence checking for translation validation and synthesis yields poor results, when such optimizations relying on undefined behaviour are enabled."
            },
            "slug": "Modeling-Undefined-Behaviour-Semantics-for-Checking-Dahiya-Bansal",
            "title": {
                "fragments": [],
                "text": "Modeling Undefined Behaviour Semantics for Checking Equivalence Across Compiler Optimizations"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper verifies equivalence checking for translation validation and synthesis across selected optimizations, when such optimizations relying on undefined behaviour are enabled."
            },
            "venue": {
                "fragments": [],
                "text": "Haifa Verification Conference"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145501438"
                        ],
                        "name": "G. Barthe",
                        "slug": "G.-Barthe",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Barthe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Barthe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058324896"
                        ],
                        "name": "Juan Manuel Crespo",
                        "slug": "Juan-Manuel-Crespo",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Crespo",
                            "middleNames": [
                                "Manuel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Manuel Crespo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372002"
                        ],
                        "name": "C\u00e9sar Kunz",
                        "slug": "C\u00e9sar-Kunz",
                        "structuredName": {
                            "firstName": "C\u00e9sar",
                            "lastName": "Kunz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C\u00e9sar Kunz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 170
                            }
                        ],
                        "text": "unrolling) to the programs so that the loop executions in the transformed programs are in correspondence, but this is brittle in the presence of unforeseen optimizations [3, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 239
                            }
                        ],
                        "text": "Program equivalence checking is commonly performed in two stages: the first stage is to construct a product program for the two programs by aligning them, and the second is proving a safety property, or invariant, of the resulting program [4, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "24 if (cp[4] == 0) return cp - str + 4;"
                    },
                    "intents": []
                }
            ],
            "corpusId": 5854250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76fee609358896578939bf3dcb34eb7aedea0e1c",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Relational program logics are formalisms for specifying and verifying properties about two programs or two runs of the same program. These properties range from correctness of compiler optimizations or equivalence between two implementations of an abstract data type, to properties like non-interference or determinism. Yet the current technology for relational verification remains underdeveloped. We provide a general notion of product program that supports a direct reduction of relational verification to standard verification. We illustrate the benefits of our method with selected examples, including non-interference, standard loop optimizations, and a state-of-the-art optimization for incremental computation. All examples have been verified using the Why tool."
            },
            "slug": "Relational-Verification-Using-Product-Programs-Barthe-Crespo",
            "title": {
                "fragments": [],
                "text": "Relational Verification Using Product Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work provides a general notion of product program that supports a direct reduction of relational verification to standard verification, and illustrates the benefits of the method with selected examples, including non-interference, standard loop optimizations, and a state-of-the-art optimization for incremental computation."
            },
            "venue": {
                "fragments": [],
                "text": "FM"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38628703"
                        ],
                        "name": "Anna Zaks",
                        "slug": "Anna-Zaks",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Zaks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Zaks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698938"
                        ],
                        "name": "A. Pnueli",
                        "slug": "A.-Pnueli",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Pnueli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pnueli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 246,
                                "start": 239
                            }
                        ],
                        "text": "Program equivalence checking is commonly performed in two stages: the first stage is to construct a product program for the two programs by aligning them, and the second is proving a safety property, or invariant, of the resulting program [4, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9568105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43c11eae3ceb570ef627e502a3f041f0cf9a0c06",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a deductive framework for proving program equivalence and its application to automatic verification of transformations performed by optimizing compilers. To leverage existing program analysis techniques, we reduce the equivalence checking problem to analysis of one system --- a cross-product of the two input programs. We show how the approach can be effectively used for checking equivalence of consonant (i.e., structurally similar) programs. Finally, we report on the prototype tool that applies the developed methodology to verify that a compiler optimization run preserves the program semantics. Unlike existing frameworks, CoVaC accommodates absence of compiler annotations and handles most of the classical intraprocedural optimizations such as constant folding, reassociation, common subexpression elimination, code motion, dead code elimination, branch optimizations, and others."
            },
            "slug": "CoVaC:-Compiler-Validation-by-Program-Analysis-of-Zaks-Pnueli",
            "title": {
                "fragments": [],
                "text": "CoVaC: Compiler Validation by Program Analysis of the Cross-Product"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A deductive framework for proving program equivalence and its application to automatic verification of transformations performed by optimizing compilers, which accommodates absence of compiler annotations and handles most of the classical intraprocedural optimizations."
            },
            "venue": {
                "fragments": [],
                "text": "FM"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1772079"
                        ],
                        "name": "Sven Verdoolaege",
                        "slug": "Sven-Verdoolaege",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Verdoolaege",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sven Verdoolaege"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272876"
                        ],
                        "name": "Gerda Janssens",
                        "slug": "Gerda-Janssens",
                        "structuredName": {
                            "firstName": "Gerda",
                            "lastName": "Janssens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerda Janssens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743332"
                        ],
                        "name": "M. Bruynooghe",
                        "slug": "M.-Bruynooghe",
                        "structuredName": {
                            "firstName": "Maurice",
                            "lastName": "Bruynooghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bruynooghe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [38] the authors use abstract interpretation to summarize loops with a polyhedral domain."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2935604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ab99e4854c526dc80f4cced40748f1369f10fad",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Designers often apply manual or semi-automatic loop and data transformations on array- and loop-intensive programs to improve performance. It is crucial that such transformations preserve the functionality of the program. This article presents an automatic method for constructing equivalence proofs for the class of static affine programs. The equivalence checking is performed on a dependence graph abstraction and uses a new approach based on widening to find the proper induction hypotheses for reasoning about recurrences. Unlike transitive-closure-based approaches, this widening approach can also handle nonuniform recurrences. The implementation is publicly available and is the first of its kind to fully support commutative operations."
            },
            "slug": "Equivalence-checking-of-static-affine-programs-to-Verdoolaege-Janssens",
            "title": {
                "fragments": [],
                "text": "Equivalence checking of static affine programs using widening to handle recurrences"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This article presents an automatic method for constructing equivalence proofs for the class of static affine programs and uses a new approach based on widening to find the proper induction hypotheses for reasoning about recurrences."
            },
            "venue": {
                "fragments": [],
                "text": "TOPL"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50981013"
                        ],
                        "name": "Shubhani Gupta",
                        "slug": "Shubhani-Gupta",
                        "structuredName": {
                            "firstName": "Shubhani",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shubhani Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9251859"
                        ],
                        "name": "Aseem Saxena",
                        "slug": "Aseem-Saxena",
                        "structuredName": {
                            "firstName": "Aseem",
                            "lastName": "Saxena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aseem Saxena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061287366"
                        ],
                        "name": "Anmol Mahajan",
                        "slug": "Anmol-Mahajan",
                        "structuredName": {
                            "firstName": "Anmol",
                            "lastName": "Mahajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anmol Mahajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145083365"
                        ],
                        "name": "Sorav Bansal",
                        "slug": "Sorav-Bansal",
                        "structuredName": {
                            "firstName": "Sorav",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorav Bansal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 109
                            }
                        ],
                        "text": "The cost of the technique is superexponential in \u03bc in the worst case and reported results are only for \u03bc = 1 [7, 16]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "We use the counterexamples from the SMT solver to eliminate other proof obligations that are demonstrably false, as in [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4645653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c4ae69a1a6928b3042f8c66757ada6fd12d0b24",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Program equivalence checking is a fundamental problem in computer science with applications to translation validation and automatic synthesis of compiler optimizations. Contemporary equivalence checkers employ SMT solvers to discharge proof obligations generated by their equivalence checking algorithm. Equivalence checkers also involve algorithms to infer invariants that relate the intermediate states of the two programs being compared for equivalence. We present a new algorithm, called invariant-sketching, that allows the inference of the required invariants through the generation of counter-examples using SMT solvers. We also present an algorithm, called query-decomposition, that allows a more capable use of SMT solvers for application to equivalence checking. Both invariant-sketching and query-decomposition help us prove equivalence across program transformations that could not be handled by previous equivalence checking algorithms."
            },
            "slug": "Effective-Use-of-SMT-Solvers-for-Program-Checking-Gupta-Saxena",
            "title": {
                "fragments": [],
                "text": "Effective Use of SMT Solvers for Program Equivalence Checking Through Invariant-Sketching and Query-Decomposition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A new algorithm is presented, called invariant-sketching, that allows the inference of the required invariants through the generation of counter-examples using SMT solvers for application to equivalence checking."
            },
            "venue": {
                "fragments": [],
                "text": "SAT"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33594132"
                        ],
                        "name": "Sudipta Kundu",
                        "slug": "Sudipta-Kundu",
                        "structuredName": {
                            "firstName": "Sudipta",
                            "lastName": "Kundu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sudipta Kundu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2272813"
                        ],
                        "name": "Zachary Tatlock",
                        "slug": "Zachary-Tatlock",
                        "structuredName": {
                            "firstName": "Zachary",
                            "lastName": "Tatlock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zachary Tatlock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145655689"
                        ],
                        "name": "Sorin Lerner",
                        "slug": "Sorin-Lerner",
                        "structuredName": {
                            "firstName": "Sorin",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorin Lerner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "For example, in [21] the authors prove the correctness of some difficult compiler optimizations, such as loop interchange, by using programmer-supplied templates which imply a correspondence between loop-free code fragments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15060665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d9579e31aabd30b752ade4064b965de76e3ce77",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Translation validation is a technique for checking that, after an optimization has run, the input and output of the optimization are equivalent. Traditionally, translation validation has been used to prove concrete, fully specified programs equivalent. In this paper we present Parameterized Equivalence Checking (PEC), a generalization of translation validation that can prove the equivalence of parameterized programs. A parameterized program is a partially specified program that can represent multiple concrete programs. For example, a parameterized program may contain a section of code whose only known property is that it does not modify certain variables. By proving parameterized programs equivalent, PEC can prove the correctness of transformation rules that represent complex optimizations once and for all, before they are ever run. We implemented our PEC technique in a tool that can establish the equivalence of two parameterized programs. To highlight the power of PEC, we designed a language for implementing complex optimizations using many-to-many rewrite rules, and used this language to implement a variety of optimizations including software pipelining, loop unrolling, loop unswitching, loop interchange, and loop fusion. Finally, to demonstrate the effectiveness of PEC, we used our PEC implementation to verify that all the optimizations we implemented in our language preserve program behavior."
            },
            "slug": "Proving-optimizations-correct-using-parameterized-Kundu-Tatlock",
            "title": {
                "fragments": [],
                "text": "Proving optimizations correct using parameterized program equivalence"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Parameterized Equivalence Checking (PEC), a generalization of translation validation that can prove the equivalence of parameterized programs, is presented."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790411"
                        ],
                        "name": "G. Necula",
                        "slug": "G.-Necula",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Necula",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Necula"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 77
                            }
                        ],
                        "text": "We believe techniques that depend on syntactic alignment of the two programs [13, 14, 26, 27, 32] fail on most or all of our benchmarks, including at least 47 benchmarks where loop unrolling has been performed (usually as part of vectorization)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 23
                            }
                        ],
                        "text": "Translation validation [26, 31, 34] uses compiler instrumentation to help generate a simulation relation to prove the correctness of compiler optimizations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 214
                            }
                        ],
                        "text": "Equivalence checking, the problem of formally proving that two functions or programs are semantically equivalent, is a long-standing and important problem; applications include verification of compiler correctness [26], superoptimization [3, 6], program synthesis [29], and verifying the correctness of code refactoring [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "Examples include past work on data-driven equivalence checking [32], Necula\u2019s well-known translation validation work [26], and others [13, 14, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2448939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "011f7da0095ac8c0d4477eeda2728e5f80a35767",
            "isKey": true,
            "numCitedBy": 486,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a translation validation infrastructure for the GNU C compiler. During the compilation the infrastructure compares the intermediate form of the program before and after each compiler pass and verifies the preservation of semantics. We discuss a general framework that the optimizer can use to communicate to the validator what transformations were performed. Our implementation however does not rely on help from the optimizer and it is quite successful by using instead a few heuristics to detect the transformations that take place.\nThe main message of this paper is that a practical translation validation infrastructure, able to check the correctness of many of the transformations performed by a realistic compiler, can be implemented with about the effort typically required to implement one compiler pass. We demonstrate this in the context of the GNU C compiler for a number of its optimizations while compiling realistic programs such as the compiler itself or the Linux kernel. We believe that the price of such an infrastructure is small considering the qualitative increase in the ability to isolate compilation errors during compiler testing and maintenance."
            },
            "slug": "Translation-validation-for-an-optimizing-compiler-Necula",
            "title": {
                "fragments": [],
                "text": "Translation validation for an optimizing compiler"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A practical translation validation infrastructure, able to check the correctness of many of the transformations performed by a realistic compiler, can be implemented with about the effort typically required to implement one compiler pass."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145781093"
                        ],
                        "name": "David A. Ramos",
                        "slug": "David-A.-Ramos",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ramos",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Ramos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373056"
                        ],
                        "name": "D. Engler",
                        "slug": "D.-Engler",
                        "structuredName": {
                            "firstName": "Dawson",
                            "lastName": "Engler",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Engler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 324,
                                "start": 320
                            }
                        ],
                        "text": "Equivalence checking, the problem of formally proving that two functions or programs are semantically equivalent, is a long-standing and important problem; applications include verification of compiler correctness [26], superoptimization [3, 6], program synthesis [29], and verifying the correctness of code refactoring [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "UC-KLEE [28] explores a finite number of paths through a pair of programs to find differences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30486598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "120c819da02fcb312986ac492f723ef9ea3223b5",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Verifying code equivalence is useful in many situations, such as checking: yesterday's code against today's, different implementations of the same (standardized) interface, or an optimized routine against a reference implementation. We present a tool designed to easily check the equivalence of two arbitrary C functions. The tool provides guarantees far beyond those possible with testing, yet it often requires less work than writing even a single test case. It automatically synthesizes inputs to the routines and uses bit-accurate, sound symbolic execution to verify that they produce equivalent outputs on a finite number of paths, even for rich, nested data structures. We show that the approach works well, even on heavily-tested code, where it finds interesting errors and gets high statement coverage, often exhausting all feasible paths for a given input size. We also show how the simple trick of checking equivalence of identical code turns the verification tool chain against itself, finding errors in the underlying compiler and verification tool."
            },
            "slug": "Practical,-Low-Effort-Equivalence-Verification-of-Ramos-Engler",
            "title": {
                "fragments": [],
                "text": "Practical, Low-Effort Equivalence Verification of Real Code"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A tool designed to easily check the equivalence of two arbitrary C functions, which provides guarantees far beyond those possible with testing, yet it often requires less work than writing even a single test case."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8599261"
                        ],
                        "name": "E. D. Angelis",
                        "slug": "E.-D.-Angelis",
                        "structuredName": {
                            "firstName": "Emanuele",
                            "lastName": "Angelis",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. D. Angelis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028917"
                        ],
                        "name": "F. Fioravanti",
                        "slug": "F.-Fioravanti",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Fioravanti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fioravanti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716699"
                        ],
                        "name": "A. Pettorossi",
                        "slug": "A.-Pettorossi",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Pettorossi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pettorossi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2415169"
                        ],
                        "name": "M. Proietti",
                        "slug": "M.-Proietti",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Proietti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Proietti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 28
                            }
                        ],
                        "text": "In contrast, the authors of [9, 10, 25] use constrained Horn clauses to summarize the entire execution of two programs and then use Horn clause solvers to prove equivalence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "To make the problem tractable, the authors introduce a transformation called predicate pairing [9], where predicates from the two programs are combined into one predicate that models both programs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7007351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4691ae5b94639bf4723078b128fe508f6d6cebec",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for verifying relational program properties, that is, properties that relate the input and the output of two programs. Our verification method is parametric with respect to the definition of the operational semantics of the programming language in which the two programs are written. That definition of the semantics consists of a set Int of constrained Horn clauses (CHCs) that encode the interpreter of the programming language. Then, given the programs and the relational property we want to verify, we generate, by using Int, a set of constrained Horn clauses whose satisfiability is equivalent to the validity of the property. Unfortunately, state-of-the-art solvers for CHCs have severe limitations in proving the satisfiability, or the unsatisfiability, of such sets of clauses. We propose some transformation techniques that increase the power of CHC solvers when verifying relational properties. We show that these transformations, based on unfold/fold rules, preserve satisfiability. Through an experimental evaluation, we show that in many cases CHC solvers are able to prove the satisfiability (or the unsatisfiability) of sets of clauses obtained by applying the transformations we propose, whereas the same solvers are unable to perform those proofs when given as input the original, untransformed sets of CHCs."
            },
            "slug": "Relational-Verification-Through-Horn-Clause-Angelis-Fioravanti",
            "title": {
                "fragments": [],
                "text": "Relational Verification Through Horn Clause Transformation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Through an experimental evaluation, it is shown that in many cases CHC solvers are able to prove the satisfiability (or the unsatisfiability) of sets of clauses obtained by applying the transformations proposed, whereas the samesolvers are unable to perform those proofs when given as input the original, untransformed sets of CHCs."
            },
            "venue": {
                "fragments": [],
                "text": "SAS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145501438"
                        ],
                        "name": "G. Barthe",
                        "slug": "G.-Barthe",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Barthe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Barthe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058324896"
                        ],
                        "name": "Juan Manuel Crespo",
                        "slug": "Juan-Manuel-Crespo",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Crespo",
                            "middleNames": [
                                "Manuel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan Manuel Crespo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108314"
                        ],
                        "name": "Sumit Gulwani",
                        "slug": "Sumit-Gulwani",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Gulwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sumit Gulwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2372002"
                        ],
                        "name": "C\u00e9sar Kunz",
                        "slug": "C\u00e9sar-Kunz",
                        "structuredName": {
                            "firstName": "C\u00e9sar",
                            "lastName": "Kunz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C\u00e9sar Kunz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144022768"
                        ],
                        "name": "Mark Marron",
                        "slug": "Mark-Marron",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Marron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Marron"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 238
                            }
                        ],
                        "text": "Equivalence checking, the problem of formally proving that two functions or programs are semantically equivalent, is a long-standing and important problem; applications include verification of compiler correctness [26], superoptimization [3, 6], program synthesis [29], and verifying the correctness of code refactoring [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "23 if (cp[3] == 0) return cp - str + 3;"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 61
                            }
                        ],
                        "text": "As a result, prior automated equivalence checking approaches [3, 7, 13, 14, 27, 32] fail on this example."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 170
                            }
                        ],
                        "text": "unrolling) to the programs so that the loop executions in the transformed programs are in correspondence, but this is brittle in the presence of unforeseen optimizations [3, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In [3] the authors suggest unrolling one loop and then attempting a syntactic alignment."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 107
                            }
                        ],
                        "text": "A similar problem arises if one labels a transition a;a\u2032 instead of ab;a\u2032, as is the case in works such as [3, 32] where loop iterations are assumed to be in one-to-one correspondence."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1632368,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "186382ac3fbef20de16612a09080c6911ba5d7e1",
            "isKey": true,
            "numCitedBy": 53,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Existing pattern-based compiler technology is unable to effectively exploit the full potential of SIMD architectures. We present a new program synthesis based technique for auto-vectorizing performance critical innermost loops. Our synthesis technique is applicable to a wide range of loops, consistently produces performant SIMD code, and generates correctness proofs for the output code. The synthesis technique, which leverages existing work on relational verification methods, is a novel combination of deductive loop restructuring, synthesis condition generation and a new inductive synthesis algorithm for producing loop-free code fragments. The inductive synthesis algorithm wraps an optimized depth-first exploration of code sequences inside a CEGIS loop. Our technique is able to quickly produce SIMD implementations (up to 9 instructions in 0.12 seconds) for a wide range of fundamental looping structures. The resulting SIMD implementations outperform the original loops by 2.0x-3.7x."
            },
            "slug": "From-relational-verification-to-SIMD-loop-synthesis-Barthe-Crespo",
            "title": {
                "fragments": [],
                "text": "From relational verification to SIMD loop synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The synthesis technique is a novel combination of deductive loop restructuring, synthesis condition generation and a new inductive synthesis algorithm for producing loop-free code fragments, which is able to quickly produce SIMD implementations for a wide range of fundamental looping structures."
            },
            "venue": {
                "fragments": [],
                "text": "PPoPP '13"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144509790"
                        ],
                        "name": "Nuno P. Lopes",
                        "slug": "Nuno-P.-Lopes",
                        "structuredName": {
                            "firstName": "Nuno",
                            "lastName": "Lopes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nuno P. Lopes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35243870"
                        ],
                        "name": "J. Monteiro",
                        "slug": "J.-Monteiro",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Monteiro",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Monteiro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [23] the authors use a sequential composition of programs (as in Figure 1c), and attempt to summarize them by solving integer recurrences."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13898279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bcf92501af0b1b8222d60817b3dfbeba2a16298",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "Proving equivalence of programs has several important applications, including algorithm recognition, regression checking, compiler optimization verification and validation, and information flow checking. Despite being a topic with so many important applications, program equivalence checking has seen little advances over the past decades due to its inherent (high) complexity. In this paper, we propose, to the best of our knowledge, the first semi-algorithm for the automatic verification of partial equivalence of two programs over the combined theory of uninterpreted function symbols and integer arithmetic (UF+IA). The proposed algorithm supports, in particular, programs with nested loops. The crux of the technique is a transformation of uninterpreted functions (UFs) applications into integer polynomials, which enables the precise summarization of loops with UF applications using recurrences. The equivalence checking algorithm then proceeds on loop-free, integer only programs. We implemented the proposed technique in CORK, a tool that automatically verifies the correctness of compiler optimizations, and we show that it can prove more optimizations correct than state-of-the-art techniques."
            },
            "slug": "Automatic-equivalence-checking-of-programs-with-and-Lopes-Monteiro",
            "title": {
                "fragments": [],
                "text": "Automatic equivalence checking of programs with uninterpreted functions and integer arithmetic"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes the first semi-algorithm for the automatic verification of partial equivalence of two programs over the combined theory of uninterpreted function symbols and integer arithmetic (UF+IA), and shows that it can prove more optimizations correct than state-of-the-art techniques."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Software Tools for Technology Transfer"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2941500"
                        ],
                        "name": "Dennis Felsing",
                        "slug": "Dennis-Felsing",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Felsing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dennis Felsing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770610"
                        ],
                        "name": "S. Grebing",
                        "slug": "S.-Grebing",
                        "structuredName": {
                            "firstName": "Sarah",
                            "lastName": "Grebing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grebing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789960"
                        ],
                        "name": "V. Klebanov",
                        "slug": "V.-Klebanov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Klebanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Klebanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739683"
                        ],
                        "name": "Philipp R\u00fcmmer",
                        "slug": "Philipp-R\u00fcmmer",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "R\u00fcmmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp R\u00fcmmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34752863"
                        ],
                        "name": "Mattias Ulbrich",
                        "slug": "Mattias-Ulbrich",
                        "structuredName": {
                            "firstName": "Mattias",
                            "lastName": "Ulbrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mattias Ulbrich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 77
                            }
                        ],
                        "text": "We believe techniques that depend on syntactic alignment of the two programs [13, 14, 26, 27, 32] fail on most or all of our benchmarks, including at least 47 benchmarks where loop unrolling has been performed (usually as part of vectorization)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 61
                            }
                        ],
                        "text": "As a result, prior automated equivalence checking approaches [3, 7, 13, 14, 27, 32] fail on this example."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 134
                            }
                        ],
                        "text": "Examples include past work on data-driven equivalence checking [32], Necula\u2019s well-known translation validation work [26], and others [13, 14, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1369504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b52c5fd1b6ad0e84065f7801a771673f765f948",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Regression verification is an approach complementing regression testing with formal verification. The goal is to formally prove that two versions of a program behave either equally or differently in a precisely specified way. In this paper, we present a novel automatic approach for regression verification that reduces the equivalence of two related imperative integer programs to Horn constraints over uninterpreted predicates. Subsequently, state-of-the-art SMT solvers are used to solve the constraints. We have implemented the approach, and our experiments show non-trivial integer programs that can now be proved equivalent without further user input."
            },
            "slug": "Automating-regression-verification-Felsing-Grebing",
            "title": {
                "fragments": [],
                "text": "Automating regression verification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A novel automatic approach for regression verification is presented that reduces the equivalence of two related imperative integer programs to Horn constraints over uninterpreted predicates and shows non-trivial integer programs that can now be proved equivalent without further user input."
            },
            "venue": {
                "fragments": [],
                "text": "Software Engineering & Management"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736858"
                        ],
                        "name": "Grigory Fedyukovich",
                        "slug": "Grigory-Fedyukovich",
                        "structuredName": {
                            "firstName": "Grigory",
                            "lastName": "Fedyukovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grigory Fedyukovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2850298"
                        ],
                        "name": "A. Gurfinkel",
                        "slug": "A.-Gurfinkel",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Gurfinkel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Gurfinkel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704762"
                        ],
                        "name": "N. Sharygina",
                        "slug": "N.-Sharygina",
                        "structuredName": {
                            "firstName": "Natasha",
                            "lastName": "Sharygina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Sharygina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 77
                            }
                        ],
                        "text": "We believe techniques that depend on syntactic alignment of the two programs [13, 14, 26, 27, 32] fail on most or all of our benchmarks, including at least 47 benchmarks where loop unrolling has been performed (usually as part of vectorization)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 61
                            }
                        ],
                        "text": "As a result, prior automated equivalence checking approaches [3, 7, 13, 14, 27, 32] fail on this example."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 134
                            }
                        ],
                        "text": "Examples include past work on data-driven equivalence checking [32], Necula\u2019s well-known translation validation work [26], and others [13, 14, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9179861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cecf35ad3902d531f9fa830c2f2f823a51132bd0",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents SimAbs, the first fully automated SMT-based approach to synthesize an abstraction of one program called target that simulates another program called source. SimAbs iteratively traverses the search space of existential abstractions of the target and choses the strongest abstraction among them that simulates the source. Deciding whether a given relation is a simulation relation is reduced to solving validity of $$\\forall \\exists $$-formulas iteratively. We present a novel algorithm for dealing with such formulas using an incremental SMT solver. In addition to deciding validity, our algorithm extracts witnessing Skolem relations which further drive simulation synthesis in SimAbs. Our evaluation confirms that SimAbs is able to efficiently discover both, simulations and abstractions, for C programs from the Software Verification Competition."
            },
            "slug": "Automated-Discovery-of-Simulation-Between-Programs-Fedyukovich-Gurfinkel",
            "title": {
                "fragments": [],
                "text": "Automated Discovery of Simulation Between Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This paper presents SimAbs, the first fully automated SMT-based approach to synthesize an abstraction of one program called target that simulates another program called source, and presents a novel algorithm for dealing with such formulas using an incremental SMT solver."
            },
            "venue": {
                "fragments": [],
                "text": "LPAR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144692078"
                        ],
                        "name": "C. Flanagan",
                        "slug": "C.-Flanagan",
                        "structuredName": {
                            "firstName": "Cormac",
                            "lastName": "Flanagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Flanagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145189240"
                        ],
                        "name": "K. Leino",
                        "slug": "K.-Leino",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Leino",
                            "middleNames": [
                                "Rustan",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Leino"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "We perform a Houdini-style [15] fixed point computation to reduce the set of learned invariants to those that can be proven by induction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1534849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02b72a79f17d7d86bb7b1d1e8ff8f659ca2bb1f0",
            "isKey": false,
            "numCitedBy": 418,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A static program checker that performs modular checking can check one program module for errors without needing to analyze the entire program. Modular checking requires that each module be accompanied by annotations that specify the module. To help reduce the cost of writing specifications, this paper presents Houdini, an annotation assistant for the modular checker ESC/Java. To infer suitable ESC/Java annotations for a given program, Houdini generates a large number of candidate annotations and uses ESC/Java to verify or refute each of these annotations. The paper describes the design, implementation, and preliminary evaluation of Houdini."
            },
            "slug": "Houdini,-an-Annotation-Assistant-for-ESC/Java-Flanagan-Leino",
            "title": {
                "fragments": [],
                "text": "Houdini, an Annotation Assistant for ESC/Java"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Houdini is presented, an annotation assistant for the modular checker ESC/Java, which generates a large number of candidate annotations and uses ESC/ Java to verify or refute each of these annotations."
            },
            "venue": {
                "fragments": [],
                "text": "FME"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145195327"
                        ],
                        "name": "Thomas Sewell",
                        "slug": "Thomas-Sewell",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Sewell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Sewell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709169"
                        ],
                        "name": "Magnus O. Myreen",
                        "slug": "Magnus-O.-Myreen",
                        "structuredName": {
                            "firstName": "Magnus",
                            "lastName": "Myreen",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Magnus O. Myreen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680559"
                        ],
                        "name": "Gerwin Klein",
                        "slug": "Gerwin-Klein",
                        "structuredName": {
                            "firstName": "Gerwin",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerwin Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 23
                            }
                        ],
                        "text": "Translation validation [26, 31, 34] uses compiler instrumentation to help generate a simulation relation to prove the correctness of compiler optimizations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10031264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "76869a552436407f5f18f3c6ba7e54153776cd28",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We extend the existing formal verification of the seL4 operating system microkernel from 9500 lines of C source code to the binary level. We handle all functions that were part of the previous verification. Like the original verification, we currently omit the assembly routines and volatile accesses used to control system hardware. More generally, we present an approach for proving refinement between the formal semantics of a program on the C source level and its formal semantics on the binary level, thus checking the validity of compilation, including some optimisations, and linking, and extending static properties proved of the source code to the executable. We make use of recent improvements in SMT solvers to almost fully automate this process. We handle binaries generated by unmodified gcc 4.5.1 at optimisation level 1, and can handle most of seL4 even at optimisation level 2."
            },
            "slug": "Translation-validation-for-a-verified-OS-kernel-Sewell-Myreen",
            "title": {
                "fragments": [],
                "text": "Translation validation for a verified OS kernel"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An approach for proving refinement between the formal semantics of a program on the C source level and its formal semantics on the binary level, thus checking the validity of compilation, including some optimisations, and linking, and extending static properties proved of the source code to the executable is presented."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI 2013"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6331083"
                        ],
                        "name": "R. Tate",
                        "slug": "R.-Tate",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Tate",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30073551"
                        ],
                        "name": "M. Stepp",
                        "slug": "M.-Stepp",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stepp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stepp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2272813"
                        ],
                        "name": "Zachary Tatlock",
                        "slug": "Zachary-Tatlock",
                        "structuredName": {
                            "firstName": "Zachary",
                            "lastName": "Tatlock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zachary Tatlock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145655689"
                        ],
                        "name": "Sorin Lerner",
                        "slug": "Sorin-Lerner",
                        "structuredName": {
                            "firstName": "Sorin",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorin Lerner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "In equality saturation [35, 37] authors build graphs of expressions for each program, transform the graphs via a series of rewrite rules, and check for equality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2138086,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f09d36476445c7f44d46555a753eae446cfed180",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer."
            },
            "slug": "Equality-saturation:-a-new-approach-to-optimization-Tate-Stepp",
            "title": {
                "fragments": [],
                "text": "Equality saturation: a new approach to optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed way of structuring optimizers has a variety of benefits over previous approaches: it obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than the authors' own."
            },
            "venue": {
                "fragments": [],
                "text": "POPL '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3095589"
                        ],
                        "name": "C. Hawblitzel",
                        "slug": "C.-Hawblitzel",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Hawblitzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hawblitzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145474353"
                        ],
                        "name": "Shuvendu K. Lahiri",
                        "slug": "Shuvendu-K.-Lahiri",
                        "structuredName": {
                            "firstName": "Shuvendu",
                            "lastName": "Lahiri",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuvendu K. Lahiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34974621"
                        ],
                        "name": "Kshama Pawar",
                        "slug": "Kshama-Pawar",
                        "structuredName": {
                            "firstName": "Kshama",
                            "lastName": "Pawar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kshama Pawar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068619995"
                        ],
                        "name": "Hammad Hashmi",
                        "slug": "Hammad-Hashmi",
                        "structuredName": {
                            "firstName": "Hammad",
                            "lastName": "Hashmi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hammad Hashmi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776563"
                        ],
                        "name": "Sedar Gokbulut",
                        "slug": "Sedar-Gokbulut",
                        "structuredName": {
                            "firstName": "Sedar",
                            "lastName": "Gokbulut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sedar Gokbulut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47635485"
                        ],
                        "name": "Lakshan Fernando",
                        "slug": "Lakshan-Fernando",
                        "structuredName": {
                            "firstName": "Lakshan",
                            "lastName": "Fernando",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lakshan Fernando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145339508"
                        ],
                        "name": "D. Detlefs",
                        "slug": "D.-Detlefs",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Detlefs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Detlefs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068713822"
                        ],
                        "name": "Scott Wadsworth",
                        "slug": "Scott-Wadsworth",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Wadsworth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Wadsworth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "SymDiff [17] models loops unsoundly by unrolling them for a"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8712721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e74f5ba5c7174e3ecf6ab2581a5e745bb69dd54",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a cross-version compiler validator and measures its effectiveness on the CLR JIT compiler. The validator checks for semantically equivalent assembly language output from various versions of the compiler, including versions across a seven-month time period, across two architectures (x86 and ARM), across two compilation scenarios (JIT and MDIL), and across optimizations levels. For month-to-month comparisons, the validator achieves a false alarm rate of just 2.2%. To help understand reported semantic differences, the validator performs a root-cause analysis on the counterexample traces generated by the underlying automated theorem proving tools. This root-cause analysis groups most of the counterexamples into a small number of buckets, reducing the number of counterexamples analyzed by hand by anywhere from 53% to 96%. The validator ran on over 500,000 methods across a large suite of test programs, finding 12 previously unknown correctness and performance bugs in the CLR compiler."
            },
            "slug": "Will-you-still-compile-me-tomorrow-static-compiler-Hawblitzel-Lahiri",
            "title": {
                "fragments": [],
                "text": "Will you still compile me tomorrow? static cross-version compiler validation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The validator ran on over 500,000 methods across a large suite of test programs, finding 12 previously unknown correctness and performance bugs in the CLR compiler."
            },
            "venue": {
                "fragments": [],
                "text": "ESEC/FSE 2013"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736921"
                        ],
                        "name": "M. Schordan",
                        "slug": "M.-Schordan",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "Schordan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1905021"
                        ],
                        "name": "Pei-Hung Lin",
                        "slug": "Pei-Hung-Lin",
                        "structuredName": {
                            "firstName": "Pei-Hung",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pei-Hung Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34895614"
                        ],
                        "name": "D. Quinlan",
                        "slug": "D.-Quinlan",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Quinlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793611"
                        ],
                        "name": "L. Pouchet",
                        "slug": "L.-Pouchet",
                        "structuredName": {
                            "firstName": "Louis-No\u00ebl",
                            "lastName": "Pouchet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pouchet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": "The authors of [30] handle affine programs where loops have constant bounds; the authors expand traces of both programs and attempt to match operations between the traces via a series of normalization and rewriting steps."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9016489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74f02bc297f7bda336c790da0e013573404042a5",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "As processors gain in complexity and heterogeneity, compilers are asked to perform program transformations of ever-increasing complexity to effectively map an input program to the target hardware. It is critical to develop methods and tools to automatically assert the correctness of programs generated by such modern optimizing compilers."
            },
            "slug": "Verification-of-Polyhedral-Optimizations-with-Loop-Schordan-Lin",
            "title": {
                "fragments": [],
                "text": "Verification of Polyhedral Optimizations with Constant Loop Bounds in Finite State Space Computations"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "As processors gain in complexity and heterogeneity, compilers are asked to perform program transformations of ever-increasing complexity to effectively map an input program to the target hardware."
            },
            "venue": {
                "fragments": [],
                "text": "ISoLA"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40445331"
                        ],
                        "name": "S. Dutta",
                        "slug": "S.-Dutta",
                        "structuredName": {
                            "firstName": "Sudakshina",
                            "lastName": "Dutta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dutta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143983899"
                        ],
                        "name": "D. Sarkar",
                        "slug": "D.-Sarkar",
                        "structuredName": {
                            "firstName": "Dipankar",
                            "lastName": "Sarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sarkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064309624"
                        ],
                        "name": "Arvind Rawat",
                        "slug": "Arvind-Rawat",
                        "structuredName": {
                            "firstName": "Arvind",
                            "lastName": "Rawat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arvind Rawat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109088016"
                        ],
                        "name": "Kulwant Singh",
                        "slug": "Kulwant-Singh",
                        "structuredName": {
                            "firstName": "Kulwant",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kulwant Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Similarly, the work [12] checks equivalence of loop parallelization and vectorization optimizations in a more general non-affine setting, but is limited to cases where the control flow of the program remains the same for all possible inputs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30566641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0922e9b86dca29d3d150a2cf8cbeace54013504",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Loop parallelization and loop vectorization of array-intensive programs are two common transformations applied by parallelizing compilers to convert a sequential program into a parallel program. Validation of such transformations carried out by untrusted compilers are extremely useful. This paper proposes a novel algorithm for construction of the dependence graph of the generated parallel programs. The transformations are then validated by checking equivalence of the dependence graphs of the original sequential program and the parallel program using a standard and fairly general algorithm reported elsewhere in the literature. The above equivalence checker still works even when the above parallelizing transformations are preceded by various enabling transformations except for loop collapsing which changes the dimensions of the arrays. To address the issue, the present work expands the scope of the checker to handle this special case by informing it of the correspondence between the index spaces of the corresponding arrays in the sequential and the parallel programs. The augmented algorithm is able to validate a large class of static affine programs. The proposed methods are implemented and tested against a set of available benchmark programs which are parallelized by the polyhedral auto-parallelizer LooPo and the auto-vectorizer Scout. During experiments, a bug of the compiler LooPo on loop parallelization has been detected."
            },
            "slug": "Validation-of-Loop-Parallelization-and-Loop-Dutta-Sarkar",
            "title": {
                "fragments": [],
                "text": "Validation of Loop Parallelization and Loop Vectorization Transformations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel algorithm for construction of the dependence graph of the generated parallel programs is proposed by expanding the scope of the checker to handle this special case by informing it of the correspondence between the index spaces of the corresponding arrays in the sequential and the parallel programs."
            },
            "venue": {
                "fragments": [],
                "text": "ENASE"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30073551"
                        ],
                        "name": "M. Stepp",
                        "slug": "M.-Stepp",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stepp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stepp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6331083"
                        ],
                        "name": "R. Tate",
                        "slug": "R.-Tate",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Tate",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145655689"
                        ],
                        "name": "Sorin Lerner",
                        "slug": "Sorin-Lerner",
                        "structuredName": {
                            "firstName": "Sorin",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorin Lerner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 23
                            }
                        ],
                        "text": "Translation validation [26, 31, 34] uses compiler instrumentation to help generate a simulation relation to prove the correctness of compiler optimizations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13218010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56202da53b5aa2e770cafd50a44741179039b8db",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We updated our Peggy tool, previously presented in [6], to perform translation validation for the LLVM compiler using a technique called Equality Saturation. We present the tool, and illustrate its effectiveness at doing translation validation on SPEC 2006 benchmarks."
            },
            "slug": "Equality-Based-Translation-Validator-for-LLVM-Stepp-Tate",
            "title": {
                "fragments": [],
                "text": "Equality-Based Translation Validator for LLVM"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The Peggy tool is updated, to perform translation validation for the LLVM compiler using a technique called Equality Saturation, and its effectiveness at doing translation validation on SPEC 2006 benchmarks is illustrated."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445900"
                        ],
                        "name": "Berkeley R. Churchill",
                        "slug": "Berkeley-R.-Churchill",
                        "structuredName": {
                            "firstName": "Berkeley",
                            "lastName": "Churchill",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berkeley R. Churchill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111333766"
                        ],
                        "name": "Rahul Sharma",
                        "slug": "Rahul-Sharma",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1666358563"
                        ],
                        "name": "J. Bastien",
                        "slug": "J.-Bastien",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bastien",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bastien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 244,
                                "start": 238
                            }
                        ],
                        "text": "Equivalence checking, the problem of formally proving that two functions or programs are semantically equivalent, is a long-standing and important problem; applications include verification of compiler correctness [26], superoptimization [3, 6], program synthesis [29], and verifying the correctness of code refactoring [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 157
                            }
                        ],
                        "text": "Performing this computation over Z264 rather than Z is expensive, but necessary because some equalities hold over Z264 that do not hold over Z (in past work [6, 32] the invariant learning routine would miss some of these equalities)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 42
                            }
                        ],
                        "text": "This is in contrast to prior work such as [6, 7, 32] where corresponding points in the two programs, sometimes called cutpoints, must be chosen based on less information; usually cutpoints are chosen syntactically."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 158
                            }
                        ],
                        "text": "ARM uses data from test cases to guess and prove relationships that ensure pointers do not alias; then the constraints are encoded with minimal use of arrays [6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "26 if (cp[6] == 0) return cp - str + 6;"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 84
                            }
                        ],
                        "text": "We used rigorously tested semantic models for x86-64 instructions developed by hand [6] and synthesized automatically [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7188487,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "fcefce81c8e810711e87170461ad6462d2f731fa",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Software fault isolation (SFI) is an important technique for the construction of secure operating systems, web browsers, and other extensible software. We demonstrate that superoptimization can dramatically improve the performance of Google Native Client, a SFI system that ships inside the Google Chrome Browser. Key to our results are new techniques for superoptimization of loops: we propose a new architecture for superoptimization tools that incorporates both a fully sound verification technique to ensure correctness and a bounded verification technique to guide the search to optimized code. In our evaluation we optimize 13 libc string functions, formally verify the correctness of the optimizations and report a median and average speedup of 25% over the libraries shipped by Google."
            },
            "slug": "Sound-Loop-Superoptimization-for-Google-Native-Churchill-Sharma",
            "title": {
                "fragments": [],
                "text": "Sound Loop Superoptimization for Google Native Client"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work demonstrates that superoptimization can dramatically improve the performance of Google Native Client, a SFI system that ships inside the Google Chrome Browser and proposes a new architecture for super Optimization tools that incorporates both a fully sound verification technique and a bounded verification technique to guide the search to optimized code."
            },
            "venue": {
                "fragments": [],
                "text": "ASPLOS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144834015"
                        ],
                        "name": "Saeed Maleki",
                        "slug": "Saeed-Maleki",
                        "structuredName": {
                            "firstName": "Saeed",
                            "lastName": "Maleki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saeed Maleki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740043"
                        ],
                        "name": "Yaoqing Gao",
                        "slug": "Yaoqing-Gao",
                        "structuredName": {
                            "firstName": "Yaoqing",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaoqing Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3106639"
                        ],
                        "name": "M. Garzar\u00e1n",
                        "slug": "M.-Garzar\u00e1n",
                        "structuredName": {
                            "firstName": "Mar\u00eda",
                            "lastName": "Garzar\u00e1n",
                            "middleNames": [
                                "Jes\u00fas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Garzar\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113248488"
                        ],
                        "name": "Tommy Wong",
                        "slug": "Tommy-Wong",
                        "structuredName": {
                            "firstName": "Tommy",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tommy Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729097"
                        ],
                        "name": "D. Padua",
                        "slug": "D.-Padua",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Padua",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Padua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "We started with 156 functions from the Test Suite for Vectorizing Compilers (TSVC), whichwas developed \u201cto assess the vectorizing capabilities of compilers\u201d and ported to C in [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9418966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de6f280d5e335c3116ed3df2183b70ddbed2f990",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Most of today's processors include vector units that have been designed to speedup single threaded programs. Although vector instructions can deliver high performance, writing vector code in assembly language or using intrinsics in high level languages is a time consuming and error-prone task. The alternative is to automate the process of vectorization by using vectorizing compilers. This paper evaluates how well compilers vectorize a synthetic benchmark consisting of 151 loops, two application from Petascale Application Collaboration Teams (PACT), and eight applications from Media Bench II. We evaluated three compilers: GCC (version 4.7.0), ICC (version 12.0) and XLC (version 11.01). Our results show that despite all the work done in vectorization in the last 40 years 45-71% of the loops in the synthetic benchmark and only a few loops from the real applications are vectorized by the compilers we evaluated."
            },
            "slug": "An-Evaluation-of-Vectorizing-Compilers-Maleki-Gao",
            "title": {
                "fragments": [],
                "text": "An Evaluation of Vectorizing Compilers"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Evaluated how well compilers vectorize a synthetic benchmark consisting of 151 loops, two application from Petascale Application Collaboration Teams (PACT), and eight applications from Media Bench II shows that despite all the work done in vectorization in the last 40 years 45-71% of the loops in the synthetic benchmark and only a few loops from the real applications are vectorized by the compilers."
            },
            "venue": {
                "fragments": [],
                "text": "2011 International Conference on Parallel Architectures and Compilation Techniques"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47825073"
                        ],
                        "name": "Weiqi Wang",
                        "slug": "Weiqi-Wang",
                        "structuredName": {
                            "firstName": "Weiqi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiqi Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680661"
                        ],
                        "name": "C. Barrett",
                        "slug": "C.-Barrett",
                        "structuredName": {
                            "firstName": "Clark",
                            "lastName": "Barrett",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Barrett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2237524"
                        ],
                        "name": "Thomas Wies",
                        "slug": "Thomas-Wies",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Wies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Wies"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Therefore we implement two memory models [39], a flat memory model, and one based on alias relationship mining (ARM)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "In these cases we also assume that stack accesses of different sizes do not alias, so we model them using separate memory stores [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13101643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "414c102616c44e6ed228c370c9c8f83275f33523",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Scalability is a key challenge in static analysis. For imperative languages like C, the approach taken for modeling memory can play a significant role in scalability. In this paper, we explore a family of memory models called partitioned memory models which divide memory up based on the results of a points-to analysis. We review Steensgaard\u2019s original and field-sensitive points-to analyses as well as Data Structure Analysis (DSA), and introduce a new cell-based points-to analysis which more precisely handles heap data structures and type-unsafe operations like pointer arithmetic and pointer casting. We give experimental results on benchmarks from the software verification competition using the program verification framework in Cascade. We show that a partitioned memory model using our cell-based points-to analysis outperforms models using other analyses."
            },
            "slug": "Partitioned-Memory-Models-for-Program-Analysis-Wang-Barrett",
            "title": {
                "fragments": [],
                "text": "Partitioned Memory Models for Program Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper explores a family of memory models called partitioned memory models which divide memory up based on the results of a points-to analysis and introduces a new cell-based points- to analysis which more precisely handles heap data structures and type-unsafe operations like pointer arithmetic and pointer casting."
            },
            "venue": {
                "fragments": [],
                "text": "VMCAI"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144996001"
                        ],
                        "name": "L. D. Moura",
                        "slug": "L.-D.-Moura",
                        "structuredName": {
                            "firstName": "Leonardo",
                            "lastName": "Moura",
                            "middleNames": [
                                "Mendon\u00e7a",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Moura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3714351"
                        ],
                        "name": "N. Bj\u00f8rner",
                        "slug": "N.-Bj\u00f8rner",
                        "structuredName": {
                            "firstName": "Nikolaj",
                            "lastName": "Bj\u00f8rner",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Bj\u00f8rner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15912959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3960dda299e0f8615a7db675b8e6905b375ecf8a",
            "isKey": false,
            "numCitedBy": 6280,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications."
            },
            "slug": "Z3:-An-Efficient-SMT-Solver-Moura-Bj\u00f8rner",
            "title": {
                "fragments": [],
                "text": "Z3: An Efficient SMT Solver"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Z3 is a new and efficient SMT Solver freely available from Microsoft Research that is used in various software verification and analysis applications."
            },
            "venue": {
                "fragments": [],
                "text": "TACAS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8599261"
                        ],
                        "name": "E. D. Angelis",
                        "slug": "E.-D.-Angelis",
                        "structuredName": {
                            "firstName": "Emanuele",
                            "lastName": "Angelis",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. D. Angelis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028917"
                        ],
                        "name": "F. Fioravanti",
                        "slug": "F.-Fioravanti",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Fioravanti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fioravanti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716699"
                        ],
                        "name": "A. Pettorossi",
                        "slug": "A.-Pettorossi",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Pettorossi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pettorossi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2415169"
                        ],
                        "name": "M. Proietti",
                        "slug": "M.-Proietti",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Proietti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Proietti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11099852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c667c2d63131979cfb7521e8ee8df9623607072",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Relational verification is a technique that aims at proving properties that relate two different program fragments, or two different program runs. It has been shown that constrained Horn clauses (CHCs) can effectively be used for relational verification by applying a CHC transformation, called predicate pairing, which allows the CHC solver to infer relations among arguments of different predicates. In this paper we study how the effects of the predicate pairing transformation can be enhanced by using various abstract domains based on linear arithmetic (i.e., the domain of convex polyhedra and some of its subdomains) during the transformation. After presenting an algorithm for predicate pairing with abstraction, we report on the experiments we have performed on over a hundred relational verification problems by using various abstract domains. The experiments have been performed by using the VeriMAP transformation and verification system, together with the Parma Polyhedra Library (PPL) and the Z3 solver for CHCs."
            },
            "slug": "Enhancing-Predicate-Pairing-with-Abstraction-for-Angelis-Fioravanti",
            "title": {
                "fragments": [],
                "text": "Enhancing Predicate Pairing with Abstraction for Relational Verification"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper studies how the effects of the predicate pairing transformation can be enhanced by using various abstract domains based on linear arithmetic during the transformation, and presents an algorithm for predicate pairing with abstraction."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8599261"
                        ],
                        "name": "E. D. Angelis",
                        "slug": "E.-D.-Angelis",
                        "structuredName": {
                            "firstName": "Emanuele",
                            "lastName": "Angelis",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. D. Angelis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2028917"
                        ],
                        "name": "F. Fioravanti",
                        "slug": "F.-Fioravanti",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Fioravanti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Fioravanti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716699"
                        ],
                        "name": "A. Pettorossi",
                        "slug": "A.-Pettorossi",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Pettorossi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pettorossi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2415169"
                        ],
                        "name": "M. Proietti",
                        "slug": "M.-Proietti",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Proietti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Proietti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 51613235,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23b0da7cd545c2d47d89f1f554fd4bce3cbce4d9",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Relational verification is a technique that aims at proving properties that relate two different program fragments, or two different program runs. It has been shown that constrained Horn clauses (CHCs) can effectively be used for relational verification by applying a CHC transformation, called Predicate Pairing, which allows the CHC solver to infer relations among arguments of different predicates. In this paper we study how the effects of the Predicate Pairing transformation can be enhanced by using various abstract domains based on Linear Arithmetic (i.e., the domain of convex polyhedra and some of its subdomains) during the transformation. After presenting an algorithm for Predicate Pairing with abstraction, we report on the experiments we have performed on over a hundred relational verification problems by using various abstract domains. The experiments have been performed by using the VeriMAP verification system, together with the Parma Polyhedra Library (PPL) and the Z3 solver for CHCs."
            },
            "slug": "Predicate-Pairing-with-Abstraction-for-Relational-Angelis-Fioravanti",
            "title": {
                "fragments": [],
                "text": "Predicate Pairing with Abstraction for Relational Verification"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is studied how the effects of the Predicate Pairing transformation can be enhanced by using various abstract domains based on Linear Arithmetic during the transformation."
            },
            "venue": {
                "fragments": [],
                "text": "LOPSTR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578198"
                        ],
                        "name": "Nimrod Partush",
                        "slug": "Nimrod-Partush",
                        "structuredName": {
                            "firstName": "Nimrod",
                            "lastName": "Partush",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nimrod Partush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743232"
                        ],
                        "name": "Eran Yahav",
                        "slug": "Eran-Yahav",
                        "structuredName": {
                            "firstName": "Eran",
                            "lastName": "Yahav",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eran Yahav"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 77
                            }
                        ],
                        "text": "We believe techniques that depend on syntactic alignment of the two programs [13, 14, 26, 27, 32] fail on most or all of our benchmarks, including at least 47 benchmarks where loop unrolling has been performed (usually as part of vectorization)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 61
                            }
                        ],
                        "text": "As a result, prior automated equivalence checking approaches [3, 7, 13, 14, 27, 32] fail on this example."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 134
                            }
                        ],
                        "text": "Examples include past work on data-driven equivalence checking [32], Necula\u2019s well-known translation validation work [26], and others [13, 14, 27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1132017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7cb1549ea79af30250c3b486822032fb6610930a",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of computing semantic differences between a program and a patched version of the program. Our goal is to obtain a precise characterization of the difference between program versions, or establish their equivalence when no difference exists."
            },
            "slug": "Abstract-Semantic-Differencing-for-Numerical-Partush-Yahav",
            "title": {
                "fragments": [],
                "text": "Abstract Semantic Differencing for Numerical Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work addresses the problem of computing semantic differences between a program and a patched version of the program and obtains a precise characterization of the difference between program versions."
            },
            "venue": {
                "fragments": [],
                "text": "SAS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3173014"
                        ],
                        "name": "Stefan Heule",
                        "slug": "Stefan-Heule",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Heule",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Heule"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1945094"
                        ],
                        "name": "Eric Schkufza",
                        "slug": "Eric-Schkufza",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Schkufza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Schkufza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111333766"
                        ],
                        "name": "Rahul Sharma",
                        "slug": "Rahul-Sharma",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "We used rigorously tested semantic models for x86-64 instructions developed by hand [6] and synthesized automatically [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1299593,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c18ec63b49ba81b4eb2b67ba30a9607b0662979",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "The x86-64 ISA sits at the bottom of the software stack of most desktop and server software. Because of its importance, many software analysis and verification tools depend, either explicitly or implicitly, on correct modeling of the semantics of x86-64 instructions. However, formal semantics for the x86-64 ISA are difficult to obtain and often written manually through great effort. We describe an automatically synthesized formal semantics of the input/output behavior for a large fraction of the x86-64 Haswell ISA\u2019s many thousands of instruction variants. The key to our results is stratified synthesis, where we use a set of instructions whose semantics are known to synthesize the semantics of additional instructions whose semantics are unknown. As the set of formally described instructions increases, the synthesis vocabulary expands, making it possible to synthesize the semantics of increasingly complex instructions. Using this technique we automatically synthesized formal semantics for 1,795 instruction variants of the x86-64 Haswell ISA. We evaluate the learned semantics against manually written semantics (where available) and find that they are formally equivalent with the exception of 50 instructions, where the manually written semantics contain an error. We further find the learned formulas to be largely as precise as manually written ones and of similar size."
            },
            "slug": "Stratified-synthesis:-automatically-learning-the-Heule-Schkufza",
            "title": {
                "fragments": [],
                "text": "Stratified synthesis: automatically learning the x86-64 instruction set"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work describes an automatically synthesized formal semantics of the input/output behavior for a large fraction of the x86-64 Haswell ISA\u2019s many thousands of instruction variants, and finds the learned formulas to be largely as precise as manually written ones and of similar size."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI 2016"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10776701"
                        ],
                        "name": "D. Mordvinov",
                        "slug": "D.-Mordvinov",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Mordvinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mordvinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736858"
                        ],
                        "name": "Grigory Fedyukovich",
                        "slug": "Grigory-Fedyukovich",
                        "structuredName": {
                            "firstName": "Grigory",
                            "lastName": "Fedyukovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Grigory Fedyukovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 28
                            }
                        ],
                        "text": "In contrast, the authors of [9, 10, 25] use constrained Horn clauses to summarize the entire execution of two programs and then use Horn clause solvers to prove equivalence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11045407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0f37e37310dbeaa063589830159467c5a6f958a9",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Simultaneous occurrences of multiple recurrence relations in a system of non-linear constrained Horn clauses are crucial for proving its satisfiability. A solution of such system is often inexpressible in the constraint language. We propose to synchronize recurrent computations, thus increasing the chances for a solution to be found. We introduce a notion of CHC product allowing to formulate a lightweight iterative algorithm of merging recurrent computations into groups and prove its soundness. The evaluation over a set of systems handling lists and linear integer arithmetic confirms that the transformed systems are drastically more simple to solve than the original ones."
            },
            "slug": "Synchronizing-Constrained-Horn-Clauses-Mordvinov-Fedyukovich",
            "title": {
                "fragments": [],
                "text": "Synchronizing Constrained Horn Clauses"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A notion of CHC product is introduced allowing to formulate a lightweight iterative algorithm of merging recurrent computations into groups and prove its soundness."
            },
            "venue": {
                "fragments": [],
                "text": "LPAR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052981539"
                        ],
                        "name": "C. Barrett",
                        "slug": "C.-Barrett",
                        "structuredName": {
                            "firstName": "Clark",
                            "lastName": "Barrett",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Barrett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3239532"
                        ],
                        "name": "Christopher L. Conway",
                        "slug": "Christopher-L.-Conway",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Conway",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher L. Conway"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144432853"
                        ],
                        "name": "Morgan Deters",
                        "slug": "Morgan-Deters",
                        "structuredName": {
                            "firstName": "Morgan",
                            "lastName": "Deters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Morgan Deters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2926176"
                        ],
                        "name": "Liana Hadarean",
                        "slug": "Liana-Hadarean",
                        "structuredName": {
                            "firstName": "Liana",
                            "lastName": "Hadarean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liana Hadarean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144453657"
                        ],
                        "name": "Dejan Jovanovic",
                        "slug": "Dejan-Jovanovic",
                        "structuredName": {
                            "firstName": "Dejan",
                            "lastName": "Jovanovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dejan Jovanovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057613884"
                        ],
                        "name": "Tim King",
                        "slug": "Tim-King",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "King",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2017837925"
                        ],
                        "name": "Andrew Reynolds",
                        "slug": "Andrew-Reynolds",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Reynolds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Reynolds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35116359"
                        ],
                        "name": "C. Tinelli",
                        "slug": "C.-Tinelli",
                        "structuredName": {
                            "firstName": "Cesare",
                            "lastName": "Tinelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Tinelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 236461198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ae2b937fca9b9adfad07059cbddfc9eefdac5a0",
            "isKey": false,
            "numCitedBy": 561,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "CVC4 is the latest version of the Cooperating Validity Checker. A joint project of NYU and U Iowa, CVC4 aims to support the useful feature set of CVC3 and SMT-LIBv2 while optimizing the design of the core system architecture and decision procedures to take advantage of recent engineering and algorithmic advances. CVC4 represents a completely new code base; it is a from-scratch rewrite of CVC3, and many subsystems have been completely redesigned. Additional decision procedures for CVC4 are currently under development, but for what it currently achieves, it is a lighter-weight and higher-performing tool than CVC3. We describe the system architecture, subsystems of note, and discuss some applications and continuing work."
            },
            "slug": "Cvc4-Barrett-Conway",
            "title": {
                "fragments": [],
                "text": "Cvc4"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The system architecture, subsystems of note, and discuss some applications and continuing work of CVC4 are described, which is a lighter-weight and higher-performing tool than CVC3."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144861543"
                        ],
                        "name": "C. Hoare",
                        "slug": "C.-Hoare",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Hoare",
                            "middleNames": [
                                "Antony",
                                "Richard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hoare"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 31
                            }
                        ],
                        "text": "Weuse relational Hoare triples [5, 19] to express proof obligations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207726175,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d8056e326d4199d157a17fbeee97a7349d2824c",
            "isKey": false,
            "numCitedBy": 2093,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper an attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics. This involves the elucidation of sets of axioms and rules of inference which can be used in proofs of the properties of computer programs. Examples are given of such axioms and rules, and a formal proof of a simple theorem is displayed. Finally, it is argued that important advantage, both theoretical and practical, may follow from a pursuance of these topics."
            },
            "slug": "An-axiomatic-basis-for-computer-programming-Hoare",
            "title": {
                "fragments": [],
                "text": "An axiomatic basis for computer programming"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "An attempt is made to explore the logical foundations of computer programming by use of techniques which were first applied in the study of geometry and have later been extended to other branches of mathematics by elucidation of sets of axioms and rules of inference."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Thus we use two solvers, Z3 [11] (commit 7f6ef0b6) and CVC4-1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Z3: An Efficient SMT Solver. In Tools and Algorithms for the Construction and Analysis of Systems (TACAS \u201908)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Thus we use two solvers, Z3 [11] (commit 7f6ef0b6) and CVC4-1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Z3: An Efficient SMT Solver. In Tools and Algorithms for the Construction and Analysis of Systems (TACAS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Philipp R\u00fcmmer, and Mattias Ulbrich"
            },
            "venue": {
                "fragments": [],
                "text": "Automated Software Engineering (ASE '14)"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "We started with 156 functions from the Test Suite for Vectorizing Compilers (TSVC), whichwas developed \u0142to assess the vectorizing capabilities of compilers\u017e and ported to C in [24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An Evaluation of Vectorizing Compilers. In Parallel Architectures and Compilation Techniques (PACT \u201911)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 19,
            "methodology": 18,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 36,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Semantic-program-alignment-for-equivalence-checking-Churchill-Padon/fc6e36c9df6521aa805bc622d066e2964d2e471d?sort=total-citations"
}