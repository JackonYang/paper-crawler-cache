{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39263338"
                        ],
                        "name": "Longxin Lin",
                        "slug": "Longxin-Lin",
                        "structuredName": {
                            "firstName": "Longxin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longxin Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Lin (1993a) and Dorigo and Colombetti (1995,1994) both used this approach, rst training the behaviors and then training the gatingfunction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 216
                            }
                        ],
                        "text": "The second network is trained in a standardsupervised mode to estimate r as a function of the input state s.Variations of this approach have been used in a variety of applications (Anderson, 1986;Barto et al., 1983; Lin, 1993b; Sutton, 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 72
                            }
                        ],
                        "text": "Variations of this approach have been used in a variety of applications [4, 9, 61, 114]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60875658,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54c4cf3a8168c1b70f91cf78a3dc98b671935492",
            "isKey": false,
            "numCitedBy": 909,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning agents are adaptive, reactive, and self-supervised. The aim of this dissertation is to extend the state of the art of reinforcement learning and enable its applications to complex robot-learning problems. In particular, it focuses on two issues. First, learning from sparse and delayed reinforcement signals is hard and in general a slow process. Techniques for reducing learning time must be devised. Second, most existing reinforcement learning methods assume that the world is a Markov decision process. This assumption is too strong for many robot tasks of interest. \nThis dissertation demonstrates how we can possibly overcome the slow learning problem and tackle non-Markovian environments, making reinforcement learning more practical for realistic robot tasks: (1) Reinforcement learning can be naturally integrated with artificial neural networks to obtain high-quality generalization, resulting in a significant learning speedup. Neural networks are used in this dissertation, and they generalize effectively even in the presence of noise and a large of binary and real-valued inputs. (2) Reinforcement learning agents can save many learning trials by using an action model, which can be learned on-line. With a model, an agent can mentally experience the effects of its actions without actually executing them. Experience replay is a simple technique that implements this idea, and is shown to be effective in reducing the number of action executions required. (3) Reinforcement learning agents can take advantage of instructive training instances provided by human teachers, resulting in a significant learning speedup. Teaching can also help learning agents avoid local optima during the search for optimal control. Simulation experiments indicate that even a small amount of teaching can save agents many learning trials. (4) Reinforcement learning agents can significantly reduce learning time by hierarchical learning--they first solve elementary learning problems and then combine solutions to the elementary problems to solve a complex problem. Simulation experiments indicate that a robot with hierarchical learning can solve a complex problem, which otherwise is hardly solvable within a reasonable time. (5) Reinforcement learning agents can deal with a wide range of non-Markovian environments by having a memory of their past. Three memory architectures are discussed. They work reasonably well for a variety of simple problems. One of them is also successfully applied to a nontrivial non-Markovian robot task. \nThe results of this dissertation rely on computer simulation, including (1) an agent operating in a dynamic and hostile environment and (2) a mobile robot operating in a noisy and non-Markovian environment. The robot simulator is physically realistic. This dissertation concludes that it is possible to build artificial agents than can acquire complex control policies effectively by reinforcement learning."
            },
            "slug": "Reinforcement-learning-for-robots-using-neural-Lin",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning for robots using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This dissertation concludes that it is possible to build artificial agents than can acquire complex control policies effectively by reinforcement learning and enable its applications to complex robot-learning problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055155244"
                        ],
                        "name": "David Chapman",
                        "slug": "David-Chapman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chapman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chapman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7213327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45d2bdf5e7072c1d5a91a38efa365715def2f45d",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Delayed reinforcement learning is an attractive framework for the unsupervised learning of action policies for autonomous agents. Some existing delayed reinforcement learning techniques have shown promise in simple domains. However, a number of hurdles must be passed before they are applicable to realistic problems. This paper describes one such difficulty, the input generalization problem (whereby the system must generalize to produce similar actions in similar situations) and an implemented solution, the G algorithm. This algorithm is based on recursive splitting of the state space based on statistical measures of differences in reinforcements received. Connectionist backpropagation has previously been used for input generalization in reinforcement learning. We compare the two techniques analytically and empirically. The G algorithm's sound statistical basis makes it easy to predict when it should and should not work, whereas the behavior of back-propagation is unpredictable. We found that a previous successful use of backpropagation can be explained by the linearity of the application domain. We found that in another domain, G reliably found the optimal policy, whereas none of a set of runs of backpropagation with many combinations of parameters did."
            },
            "slug": "Input-Generalization-in-Delayed-Reinforcement-An-Chapman-Kaelbling",
            "title": {
                "fragments": [],
                "text": "Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes the input generalization problem (whereby the system must generalize to produce similar actions in similar situations) and an implemented solution, the G algorithm, which is based on recursive splitting of the state space based on statistical measures of differences in reinforcements received."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39263338"
                        ],
                        "name": "Longxin Lin",
                        "slug": "Longxin-Lin",
                        "structuredName": {
                            "firstName": "Longxin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longxin Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18783919,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2eb733470921af04df5c611a6a3c76c281ce498",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning is a type of unsupervised learning for sequential decision making. Q-learning is probably the best-understood reinforcement learning algorithm. In Q-learning, the agent learns a mapping from states and actions to their utilities. An important assumption of Q-learning is the Markovian environment assumption, meaning that any information needed to determine the optimal actions is reflected in the agent''s state representation. Consider an agent whose state representation is based solely on its immediate perceptual sensations. When its sensors are not able to make essential distinctions among world states, the Markov assumption is violated, causing a problem called perceptual aliasing. For example, when facing a closed box, an agent based on its current visual sensation cannot act optimally if the optimal action depends on the contents of the box. There are two basic approaches to addressing this problem -- using more sensors or using history to figure out the current world state. This paper studies three connectionist approaches which learn to use history to handle perceptual aliasing: the window-Q, recurrent-Q, and recurrent-model architectures. Empirical study of these architectures is presented. Their relative strengths and weaknesses are also discussed."
            },
            "slug": "Memory-Approaches-to-Reinforcement-Learning-in-Lin-Mitchell",
            "title": {
                "fragments": [],
                "text": "Memory Approaches to Reinforcement Learning in Non-Markovian Domains"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper studies three connectionist approaches which learn to use history to handle perceptual aliasing: the window-Q, recurrent- Q, and recurrent-model architectures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41302553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "168bd65ae047afecf3eea54786d8e6becba932f7",
            "isKey": false,
            "numCitedBy": 795,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation addresses the problem of designing algorithms for learning in embedded systems. This problem differs from the traditional supervised learning problem. An agent, finding itself in a particular input situation must generate an action. It then receives a reinforcement value from the environment, indicating how valuable the current state of the environment is for the agent. The agent cannot, however, deduce the reinforcement value that would have resulted from executing any of its other actions. A number of algorithms for learning action strategies from reinforcement values are presented and compared empirically with existing reinforcement-learning algorithms. \nThe interval-estimation algorithm uses the statistical notion of confidence intervals to guide its generation of actions in the world, trading off acting to gain information against acting to gain reinforcement. It performs well in simple domains but does not exhibit any generalization and is computationally complex. \nThe cascade algorithm is a structural credit-assignment method that allows an action strategy with many output bits to be learned by a collection of reinforcement-learning modules that learn Boolean functions. This method represents an improvement in computational complexity and often in learning rate. \nTwo algorithms for learning Boolean functions in k-DNF are described. Both are based on Valiant's algorithm for learning such functions from input-output instances. The first uses Sutton's techniques for linear association and reinforcement comparison, while the second uses techniques from the interval estimation algorithm. They both perform well and have tractable complexity. \nA generate-and-test reinforcement-learning algorithm is presented. It allows symbolic representations of Boolean functions to be constructed incrementally and tested in the environment. It is highly parametrized and can be tuned to learn a broad range of function classes. Low-complexity functions can be learned very efficiently even in the presence of large numbers of irrelevant input bits. This algorithm is extended to construct simple sequential networks using a set-reset operator, which allows the agent to learn action strategies with state. \nThese algorithms, in addition to being studied in simulation, were implemented and tested on a physical mobile robot."
            },
            "slug": "Learning-in-embedded-systems-Kaelbling",
            "title": {
                "fragments": [],
                "text": "Learning in embedded systems"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This dissertation addresses the problem of designing algorithms for learning in embedded systems using Sutton's techniques for linear association and reinforcement comparison, while the interval estimation algorithm uses the statistical notion of confidence intervals to guide its generation of actions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2590047"
                        ],
                        "name": "C. Fiechter",
                        "slug": "C.-Fiechter",
                        "structuredName": {
                            "firstName": "Claude-Nicolas",
                            "lastName": "Fiechter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fiechter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 96
                            }
                        ],
                        "text": "In spite of the mismatch between embedded reinforcement learning and the train/testperspective, Fiechter (1994) provides a PAC analysis for Q-learning (described inSection 4.2) that sheds some light on the connection between the two views."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18733448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81bba615887d994218f20aa7a4b932665ba10d83",
            "isKey": true,
            "numCitedBy": 12,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a new formal model for studying reinforcement learning, based on Valiant's PAC framework. In our model the learner does not have direct access to every state of the environment. Instead , every sequence of experiments starts in a xed initial state and the learner is provided with a \\reset\" operation that interrupts the current sequence of experiments and starts a new one (from the initial state). We do not require the agent to learn the optimal policy but only a good approximation of it with high probability. More precisely, we require the learner to produce a policy whose expected value from the initial state is \"-close to that of the optimal policy, with probability no less than 1 ?. For this model, we describe an algorithm that produces such an (\",)-optimal policy, for any environment, in time polynomial in N, K, 1=\", 1==, 1=(1 ?) and r max , where N is the number of states of the environment, K is the maximum number of actions in a state, is the discount factor and r max is the maximum reward on any transition."
            },
            "slug": "Eecient-Reinforcement-Learning-Fiechter",
            "title": {
                "fragments": [],
                "text": "Eecient Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new formal model for studying reinforcement learning, based on Valiant's PAC framework, that requires the learner to produce a policy whose expected value from the initial state is \"-close to that of the optimal policy, with probability no less than 1 ?\"."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850503"
                        ],
                        "name": "S. Mahadevan",
                        "slug": "S.-Mahadevan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Mahadevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mahadevan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 267
                            }
                        ],
                        "text": "Although his R-learning algorithm seems to exhibit convergence problems for some MDPs, several researchers have found the average-reward criterion closer to the true problem they wish to solve than a discounted criterion and therefore prefer R-learning to Q-learning (Mahadevan, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 265
                            }
                        ],
                        "text": "Although his R-learning algorithm seems to exhibit convergence problems forsome MDPs, several researchers have found the average-reward criterion closer to the trueproblem they wish to solve than a discounted criterion and therefore prefer R-learning toQ-learning (Mahadevan, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6470940,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "481421dd6b320e9aa2325420cc27cdee22ee36cd",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "To-Discount-or-Not-to-Discount-in-Reinforcement-A-R-Mahadevan",
            "title": {
                "fragments": [],
                "text": "To Discount or Not to Discount in Reinforcement Learning: A Case Study Comparing R Learning and Q Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 72
                            }
                        ],
                        "text": "Variations of this approach have been used in a variety of applications (Anderson, 1986; Barto et al., 1983; Lin, 1993b; Sutton, 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 181
                            }
                        ],
                        "text": "The second network is trained in a standardsupervised mode to estimate r as a function of the input state s.Variations of this approach have been used in a variety of applications (Anderson, 1986;Barto et al., 1983; Lin, 1993b; Sutton, 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18649966,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11463e2a6ed218e87e22cba2c2f24fb5992d0293",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "THE DIFFICULTIES OF LEARNING IN MULTILAYERED NETWORKS OF COMPUTATIONAL UNITS HAS LIMITED THE USE OF CONNECTIONIST SYSTEMS IN COMPLEX DOMAINS. THIS DISSERTATION ELUCIDATES THE ISSUES OF LEARNING IN A NETWORK''S HIDDEN UNITS, AND REVIEWS METHODS FOR ADDRESSING THESE ISSUES THAT HAVE BEEN DEVELOPED THROUGH THE YEARS. ISSUES OF LEARNING IN HIDDEN UNITS ARE SHOWN TO BE ANALOGOUS TO LEARNING ISSUES FOR MULTILAYER SYSTEMS EMPLOYING SYMBOLIC REPRSENTATIONS. COMPARISONS OF A NUMBER OF ALGORITHMS FOR LEARNING IN HIDDEN UNITS ARE MADE BY APPLYING THEM IN A CONSISTENT MANNER TO SEVERAL TASKS. RECENTLY DEVELOPED ALGORITHMS, INCLUDING RUMELHART, ET AL''S, ERROR BACK-PROPOGATIONS ALGORITHM AND BARTO, ET AL''S, REINFORCEMENT-LEARNING ALGORITHMS, LEARN THE SOLUTIONS TO THE TASKS MUCH MORE SUCCESSFULLY THAN METHODS OF THE PAST. A NOVEL ALGORITHM IS EXAMINED THAT COMBINES ASPECTS OF REINFORCEMENT LEARNING AND A DATA-DIRECTED SEARCH FOR USEFUL WEIGHTS, AND IS SHOWN TO OUT PERFORM REINFORMCEMENT-LEARNING ALGORITHMS. A CONNECTIONIST FRAMEWORK FOR THE LEARNING OF STRATEGIES IS DESCRIBED WHICH COMBINES THE ERROR BACK-PROPOGATION ALGORITHM FOR LEARNING IN HIDDEN UNITS WITH SUTTON''S AHC ALGORITHM TO LEARN EVALUATION FUNCTIONS AND WITH A REINFORCEMENT-LEARNING ALGORITHM TO LEARN SEARCH HEURISTICS. THE GENERAL- ITY OF THIS HYBRID SYSTEM IS DEMONSTRATED THROUGH SUCCESSFUL APPLICATIONS"
            },
            "slug": "Learning-and-problem-solving-with-multilayer-neural-Anderson",
            "title": {
                "fragments": [],
                "text": "Learning and problem-solving with multilayer connectionist systems (adaptive, strategy learning, neural networks, reinforcement learning)"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel algorithm is examined that combines ASPECTS of REINFORCEMENT LEARNING and a DATA-DIRECTED SEARCH for USEFUL WEIGHTS, and is shown to out perform reinFORMCEMENT-LEARNING ALGORITHMS."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3466704"
                        ],
                        "name": "Gavin Adrian Rummery",
                        "slug": "Gavin-Adrian-Rummery",
                        "structuredName": {
                            "firstName": "Gavin",
                            "lastName": "Rummery",
                            "middleNames": [
                                "Adrian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gavin Adrian Rummery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145387873"
                        ],
                        "name": "M. Niranjan",
                        "slug": "M.-Niranjan",
                        "structuredName": {
                            "firstName": "Mahesan",
                            "lastName": "Niranjan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Niranjan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 38
                            }
                        ],
                        "text": "A di erent learning algorithm: SARSA (Rummery & Niranjan, 1994) instead of valueiteration.4."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "A di erent learning algorithm: SARSA [95] instead of value iteration."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59872172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a09464f26e18a25a948baaa736270bfb84b5e12",
            "isKey": false,
            "numCitedBy": 1412,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning algorithms are a powerful machine learning technique. However, much of the work on these algorithms has been developed with regard to discrete nite-state Markovian problems, which is too restrictive for many real-world environments. Therefore, it is desirable to extend these methods to high dimensional continuous state-spaces, which requires the use of function approximation to generalise the information learnt by the system. In this report, the use of back-propagation neural networks (Rumelhart, Hinton and Williams 1986) is considered in this context. We consider a number of di erent algorithms based around Q-Learning (Watkins 1989) combined with the Temporal Di erence algorithm (Sutton 1988), including a new algorithm (Modi ed Connectionist Q-Learning), and Q( ) (Peng and Williams 1994). In addition, we present algorithms for applying these updates on-line during trials, unlike backward replay used by Lin (1993) that requires waiting until the end of each trial before updating can occur. On-line updating is found to be more robust to the choice of training parameters than backward replay, and also enables the algorithms to be used in continuously operating systems where no end of trial conditions occur. We compare the performance of these algorithms on a realistic robot navigation problem, where a simulated mobile robot is trained to guide itself to a goal position in the presence of obstacles. The robot must rely on limited sensory feedback from its surroundings, and make decisions that can be generalised to arbitrary layouts of obstacles. These simulations show that on-line learning algorithms are less sensitive to the choice of training parameters than backward replay, and that the alternative update rules of MCQ-L and Q( ) are more robust than standard Q-learning updates. 1"
            },
            "slug": "On-line-Q-learning-using-connectionist-systems-Rummery-Niranjan",
            "title": {
                "fragments": [],
                "text": "On-line Q-learning using connectionist systems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Simulations show that on-line learning algorithms are less sensitive to the choice of training parameters than backward replay, and that the alternative update rules of MCQ-L and Q( ) are more robust than standard Q-learning updates."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 64
                            }
                        ],
                        "text": "CRBP The complementary reinforcement backpropagation algorithm (Ackley & Littman,1990) (crbp) consists of a feed-forward network mapping an encoding of the state to anencoding of the action."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16277413,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3d1de926d5e8649f3c9c8f0224092a6c1d83850",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In associative reinforcement learning, an environment generates input vectors, a learning system generates possible output vectors, and a reinforcement function computes feedback signals from the input-output pairs. The task is to discover and remember input-output pairs that generate rewards. Especially difficult cases occur when rewards are rare, since the expected time for any algorithm can grow exponentially with the size of the problem. Nonetheless, if a reinforcement function possesses regularities, and a learning algorithm exploits them, learning time can be reduced below that of non-generalizing algorithms. This paper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results on problems designed to offer differing opportunities for generalization."
            },
            "slug": "Generalization-and-Scaling-in-Reinforcement-Ackley-Littman",
            "title": {
                "fragments": [],
                "text": "Generalization and Scaling in Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes a neural network algorithm called complementary reinforcement back-propagation (CRBP), and reports simulation results on problems designed to offer differing opportunities for generalization."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2590047"
                        ],
                        "name": "C. Fiechter",
                        "slug": "C.-Fiechter",
                        "structuredName": {
                            "firstName": "Claude-Nicolas",
                            "lastName": "Fiechter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fiechter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 96
                            }
                        ],
                        "text": "In spite of the mismatch between embedded reinforcement learning and the train/testperspective, Fiechter (1994) provides a PAC analysis for Q-learning (described inSection 4.2) that sheds some light on the connection between the two views."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8783611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f67cd79df4f62742d6f3d9f1e8cd5c0cc9194d38",
            "isKey": true,
            "numCitedBy": 99,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a new formal model for studying reinforcement learning, based on Valiant's PAC framework.\nIn our model the learner does not have direct access to every state of the environment. Instead, every sequence of experiments starts in a fixed initial state and the learner is provided with a \u201creset\u201d operation that interrupts the current sequence of experiments and starts a new one (from the initial state).\nWe do not require the agent to learn the optimal policy but only a good approximation of it with high probability. More precisely, we require the learner to produce a policy whose expected value from the initial state is \u03b5-close to that of the optimal policy, with probability no less than 1\u2212\u03b4.\nFor this model, we describe an algorithm that produces such an (\u03b5,\u03b4)-optimal policy for any environment, in time polynomial in <italic>N,K</italic>,1/\u03b5,1/\u03b4,1/(1\u2212\u03b2) and <italic>r<subscrpt>max</subscrpt></italic>, where <italic>N</italic> is the number of states of the environment, <italic>K</italic> is the maximum number of actions in a state, \u03b2 is the discount factor and <italic>r<subscrpt>max</subscrpt></italic> is the maximum reward on any transition."
            },
            "slug": "Efficient-reinforcement-learning-Fiechter",
            "title": {
                "fragments": [],
                "text": "Efficient reinforcement learning"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A new formal model for studying reinforcement learning, based on Valiant's PAC framework, that requires the learner to produce a policy whose expected value from the initial state is \u03b5-close to that of the optimal policy, with probability no less than 1\u2212\u03b4."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '94"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742183"
                        ],
                        "name": "M. Matari\u0107",
                        "slug": "M.-Matari\u0107",
                        "structuredName": {
                            "firstName": "Maja",
                            "lastName": "Matari\u0107",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Matari\u0107"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7304077,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "563cf7359970015db2f8a05c41e9efef7025dda2",
            "isKey": false,
            "numCitedBy": 447,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reward-Functions-for-Accelerated-Learning-Matari\u0107",
            "title": {
                "fragments": [],
                "text": "Reward Functions for Accelerated Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11023955"
                        ],
                        "name": "Mark B. Ring",
                        "slug": "Mark-B.-Ring",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Ring",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark B. Ring"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Ring [92] has a neural-network approach that uses a variable history window, adding history when necessary to disambiguate situations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Ring (1994) has a neural-network approach that uses a variable history window,adding history when necessary to disambiguate situations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27150180,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "082b1f5c791cadef18c4920ecc1396615a3fe7cb",
            "isKey": false,
            "numCitedBy": 226,
            "numCiting": 133,
            "paperAbstract": {
                "fragments": [],
                "text": "Continual learning is the constant development of complex behaviors with no nal end in mind. It is the process of learning ever more complicated skills by building on those skills already developed. In order for learning at one stage of development to serve as the foundation for later learning, a continual-learning agent should learn hierarchically. CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development is proposed, described, tested, and evaluated in this dissertation. CHILD accumulates useful behaviors in reinforcement environments by using the Temporal Transition Hierarchies learning algorithm, also derived in the dissertation. This constructive algorithm generates a hierarchical, higher-order neural network that can be used for predicting context-dependent temporal sequences and can learn sequential-task benchmarks more than two orders of magnitude faster than competing neural-network systems. Consequently, CHILD can quickly solve complicated non-Markovian reinforcement-learning tasks and can then transfer its skills to similar but even more complicated tasks, learning these faster still. This continual-learning approach is made possible by the unique properties of Temporal Transition Hierarchies, which allow existing skills to be amended and augmented in precisely the same way that they were constructed in the rst place. Table of"
            },
            "slug": "Continual-learning-in-reinforcement-environments-Ring",
            "title": {
                "fragments": [],
                "text": "Continual learning in reinforcement environments"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "CHILD, an agent capable of Continual, Hierarchical, Incremental Learning and Development is proposed, described, tested, and evaluated in this dissertation and generates a hierarchical, higher-order neural network that can be used for predicting context-dependent temporal sequences and can learn sequential-task benchmarks more than two orders of magnitude faster than competing neural-network systems."
            },
            "venue": {
                "fragments": [],
                "text": "GMD-Bericht"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 60
                            }
                        ],
                        "text": "2 Compositional Q-learning Singh's compositional Q-learning [110, 109] (C-QL) consists of a hierarchy based on the temporal sequencing of subgoals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5972929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5cc2b2829a1fcf0260a1405f5f931efb21ffd5a",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning (RL) algorithms have traditionally been thought of as trial and error learning methods that use actual control experience to incrementally improve a control policy. Sutton's DYNA architecture demonstrated that RL algorithms can work as well using simulated experience from an environment model, and that the resulting computation was similar to doing one-step lookahead planning. Inspired by the literature on hierarchical planning, I propose learning a hierarchy of models of the environment that abstract temporal detail as a means of improving the scalability of RL algorithms. I present H-DYNA (Hierarchical DYNA), an extension to Sutton's DYNA architecture that is able to learn such a hierarchy of abstract models. H-DYNA differs from hierarchical planners in two ways: first, the abstract models are learned using experience gained while learning to solve other tasks in the same environment, and second, the abstract models can be used to solve stochastic control tasks. Simulations on a set of compositionally-structured navigation tasks show that H-DYNA can learn to solve them faster than conventional RL algorithms. The abstract models also serve as mechanisms for achieving transfer of learning across multiple tasks."
            },
            "slug": "Reinforcement-Learning-with-a-Hierarchy-of-Abstract-Singh",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning with a Hierarchy of Abstract Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulations on a set of compositionally-structured navigation tasks show that H-DYNA can learn to solve them faster than conventional RL algorithms, and the abstract models can be used to solve stochastic control tasks."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720087"
                        ],
                        "name": "S. Whitehead",
                        "slug": "S.-Whitehead",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Whitehead",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Whitehead"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 49
                            }
                        ],
                        "text": "Goal1 2 3 nFigure 5: In this environment, due to Whitehead (1991), random exploration would taketake O(2n) steps to reach the goal even once, whereas a more intelligent explo-ration strategy (e.g. \\assume any untried action leads directly to goal\") wouldrequire only O(n2) steps.the\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 209
                            }
                        ],
                        "text": "How should it gather data about the environment initially? Random exploration might be dangerous, and in some environments is an immensely ine cient method of gathering data, requiring exponentially more data (Whitehead, 1991) than a system that interleaves experience gathering with policy-building more tightly (Koenig & Simmons, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 149
                            }
                        ],
                        "text": "Random explorationmight be dangerous, and in some environments is an immensely ine cient method ofgathering data, requiring exponentially more data (Whitehead, 1991) than a systemthat interleaves experience gathering with policy-building more tightly (Koenig &Simmons, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44974425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58435c05d20a0ba3ea1d342b03e9d9dd2852835d",
            "isKey": true,
            "numCitedBy": 90,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Complexity-and-Cooperation-in-Q-Learning-Whitehead",
            "title": {
                "fragments": [],
                "text": "Complexity and Cooperation in Q-Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 28
                            }
                        ],
                        "text": "This type of sample backup (Singh, 1993) is criticalto the operation of the model-free methods discussed in the next section."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 27
                            }
                        ],
                        "text": "This type of sample backup (Singh, 1993) is critical to the operation of the model-free methods discussed in the next section."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 59715909,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c376317f738fa4795cb47c3b5c8918c9348ca965",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation is about building learning control architectures for agents embedded in finite, stationary, and Markovian environments. Such architectures give embedded agents the ability to improve autonomously the efficiency with which they can achieve goals. Machine learning researchers have developed reinforcement learning (RL) algorithms based on dynamic programming (DP) that use the agent''s experience in its environment to improve its decision policy incrementally. This is achieved by adapting an evaluation function in such a way that the decision policy that is ``greedy'''' with respect to it improves with experience. This dissertation focuses on finite, stationary and Markovian environments for two reasons: it allows the development and use of a strong theory of RL, and there are many challenging real-world RL tasks that fall into this category. This dissertation establishes a novel connection between stochastic approximation theory and RL that provides a uniform framework for understanding all the different RL algorithms that have been proposed to date. It also highlights a dimension that clearly separates all RL research from prior work on DP. Two other theoretical results showing how approximations affect performance in RL provide partial justification for the use of compact function approximators in RL. In addition, a new family of ``soft'''' DP algorithms is presented. These algorithms converge to solutions that are more robust than the solutions found by classical DP algorithms. Despite all of the theoretical progress, conventional RL architectures scale poorly enough to make them impractical for many real-world problems. This dissertation studies two aspects of the scaling issue: the need to accelerate RL, and the need to build RL architectures that can learn to solve multiple tasks. It presents three RL architectures, CQ-L, H-DYNA, and BB-RL, that accelerate learning by facilitating transfer of training from simple to complex tasks. Each architecture uses a different method to achieve transfer of training: CQ-L uses the evaluation functions for simple tasks as building blocks to construct the evaluation function for complex tasks. H-DYNA uses the evaluation functions for simple tasks to build an abstract environment model, and BB-RL uses the decision policies found for the simple tasks as the primitive actions for the complex tasks. A mixture of theoretical and empirical results are presented to support the new RL architectures developed in this dissertation."
            },
            "slug": "Learning-to-Solve-Markovian-Decision-Processes-Singh",
            "title": {
                "fragments": [],
                "text": "Learning to Solve Markovian Decision Processes"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This dissertation establishes a novel connection between stochastic approximation theory and RL that provides a uniform framework for understanding all the different RL algorithms that have been proposed to date and highlights a dimension that clearly separates all RL research from prior work on DP."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059884"
                        ],
                        "name": "V. Gullapalli",
                        "slug": "V.-Gullapalli",
                        "structuredName": {
                            "firstName": "Vijaykumar",
                            "lastName": "Gullapalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Gullapalli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Gullapalli (1990, 1992) has developed a \\neural\" reinforcement-learning unit for use incontinuous action spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21486916,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e9dc8d71572719cec58ec815bbd331fbd07fa15",
            "isKey": false,
            "numCitedBy": 317,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-stochastic-reinforcement-learning-algorithm-for-Gullapalli",
            "title": {
                "fragments": [],
                "text": "A stochastic reinforcement learning algorithm for learning real-valued functions"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844179"
                        ],
                        "name": "L. Baird",
                        "slug": "L.-Baird",
                        "structuredName": {
                            "firstName": "Leemon",
                            "lastName": "Baird",
                            "middleNames": [
                                "C."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baird"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2650569"
                        ],
                        "name": "A. Klopf",
                        "slug": "A.-Klopf",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Klopf",
                            "middleNames": [
                                "Harry"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Klopf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 56506644,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc1a6753de779a5871ee5481e455fa5e00404f1d",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Many reinforcement learning systems, such as Q-learning (Watkins, 1989), or advantage updating (Baird, 1993), require that a function f(x,u) be learned, and that the value of argmax f(x,u) be calculated quickly for any given x. The function f could be learned by a function approximation system such as a multilayer preceptron, but the maximum of f for a given x cannot found analytically and is difficult to approximate numerically for high-dimensional u vectors. A new method is proposed, wire fitting, in which a function approximation system is used to learn a set of functions called control wires, and the function f is found by fitting a surface to the control wires. Wire fitting has the following four properties: (1) any continuous f function can represented to any desired accuracy given sufficient parameters; (2) the function f(x,u) can be evaluated quickly; (3) argmax f(x,u) can found exactly in constant time after evaluating f(x,U); (4) wire fitting can incorporate any general function approximation system. These four properties are discussed and it is shown how wire fitting can be combined with a memory-based learning system and Q-learning to control an inverted-pendulum system"
            },
            "slug": "Reinforcement-Learning-With-High-Dimensional,-Baird-Klopf",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning With High-Dimensional, Continuous Actions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149607686"
                        ],
                        "name": "Anton Schwartz",
                        "slug": "Anton-Schwartz",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anton Schwartz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Schwartz (1993) examined the problem of adapting Q-learning to an average-rewardframework."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 10
                            }
                        ],
                        "text": "Thrun and Schwartz (1993) theorize that function approximation of value functionsis also dangerous because the errors in value functions due to generalization can becomecompounded by the \\max\" operator in the de nition of the value function."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10564390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99b2fd28dcab3657c5f1271a05223f4740e4b65c",
            "isKey": false,
            "numCitedBy": 328,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Reinforcement-Learning-Method-for-Maximizing-Schwartz",
            "title": {
                "fragments": [],
                "text": "A Reinforcement Learning Method for Maximizing Undiscounted Rewards"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "Sutton (1996) shows how modi ed versions of Boyan and Moore's examples can convergesuccessfully."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 144
                            }
                        ],
                        "text": "\u2026more e cient (Cichosz & Mulawka, 1995)and on changing the de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC can be accomplished in a uni ed manner\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10253791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbeb58496711887bf563ad7b0a2860fd46bcd725",
            "isKey": true,
            "numCitedBy": 1282,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "On large problems, reinforcement learning systems must use parameterized function approximators such as neural networks in order to generalize between similar situations and actions. In these cases there are no strong theoretical results on the accuracy of convergence, and computational results have been mixed. In particular, Boyan and Moore reported at last year's meeting a series of negative results in attempting to apply dynamic programming together with function approximation to simple control problems with continuous state spaces. In this paper, we present positive results for all the control tasks they attempted, and for one that is significantly larger. The most important differences are that we used sparse-coarse-coded function approximators (CMACs) whereas they used mostly global function approximators, and that we learned online whereas they learned offline. Boyan and Moore and others have suggested that the problems they encountered could be solved by using actual outcomes (\"rollouts\"), as in classical Monte Carlo methods, and as in the TD(\u03bb) algorithm when \u03bb = 1. However, in our experiments this always resulted in substantially poorer performance. We conclude that reinforcement learning can work robustly in conjunction with function approximators, and that there is little justification at present for avoiding the case of general \u03bb."
            },
            "slug": "Generalization-in-Reinforcement-Learning:-Examples-Sutton",
            "title": {
                "fragments": [],
                "text": "Generalization in Reinforcement Learning: Successful Examples Using Sparse Coarse Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "It is concluded that reinforcement learning can work robustly in conjunction with function approximators, and that there is little justification at present for avoiding the case of general \u03bb."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "\u2026the interval exploration method (Kaelbling, 1993b) (described shortly), the ex-ploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a),and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993).2.2.2 Randomized StrategiesAnother simple\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 107
                            }
                        ],
                        "text": "Thisapproach has been used by a number of researchers (Meeden, McGraw, & Blank, 1993; Lin& Mitchell, 1992; Schmidhuber, 1991b)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 895695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d33f8ba2a2e0456ef282286eed74025be0bb1af",
            "isKey": true,
            "numCitedBy": 127,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This work addresses three problems with reinforcement learning and adaptive neuro-control: 1. Non-Markovian interfaces between learner and environment. 2. On-line learning based on system realization. 3. Vector-valued adaptive critics. An algorithm is described which is based on system realization and on two interacting fully recurrent continually running networks which may learn in parallel. Problems with parallel learning are attacked by 'adaptive randomness'. It is also described how interacting model/controller systems can be combined with vector-valued 'adaptive critics' (previous critics have been scalar)."
            },
            "slug": "Reinforcement-Learning-in-Markovian-and-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning in Markovian and Non-Markovian Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work addresses three problems with reinforcement learning and adaptive neuro-control: non-Markovian interfaces between learner and environment, problems with parallel learning and how interacting model/controller systems can be combined with vector-valued 'adaptive critics'."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 119
                            }
                        ],
                        "text": "1 Adaptive Heuristic Critic and TD( ) The adaptive heuristic critic algorithm is a learning analog of policy iteration [7] in which the value-function computation is no longer implemented by solving a set of linear equations, but is instead computed by an algorithm called TD(0)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 196
                            }
                        ],
                        "text": "The second network is trained in a standardsupervised mode to estimate r as a function of the input state s.Variations of this approach have been used in a variety of applications (Anderson, 1986;Barto et al., 1983; Lin, 1993b; Sutton, 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 72
                            }
                        ],
                        "text": "Variations of this approach have been used in a variety of applications [3, 7, 46, 89]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1522994,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a7acaf6469c06ae5876d92f013184db5897bb13",
            "isKey": false,
            "numCitedBy": 3247,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem. The task is to balance a pole that is hinged to a movable cart by applying forces to the cart's base. It is argued that the learning problems faced by adaptive elements that are components of adaptive networks are at least as difficult as this version of the pole-balancing problem. The learning system consists of a single associative search element (ASE) and a single adaptive critic element (ACE). In the course of learning to balance the pole, the ASE constructs associations between input and output by searching under the influence of reinforcement feedback, and the ACE constructs a more informative evaluation function than reinforcement feedback alone can provide. The differences between this approach and other attempts to solve problems using neurolike elements are discussed, as is the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences."
            },
            "slug": "Neuronlike-adaptive-elements-that-can-solve-control-Barto-Sutton",
            "title": {
                "fragments": [],
                "text": "Neuronlike adaptive elements that can solve difficult learning control problems"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "It is shown how a system consisting of two neuronlike adaptive elements can solve a difficult learning control problem and the relation of this work to classical and instrumental conditioning in animal learning studies and its possible implications for research in the neurosciences."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304972"
                        ],
                        "name": "H. Berenji",
                        "slug": "H.-Berenji",
                        "structuredName": {
                            "firstName": "Hamid",
                            "lastName": "Berenji",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Berenji"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 76
                            }
                        ],
                        "text": "Popular techniques include various neural-network methods [94], fuzzy logic [11, 58]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 103
                            }
                        ],
                        "text": "Popular techniques include various neural-network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 24883790,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9eda9334d9b9b89124673774e51f855962d8d388",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The previous models of artificial neural networks for control do not use the existing knowledge of a physical system's behavior and train the network from scratch. The learning process is usually long, and even after the learing is completed, the resulting network can not be easily explained. On the other hand, approximate reasoning-based controllers provide a clear understanding of the control strategy but can not learn from experience. In this paper, we introduce a new method for learning to refine the control rules of approximate reasoning-based controllers. A reinforcement learning technique is used in conjunction with a multi-layer neural network model of an approximate reasoning-based controller. The model learns by updating its prediction of the physical system's behavior. Unlike previous models, our model can use the control knowledge of an experienced operator and fine-tune it through the process of learning. We discuss some of the space domains suitable for application of the new model such as rendezvous and docking, camera tracking, and tethered systems control."
            },
            "slug": "Artificial-Neural-Networks-and-Approximate-for-in-Berenji",
            "title": {
                "fragments": [],
                "text": "Artificial Neural Networks and Approximate Reasoning for Intelligent Control in Space"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A reinforcement learning technique is used in conjunction with a multi-layer neural network model of an approximate reasoning-based controller that can use the control knowledge of an experienced operator and fine-tune it through the process of learning."
            },
            "venue": {
                "fragments": [],
                "text": "1991 American Control Conference"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 120
                            }
                        ],
                        "text": "Many of the other hierarchical learning methods can be cast in this framework.6.3.1 Feudal Q-learningFeudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learningmodules."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 18
                            }
                        ],
                        "text": "Feudal Q-learning [31, 128] involves a hierarchy of learning modules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2801572,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "1678bd32846b1aded5b1e80a617170812e80f562",
            "isKey": true,
            "numCitedBy": 643,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "One way to speed up reinforcement learning is to enable learning to happen simultaneously at multiple resolutions in space and time. This paper shows how to create a Q-learning managerial hierarchy in which high level managers learn how to set tasks to their submanagers who, in turn, learn how to satisfy them. Submanagers need not initially understand their managers' commands. They simply learn to maximise their reinforcement in the context of the current command. \n \nWe illustrate the system using a simple maze task. As the system learns how to get around, satisfying commands at the multiple levels, it explores more efficiently than standard, flat, Q-learning and builds a more comprehensive map."
            },
            "slug": "Feudal-Reinforcement-Learning-Dayan-Hinton",
            "title": {
                "fragments": [],
                "text": "Feudal Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper shows how to create a Q-learning managerial hierarchy in which high level managers learning how to set tasks to their submanagers who, in turn, learn how to satisfy them."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145314818"
                        ],
                        "name": "Sven Koenig",
                        "slug": "Sven-Koenig",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Koenig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sven Koenig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719955"
                        ],
                        "name": "R. Simmons",
                        "slug": "R.-Simmons",
                        "structuredName": {
                            "firstName": "Reid",
                            "lastName": "Simmons",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Simmons"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13287928,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7cadb4ff24e7419600b8d273b581402b662dbf92",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper analyzes the complexity of on-line reinforcement learning algorithms, namely asynchronous realtime versions of Q-learning and value-iteration, applied to the problem of reaching a goal state in deterministic domains. Previous work had concluded that, in many cases, tabula rasa reinforcement learning was exponential for such problems, or was tractable only if the learning algorithm was augmented. We show that, to the contrary, the algorithms are tractable with only a simple change in the task representation or initialization. We provide tight bounds on the worst-case complexity, and show how the complexity is even smaller if the reinforcement learning algorithms have initial knowledge of the topology of the state space or the domain has certain special properties. We also present a novel bidirectional Q-learning algorithm to find optimal paths from all states to a goal state and show that it is no more complex than the other algorithms."
            },
            "slug": "Complexity-Analysis-of-Real-Time-Reinforcement-Koenig-Simmons",
            "title": {
                "fragments": [],
                "text": "Complexity Analysis of Real-Time Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper analyzes the complexity of on-line reinforcement learning algorithms, namely asynchronous realtime versions of Q-learning and value-iteration, applied to the problem of reaching a goal state in deterministic domains and shows that the algorithms are tractable with only a simple change in the task representation or initialization."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844179"
                        ],
                        "name": "L. Baird",
                        "slug": "L.-Baird",
                        "structuredName": {
                            "firstName": "Leemon",
                            "lastName": "Baird",
                            "middleNames": [
                                "C."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18786951,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4d262acad49ff3302a8a666da81088450769914",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies algorithms based on an incremental dynamic programming abstraction of one of the key issues in understanding the behavior of actor-critic learning systems. The prime example of such a learning system is the ASE/ACE architecture introduced by Barto, Sutton, and Anderson (1983). Also related are Witten's adaptive controller (1977) and Holland's bucket brigade algorithm (1986). The key feature of such a system is the presence of separate adaptive components for action selection and state evaluation, and the key issue focused on here is the extent to which their joint adaptation is guaranteed to lead to optimal behavior in the limit. In the incremental dynamic programming point of view taken here, these questions are formulated in terms of the use of separate data structures for the current best choice of policy and current best estimate of state values, with separate operations used to update each at individual states. Particular emphasis here is on the e ect of complete asynchrony in the updating of these data structures across states. The main results are that, while convergence to optimal performance is not guaranteed in general, there are a number of situations in which such convergence is assured. Since the algorithms investigated represent a certain idealized abstraction of actor-critic learning systems, these results are not directly applicable to current versions of such learning systems but may be viewed instead as providing a useful rst step toward more complete understanding of such systems. Another useful perspective on the algorithms analyzed here is that they represent a broad class of asynchronous dynamic programming procedures based on policy iteration."
            },
            "slug": "Analysis-of-Some-Incremental-Variants-of-Policy-Williams-Baird",
            "title": {
                "fragments": [],
                "text": "Analysis of Some Incremental Variants of Policy Iteration: First Steps Toward Understanding Actor-Cr"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This paper studies algorithms based on an incremental dynamic programming abstraction of one of the key issues in understanding the behavior of actor-critic learning systems, and finds that, while convergence to optimal performance is not guaranteed in general, there are a number of situations in which such convergence is assured."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157630384"
                        ],
                        "name": "L.-J. Lin",
                        "slug": "L.-J.-Lin",
                        "structuredName": {
                            "firstName": "L.-J.",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L.-J. Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Lin (1993a) and Dorigo and Colombetti (1995,1994) both used this approach, rst training the behaviors and then training the gatingfunction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 216
                            }
                        ],
                        "text": "The second network is trained in a standardsupervised mode to estimate r as a function of the input state s.Variations of this approach have been used in a variety of applications (Anderson, 1986;Barto et al., 1983; Lin, 1993b; Sutton, 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "Lin [60] and Dorigo and Colombetti [38, 37] both used this approach, rst training the behaviors and then training the gating function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62755886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c841b66e4d9835230554af30d1bbd6d5d4bf56c",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown how reinforcement learning can be made practical for complex problems by introducing hierarchical learning. The agent at first learns elementary skills for solving elementary problems. To learn a new skill for solving a complex problem later on, the agent can ignore the low-level details and focus on the problem of coordinating the elementary skills it has developed. A physically-realistic mobile robot simulator is used to demonstrate the success and importance of hierarchical learning. For fast learning, artificial neural networks are used to generalize experiences, and a teaching technique is employed to save many learning trials of the simulated robot.<<ETX>>"
            },
            "slug": "Hierarchical-learning-of-robot-skills-by-Lin",
            "title": {
                "fragments": [],
                "text": "Hierarchical learning of robot skills by reinforcement"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "It is shown how reinforcement learning can be made practical for complex problems by introducing hierarchical learning and artificial neural networks are used to generalize experiences."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850503"
                        ],
                        "name": "S. Mahadevan",
                        "slug": "S.-Mahadevan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Mahadevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mahadevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706614"
                        ],
                        "name": "J. Connell",
                        "slug": "J.-Connell",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Connell",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Connell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Mahadevan and Connell (1991a) discuss a task in which a mobile robot pushes largeboxes for extended periods of time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Mahadevan and Connell (1991b) used thedual approach: they xed the gating function, and supplied reinforcement functions for theindividual behaviors, which were learned."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 114918,
            "fieldsOfStudy": [
                "Computer Science",
                "Psychology"
            ],
            "id": "9d612473d6bba6fb7b611b88d0b5f7fff6c17fdd",
            "isKey": false,
            "numCitedBy": 734,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Programming-of-Behavior-Based-Robots-Mahadevan-Connell",
            "title": {
                "fragments": [],
                "text": "Automatic Programming of Behavior-Based Robots Using Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 148
                            }
                        ],
                        "text": "\u2026reinforcement learning algorithmsincluding the interval exploration method (Kaelbling, 1993b) (described shortly), the ex-ploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a),and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993).2.2.2\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 193
                            }
                        ],
                        "text": "Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the exploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7962049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5f8a0858fb82ce0e50b55446577a70e40137aaf",
            "isKey": true,
            "numCitedBy": 1556,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Integrated-Architectures-for-Learning,-Planning,-on-Sutton",
            "title": {
                "fragments": [],
                "text": "Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059884"
                        ],
                        "name": "V. Gullapalli",
                        "slug": "V.-Gullapalli",
                        "structuredName": {
                            "firstName": "Vijaykumar",
                            "lastName": "Gullapalli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Gullapalli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 11
                            }
                        ],
                        "text": "Gullapalli [43, 44] has developed a \\neural\" reinforcement-learning unit for use in continuous action spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Gullapalli (1990, 1992) has developed a \\neural\" reinforcement-learning unit for use incontinuous action spaces."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17944239,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb99d62aaddfe2bc8d484304fbb503ac5b5dd9a1",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 177,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning control involves modifying a controller's behavior to improve its performance as measured by some predefined index of performance (IP). If control actions that improve performance as measured by the IP are known, supervised learning methods, or methods for learning from examples, can be used to train the controller. But when such control actions are not known a priori, appropriate control behavior has to be inferred from observations of the IP. One can distinguish between two classes of methods for training controllers under such circumstances. Indirect methods involve constructing a model of the problem's IP and using the model to obtain training information for the controller. On the other hand, direct, or model-free, methods obtain the requisite training information by observing the effects of perturbing the controlled process on the IP. Despite its reputation for inefficiency, we argue that for certain types of problems the latter approach, of which reinforcement learning is an example, can yield faster, more reliable learning. Using several control problems as examples, we illustrate how the complexity of model construction can often exceed that of solving the original control problem using direct reinforcement learning methods, making indirect methods relatively inefficient. These results indicate the importance of considering direct reinforcement learning methods as tools for learning to solve control problems. We also present several techniques for augmenting the power of reinforcement learning methods. These include (1) the use of local models to guide assigning credit to the components of a reinforcement learning system, (2) implementing a procedure from experimental psychology called \"shaping\" to improve the efficiency of learning, thereby making more complex problems amenable to solution, and (3) implementing a multi-level learning architecture designed for exploiting task decomposability by using previously-learned behaviors as primitives for learning more complex tasks."
            },
            "slug": "Reinforcement-learning-and-its-application-to-Gullapalli",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning and its application to control"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that for certain types of problems the latter approach, of which reinforcement learning is an example, can yield faster, more reliable learning, while the former approach is relatively inefficient."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39263338"
                        ],
                        "name": "Longxin Lin",
                        "slug": "Longxin-Lin",
                        "structuredName": {
                            "firstName": "Longxin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longxin Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Lin (1993a) and Dorigo and Colombetti (1995,1994) both used this approach, rst training the behaviors and then training the gatingfunction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore [18] used local memory-based methods in conjunction with value iteration; Lin [59] used backpropagation networks for Q-learning; Watkins [128] used CMAC for Q-learning; Tesauro [118, 120] used backpropagation for learning the value function in backgammon (described in Section 8."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Linear programming (Schrijver, 1986) is an extremely general problem, and MDPs canbe solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux,1963; Ho man & Karp, 1966)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "Thisapproach has been used by a number of researchers (Meeden, McGraw, & Blank, 1993; Lin& Mitchell, 1992; Schmidhuber, 1991b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "Ph.D. thesis,Carnegie Mellon University, Pittsburgh, PA.Lin, L.-J., & Mitchell, T. M. (1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Lin, L.-J. (1993a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 145
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 216
                            }
                        ],
                        "text": "The second network is trained in a standardsupervised mode to estimate r as a function of the input state s.Variations of this approach have been used in a variety of applications (Anderson, 1986;Barto et al., 1983; Lin, 1993b; Sutton, 1984)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "Shaping has been used in supervised-learning systems, and can be used to train hierarchical reinforcement-learning systems from the bottom up [59], and to alleviate problems of delayed reinforcement by decreasing the delay until the problem is well understood [37, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "Theory of Linear and Integer Programming."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Lin, L.-J. (1991)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 293,
                                "start": 284
                            }
                        ],
                        "text": "In applications in which it is possible to compute a gradient,rewarding the agent for taking steps up the gradient, rather than just for achievingthe nal goal, can speed learning signi cantly (Mataric, 1994).imitation: An agent can learn by \\watching\" another agent perform the task (Lin, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 141
                            }
                        ],
                        "text": "Shaping has been used in supervised-learning systems,and can be used to train hierarchical reinforcement-learning systems from the bottomup (Lin, 1991), and to alleviate problems of delayed reinforcement by decreasing thedelay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 141
                            }
                        ],
                        "text": "Shaping has been used in supervised-learning systems,and can be used to train hierarchical reinforcement-learning systems from the bottomup (Lin, 1991), and to alleviate problems of delayed reinforcement by decreasing thedelay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995).local reinforcement signals: Whenever possible, agents should be given reinforcementsignals that are local."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 145
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Lin, L.-J. (1993b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Lin andMitchell (1992) used a xed-width nite history window to learn a pole balancing task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "imitation: An agent can learn by \\watching\" another agent perform the task [59]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 32035341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86ef873884ecc6361b8616aa1ac5695d89167f5e",
            "isKey": true,
            "numCitedBy": 199,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Programming robots is a tedious task. So, there is growing interest in building robots which can learn by themselves. Self-improving, which involves trial and error, however, is often a slow process and could be hazardous in a hostile environment. By teaching robots how tasks can be achieved, learning time can be shortened and hazard can be minimized. This paper presents a general approach to making robots which can improve their performance from experiences as well as from being taught. Based on this proposed approach and other learning speedup techniques, a simulated learning robot was developed and could learn three moderately complex behaviors, which were then integrated in a subsumption style so that the robot could navigate and recharge itself. Interestingly, a real robot could actually use what was learned in the simulator to operate in the real world quite successfully."
            },
            "slug": "Programming-Robots-Using-Reinforcement-Learning-and-Lin",
            "title": {
                "fragments": [],
                "text": "Programming Robots Using Reinforcement Learning and Teaching"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a general approach to making robots which can improve their performance from experiences as well as from being taught, and develops a simulated learning robot which could learn three moderately complex behaviors and use what was learned in the simulator to operate in the real world quite successfully."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2453007"
                        ],
                        "name": "A. Cassandra",
                        "slug": "A.-Cassandra",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Cassandra",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cassandra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 118
                            }
                        ],
                        "text": "This method is computationally intractable, but may serve as inspiration for methods that make further approximations [20, 65]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 72
                            }
                        ],
                        "text": "There is no known tight worst-case bound availablefor policy iteration (Littman et al., 1995b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11466814,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a666a97335c9ec87cce86b359bd28e85fecae580",
            "isKey": false,
            "numCitedBy": 733,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-Policies-for-Partially-Observable-Scaling-Littman-Cassandra",
            "title": {
                "fragments": [],
                "text": "Learning Policies for Partially Observable Environments: Scaling Up"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153570946"
                        ],
                        "name": "M. Dorigo",
                        "slug": "M.-Dorigo",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Dorigo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dorigo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810016"
                        ],
                        "name": "M. Colombetti",
                        "slug": "M.-Colombetti",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Colombetti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Colombetti"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 16
                            }
                        ],
                        "text": "Lin (1993a) and Dorigo and Colombetti (1995,1994) both used this approach, rst training the behaviors and then training the gatingfunction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 152
                            }
                        ],
                        "text": "\u2026systems from the bottomup (Lin, 1991), and to alleviate problems of delayed reinforcement by decreasing thedelay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995).local reinforcement signals: Whenever possible, agents should be given reinforcementsignals that are local."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 136
                            }
                        ],
                        "text": "Dorigo and Colombetti applied classi er systems to a moderately complex problem of learning robot behavior from immediate reinforcement [38, 37]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 14
                            }
                        ],
                        "text": "Dorigo, M., & Colombetti, M. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 35
                            }
                        ],
                        "text": "Lin [60] and Dorigo and Colombetti [38, 37] both used this approach, rst training the behaviors and then training the gating function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 150
                            }
                        ],
                        "text": "Dorigo and Colombetti applied classi er systems to a moderately complex problem oflearning robot behavior from immediate reinforcement (Dorigo, 1995; Dorigo & Colombetti,1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 284,
                                "start": 274
                            }
                        ],
                        "text": "Shaping has been used in supervised-learning systems,and can be used to train hierarchical reinforcement-learning systems from the bottomup (Lin, 1991), and to alleviate problems of delayed reinforcement by decreasing thedelay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995).local reinforcement signals: Whenever possible, agents should be given reinforcementsignals that are local."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2787851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73fb548322b36310483809c5c9dff9f3bee1872a",
            "isKey": true,
            "numCitedBy": 291,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Robot-Shaping:-Developing-Autonomous-Agents-Through-Dorigo-Colombetti",
            "title": {
                "fragments": [],
                "text": "Robot Shaping: Developing Autonomous Agents Through Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850503"
                        ],
                        "name": "S. Mahadevan",
                        "slug": "S.-Mahadevan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Mahadevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mahadevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706614"
                        ],
                        "name": "J. Connell",
                        "slug": "J.-Connell",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Connell",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Connell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "Mahadevan and Connell [72] used the dual approach: they xed the gating function, and supplied reinforcement functions for the individual behaviors, which were learned."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 17
                            }
                        ],
                        "text": "Mahadevan, S., & Connell, J. (1991a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 97
                            }
                        ],
                        "text": "Most interesting examples of robotic reinforcementlearning employ this technique to some extent (Connell & Mahadevan, 1993).re exes: One thing that keeps agents that know nothing from learning anything is thatthey have a hard time even nding the interesting parts of the space; they wander275\nKaelbling, Littman, & Moorearound at random never getting near the goal, or they are always \\killed\" immediately."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Mahadevan and Connell (1991a) discuss a task in which a mobile robot pushes largeboxes for extended periods of time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Connell, J., & Mahadevan, S. (1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 102
                            }
                        ],
                        "text": "In Proceedings of the Ninth National Conference onArti cial Intelligence Anaheim, CA.Mahadevan, S., & Connell, J. (1991b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Mahadevan and Connell (1991b) used thedual approach: they xed the gating function, and supplied reinforcement functions for theindividual behaviors, which were learned."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37804840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48c154af7f4cec7bc8b106cc0b1d5b44d8e0afe0",
            "isKey": true,
            "numCitedBy": 83,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Scaling-Reinforcement-Learning-to-Robotics-by-the-Mahadevan-Connell",
            "title": {
                "fragments": [],
                "text": "Scaling Reinforcement Learning to Robotics by Exploiting the Subsumption Architecture"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32358325"
                        ],
                        "name": "D. Sofge",
                        "slug": "D.-Sofge",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Sofge",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sofge"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12082697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a37a7bbd8ba95e134089621e2674a7ac753fa4f5",
            "isKey": false,
            "numCitedBy": 193,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "Whenever an intelligent agent learns to control an unknown environment, two opposing objectives have to be combined. On the one hand, the environment must be su ciently explored in order to identify a (sub-) optimal controller. For instance, a robot facing an unknown environment has to spend time moving around and acquiring knowledge. On the other hand, the environment must also be exploited during learning, i.e., experience made during learning must also be considered for action selection, if one is interested in minimizing costs of learning. For example, although a robot has to explore its environment, it should avoid collisions with obstacles once it has received some negative reward for collisions. For e cient learning, actions should thus be generated in such a way that the environment is explored and pain is avoided. This fundamental trade-o between exploration and exploitation demands e cient exploration capabilities, maximizing the e ect of learning while minimizing the costs of exploration."
            },
            "slug": "THE-ROLE-OF-EXPLORATION-IN-LEARNING-CONTROL-Sofge",
            "title": {
                "fragments": [],
                "text": "THE ROLE OF EXPLORATION IN LEARNING CONTROL"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "Whenever an intelligent agent learns to control an unknown environment, two opposing objectives have to be combined: the environment must be explored in order to identify a (sub-) optimal controller and experience made during learning must also be considered for action selection."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2624386"
                        ],
                        "name": "L. Chrisman",
                        "slug": "L.-Chrisman",
                        "structuredName": {
                            "firstName": "Lonnie",
                            "lastName": "Chrisman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chrisman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Chrisman (1992) showed how the forward-backward algorithm for learning HMMs couldbe adapted to learning POMDPs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1963904,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a1b055577a86141df13f13a3203c76a32bffdc3a",
            "isKey": false,
            "numCitedBy": 395,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "It is known that Perceptual Aliasing may significantly diminish the effectiveness of reinforcement learning algorithms [Whitehead and Ballard, 1991]. Perceptual aliasing occurs when multiple situations that are indistinguishable from immediate perceptual input require different responses from the system. For example, if a robot can only see forward, yet the presence of a battery charger behind it determines whether or not it should backup, immediate perception alone is insufficient for determining the most appropriate action. It is problematic since reinforcement algorithms typically learn a control policy from immediate perceptual input to the optimal choice of action. \n \nThis paper introduces the predictive distinctions approach to compensate for perceptual aliasing caused from incomplete perception of the world. An additional component, a predictive model, is utilized to track aspects of the world that may not be visible at all times. In addition to the control policy, the model must also be learned, and to allow for stochastic actions and noisy perception, a probabilistic model is learned from experience. In the process, the system must discover, on its own, the important distinctions in the world. Experimental results are given for a simple simulated domain, and additional issues are discussed."
            },
            "slug": "Reinforcement-Learning-with-Perceptual-Aliasing:-Chrisman",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The predictive distinctions approach to compensate for perceptual aliasing caused from incomplete perception of the world is introduced and Experimental results are given for a simple simulated domain, and additional issues are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66846174"
                        ],
                        "name": "Pawe\u0142 Cichosz",
                        "slug": "Pawe\u0142-Cichosz",
                        "structuredName": {
                            "firstName": "Pawe\u0142",
                            "lastName": "Cichosz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pawe\u0142 Cichosz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053548"
                        ],
                        "name": "J. Mulawka",
                        "slug": "J.-Mulawka",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Mulawka",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mulawka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 67
                            }
                        ],
                        "text": "Therehas been some recent work on making the updates more e cient (Cichosz & Mulawka, 1995)and on changing the de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 52864793,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b00ab15ab0c5a277cc6a08c2ff3e475cc84947fe",
            "isKey": true,
            "numCitedBy": 17,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-and-Efficient-Reinforcement-Learning-with-Cichosz-Mulawka",
            "title": {
                "fragments": [],
                "text": "Fast and Efficient Reinforcement Learning with Truncated Temporal Differences"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710442"
                        ],
                        "name": "R. Grupen",
                        "slug": "R.-Grupen",
                        "structuredName": {
                            "firstName": "Roderic",
                            "lastName": "Grupen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grupen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10969147"
                        ],
                        "name": "C. Connolly",
                        "slug": "C.-Connolly",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Connolly",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Connolly"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2683481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c3e83a0e62a4bd025622c9b7d3fdaf73c964d3a",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "While exploring to find better solutions, an agent performing online reinforcement learning (RL) can perform worse than is acceptable. In some cases, exploration might have unsafe, or even catastrophic, results, often modeled in terms of reaching 'failure' states of the agent's environment. This paper presents a method that uses domain knowledge to reduce the number of failures during exploration. This method formulates the set of actions from which the RL agent composes a control policy to ensure that exploration is conducted in a policy space that excludes most of the unacceptable policies. The resulting action set has a more abstract relationship to the task being solved than is common in many applications of RL. Although the cost of this added safety is that learning may result in a suboptimal solution, we argue that this is an appropriate tradeoff in many problems. We illustrate this method in the domain of motion planning."
            },
            "slug": "Robust-Reinforcement-Learning-in-Motion-Planning-Singh-Barto",
            "title": {
                "fragments": [],
                "text": "Robust Reinforcement Learning in Motion Planning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a method that uses domain knowledge to reduce the number of failures during exploration and formulates the set of actions from which the RL agent composes a control policy to ensure that exploration is conducted in a policy space that excludes most of the unacceptable policies."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053548"
                        ],
                        "name": "J. Mulawka",
                        "slug": "J.-Mulawka",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Mulawka",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mulawka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 67
                            }
                        ],
                        "text": "Therehas been some recent work on making the updates more e cient (Cichosz & Mulawka, 1995)and on changing the de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 18863292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a2ff5519a7348659ca9cac5304b5b06649acfcbe",
            "isKey": true,
            "numCitedBy": 6,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of temporal credit assignment in reinforcement learning is typically solved using algorithms based on the methods of temporal diierences TD(). Of those, Q-learning is currently best understood and most widely used. Using TD-based algorithms with > 0 often allows one to speed up the propagation of credit signiicantly, but it involves certain implementational problems. The traditional implementation of TD(> 0) based on eligibility traces suuers from lack of generality and computational ineeciency. The TTD (Truncated Temporal Diierences) procedure is a simple TD() approximation technique that appears to overcome these drawbacks of eligibility traces. The paper outlines this technique, discusses its computational eeciency advantages, and presents experimental studies with the combination of TTD and Q-learning in deter-ministic and stochastic environments. These experiments show that TTD makes it possible to obtain a signiicant learning speedup without reducing reliability at essentially the same computational cost as usual TD(0) learning. We conclude that the TTD procedure is probably the most promising way of using TD methods for reinforcement learning , especially for tasks with large state spaces and a hard temporal credit assignment problem."
            },
            "slug": "Fast-and-Eecient-Reinforcement-Learning-with-Mulawka",
            "title": {
                "fragments": [],
                "text": "Fast and Eecient Reinforcement Learning with Truncated Temporal Diierences"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The paper outlines this technique, discusses its computational eeciency advantages, and presents experimental studies with the combination of TTD and Q-learning in deter-ministic and stochastic environments, concluding that the TTD procedure is probably the most promising way of using TD methods for reinforcement learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1819261"
                        ],
                        "name": "S. Bradtke",
                        "slug": "S.-Bradtke",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bradtke",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bradtke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13624034,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eaec01700f5ea63af311cfd7a70a3869460ce080",
            "isKey": false,
            "numCitedBy": 1307,
            "numCiting": 170,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-to-Act-Using-Real-Time-Dynamic-Programming-Barto-Bradtke",
            "title": {
                "fragments": [],
                "text": "Learning to Act Using Real-Time Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 104
                            }
                        ],
                        "text": "Nonetheless, reinforcement-learning algorithms can be adapted to work for avery general class of games (Littman, 1994a) and many researchers have used reinforcementlearning in these environments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 15
                            }
                        ],
                        "text": "It is NP-hard (Littman, 1994b) to nd this mapping, and even thebest mapping can have very poor performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 239
                            }
                        ],
                        "text": "POMDP Approach Another strategy consists of using hidden Markov model (HMM)techniques to learn a model of the environment, including the hidden state, then to use thatmodel to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994;Lovejoy, 1991; Monahan, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 104
                            }
                        ],
                        "text": "Nonetheless, reinforcement-learning algorithms can be adapted to work for a very general class of games (Littman, 1994a) and many researchers have used reinforcement learning in these environments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8108362,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7fbf55baccbc5fdc7ded1ba18330605909aef5e5",
            "isKey": false,
            "numCitedBy": 2282,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Markov-Games-as-a-Framework-for-Multi-Agent-Littman",
            "title": {
                "fragments": [],
                "text": "Markov Games as a Framework for Multi-Agent Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 13
                            }
                        ],
                        "text": "Zhang, W., & Dietterich, T. G. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 181
                            }
                        ],
                        "text": "\u2026networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 378,
                                "start": 358
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 25
                            }
                        ],
                        "text": "1); Zhang and Dietterich [136] used backpropagation and TD( ) to learn good strategies for job-shop scheduling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6822470,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "b550e3e05701cbf6c76a8c71e91beb95f950b080",
            "isKey": true,
            "numCitedBy": 428,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply reinforcement learning methods to learn domain-specific heuristics for job shop scheduling. A repair-based scheduler starts with a critical-path schedule and incrementally repairs constraint violations with the goal of finding a short conflict-free schedule. The temporal difference algorithm TD(\u03bb) is applied to tram a neural network to learn a heuristic evaluation function over states. This evaluation function is used by a one-step lookahead search procedure to find good solutions to new scheduling problems. We evaluate this approach on synthetic problems and on problems from a NASA space shuttle pay load processing task. The evaluation function is trained on problems involving a small number of jobs and then tested on larger problems. The TD scheduler performs better than the best known existing algorithm for this task--Zwehen's iterative repair method based on simulated annealing. The results suggest that reinforcement learning can provide a new method for constructing high-performance scheduling systems."
            },
            "slug": "A-Reinforcement-Learning-Approach-to-job-shop-Zhang-Dietterich",
            "title": {
                "fragments": [],
                "text": "A Reinforcement Learning Approach to job-shop Scheduling"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Reinforcement learning methods are applied to learn domain-specific heuristics for job shop scheduling to suggest that reinforcement learning can provide a new method for constructing high-performance scheduling systems."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 15
                            }
                        ],
                        "text": "More recently, Tesauro (1992, 1994, 1995) applied the temporal di erence algorithmto backgammon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14742574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b11305f69641ecb8bd4a5e59cfebe41ad9ed989",
            "isKey": false,
            "numCitedBy": 845,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "TD-Gammon is a neural network that is able to teach itself to play backgammon solely by playing against itself and learning from the results, based on the TD() reinforcement learning algorithm (Sutton 1988). Despite starting from random initial weights (and hence random initial strategy), TD-Gammon achieves a surprisingly strong level of play. With zero knowledge built in at the start of learning (i.e., given only a raw description of the board state), the network learns to play at a strong intermediate level. Furthermore, when a set of hand-crafted features is added to the network's input representation, the result is a truly staggering level of performance: the latest version of TD-Gammon is now estimated to play at a strong master level that is extremely close to the world's best human players."
            },
            "slug": "TD-Gammon,-a-Self-Teaching-Backgammon-Program,-Play-Tesauro",
            "title": {
                "fragments": [],
                "text": "TD-Gammon, a Self-Teaching Backgammon Program, Achieves Master-Level Play"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The latest version of TD-Gammon is now estimated to play at a strong master level that is extremely close to the world's best human players."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2104930927"
                        ],
                        "name": "VehicleLisa Meedeny",
                        "slug": "VehicleLisa-Meedeny",
                        "structuredName": {
                            "firstName": "VehicleLisa",
                            "lastName": "Meedeny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "VehicleLisa Meedeny"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081634616"
                        ],
                        "name": "Douglas BlankyyDepartment",
                        "slug": "Douglas-BlankyyDepartment",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "BlankyyDepartment",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas BlankyyDepartment"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15771338,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f6e23ed9b38fdf75a426c28fd76507faf22d8c7a",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We use a connectionist network trained with reinforcement to control both an autonomous robot vehicle and a simulated robot. We show that given appropriate sensory data and architectural structure , a network can learn to control the robot for a simple navigation problem. We then investigate a more complex goal-based problem and examine the plan-like behavior that emerges. An autonomous agent can be abstractly deened as a mapping from a sequence of sensory inputs to an appropriate action in response to these percepts. Such an agent is autonomous to the extent that its behavior is determined by its immediate inputs and past experience, rather than by its built-in control (Russell & Wefald, 1991). We are interested in investigating the cognitive capabilities of autonomous agents. We believe that cog-nitive behavior can emerge from the reactive, situated activity of autonomous agents. Some consensus exists about how to design autonomous agents. There should be a relatively direct coupling between perception and action, control should be distributed and decentralized, and most importantly, there should be a dynamic interaction between the environment and the agent (Maes, 1990). However, no such consensus exists on the best method to implement these design features. Connectionist networks can easily accommodate all these design features and we believe they can be eeective mechanisms for controlling autonomous agents. This paper focuses on exploring connectionist designs for controlling simple navigation in an autonomous vehicle. We conclude by applying the successful design features to a more diicult problem and examine the plan-like behavior that emerges. Methodology Given that a connectionist controller will be used, there are still a number of implementation questions to be resolved. First, what sorts of sensory abilities will the controller need to simply react eeectively to the envi-ronment? Second, what sort of memory or training sub-tasks would enable the controller to produce more complex responses to the environment? Third, how should the controller be trained so that the behavior emerges rather than being speciied explicitly? We examine these implementation questions by testing network controllers for both a real and simulated robot in a very simple environment, using as experimental variables both type of sensory data, type of training subtasks, and amount of memory. To train the network controllers, we use a reinforcement learning algorithm which converts abstract measures of goodness (reward and punishment) into speciic teacher signals. The environment, called the \\playpen\", is a rectangular box (2 4 feet) \u2026"
            },
            "slug": "Emergent-Control-and-Planning-in-an-Autonomous-Meedeny-BlankyyDepartment",
            "title": {
                "fragments": [],
                "text": "Emergent Control and Planning in an Autonomous"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785805"
                        ],
                        "name": "M. Thathachar",
                        "slug": "M.-Thathachar",
                        "structuredName": {
                            "firstName": "Mandayam",
                            "lastName": "Thathachar",
                            "middleNames": [
                                "A.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thathachar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17464562,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5884b40f776c85d7689ea2c440f7982cb1bec46",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic automata operating in an unknown random environment have been proposed earlier as models of learning. These automata update their action probabilities in accordance with the inputs received from the environment and can improve their own performance during operation. In this context they are referred to as learning automata. A survey of the available results in the area of learning automata has been attempted in this paper. Attention has been focused on the norms of behavior of learning automata, issues in the design of updating schemes, convergence of the action probabilities, and interaction of several automata. Utilization of learning automata in parameter optimization and hypothesis testing is discussed, and potential areas of application are suggested."
            },
            "slug": "Learning-Automata-A-Survey-Narendra-Thathachar",
            "title": {
                "fragments": [],
                "text": "Learning Automata - A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "Attention has been focused on the norms of behavior of learning automata, issues in the design of updating schemes, convergence of the action probabilities, and interaction of several automata."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214276"
                        ],
                        "name": "J. Boyan",
                        "slug": "J.-Boyan",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Boyan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Boyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 70
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore [18] used local memory-based methods in conjunction with value iteration; Lin [59] used backpropagation networks for Q-learning; Watkins [128] used CMAC for Q-learning; Tesauro [118, 120] used backpropagation for learning the value function in backgammon (described in Section 8."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Boyan and Moore [18] report that their counter-examples can be made to work with problem-speci c hand-tuning despite the unreliability of untuned algorithms that provably converge in discrete domains."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "These issues are discussed by Boyan and Moore [18], who give some simple examples of value function errors growing arbitrarily large when generalization is used with value iteration."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 44
                            }
                        ],
                        "text": "Sutton (1996) shows how modi ed versions of Boyan and Moore's examples can convergesuccessfully."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 54
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 78
                            }
                        ],
                        "text": "In Sutton's com-262\nReinforcement Learning: A Surveyparative experiments with Boyan and Moore's counter-examples, he changes four aspectsof the experiments:1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Boyan and Moore sampled states uniformly in state space,whereas Sutton's method sampled along empirical trajectories."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 29
                            }
                        ],
                        "text": "Theseissues are discussed by Boyan and Moore (1995), who give some simple examples of valuefunction errors growing arbitrarily large when generalization is used with value iteration."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Boyan and Moore (1995)report that their counter-examples can be made to work with problem-speci c hand-tuningdespite the unreliability of untuned algorithms that provably converge in discrete domains."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 54
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7799595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a8f4c3f0d317fd1f0bfe0b2e3e51aac893b8144",
            "isKey": false,
            "numCitedBy": 664,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A straightforward approach to the curse of dimensionality in reinforcement learning and dynamic programming is to replace the lookup table with a generalizing function approximator such as a neural net. Although this has been successful in the domain of backgammon, there is no guarantee of convergence. In this paper, we show that the combination of dynamic programming and function approximation is not robust, and in even very benign cases, may produce an entirely wrong policy. We then introduce Grow-Support, a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization."
            },
            "slug": "Generalization-in-Reinforcement-Learning:-Safely-Boyan-Moore",
            "title": {
                "fragments": [],
                "text": "Generalization in Reinforcement Learning: Safely Approximating the Value Function"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Grow-Support is introduced, a new algorithm which is safe from divergence yet can still reap the benefits of successful generalization, and which is not robust, and in even very benign cases, may produce an entirely wrong policy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716694"
                        ],
                        "name": "J. Mill\u00e1n",
                        "slug": "J.-Mill\u00e1n",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Mill\u00e1n",
                            "middleNames": [
                                "del",
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mill\u00e1n"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 19195809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4a77730aaa6397e14a2dda6c9037cde9f82bb623",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a reinforcement connectionist learning architecture that allows an autonomous robot to acquire efficient navigation strategies in a few trials. Besides rapid learning, the architecture has three further appealing features. First, the robot improves its performance incrementally as it interacts with an initially unknown environment, and it ends up learning to avoid collisions even in those situations in which its sensors cannot detect the obstacles. This is a definite advantage over nonlearning reactive robots. Second, since it learns from basic reflexes, the robot is operational from the very beginning and the learning process is safe. Third, the robot exhibits high tolerance to noisy sensory data and good generalization abilities. All these features make this learning robot's architecture very well suited to real-world applications. We report experimental results obtained with a real mobile robot in an indoor environment that demonstrate the appropriateness of our approach to real autonomous robot control."
            },
            "slug": "Rapid,-safe,-and-incremental-learning-of-navigation-Mill\u00e1n",
            "title": {
                "fragments": [],
                "text": "Rapid, safe, and incremental learning of navigation strategies"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A reinforcement connectionist learning architecture that allows an autonomous robot to acquire efficient navigation strategies in a few trials and has high tolerance to noisy sensory data and good generalization abilities is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern. Part B"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6042780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2cca494cd58f483547a4bd059b319a915e5751bc",
            "isKey": false,
            "numCitedBy": 862,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent developments in the area of reinforcement learning have yielded a number of new algorithms for the prediction and control of Markovian environments. These algorithms, including the TD() algorithm of Sutton (1988) and the Q-learning algorithm of Watkins (1989), can be motivated heuristically as approximations to dynamic programming (DP). In this paper we provide a rigorous proof of convergence of these DP-based learning algorithms by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem. The theorem establishes a general class of convergent algorithms to which both TD() and Q-learning belong."
            },
            "slug": "On-the-Convergence-of-Stochastic-Iterative-Dynamic-Jaakkola-Jordan",
            "title": {
                "fragments": [],
                "text": "On the Convergence of Stochastic Iterative Dynamic Programming Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A rigorous proof of convergence of DP-based learning algorithms is provided by relating them to the powerful techniques of stochastic approximation theory via a new convergence theorem, which establishes a general class of convergent algorithms to which both TD() and Q-learning belong."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "CMAC [3], and local memory-based methods [84], such as generalizations of nearest neighbor methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13297119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8fe1e02acecc5fbab9e0fe48edbd1d1af4c47e97",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "A challenge for learning control systems is to build a model of a nonlinear plant from samples of the plant's inputs and outputs. We have used a memory-based technique to approach this problem. Samples of the plant's inputs and outputs are stored in a memory. Predictions of the plant's outputs given an input vector (or vice versa) are made by constructing a local model from nearby samples, and using the local model to interpolate between and extrapolate from samples in the database. This approach implements a philosophy of modeling a complex plant with many simple local models. A local model is constructed by performing a regression of a polynomial surface on the data. The regression is locally weighted by a weighting function of the distance between the desired vector and each vector stored in memory. In order to find relevant information in the memory and to perform the locally weighted regression a distance metric, weight function shape parameter, and ridge regression parameters are required. These parameters are found using a cross-validation approach. The parameters indicate which variables and terms in the local model are relevant to modeling the plant."
            },
            "slug": "Memory-Based-Learning-Control-Atkeson",
            "title": {
                "fragments": [],
                "text": "Memory-Based Learning Control"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This approach implements a philosophy of modeling a complex plant with many simple local models by using a memory-based technique to approach the problem of building a model of a nonlinear plant from samples of the plant's inputs and outputs."
            },
            "venue": {
                "fragments": [],
                "text": "1991 American Control Conference"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145676014"
                        ],
                        "name": "D. Cliff",
                        "slug": "D.-Cliff",
                        "structuredName": {
                            "firstName": "Dave",
                            "lastName": "Cliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cliff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068082728"
                        ],
                        "name": "Susi Ross",
                        "slug": "Susi-Ross",
                        "structuredName": {
                            "firstName": "Susi",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susi Ross"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Cli , D., & Ross, S. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Cli and Ross [21] start with Wilson's zeroth-level classi er system [108] and add one and two-bit memory registers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Cli and Ross (1994) start with Wilson's zeroth-268\nReinforcement Learning: A Survey i\nb a SE \u03c0Figure 10: Structure of a POMDP agent.level classi er system (Wilson, 1995) and add one and two-bit memory registers."
                    },
                    "intents": []
                }
            ],
            "corpusId": 21342018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d611b1049f336528b736380a995150acbf70fa3",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "In a recent article, Wilson (1994) described a \"zeroth-level\" classifier system (ZCS). ZCS employs a reinforcement learning technique comparable to Q-learning (Watkins, 1989). This article presents results from the first reconstruction of ZCS. Having replicated Wilson's results, we extend ZCS in a manner suggested by Wilson: The original formulation of ZCS has no memory mechanisms, but Wilson (1994b) suggested how internal \"temporary memory\" registers could be added. We show results from adding one-bit and two-bit memory registers to ZCS. Our results demonstrate that ZCS can exploit memory facilities efficiently in non-Markov environments. We also show that the memoryless ZCS can converge on near-optimal stochastic solutions in non-Markov environments. We then present results from trials using ZCS in Markov environments that require increasingly long chains of actions before reward is received. Our results indicate that inaccurate overgeneral classifiers can interact with the classifier-generation mechanisms to cause catastrophic breakdowns in overall system performance. Basing classifier fitness on accuracy may alleviate this problem. We conclude that the memory mechanism in its current form is unlikely to scale well for situations requiring large amounts of temporary memory. Nevertheless, the ability to find stochastic solutions when there is insufficient memory might offset this problem somewhat."
            },
            "slug": "Adding-Temporary-Memory-to-ZCS-Cliff-Ross",
            "title": {
                "fragments": [],
                "text": "Adding Temporary Memory to ZCS"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Results from the first reconstruction of ZCS are presented, showing that inaccurate overgeneral classifiers can interact with the classifier-generation mechanisms to cause catastrophic breakdowns in overall system performance and finding stochastic solutions when there is insufficient memory."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1952931"
                        ],
                        "name": "M. Salganicoff",
                        "slug": "M.-Salganicoff",
                        "structuredName": {
                            "firstName": "Marcos",
                            "lastName": "Salganicoff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Salganicoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1412391493"
                        ],
                        "name": "L. Ungar",
                        "slug": "L.-Ungar",
                        "structuredName": {
                            "firstName": "Lyle",
                            "lastName": "Ungar",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ungar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40294919,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e7555dbcf60f8c4ffa6944d2f762d7eed8c2314",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Active-Exploration-and-Learning-in-real-Valued-Salganicoff-Ungar",
            "title": {
                "fragments": [],
                "text": "Active Exploration and Learning in real-Valued Spaces using Multi-Armed Bandit Allocation Indices"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064587073"
                        ],
                        "name": "C. K. Tham",
                        "slug": "C.-K.-Tham",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Tham",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. K. Tham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680968"
                        ],
                        "name": "R. Prager",
                        "slug": "R.-Prager",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Prager",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Prager"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 15
                            }
                        ],
                        "text": "Tham, C.-K., & Prager, R. W. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "This method was used by Tham and Prager [121] to learn to control a simulated multi-link robot arm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 24
                            }
                        ],
                        "text": "This method was used by Tham and Prager (1994) to learn to control asimulated multi-link robot arm.6.3.3 Hierarchical Distance to GoalEspecially if we consider reinforcement learning modules to be part of larger agent archi-tectures, it is important to consider problems in which goals are dynamically input to thelearner."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 24
                            }
                        ],
                        "text": "This method was used by Tham and Prager (1994) to learn to control asimulated multi-link robot arm.6.3.3 Hierarchical Distance to GoalEspecially if we consider reinforcement learning modules to be part of larger agent archi-tectures, it is important to consider problems in which goals are\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9232962,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51bf6766c414601a89a7488682fc0ac2e3479b98",
            "isKey": true,
            "numCitedBy": 57,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Modular-Q-Learning-Architecture-for-Manipulator-Tham-Prager",
            "title": {
                "fragments": [],
                "text": "A Modular Q-Learning Architecture for Manipulator Task Decomposition"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21889436"
                        ],
                        "name": "Geoffrey J. Gordon",
                        "slug": "Geoffrey-J.-Gordon",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Gordon",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey J. Gordon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11137395,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "0c4edc609f977cefb305f76a991514a83f8088e3",
            "isKey": false,
            "numCitedBy": 591,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Stable-Function-Approximation-in-Dynamic-Gordon",
            "title": {
                "fragments": [],
                "text": "Stable Function Approximation in Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701876"
                        ],
                        "name": "P. Maes",
                        "slug": "P.-Maes",
                        "structuredName": {
                            "firstName": "Pattie",
                            "lastName": "Maes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Maes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72419159"
                        ],
                        "name": "R. Brooks",
                        "slug": "R.-Brooks",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Brooks",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brooks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 89
                            }
                        ],
                        "text": "Annals of Operations Research, 28, 47{66.280\nReinforcement Learning: A SurveyMaes, P., & Brooks, R. A. (1990)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Maes and Brooks (1990) useda version of this architecture in which the individual behaviors were xed a priori and thegating function was learned from reinforcement."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Maes and Brooks [68] used a version of this architecture in which the individual behaviors were xed a priori and the gating function was learned from reinforcement."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1566702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "269047d9b8ea3594c665399e4d029b1990307ed4",
            "isKey": true,
            "numCitedBy": 480,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an algorithm which allows a behavior-based robot to learn on the basis of positive and negative feedback when to activate its behaviors. In accordance with the philosophy of behavior-based robots, the algorithm is completely distributed: each of the behaviors independently tries to sensors find out (i) whether it is relevant (i.e. whether it is at all correlated to positive feedback) and (ii) what the conditions are under which it becomes reliable (i.e. the conditions under which it maximises the probability of receiving positive feedback and minimises the probability of receiving negative feedback). The algorithm has been tested successfully on an autonomous 6-legged robot which had to learn how to coordinate its legs so as to walk forward."
            },
            "slug": "Learning-to-Coordinate-Behaviors-Maes-Brooks",
            "title": {
                "fragments": [],
                "text": "Learning to Coordinate Behaviors"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "An algorithm which allows a behavior-based robot to learn on the basis of positive and negative feedback when to activate its behaviors has been described and tested successfully on an autonomous 6-legged robot."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844179"
                        ],
                        "name": "L. Baird",
                        "slug": "L.-Baird",
                        "structuredName": {
                            "firstName": "Leemon",
                            "lastName": "Baird",
                            "middleNames": [
                                "C."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baird"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 36
                            }
                        ],
                        "text": "Baird's residual gradient technique (Baird, 1995) provides guaranteed convergence to locally optimal solutions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 37
                            }
                        ],
                        "text": "Baird's residual gradient technique (Baird, 1995) provides guaranteedconvergence to locally optimal solutions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 621595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f518bffb712a298bff18248c67f6fc0181018ae6",
            "isKey": false,
            "numCitedBy": 1066,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Residual-Algorithms:-Reinforcement-Learning-with-Baird",
            "title": {
                "fragments": [],
                "text": "Residual Algorithms: Reinforcement Learning with Function Approximation"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 14
                            }
                        ],
                        "text": "He, and later McCallum (1993), also gave heuristic state-splitting rules to attempt to learn the smallest possible model for a given environment."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17063561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ae6fbda6080f3ed6ec29a8d62a8c8941d7263d4",
            "isKey": false,
            "numCitedBy": 204,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Overcoming-Incomplete-Perception-with-Utile-Memory-McCallum",
            "title": {
                "fragments": [],
                "text": "Overcoming Incomplete Perception with Utile Distinction Memory"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2748374"
                        ],
                        "name": "R. Crites",
                        "slug": "R.-Crites",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Crites",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Crites"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 58
                            }
                        ],
                        "text": "Q-learning has been used in an elevator dispatching task (Crites & Barto, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "Q-learning has been used in an elevator dispatching task [29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14551518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7e5a400e63e74f44d07be8b4742472c981ca5b8",
            "isKey": false,
            "numCitedBy": 637,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the application of reinforcement learning (RL) to the difficult real world problem of elevator dispatching. The elevator domain poses a combination of challenges not seen in most RL research to date. Elevator systems operate in continuous state spaces and in continuous time as discrete event dynamic systems. Their states are not fully observable and they are nonstationary due to changing passenger arrival rates. In addition, we use a team of RL agents, each of which is responsible for controlling one elevator car. The team receives a global reinforcement signal which appears noisy to each agent due to the effects of the actions of the other agents, the random nature of the arrivals and the incomplete observation of the state. In spite of these complications, we show results that in simulation surpass the best of the heuristic elevator control algorithms of which we are aware. These results demonstrate the power of RL on a very large scale stochastic dynamic optimization problem of practical utility."
            },
            "slug": "Improving-Elevator-Performance-Using-Reinforcement-Crites-Barto",
            "title": {
                "fragments": [],
                "text": "Improving Elevator Performance Using Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Results in simulation surpass the best of the heuristic elevator control algorithms of which the author is aware and demonstrate the power of RL on a very large scale stochastic dynamic optimization problem of practical utility."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745219"
                        ],
                        "name": "S. Schaal",
                        "slug": "S.-Schaal",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schaal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Schaal and Atkeson (1994) constructed a two-armed robot, shown in Figure 11, thatlearns to juggle a device known as a devil-stick."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2004600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82479b544924ee734fb22f9dce78aace0f90cd3c",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 134,
            "paperAbstract": {
                "fragments": [],
                "text": "Issues involved in implementing robot learning for a challenging dynamic task are explored in this article, using a case study from robot juggling. We use a memory-based local modeling approach (locally weighted regression) to represent a learned model of the task to be performed. Statistical tests are given to examine the uncertainty of a model, to optimize its prediction quality, and to deal with noisy and corrupted data. We develop an exploration algorithm that explicitly deals with prediction accuracy requirements during exploration. Using all these ingredients in combination with methods from optimal control, our robot achieves fast real-time learning of the task within 40 to 100 trials.<<ETX>>"
            },
            "slug": "Robot-juggling:-implementation-of-memory-based-Schaal-Atkeson",
            "title": {
                "fragments": [],
                "text": "Robot juggling: implementation of memory-based learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A memory-based local modeling approach (locally weighted regression) is used to represent a learned model of the task to be performed, and an exploration algorithm is developed that explicitly deals with prediction accuracy requirements during exploration."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Control Systems"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 34
                            }
                        ],
                        "text": "2 Dyna Sutton's Dyna architecture [116, 117] exploits a middle ground, yielding strategies that are both more e ective than model-free learning and more computationally e cient than the certainty-equivalence approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15773832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "87bfc4daa478433d8b18a47edf9112a25098cada",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Planning-by-Incremental-Dynamic-Programming-Sutton",
            "title": {
                "fragments": [],
                "text": "Planning by Incremental Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2341152"
                        ],
                        "name": "Stewart W. Wilson",
                        "slug": "Stewart-W.-Wilson",
                        "structuredName": {
                            "firstName": "Stewart",
                            "lastName": "Wilson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stewart W. Wilson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "Cli and Ross (1994) start with Wilson's zeroth-268\nReinforcement Learning: A Survey i\nb a SE \u03c0Figure 10: Structure of a POMDP agent.level classi er system (Wilson, 1995) and add one and two-bit memory registers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18341635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "844ebc6d5d79cb1acb71f787c424ab0d59b248d7",
            "isKey": false,
            "numCitedBy": 1472,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "In many classifier systems, the classifier strength parameter serves as a predictor of future payoff and as the classifier's fitness for the genetic algorithm. We investigate a classifier system, XCS, in which each classifier maintains a prediction of expected payoff, but the classifier's fitness is given by a measure of the prediction's accuracy. The system executes the genetic algorithm in niches defined by the match sets, instead of panmictically. These aspects of XCS result in its population tending to form a complete and accurate mapping X A P from inputs and actions to payoff predictions. Further, XCS tends to evolve classifiers that are maximally general, subject to an accuracy criterion. Besides introducing a new direction for classifier system research, these properties of XCS make it suitable for a wide range of reinforcement learning situations where generalization over states is desirable."
            },
            "slug": "Classifier-Fitness-Based-on-Accuracy-Wilson",
            "title": {
                "fragments": [],
                "text": "Classifier Fitness Based on Accuracy"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A classifier system, XCS, is investigated, in which each classifier maintains a prediction of expected payoff, but the classifier's fitness is given by a measure of the prediction's accuracy, making it suitable for a wide range of reinforcement learning situations where generalization over states is desirable."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 95
                            }
                        ],
                        "text": "Many algorithms come with a provable guar-antee of asymptotic convergence to optimal behavior (Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 295,
                                "start": 274
                            }
                        ],
                        "text": "\u2026de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC can be accomplished in a uni ed manner byWatkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6134427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aab43c9c33af00b718cf2ae374b861d49862a563",
            "isKey": true,
            "numCitedBy": 15782,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine Learning is the study of methods for programming computers to learn. Computers are applied to a wide range of tasks, and for most of these it is relatively easy for programmers to design and implement the necessary software. However, there are many tasks for which this is difficult or impossible. These can be divided into four general categories. First, there are problems for which there exist no human experts. For example, in modern automated manufacturing facilities, there is a need to predict machine failures before they occur by analyzing sensor readings. Because the machines are new, there are no human experts who can be interviewed by a programmer to provide the knowledge necessary to build a computer system. A machine learning system can study recorded data and subsequent machine failures and learn prediction rules. Second, there are problems where human experts exist, but where they are unable to explain their expertise. This is the case in many perceptual tasks, such as speech recognition, hand-writing recognition, and natural language understanding. Virtually all humans exhibit expert-level abilities on these tasks, but none of them can describe the detailed steps that they follow as they perform them. Fortunately, humans can provide machines with examples of the inputs and correct outputs for these tasks, so machine learning algorithms can learn to map the inputs to the outputs. Third, there are problems where phenomena are changing rapidly. In finance, for example, people would like to predict the future behavior of the stock market, of consumer purchases, or of exchange rates. These behaviors change frequently, so that even if a programmer could construct a good predictive computer program, it would need to be rewritten frequently. A learning program can relieve the programmer of this burden by constantly modifying and tuning a set of learned prediction rules. Fourth, there are applications that need to be customized for each computer user separately. Consider, for example, a program to filter unwanted electronic mail messages. Different users will need different filters. It is unreasonable to expect each user to program his or her own rules, and it is infeasible to provide every user with a software engineer to keep the rules up-to-date. A machine learning system can learn which mail messages the user rejects and maintain the filtering rules automatically. Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis. Statistics focuses on understanding the phenomena that have generated the data, often with the goal of testing different hypotheses about those phenomena. Data mining seeks to find patterns in the data that are understandable by people. Psychological studies of human learning aspire to understand the mechanisms underlying the various learning behaviors exhibited by people (concept learning, skill acquisition, strategy change, etc.)."
            },
            "slug": "Machine-learning-Dietterich",
            "title": {
                "fragments": [],
                "text": "Machine learning"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Machine learning addresses many of the same research questions as the fields of statistics, data mining, and psychology, but with differences of emphasis."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 46
                            }
                        ],
                        "text": "Other games that have been studied include Go [104] and Chess [122]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6314186,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73382b3efd243d330a902e6a1eb0f6b32dbc5f29",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The game of Go has a high branching factor that defeats the tree search approach used in computer chess, and long-range spatiotemporal interactions that make position evaluation extremely difficult. Development of conventional Go programs is hampered by their knowledge-intensive nature. We demonstrate a viable alternative by training networks to evaluate Go positions via temporal difference (TD) learning. \n \nOur approach is based on network architectures that reflect the spatial organization of both input and reinforcement signals on the Go board, and training protocols that provide exposure to competent (though unlabelled) play. These techniques yield far better performance than undifferentiated networks trained by selfplay alone. A network with less than 500 weights learned within 3,000 games of 9\u00d79 Go a position evaluation function that enables a primitive one-ply search to defeat a commercial Go program at a low playing level."
            },
            "slug": "Temporal-Difference-Learning-of-Position-Evaluation-Schraudolph-Dayan",
            "title": {
                "fragments": [],
                "text": "Temporal Difference Learning of Position Evaluation in the Game of Go"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work demonstrates a viable alternative by training networks to evaluate Go positions via temporal difference (TD) learning, based on network architectures that reflect the spatial organization of both input and reinforcement signals on the Go board, and training protocols that provide exposure to competent (though unlabelled) play."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "McCallum [60] suggests some related tree-structured methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "Thissystem has had excellent results in a very complex driving-simulation domain (McCallum,1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 99
                            }
                        ],
                        "text": "It outperformed Q-learning with backpropagation in a simple video-game environment and was used by McCallum (1995) (in conjunction with other techniquesfor dealing with partial observability) to learn behaviors in a complex driving-simulator."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "McCallum (1995) suggests some related tree-structuredmethods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "McCallum (1995) describes the \\utile su x memory\" which learns a variable-width windowthat serves simultaneously as a model of the environment and a nite-memory policy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "McCallum [60] describes the \\utile su x memory\" which learns a variable-width window that serves simultaneously as a model of the environment and a nite-memory policy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5484591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f555d8cd2cb4643f7367b3beecc9a1a0f82b41d3",
            "isKey": true,
            "numCitedBy": 210,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Instance-Based-Utile-Distinctions-for-Reinforcement-McCallum",
            "title": {
                "fragments": [],
                "text": "Instance-Based Utile Distinctions for Reinforcement Learning with Hidden State"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741124"
                        ],
                        "name": "L. Valiant",
                        "slug": "L.-Valiant",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Valiant",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Valiant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 45
                            }
                        ],
                        "text": "For example, in the well-knownPAC framework (Valiant, 1984), there is a learning period during which mistakes donot count, then a performance period during which they do."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 45
                            }
                        ],
                        "text": "For example, in the well-known PAC framework (Valiant, 1984), there is a learning period during which mistakes do not count, then a performance period during which they do."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10ddb646feddc12337b5a755c72e153e37088c02",
            "isKey": false,
            "numCitedBy": 4243,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Humans appear to be able to learn new concepts without needing to be programmed explicitly in any conventional sense. In this paper we regard learning as the phenomenon of knowledge acquisition in the absence of explicit programming. We give a precise methodology for studying this phenomenon from a computational viewpoint. It consists of choosing an appropriate information gathering mechanism, the learning protocol, and exploring the class of concepts that can be learnt using it in a reasonable (polynomial) number of steps. We find that inherent algorithmic complexity appears to set serious limits to the range of concepts that can be so learnt. The methodology and results suggest concrete principles for designing realistic learning systems."
            },
            "slug": "A-theory-of-the-learnable-Valiant",
            "title": {
                "fragments": [],
                "text": "A theory of the learnable"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper regards learning as the phenomenon of knowledge acquisition in the absence of explicit programming, and gives a precise methodology for studying this phenomenon from a computational viewpoint."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '84"
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055652033"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "Jiirgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "\u2026the interval exploration method (Kaelbling, 1993b) (described shortly), the ex-ploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a),and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993).2.2.2 Randomized StrategiesAnother simple\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 107
                            }
                        ],
                        "text": "Thisapproach has been used by a number of researchers (Meeden, McGraw, & Blank, 1993; Lin& Mitchell, 1992; Schmidhuber, 1991b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 238
                            }
                        ],
                        "text": "Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method (Kaelbling, 1993b) (described shortly), the exploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a), and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17874844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "94db34f4b68189bfcba22beab33ee3b54f10b876",
            "isKey": true,
            "numCitedBy": 608,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel curious model-building control system is described which actively tries to provoke situations for which it learned to expect to learn something about the environment. Such a system has been implemented as a four-network system based on Watkins' Q-learning algorithm which can be used to maximize the expectation of the temporal derivative of the adaptive assumed reliability of future predictions. An experiment with an artificial nondeterministic environment demonstrates that the system can be superior to previous model-building control systems, which do not address the problem of modeling the reliability of the world model's predictions in uncertain environments and use ad-hoc methods (like random search) to train the world model.<<ETX>>"
            },
            "slug": "Curious-model-building-control-systems-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Curious model-building control systems"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A novel curious model-building control system is described which actively tries to provoke situations for which it learned to expect to learn something about the environment, based on Watkins' Q-learning algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] 1991 IEEE International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39876445"
                        ],
                        "name": "Chuen-Chien Lee",
                        "slug": "Chuen-Chien-Lee",
                        "structuredName": {
                            "firstName": "Chuen-Chien",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuen-Chien Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 101
                            }
                        ],
                        "text": "Popular techniques include various neuralnetwork methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 118
                            }
                        ],
                        "text": "Popular techniques include various neural-network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 32614664,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51b7f5ed41ef96ef3b8c36d227d95ff61cc46739",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A self\u2010learning controller is proposed as an intelligent controller for dynamic processes, employing a control policy which evolves and improves automatically. A key component of the controller is a rule\u2010based system which provides a linguistic description of control strategy. This strategy has the form of a collection of fuzzy conditional statements which are implemented and manipulated using fuzzy set theory. the inference engine of the controller is based on the principles of approximate reasoning, while its learning capability is provided by neuron\u2010like elements, which are derived from animal conditioning theory. It is shown that the system can solve a fairly difficult control learning problem. More concretely, the task is 1\u2010D pole balancing, in which a pole is hinged to a movable cart to which a continously variable control force is applied. Simulation results demonstrate that improved learning performance can be achieved in relation to previously described systems employing bang\u2010bang control. Furthermore, the proposed controller is relatively insensitive to variations in the parameters of the system, for example, changes in the length and mass of the pole, initial angle, failure criteria, and slanting the base of the car\u2010pole system."
            },
            "slug": "A-self\u2010learning-rule\u2010based-controller-employing-and-Lee",
            "title": {
                "fragments": [],
                "text": "A self\u2010learning rule\u2010based controller employing approximate reasoning and neural net concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A self\u2010learning controller is proposed as an intelligent controller for dynamic processes, employing a control policy which evolves and improves automatically, and a key component is a rule\u2010based system which provides a linguistic description of control strategy."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Intell. Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37814588"
                        ],
                        "name": "M. Puterman",
                        "slug": "M.-Puterman",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Puterman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Puterman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Puterman (1994) discusses another stoppingcriterion, based on the span semi-norm, which may result in earlier termination."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 90
                            }
                        ],
                        "text": "There are many good references toMDP models (Bellman, 1957; Bertsekas, 1987; Howard,1960; Puterman, 1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122678161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8090121ad488b4af27bc59bf91b62e9c6a6f49c6",
            "isKey": false,
            "numCitedBy": 11670,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThe past decade has seen considerable theoretical and applied research on Markov decision processes, as well as the growing use of these models in ecology, economics, communications engineering, and other fields where outcomes are uncertain and sequential decision-making processes are needed. A timely response to this increased activity, Martin L. Puterman's new work provides a uniquely up-to-date, unified, and rigorous treatment of the theoretical, computational, and applied research on Markov decision process models. It discusses all major research directions in the field, highlights many significant applications of Markov decision processes models, and explores numerous important topics that have previously been neglected or given cursory coverage in the literature. Markov Decision Processes focuses primarily on infinite horizon discrete time models and models with discrete time spaces while also examining models with arbitrary state spaces, finite horizon models, and continuous-time discrete state models. The book is organized around optimality criteria, using a common framework centered on the optimality (Bellman) equation for presenting results. The results are presented in a \"theorem-proof\" format and elaborated on through both discussion and examples, including results that are not available in any other book. A two-state Markov decision process model, presented in Chapter 3, is analyzed repeatedly throughout the book and demonstrates many results and algorithms. Markov Decision Processes covers recent research advances in such areas as countable state space models with average reward criterion, constrained models, and models with risk sensitive optimality criteria. It also explores several topics that have received little or no attention in other books, including modified policy iteration, multichain models with average reward criterion, and sensitive optimality. In addition, a Bibliographic Remarks section in each chapter comments on relevant historic"
            },
            "slug": "Markov-Decision-Processes:-Discrete-Stochastic-Puterman",
            "title": {
                "fragments": [],
                "text": "Markov Decision Processes: Discrete Stochastic Dynamic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Markov Decision Processes covers recent research advances in such areas as countable state space models with average reward criterion, constrained models, and models with risk sensitive optimality criteria, and explores several topics that have received little or no attention in other books."
            },
            "venue": {
                "fragments": [],
                "text": "Wiley Series in Probability and Statistics"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37814588"
                        ],
                        "name": "M. Puterman",
                        "slug": "M.-Puterman",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Puterman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Puterman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24313010"
                        ],
                        "name": "M. C. Shin",
                        "slug": "M.-C.-Shin",
                        "structuredName": {
                            "firstName": "Moon",
                            "lastName": "Shin",
                            "middleNames": [
                                "Chirl"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. C. Shin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 47
                            }
                        ],
                        "text": "Puterman's modi ed policy iteration algorithm (Puterman & Shin, 1978)provides a method for trading iteration time for iteration improvement in a smoother way."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 26
                            }
                        ],
                        "text": "Modi ed policy iteration (Puterman & Shin,1978) seeks a trade-o between cheap and e ective iterations and is preferred by somepractictioners (Rust, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122017624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7b608eff19a85f85b34cda3ecb66d0a3e572583",
            "isKey": false,
            "numCitedBy": 280,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we study a class of modified policy iteration algorithms for solving Markov decision problems. These correspond to performing policy evaluation by successive approximations. We discuss the relationship of these algorithms to Newton-Kantorovich iteration and demonstrate their covergence. We show that all of these algorithms converge at least as quickly as successive approximations and obtain estimates of their rates of convergence. An analysis of the computational requirements of these algorithms suggests that they may be appropriate for solving problems with either large numbers of actions, large numbers of states, sparse transition matrices, or small discount rates. These algorithms are compared to policy iteration, successive approximations, and Gauss-Seidel methods on large randomly generated test problems."
            },
            "slug": "Modified-Policy-Iteration-Algorithms-for-Discounted-Puterman-Shin",
            "title": {
                "fragments": [],
                "text": "Modified Policy Iteration Algorithms for Discounted Markov Decision Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A class of modified policy iteration algorithms for solving Markov decision problems correspond to performing policy evaluation by successive approximations and it is shown that all of these algorithms converge at least as quickly as successive approxIMations and estimates of their rates of convergence are obtained."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10566652,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a82db864e472b5aa6313596ef9919f64e3363b1f",
            "isKey": false,
            "numCitedBy": 10134,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "The leading and most up-to-date textbook on the far-ranging algorithmic methododogy of Dynamic Programming, which can be used for optimal control, Markovian decision problems, planning and sequential decision making under uncertainty, and discrete/combinatorial optimization. The treatment focuses on basic unifying themes, and conceptual foundations. It illustrates the versatility, power, and generality of the method with many examples and applications from engineering, operations research, and other fields. It also addresses extensively the practical application of the methodology, possibly through the use of approximations, and provides an extensive treatment of the far-reaching methodology of Neuro-Dynamic Programming/Reinforcement Learning."
            },
            "slug": "Dynamic-Programming-and-Optimal-Control-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming and Optimal Control"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The leading and most up-to-date textbook on the far-ranging algorithmic methododogy of Dynamic Programming, which can be used for optimal control, Markovian decision problems, planning and sequential decision making under uncertainty, and discrete/combinatorial optimization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844179"
                        ],
                        "name": "L. Baird",
                        "slug": "L.-Baird",
                        "structuredName": {
                            "firstName": "Leemon",
                            "lastName": "Baird",
                            "middleNames": [
                                "C."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baird"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16785747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a87ed531515e45a56b6a1cd608b62a644e38a9b",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider a given value function on states of a Markov decision problem, as might result from applying a reinforcement learning algorithm. Unless this value function equals the corresponding optimal value function , at some states there will be a discrepancy, which is natural to call the Bellman residual, between what the value function speciies at that state and what is obtained by a one-step lookahead along the seemingly best action at that state using the given value function to evaluate all succeeding states. This paper derives a tight bound on how far from optimal the discounted return for a greedy policy based on the given value function will be as a function of the maximum norm magnitude of this Bellman residual. A corresponding result is also obtained for value functions deened on state-action pairs, as are used in Q-learning. One signiicant application of these results is to problems where a function approximator is used to learn a value function, with training of the approxi-mator based on trying to minimize the Bellman residual across states or state-action pairs. When control is based on the use of the resulting value function, this result provides a link between how well the objectives of function approximator training are met and the quality of the resulting control."
            },
            "slug": "Tight-Performance-Bounds-on-Greedy-Policies-Based-Williams-Baird",
            "title": {
                "fragments": [],
                "text": "Tight Performance Bounds on Greedy Policies Based on Imperfect Value Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A tight bound is derived on how far from optimal the discounted return for a greedy policy based on the given value function will be as a function of the maximum norm magnitude of this Bellman residual."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "These problems are addressed by prioritized sweeping [66] and Queue-Dyna [70], which are two independently-developed but very similar techniques."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 242,
                                "start": 238
                            }
                        ],
                        "text": "Techniques like this have been used in several reinforcement learning algorithms including the interval exploration method [39] (described shortly), the exploration bonus in Dyna [91] and the exploration mechanism in prioritized sweeping [66]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2294833,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "049c6719ac3470e03316d75b3d605b79eccd24e5",
            "isKey": false,
            "numCitedBy": 263,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new algorithm, Prioritized Sweeping, for e cient prediction and control of stochastic Markov systems. Incremental learning methods such as Temporal Di erencing and Qlearning have fast real time performance. Classical methods are slower, but more accurate, because they make full use of the observations. Prioritized Sweeping aims for the best of both worlds. It uses all previous experiences both to prioritize important dynamic programming sweeps and to guide the exploration of state-space. We compare Prioritized Sweeping with other reinforcement learning schemes for a number of di erent stochastic optimal control problems. It successfully solves large state-space real time problems with which other methods have"
            },
            "slug": "Prioritized-Sweeping-:-Reinforcement-Learning-with-Moore-Atkeson",
            "title": {
                "fragments": [],
                "text": "Prioritized Sweeping : Reinforcement Learning with Less Data and Less Real Time"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Prioritized Sweeping successfully solves large state-space real time problems with which other methods have failed, and is compared with other reinforcement learning schemes for a number of stochastic optimal control problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47200559"
                        ],
                        "name": "John Rust",
                        "slug": "John-Rust",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Rust",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Rust"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 143
                            }
                        ],
                        "text": "Modi ed policy iteration (Puterman & Shin, 1978) seeks a trade-o between cheap and e ective iterations and is preferred by some practictioners (Rust, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 142
                            }
                        ],
                        "text": "Modi ed policy iteration (Puterman & Shin,1978) seeks a trade-o between cheap and e ective iterations and is preferred by somepractictioners (Rust, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12984000,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67de670898ebe2c982dab0429704370cd8d9a327",
            "isKey": false,
            "numCitedBy": 349,
            "numCiting": 266,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-dynamic-programming-in-economics-Rust",
            "title": {
                "fragments": [],
                "text": "Numerical dynamic programming in economics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 59
                            }
                        ],
                        "text": "Popular techniques include various neural-network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60899176,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "249ce7a85b158c16ba108451070c07aa1156e7eb",
            "isKey": false,
            "numCitedBy": 1286,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Very rarely, a book is published which not only advances our knowledge of a particular topic, but fundamentally recasts our methods of investigating and thinking about large tracts of the map of learning. Linguists remember 1957 as the publication year of Noam Chomsky's Syntactic structures-a book whose ostensible subjects were the structure of English grammatical rules and the goals of grammatical description, but which can be seen with hindsight as the first shot in an intellectual revolution which ended by radically changing the texture of day-to-day research activity and discourse throughout almost all of linguistics, and in substantial parts of other cognition-related disciplines. In decades to come, perhaps 1986 will be remembered by academics as the year of publication of the pair of volumes reviewed here: they constitute the first large-scale public statement of an intellectual paradigm fully as revolutionary as the generative paradigm ever was (there have been scattered journal articles in the preceding four or five years). I would go further and suggest that, if the promises of this book can be redeemed, the contrast in linguistics and neighboring disciplines between the 1990's and the 1970's will be significantly greater than the contrast between the 1970's and the 1950's. (I need hardly add, of course, that it is one thing to fire an opening salvo, but another to achieve ultimate predominance.) The new paradigm is called Parallel Distributed Processing by the sixteen writers who contributed to this book, many of whom work either at the University of California, San Diego, or at Carnegie-Mellon University in Pittsburgh. Some other researchers (e.g. Feldman 1985) use the term 'connectionism' for the same concept. These two volumes comprise 26 chapters which, among them, (i) explain the over-all nature and aims of PDP/connectionist models, (ii) define a family of specific variants of the general paradigm, and (iii) exemplify it by describing experiments in which PDP models were used to simulate human performance in various cognitive domains. The experiments, inevitably, treat their respective domains in a simplified, schematic way by comparison with the endless complexity found in any real-life cognitive area; but simplification in this case does not mean trivialization. There are also auxiliary chapters on relevant related topics; thus Chap. 9, by M. I. JORDAN, is a tutorial on linear algebra, a branch of mathematics having special significance for the PDP paradigm. (Each chapter is attributed to a particular author or"
            },
            "slug": "Parallel-Distributed-Processing:-Explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the Microstructures of Cognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "In decades to come, perhaps 1986 will be remembered by academics as the year of publication of the pair of volumes reviewed here: they constitute the first large-scale public statement of an intellectual paradigm fully as revolutionary as the generative paradigm ever was."
            },
            "venue": {
                "fragments": [],
                "text": "Language"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680510"
                        ],
                        "name": "T. Dean",
                        "slug": "T.-Dean",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dean",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145500761"
                        ],
                        "name": "J. Kirman",
                        "slug": "J.-Kirman",
                        "structuredName": {
                            "firstName": "Jak",
                            "lastName": "Kirman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kirman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738567"
                        ],
                        "name": "A. Nicholson",
                        "slug": "A.-Nicholson",
                        "structuredName": {
                            "firstName": "Ann",
                            "lastName": "Nicholson",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nicholson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 27
                            }
                        ],
                        "text": "The Plexus planning system [33, 55] exploits a similar intuition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9725192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c8b044a273f14cb5772202980051d7e03b514e4",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We provide a method, based on the theory of Markov decision problems, for efficient planning in stochastic domains. Goals are encoded as reward functions, expressing the desirability of each world state; the planner must find a policy (mapping from states to actions) that maximizes future rewards. Standard goals of achievement, as well as goals of maintenance and prioritized combinations of goals, can be specified in this way. An optimal policy can be found using existing methods, but these methods are at best polynomial in the number of states in the domain, where the number of states is exponential in the number of propositions (or state variables). By using information about the starting state, the reward function, and the transition probabilities of the domain, we can restrict the planner's attention to a set of world states that are likely to be encountered in satisfying the goal. Furthermore, the planner can generate more or less complete plans depending on the time it has available. We describe experiments involving a mobile robotics application and consider the problem of schedulilng different phases of the planning algorithm given time constraints."
            },
            "slug": "Planning-With-Deadlines-in-Stochastic-Domains-Dean-Kaelbling",
            "title": {
                "fragments": [],
                "text": "Planning With Deadlines in Stochastic Domains"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A method, based on the theory of Markov decision problems, for efficient planning in stochastic domains, that can restrict the planner's attention to a set of world states that are likely to be encountered in satisfying the goal."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 122
                            }
                        ],
                        "text": "But another strategy is to have a human supply appropriate motor commands to a robot through a joystick or steering wheel (Pomerleau, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 121
                            }
                        ],
                        "text": "Butanother strategy is to have a human supply appropriate motor commands to a robotthrough a joystick or steering wheel (Pomerleau, 1993).problem decomposition: Decomposing a huge learning problem into a collection of smallerones, and providing useful reinforcement signals for the subproblems is a\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 110954972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c67f0f00eaab360db1b9bd377e783c27c922dc86",
            "isKey": false,
            "numCitedBy": 597,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nVision based mobile robot guidance has proven difficult for classical machine vision methods because of the diversity and real time constraints inherent in the task. This book describes a connectionist system called ALVINN (Autonomous Land Vehicle In a Neural Network) that overcomes these difficulties. ALVINN learns to guide mobile robots using the back-propagation training algorithm. Because of its ability to learn from example, ALVINN can adapt to new situations and therefore cope with the diversity of the autonomous navigation task. But real world problems like vision based mobile robot guidance present a different set of challenges for the connectionist paradigm. Among them are: how to develop a general representation from a limited amount of real training data, how to understand the internal representations developed by artificial neural networks, how to estimate the reliability of individual networks, how to combine multiple networks trained for different situations into a single system, and how to combine connectionist perception with symbolic reasoning. Neural Network Perception for Mobile Robot Guidance presents novel solutions to each of these problems. Using these techniques, the ALVINN system can learn to control an autonomous van in under 5 minutes by watching a person drive. Once trained, individual ALVINN networks can drive in a variety of circumstances, including single-lane paved and unpaved roads, and multi-lane lined and unlined roads, at speeds of up to 55 miles per hour. The techniques also are shown to generalize to the task of controlling the precise foot placement of a walking robot."
            },
            "slug": "Neural-Network-Perception-for-Mobile-Robot-Guidance-Pomerleau",
            "title": {
                "fragments": [],
                "text": "Neural Network Perception for Mobile Robot Guidance"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book describes a connectionist system called ALVINN (Autonomous Land Vehicle In a Neural Network) that overcomes difficulties and can learn to control an autonomous van in under 5 minutes by watching a person drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2453007"
                        ],
                        "name": "A. Cassandra",
                        "slug": "A.-Cassandra",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Cassandra",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cassandra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16792751,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fa4be2647b4d088ee09320aaa008008728e4af8",
            "isKey": false,
            "numCitedBy": 718,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe the partially observable Markov decision process (POMDP) approach to finding optimal or near-optimal control strategies for partially observable stochastic environments, given a complete model of the environment. The POMDP approach was originally developed in the operations research community and provides a formal basis for planning problems that have been of interest to the AI community. We found the existing algorithms for computing optimal control strategies to be highly computationally inefficient and have developed a new algorithm that is empirically more efficient. We sketch this algorithm and present preliminary results on several small problems that illustrate important properties of the POMDP approach."
            },
            "slug": "Acting-Optimally-in-Partially-Observable-Stochastic-Cassandra-Kaelbling",
            "title": {
                "fragments": [],
                "text": "Acting Optimally in Partially Observable Stochastic Domains"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The existing algorithms for computing optimal control strategies for partially observable stochastic environments are found to be highly computationally inefficient and a new algorithm is developed that is empirically more efficient."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404817"
                        ],
                        "name": "J. Holland",
                        "slug": "J.-Holland",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holland",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Holland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 37
                            }
                        ],
                        "text": "Classi er Systems Classi er systems (Holland, 1975; Goldberg, 1989) were explicitlydeveloped to solve problems with delayed reward, including those requiring short-termmemory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 36
                            }
                        ],
                        "text": "Classi er Systems Classi er systems (Holland, 1975; Goldberg, 1989) were explicitly developed to solve problems with delayed reward, including those requiring short-term memory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58781161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b4279db68b16e20fbc56f9d41980a950191d30a",
            "isKey": false,
            "numCitedBy": 38845,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Name of founding work in the area. Adaptation is key to survival and evolution. Evolution implicitly optimizes organisims. AI wants to mimic biological optimization { Survival of the ttest { Exploration and exploitation { Niche nding { Robust across changing environments (Mammals v. Dinos) { Self-regulation,-repair and-reproduction 2 Artiicial Inteligence Some deenitions { \"Making computers do what they do in the movies\" { \"Making computers do what humans (currently) do best\" { \"Giving computers common sense; letting them make simple deci-sions\" (do as I want, not what I say) { \"Anything too new to be pidgeonholed\" Adaptation and modiication is root of intelligence Some (Non-GA) branches of AI: { Expert Systems (Rule based deduction)"
            },
            "slug": "Adaptation-in-natural-and-artificial-systems-Holland",
            "title": {
                "fragments": [],
                "text": "Adaptation in natural and artificial systems"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Names of founding work in the area of Adaptation and modiication, which aims to mimic biological optimization, and some (Non-GA) branches of AI."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1985966"
                        ],
                        "name": "W. Lovejoy",
                        "slug": "W.-Lovejoy",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Lovejoy",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Lovejoy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 253
                            }
                        ],
                        "text": "POMDP Approach Another strategy consists of using hidden Markov model (HMM)techniques to learn a model of the environment, including the hidden state, then to use thatmodel to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994;Lovejoy, 1991; Monahan, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121258294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b631510618b13c32a87ac134f11dcb22cec8c9a1",
            "isKey": false,
            "numCitedBy": 606,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A partially observed Markov decision process (POMDP) is a generalization of a Markov decision process that allows for incomplete information regarding the state of the system. The significant applied potential for such processes remains largely unrealized, due to an historical lack of tractable solution methodologies. This paper reviews some of the current algorithmic alternatives for solving discrete-time, finite POMDPs over both finite and infinite horizons. The major impediment to exact solution is that, even with a finite set of internal system states, the set of possible information states is uncountably infinite. Finite algorithms are theoretically available for exact solution of the finite horizon problem, but these are computationally intractable for even modest-sized problems. Several approximation methodologies are reviewed that have the potential to generate computationally feasible, high precision solutions."
            },
            "slug": "A-survey-of-algorithmic-methods-for-partially-Lovejoy",
            "title": {
                "fragments": [],
                "text": "A survey of algorithmic methods for partially observed Markov decision processes"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Several approximation methodologies are reviewed that have the potential to generate computationally feasible, high precision solutions for solving discrete-time, finite POMDPs over both finite and infinite horizons."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1422562562"
                        ],
                        "name": "F. D'epenoux",
                        "slug": "F.-D'epenoux",
                        "structuredName": {
                            "firstName": "Fran\u00e7oise",
                            "lastName": "D'epenoux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. D'epenoux"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 156
                            }
                        ],
                        "text": "Linear programming (Schrijver, 1986) is an extremely general problem, and MDPs canbe solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux,1963; Ho man & Karp, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 57033158,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "67d4e38cbfb6395c601e3d6f4c6314202efbf128",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "R. Howard (R. Howard.1960. Dynamic Programming and Markov Processes. John Wiley and Sons, Inc., New York.) and A. Manne (A. Manne. 1960. Linear programming and sequential decisions. Management Science, April.) have suggested two methods for optimizing average costs per unit time by means of a sequential decision rule. This rule gives order quantities as a function of initial stock levels, and is based upon the assumption of a Markov process which is both homogenous and discrete. The formulation does not include any restrictive hypothesis on the structure of costs. This paper presents a synthesis of both dynamic and linear programming methods applicable to the same kind of model for the case of time discounting."
            },
            "slug": "A-Probabilistic-Production-and-Inventory-Problem-D'epenoux",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Production and Inventory Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a synthesis of both dynamic and linear programming methods applicable to the same kind of model for the case of time discounting, and gives order quantities as a function of initial stock levels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 15
                            }
                        ],
                        "text": "More recently, Tesauro (1992, 1994, 1995) applied the temporal di erence algorithmto backgammon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 3
                            }
                        ],
                        "text": "In Tesauro, G., Touretzky, D. S., & Leen, T. K.(Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Tesauro, G. (1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 17
                            }
                        ],
                        "text": "In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 3
                            }
                        ],
                        "text": "In Tesauro, G., Touretzky, D. S., &Leen, T. K. (Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 23
                            }
                        ],
                        "text": "More recently, Tesauro [93, 94, 95] applied the temporal di erence algorithm to backgammon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 17
                            }
                        ],
                        "text": "In Cowan, J. D., Tesauro, G., & Alspector, J.(Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 191
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "\u2026methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 17
                            }
                        ],
                        "text": "In Cowan, J. D., Tesauro, G., & Alspector,J. (Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 255,
                                "start": 247
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore [14] used local memory-based methods in conjunction with value iteration; Lin [44] used backpropagation networks for Q-learning; Watkins [101] used CMAC for Q-learning; Tesauro [93, 95] used backpropagation for learning the value function in backgammon (described in Section 8."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Tesauro, G. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Tesauro, G. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 9
                            }
                        ],
                        "text": "Instead, Tesauro used a backpropagation-based three-layer270\nReinforcement Learning: A SurveyTrainingGames HiddenUnits ResultsBasic PoorTD 1.0 300,000 80 Lost by 13 points in 51gamesTD 2.0 800,000 40 Lost by 7 points in 38gamesTD 2.1 1,500,000 80 Lost by 1 point in 40gamesTable 2: TD-Gammon's performance in games against the top human professional players."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6023746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ed59f49c1bb7de06cfa2a9467d5efb535103277",
            "isKey": true,
            "numCitedBy": 906,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome."
            },
            "slug": "Temporal-difference-learning-and-TD-Gammon-Tesauro",
            "title": {
                "fragments": [],
                "text": "Temporal difference learning and TD-Gammon"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "Popular methods include various neural-network methods [76], CMAC [2], and local memory-based methods [67], such as generalizations of nearest neighbor methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15291527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff2c2e3e83d1e8828695484728393c76ee07a101",
            "isKey": false,
            "numCitedBy": 15736,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system."
            },
            "slug": "Parallel-distributed-processing:-explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 96
                            }
                        ],
                        "text": "Other games thathave been studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) and Chess (Thrun,1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 96
                            }
                        ],
                        "text": "Other games that have been studied include Go (Schraudolph, Dayan, & Sejnowski, 1994) and Chess (Thrun, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12298610,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bc7a6dcb9e0e6c7a26800532e2a00f5572eea47",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents NeuroChess, a program which learns to play chess from the final outcome of games. NeuroChess learns chess board evaluation functions, represented by artificial neural networks. It integrates inductive neural network learning, temporal differencing, and a variant of explanation-based learning. Performance results illustrate some of the strengths and weaknesses of this approach."
            },
            "slug": "Learning-to-Play-the-Game-of-Chess-Thrun",
            "title": {
                "fragments": [],
                "text": "Learning to Play the Game of Chess"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "NeuroChess, a program which learns to play chess from the final outcome of games, is presented, which integrates inductive neural network learning, temporal differencing, and a variant of explanation-based learning."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 15784464,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4c6240b68e97d6f3b9bc67a701f10e49a1b1dab",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Hierarchical-Learning-in-Stochastic-Domains:-Kaelbling",
            "title": {
                "fragments": [],
                "text": "Hierarchical Learning in Stochastic Domains: Preliminary Results"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73488853"
                        ],
                        "name": "John Alan Kirman",
                        "slug": "John-Alan-Kirman",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kirman",
                            "middleNames": [
                                "Alan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Alan Kirman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 72
                            }
                        ],
                        "text": "The Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman,1994) exploits a similar intuition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 27
                            }
                        ],
                        "text": "The Plexus planning system (Dean, Kaelbling, Kirman, & Nicholson, 1993; Kirman, 1994) exploits a similar intuition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63448291,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "218c01d6c796287a87c77c91d50f027bae62b241",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a large class of problems that involve real-time planning and execution in stochastic domains. Classical solutions either ignore the random element while planning and then modify the plan to deal with failures, or compute the best action to perform in any circumstance. These solutions are often either too brittle or too expensive computationally. I present an approach that interleaves planning and execution and reduces the set of circumstances that need to be considered at a given time. In previous work with other researchers I have shown that this approach is practical for some stochastic problems, but left open the question of which problems the approach can be applied to effectively. I present a classification of real-time stochastic decision problems that allows the prediction of the performance of this planning system (and others) without resorting to exhaustive empirical performance estimation. For a number of such planning problems, I present results showing the correlation between the predicted performance and the actual performance as estimated empirically."
            },
            "slug": "Predicting-real-time-planner-performance-by-domain-Kirman",
            "title": {
                "fragments": [],
                "text": "Predicting real-time planner performance by domain characterization"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "An approach that interleaves planning and execution and reduces the set of circumstances that need to be considered at a given time is presented, which allows the prediction of the performance of this planning system (and others) without resorting to exhaustive empirical performance estimation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715339"
                        ],
                        "name": "D. Goldberg",
                        "slug": "D.-Goldberg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 52
                            }
                        ],
                        "text": "Classi er Systems Classi er systems (Holland, 1975; Goldberg, 1989) were explicitlydeveloped to solve problems with delayed reward, including those requiring short-termmemory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 36
                            }
                        ],
                        "text": "Classi er Systems Classi er systems (Holland, 1975; Goldberg, 1989) were explicitly developed to solve problems with delayed reward, including those requiring short-term memory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38613589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e62d1345b340d5fda3b092c460264b9543bc4b5",
            "isKey": false,
            "numCitedBy": 58251,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. \n \nMajor concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required."
            },
            "slug": "Genetic-Algorithms-in-Search-Optimization-and-Goldberg",
            "title": {
                "fragments": [],
                "text": "Genetic Algorithms in Search Optimization and Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This book brings together the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680510"
                        ],
                        "name": "T. Dean",
                        "slug": "T.-Dean",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dean",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "There are many good references toMDP models (Bellman, 1957; Bertsekas, 1987; Howard,1960; Puterman, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 88
                            }
                        ],
                        "text": "The resulting formal model is called a partially observableMarkov decision process or POMDP.7.1 State-Free Deterministic PoliciesThe most naive strategy for dealing with partial observability is to ignore it."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 172
                            }
                        ],
                        "text": "In addition, it may converge quite slowly to a good policy.4.3 Model-free Learning With Average RewardAs described, Q-learning can be applied to discounted in nite-horizon MDPs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "An MDP consists of a set of states S, a set of actions A, 247\nKaelbling, Littman, & Moore a reward function R : S A !"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "Linear programming (Schrijver, 1986) is an extremely general problem, and MDPs canbe solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux,1963; Ho man & Karp, 1966)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "Although general MDPs may have in nite (even uncountable) state and action spaces,we will only discuss methods for solving nite-state and nite-action problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "In this section, we will consider extensions to the basic MDP framework for solvingpartially observable problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 161
                            }
                        ],
                        "text": "However, in the worst case the number of iterations grows polynomially in 1=(1 ), so the convergence rate slows considerably as the discount factor approaches 1 [66]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "Learning an Optimal Policy: Model-free MethodsIn the previous section we reviewed methods for obtaining an optimal policy for an MDPassuming that we already had a model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "Chrisman (1992) showed how the forward-backward algorithm for learning HMMs couldbe adapted to learning POMDPs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 183
                            }
                        ],
                        "text": "In section 6,we discuss methods for solving problems with continuous input and output spaces.3.2 Finding a Policy Given a ModelBefore we consider algorithms for learning to behave in MDP environments, we will ex-plore techniques for determining the optimal policy given a correct model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 2
                            }
                        ],
                        "text": "POMDP Approach Another strategy consists of using hidden Markov model (HMM)techniques to learn a model of the environment, including the hidden state, then to use thatmodel to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994;Lovejoy, 1991; Monahan, 1982)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "It starts by making an approximate version of the MDPwhich is much smaller than the original one."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 242
                            }
                        ],
                        "text": "The agentmust be able to learn which of its actions are desirable based on reward that can take placearbitrarily far in the future.3.1 Markov Decision ProcessesProblems with delayed reinforcement are well modeled asMarkov decision processes (MDPs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "It is known that the running time is pseudopolynomial and that for any xed discount factor, there is a polynomial bound in the total size of the MDP [66]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "Cli and Ross (1994) start with Wilson's zeroth-268\nReinforcement Learning: A Survey i\nb a SE \u03c0Figure 10: Structure of a POMDP agent.level classi er system (Wilson, 1995) and add one and two-bit memory registers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 36
                            }
                        ],
                        "text": "This problem can be formulated as anMDP, but it is di cult to solve using the techniquesdescribed earlier, because the input space is continuous."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 91
                            }
                        ],
                        "text": "From a theoretic perspective, linear programming is the only knownalgorithm that can solve MDPs in polynomial time, although the theoretically e cientalgorithms have not been shown to be e cient in practice.4."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 143
                            }
                        ],
                        "text": "It is known that the running time is pseudopolynomial and that forany xed discount factor, there is a polynomial bound in the total size of theMDP (Littmanet al., 1995b).3.2.3 Enhancement to Value Iteration and Policy IterationIn practice, value iteration is much faster per iteration, but policy iteration takes feweriterations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "There is no known tight worst-case bound available for policy iteration [66]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 72
                            }
                        ],
                        "text": "There is no known tight worst-case bound availablefor policy iteration (Littman et al., 1995b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "4 Other Model-Based MethodsMethods proposed for solving MDPs given a model can be used in the context of model-based methods as well."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "The approximateMDP contains a set of states,called the envelope, that includes the agent's current state and the goal state, if there is one."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "The planningprocess is an alternation between nding an optimal policy on the approximate MDP andadding useful states to the envelope."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "Although his R-learning algorithm seems to exhibit convergence problems forsome MDPs, several researchers have found the average-reward criterion closer to the trueproblem they wish to solve than a discounted criterion and therefore prefer R-learning toQ-learning (Mahadevan, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 80
                            }
                        ],
                        "text": "Unfortunately, complete observabilityis necessary for learning methods based on MDPs."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2080151,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ae91735c6e5f4ab44bfa95bd144663f057d1935",
            "isKey": false,
            "numCitedBy": 539,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "Markov decision problems (MDPs) provide the foundations for a number of problems of interest to AI researchers studying automated planning and reinforcement learning. In this paper, we summarize results regarding the complexity of solving MDPs and the running time of MDP solution algorithms. We argue that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly. To encourage future research, we sketch some alternative methods of analysis that rely on the structure of MDPs."
            },
            "slug": "On-the-Complexity-of-Solving-Markov-Decision-Littman-Dean",
            "title": {
                "fragments": [],
                "text": "On the Complexity of Solving Markov Decision Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is argued that, although MDPs can be solved efficiently in theory, more study is needed to reveal practical algorithms for solving large problems quickly and to encourage future research, some alternative methods of analysis are sketched that rely on the structure of M DPs."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259367"
                        ],
                        "name": "J. Albus",
                        "slug": "J.-Albus",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Albus",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Albus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 53
                            }
                        ],
                        "text": "A very di erent kind of function approximator (CMAC (Albus, 1975)) that has weakgeneralization.3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2302184,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e54f00da3befa0c91e19a7a12a4242cf42433cd3",
            "isKey": false,
            "numCitedBy": 2222,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The Cerebellar Model Articulation Controller (CMAC) [1, 2] is a neural network that models the structure and function of the part of the brain known as the cerebellum. The cerebellum provides precise coordination of motor control for such body parts as the eyes, arms, fingers, legs, and wings. It stores and retrieves information required to control thousands of muscles in producing coordinated behavior as a function of time. CMAC was designed to provide this kind of motor control for robotic manipulators. CMAC is a kind of memory, or table look-up mechanism, that is capable of learning motor behavior. It exhibits properties such as generalization, learning interference, discrimination, and forgetting that are characteristic of motor learning in biological creatures. In a biological motor system, the drive signal for each muscle is a function of many variables. These include feedback from sensors that measure position, velocity, and acceleration of the limb; stretch in muscles; tension in tendons; and tactile sensations from various points on the skin. Feedback also includes information from the eyes via the superior colliculus and visual cortex about the positions of the hands and feet relative to their intended targets. Drive signals to the muscles also depend on higher level ideas, plans, intentions, motives, and urges. These may be specified by variables that identify the name of the task to be performed and specify the goals that are desired, the procedures and knowledge required to achieve those goals, and the priorities that have been assigned to achieving those goals. A block diagram of a typical CMAC is shown in Fig. 1. CMAC modules are designed to accept both input command variables from higher levels and feedback variables from sensors. Each CMAC merges these two inputs into a set of memory addresses wherein are stored the correct motor response. The combined input selects a set of memory locations from a large pool of memory locations. The output is the sum of the contents"
            },
            "slug": "New-Approach-to-Manipulator-Control:-The-Cerebellar-Albus",
            "title": {
                "fragments": [],
                "text": "New Approach to Manipulator Control: The Cerebellar Model Articulation Controller (CMAC)1"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Cerebellar Model Articulation Controller is a neural network that models the structure and function of the part of the brain known as the cerebellum that exhibits properties such as generalization, learning interference, discrimination, and forgetting that are characteristic of motor learning in biological creatures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1392419536"
                        ],
                        "name": "P. Jones",
                        "slug": "P.-Jones",
                        "structuredName": {
                            "firstName": "Peter W.",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Jones"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 198
                            }
                        ],
                        "text": "The simplest possible reinforcement-learning problem is known as the k-armed bandit problem, which has been the subject of a great deal of study in the statistics and applied mathematics literature [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 142
                            }
                        ],
                        "text": "We will consider a representativeselection of them, but for a deeper discussion and a number of important theoretical results,see the book by Berry and Fristedt (1985)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 205
                            }
                        ],
                        "text": "The simplest possible reinforcement-learning problem is known as the k-armed banditproblem, which has been the subject of a great deal of study in the statistics and appliedmathematics literature (Berry & Fristedt, 1985)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 163
                            }
                        ],
                        "text": "We will consider a representative selection of them, but for a deeper discussion and a number of important theoretical results, see the book by Berry and Fristedt [12]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 16
                            }
                        ],
                        "text": "Berry, D. A., & Fristedt, B. (1985)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 128
                            }
                        ],
                        "text": "If the agent is going to be acting for a total of h steps, it can use basic Bayesian reasoning to solve for an optimal strategy [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 41
                            }
                        ],
                        "text": "This measure is known as regret (Berry & Fristedt, 1985)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 264
                            }
                        ],
                        "text": "Although it is instructive, the methods it provides do not scale well to more complexproblems.2.1.1 Dynamic-Programming ApproachIf the agent is going to be acting for a total of h steps, it can use basic Bayesian reasoningto solve for an optimal strategy (Berry & Fristedt, 1985)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62177003,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4690cd528cae72cc9afbca0506ca595dad9434b",
            "isKey": true,
            "numCitedBy": 570,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Bandit-Problems,-Sequential-Allocation-of-Jones",
            "title": {
                "fragments": [],
                "text": "Bandit Problems, Sequential Allocation of Experiments"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3091735"
                        ],
                        "name": "L. Meeden",
                        "slug": "L.-Meeden",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Meeden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Meeden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144629612"
                        ],
                        "name": "G. McGraw",
                        "slug": "G.-McGraw",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "McGraw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McGraw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755154"
                        ],
                        "name": "Douglas S. Blank",
                        "slug": "Douglas-S.-Blank",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Blank",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas S. Blank"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15874868,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "fd8272c5950ba379ce530ea6c31b8c13a6d025e7",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We use a connectionist network trained with reinforcement to control both an autonomous robot vehicle and a simulated robot. We show that given appropriate sensory data and architectural structure, a network can learn to control the robot for a simple navigation problem. We then investigate a more complex goal-based problem and examine the plan-like behavior that emerges."
            },
            "slug": "Emergent-Control-and-Planning-in-an-Autonomous-Meeden-McGraw",
            "title": {
                "fragments": [],
                "text": "Emergent Control and Planning in an Autonomous Vehicle"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work uses a connectionist network trained with reinforcement to control both an autonomous robot vehicle and a simulated robot, showing that given appropriate sensory data and architectural structure, a network can learn to control the robot for a simple navigation problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706614"
                        ],
                        "name": "J. Connell",
                        "slug": "J.-Connell",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Connell",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Connell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850503"
                        ],
                        "name": "S. Mahadevan",
                        "slug": "S.-Mahadevan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Mahadevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mahadevan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 97
                            }
                        ],
                        "text": "Most interesting examples of robotic reinforcementlearning employ this technique to some extent (Connell & Mahadevan, 1993).re exes: One thing that keeps agents that know nothing from learning anything is thatthey have a hard time even nding the interesting parts of the space; they\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60682090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6ddc400d0cff2c4d176352f761cd85b1ef0e5ec",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "For learning to be useful on real robots, whatever algorithm is used must converge in some \u201creasonable\u201d amount of time. If each trial step takes on the order of seconds, a million steps would take several months of continuous run time. In many cases such extended runs are neither desirable nor practical. In this chapter we discuss how learning can be speeded up by exploiting properties of the task, sensor configuration, environment, and existing control structure."
            },
            "slug": "Rapid-Task-Learning-for-Real-Robots-Connell-Mahadevan",
            "title": {
                "fragments": [],
                "text": "Rapid Task Learning for Real Robots"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This chapter discusses how learning can be speeded up by exploiting properties of the task, sensor configuration, environment, and existing control structure."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785805"
                        ],
                        "name": "M. Thathachar",
                        "slug": "M.-Thathachar",
                        "structuredName": {
                            "firstName": "Mandayam",
                            "lastName": "Thathachar",
                            "middleNames": [
                                "A.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Thathachar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 16
                            }
                        ],
                        "text": "Narendra, K., & Thathachar, M. A. L. (1989)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 54
                            }
                        ],
                        "text": "Prentice-Hall, Englewood Cli s, NJ.Narendra, K. S., & Thathachar, M. A. L. (1974)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 177
                            }
                        ],
                        "text": "Unfortunately, it does not alwaysconverge to the correct action; but the probability that it converges to the wrong one canbe made arbitrarily small by making small (Narendra & Thathachar, 1974)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "3 Learning Automata A branch of the theory of adaptive control is devoted to learning automata, surveyed by Narendra and Thathachar [85], which were originally described explicitly as nite state automata."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 216
                            }
                        ],
                        "text": "Unfortunately, no one has yet beenable to nd an analog of index values for delayed reinforcement problems.2.1.3 Learning AutomataA branch of the theory of adaptive control is devoted to learning automata, surveyed byNarendra and Thathachar (1989), which were originally described explicitly as nite stateautomata."
                    },
                    "intents": []
                }
            ],
            "corpusId": 42185255,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "44eeb93197dcf2e7bf4ed9172a82f81de9c05365",
            "isKey": true,
            "numCitedBy": 1598,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the combination of knowledge and actions, someone can improve their skill and ability. It will lead them to live and work much better. This is why, the students, workers, or even employers should have reading habit for books. Any book will give certain knowledge to take all benefits. This is what this learning automata an introduction tells you. It will add more knowledge of you to life and work better. Try it and prove it."
            },
            "slug": "Learning-automata-an-introduction-Narendra-Thathachar",
            "title": {
                "fragments": [],
                "text": "Learning automata - an introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "From the combination of knowledge and actions, someone can improve their skill and ability and this learning automata an introduction tells you that any book will give certain knowledge to take all benefits."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752462"
                        ],
                        "name": "D. Casta\u00f1\u00f3n",
                        "slug": "D.-Casta\u00f1\u00f3n",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Casta\u00f1\u00f3n",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Casta\u00f1\u00f3n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15863223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7757a99904457070707b8601267c277e749af9df",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A class of iterative aggregation algorithms for solving infinite horizon dynamic programming problems is proposed. The idea is to interject aggregation iterations in the course of the usual successive approximation method. An important feature that sets this method apart from earlier ones is that the aggregate groups of states change adaptively from one aggregation iteration to the next, depending on the progress of the computation. This allows acceleration of convergence in difficult problems involving multiple-ergodic classes for which methods using fixed groups of aggregate states are ineffective. No knowledge of special problem structure is utilized by the algorithms. >"
            },
            "slug": "Adaptive-aggregation-methods-for-infinite-horizon-Bertsekas-Casta\u00f1\u00f3n",
            "title": {
                "fragments": [],
                "text": "Adaptive aggregation methods for infinite horizon dynamic programming"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A class of iterative aggregation algorithms for solving infinite horizon dynamic programming problems is proposed, to interject aggregation iterations in the course of the usual successive approximation method, which allows acceleration of convergence in difficult problems involving multiple-ergodic classes."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731282"
                        ],
                        "name": "Benjamin Van Roy",
                        "slug": "Benjamin-Van-Roy",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Van Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Van Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 23
                            }
                        ],
                        "text": "Several recent results [42, 126] show how the appropriate choice of function approximator can guarantee convergence, though not necessarily to the optimal values."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2202186,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "4a5a283e178f187e64d0c1019314d0a6e1d4ca96",
            "isKey": false,
            "numCitedBy": 481,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a methodological framework and present a few different ways in which dynamic programming and compact representations can be combined to solve large scale stochastic control problems. In particular, we develop algorithms that employ two types of feature-based compact representations; that is, representations that involve feature extraction and a relatively simple approximation architecture. We prove the convergence of these algorithms and provide bounds on the approximation error. As an example, one of these algorithms is used to generate a strategy for the game of Tetris. Furthermore, we provide a counter-example illustrating the difficulties of integrating compact representations with dynamic programming, which exemplifies the shortcomings of certain simple approaches."
            },
            "slug": "Feature-based-methods-for-large-scale-dynamic-Tsitsiklis-Roy",
            "title": {
                "fragments": [],
                "text": "Feature-based methods for large scale dynamic programming"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A methodological framework is developed and algorithms that employ two types of feature-based compact representations; that is, representations that involve feature extraction and a relatively simple approximation architecture are developed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 34th IEEE Conference on Decision and Control"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35157864"
                        ],
                        "name": "W. Cleveland",
                        "slug": "W.-Cleveland",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Cleveland",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Cleveland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46660600"
                        ],
                        "name": "S. J. Devlin",
                        "slug": "S.-J.-Devlin",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Devlin",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. J. Devlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 172
                            }
                        ],
                        "text": "The juggling robot learned a world model from experience, which was generalized to unvisited states by a function approximation scheme known as locally weighted regression [20, 65]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 171
                            }
                        ],
                        "text": "The juggling robot learned a world model from experience, which was generalizedto unvisited states by a function approximation scheme known as locally weightedregression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14960635,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b81e61b920b986ff1495af426aad7437c9011d85",
            "isKey": false,
            "numCitedBy": 4970,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract Locally weighted regression, or loess, is a way of estimating a regression surface through a multivariate smoothing procedure, fitting a function of the independent variables locally and in a moving fashion analogous to how a moving average is computed for a time series. With local fitting we can estimate a much wider class of regression surfaces than with the usual classes of parametric functions, such as polynomials. The goal of this article is to show, through applications, how loess can be used for three purposes: data exploration, diagnostic checking of parametric models, and providing a nonparametric regression surface. Along the way, the following methodology is introduced: (a) a multivariate smoothing procedure that is an extension of univariate locally weighted regression; (b) statistical procedures that are analogous to those used in the least-squares fitting of parametric functions; (c) several graphical methods that are useful tools for understanding loess estimates and checking the a..."
            },
            "slug": "Locally-Weighted-Regression:-An-Approach-to-by-Cleveland-Devlin",
            "title": {
                "fragments": [],
                "text": "Locally Weighted Regression: An Approach to Regression Analysis by Local Fitting"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699665"
                        ],
                        "name": "A. Hoffman",
                        "slug": "A.-Hoffman",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Hoffman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47546648"
                        ],
                        "name": "R. Karp",
                        "slug": "R.-Karp",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Karp",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Karp"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 160
                            }
                        ],
                        "text": "Reinforcement Learning: A Survey Linear programming [105] is an extremely general problem, and MDPs can be solved by generalpurpose linear-programming packages [35, 34, 46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 173
                            }
                        ],
                        "text": "Linear programming (Schrijver, 1986) is an extremely general problem, and MDPs canbe solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux,1963; Ho man & Karp, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122743928,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c3fe99c0dca1e7adbd2f987b4a77ab2c08fef549",
            "isKey": false,
            "numCitedBy": 242,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "A stochastic game is played in a sequence of steps; at each step the play is said to be in some state i, chosen from a finite collection of states. If the play is in state i, the first player chooses move k and the second player chooses move l, then the first player receives a reward akli, and, with probability pklij, the next state is j. \n \nThe concept of stochastic games was introduced by Shapley with the proviso that, with probability 1, play terminates. The authors consider the case when play never terminates, and show properties of such games and offer a convergent algorithm for their solution. In the special case when one of the players is a dummy, the nonterminating stochastic game reduces to a Markovian decision process, and the present work can be regarded as the extension to a game theoretic context of known results on Markovian decision processes."
            },
            "slug": "On-Nonterminating-Stochastic-Games-Hoffman-Karp",
            "title": {
                "fragments": [],
                "text": "On Nonterminating Stochastic Games"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60492634,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "638df1b831feb3647a9bf5496780b38890573d4d",
            "isKey": false,
            "numCitedBy": 5448,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "gineering, computer science, operations research, and applied mathematics. It is essentially a self-contained work, with the development of the material occurring in the main body of the text and excellent appendices on linear algebra and analysis, graph theory, duality theory, and probability theory and Markov chains supporting it. The introduction discusses parallel and distributed architectures, complexity measures, and communication and synchronization issues, and it presents both Jacobi and Gauss-Seidel iterations, which serve as algorithms of reference for many of the computational approaches addressed later. After the introduction, the text is organized in two parts: synchronous algorithms and asynchronous algorithms. The discussion of synchronous algorithms comprises four chapters, with Chapter 2 presenting both direct methods (converging to the exact solution within a finite number of steps) and iterative methods for linear"
            },
            "slug": "Parallel-and-Distributed-Computation:-Numerical-Bertsekas-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "Parallel and Distributed Computation: Numerical Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work discusses parallel and distributed architectures, complexity measures, and communication and synchronization issues, and it presents both Jacobi and Gauss-Seidel iterations, which serve as algorithms of reference for many of the computational approaches addressed later."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47918185"
                        ],
                        "name": "Jing Peng",
                        "slug": "Jing-Peng",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 25
                            }
                        ],
                        "text": "3 Prioritized Sweeping / Queue-DynaAlthough Dyna is a great improvement on previous methods, it su ers from being relativelyundirected."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "These problems are addressedby prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna (Peng & Williams,1993), which are two independently-developed but very similar techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "These problems are addressed by prioritized sweeping [83] and Queue-Dyna [87], which are two independently-developed but very similar techniques."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13928975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c980493673bad9bcb888dd18788d2b4b3ecb7a2e",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Sutton's Dyna framework provides a novel and computationally appealing way to integrate learning, planning, and reacting in autonomous agents. Examined here is a class of strategies designed to enhance the learning and planning power of Dyna systems by increasing their computational efficiency. The benefit of using these strategies is demonstrated on some simple abstract learning tasks."
            },
            "slug": "Efficient-Learning-and-Planning-Within-the-Dyna-Peng-Williams",
            "title": {
                "fragments": [],
                "text": "Efficient Learning and Planning Within the Dyna Framework"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A class of strategies designed to enhance the learning and planning power of Dyna systems by increasing their computational efficiency are examined."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7991309"
                        ],
                        "name": "A. Samuel",
                        "slug": "A.-Samuel",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Samuel",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Samuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 91
                            }
                        ],
                        "text": "One application, spectacularly far ahead of its time, was Samuel's checkers playing system (Samuel, 1959)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 91
                            }
                        ],
                        "text": "One application, spectacularly far ahead of its time, wasSamuel's checkers playing system (Samuel, 1959)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2126705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772",
            "isKey": false,
            "numCitedBy": 3147,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method. Full use is made of the so-called \u201calpha-beta\u201d pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the program to look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program's playing ability has been greatly improved.tplay checker masters, the"
            },
            "slug": "Some-Studies-in-Machine-Learning-Using-the-Game-of-Samuel",
            "title": {
                "fragments": [],
                "text": "Some Studies in Machine Learning Using the Game of Checkers"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method and to permit the program to look ahead to a much greater depth than it otherwise could do."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786249"
                        ],
                        "name": "D. Bertsekas",
                        "slug": "D.-Bertsekas",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Bertsekas",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bertsekas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60805494,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c3337861b56120ff0b5b956f5c4e1084973bb45",
            "isKey": false,
            "numCitedBy": 1360,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "If you really want to be smarter, reading can be one of the lots ways to evoke and realize. Many people who like reading will have more knowledge and experiences. Reading can be a way to gain information from economics, politics, science, fiction, literature, religion, and many others. As one of the part of book categories, dynamic programming deterministic and stochastic models always becomes the most wanted book. Many people are absolutely searching for this book. It means that many love to read this kind of book."
            },
            "slug": "Dynamic-Programming:-Deterministic-and-Stochastic-Bertsekas",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming: Deterministic and Stochastic Models"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "As one of the part of book categories, dynamic programming deterministic and stochastic models always becomes the most wanted book."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157819406"
                        ],
                        "name": "P. Kumar",
                        "slug": "P.-Kumar",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Kumar",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713762"
                        ],
                        "name": "P. Varaiya",
                        "slug": "P.-Varaiya",
                        "structuredName": {
                            "firstName": "Pravin",
                            "lastName": "Varaiya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Varaiya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 45
                            }
                        ],
                        "text": "Thismethod is known as certainty equivlance (Kumar & Varaiya, 1986)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "This method is known as certainty equivlance [57]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57169308,
            "fieldsOfStudy": [
                "Mathematics",
                "Engineering"
            ],
            "id": "8ae83806465bb89763a6ec4c1f87add1d5ee1841",
            "isKey": false,
            "numCitedBy": 1074,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "stochastic systems estimation identification and adaptive stochastic adaptive control eolss stochastic systems estimation identification and adaptive stochastic systems estimation identification and adaptive control of stochastic systems eolss stochastic systems estimation identification and adaptive stochastic systems: estimation, identification and adaptation in stochastic dynamic systems survey and new identification and stochastic adaptive control (systems identification and adaptive control methods for some robust stochastic adaptive control dspace@mit: home adaptation in stochastic dynamic systems survey and new chapter 1: introduction to adaptive control stochastic systems estimation identification and adaptive stochastic adaptive nash certainty equivalence control coefficient estimation in adaptive control systems maximum likelihood identification and realization of (size 44,85mb) download ebook stable adaptive systems optimal adaptive control of uncertain stochastic discrete 19,42mb file download system identification adaptive on-line identification and adaptive trajectory tracking ece686: filtering and control of stochastic linear systems robustness and convergence of least-squares identification 68,58mb file system identification adaptive control bahram adaptation in stochastic dynamic systems survey and new identification and system parameter estimation 1991 gbv stochastic adaptive control via consistent parameter adaptive control of stochastic sage pub stochastic delay estimation and adaptive control of eece 574 adaptive control basics of system identification robust identification of stochastic linear systems with robust adaptive els-qr algorithm for linear discrete time stochastic systems: the mathematics of filtering and ee/ise 556: stochastic systems fall 2013 usc search identification and system parameter estimation 1991 gbv"
            },
            "slug": "Stochastic-Systems:-Estimation,-Identification,-and-Kumar-Varaiya",
            "title": {
                "fragments": [],
                "text": "Stochastic Systems: Estimation, Identification, and Adaptive Control"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The mathematics of filtering and ee/ise 556: stochastic systems fall 2013 usc search identification and system parameter estimation 1991 gbv is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70851852"
                        ],
                        "name": "E. Shoesmith",
                        "slug": "E.-Shoesmith",
                        "structuredName": {
                            "firstName": "Eddie",
                            "lastName": "Shoesmith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Shoesmith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145948621"
                        ],
                        "name": "G. Box",
                        "slug": "G.-Box",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Box",
                            "middleNames": [
                                "E.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Box"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40159987"
                        ],
                        "name": "N. Draper",
                        "slug": "N.-Draper",
                        "structuredName": {
                            "firstName": "Norman",
                            "lastName": "Draper",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Draper"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 104
                            }
                        ],
                        "text": "It is also relatedto a certain class of statistical techniques known as experiment design methods (Box &Draper, 1987), which are used for comparing multiple treatments (for example, fertilizersor drugs) to determine which treatment (if any) is best in as small a set of experiments aspossible.2.3\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117364770,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "5d090074d5fa968d852e4468820cb683040d075a",
            "isKey": true,
            "numCitedBy": 4650,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Introduction to Response Surface Methodology. The Use of Graduating Functions. Least Squares for Response Surface Work. Factorial Designs at Two Levels. Blocking and Fractionating 2 k Factorial Designs. The Use of Steepest Ascent to Achieve System Improvement. Fitting Second--Order Models. Adequacy of Estimation and the Use of Transformation. Exploration of Maxima and Ridge Systems with Second--Order Response Surfaces. Occurrence and Elucidation of Ridge Systems, I. Occurrence and Elucidation of Ridge Systems, II. Links Between Emprirical and Theoretical Models. Design Aspects of Variance, Bias, and Lack of Fit. Variance----Optimal Designs. Practical Choice of a Response Surface Design. Subject Index. Index."
            },
            "slug": "Empirical-Model\u2010Building-and-Response-Surfaces-Shoesmith-Box",
            "title": {
                "fragments": [],
                "text": "Empirical Model\u2010Building and Response Surfaces"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work discusses the use of Graduating Functions, design Aspects of Variance, Bias, and Lack of Fit, and Practical Choice of a Response Surface Design in relation to Second--Order Response Surfaces."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49140741"
                        ],
                        "name": "Ming Li",
                        "slug": "Ming-Li",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46641506"
                        ],
                        "name": "P. Vit\u00e1nyi",
                        "slug": "P.-Vit\u00e1nyi",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Vit\u00e1nyi",
                            "middleNames": [
                                "M.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Vit\u00e1nyi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 63405852,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "063d43c5a12a3a4ecfa205b826f5799c5454c550",
            "isKey": false,
            "numCitedBy": 458,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The doctoral program in Educational Policy and Leadership is designed to foster the development of scholar-practitioners. It asks students not only to inquire deeply into the process of teaching and learning, but also how the organization of schools shapes the process. In addition, the program asks students to acquire adjacent disciplinary strengths that provide contexts for considering what knowledge is of most worth, how forms of knowledge are socially distributed, and what educational measures might help bring about a more just society. Students are expected to gain expertise in research that will enable them to contribute to the ways we think about education and to develop technological and other practical skills that will enable them to implement strategies for change."
            },
            "slug": "Theories-of-learning-Li-Vit\u00e1nyi",
            "title": {
                "fragments": [],
                "text": "Theories of learning"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The doctoral program in Educational Policy and Leadership is designed to foster the development of scholar-practitioners by asking students not only to inquire deeply into the process of teaching and learning, but also how the organization of schools shapes the process."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143926762"
                        ],
                        "name": "A. Condon",
                        "slug": "A.-Condon",
                        "structuredName": {
                            "firstName": "Anne",
                            "lastName": "Condon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Condon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 89
                            }
                        ],
                        "text": "However, the number of iterations required can grow exponentially in the discount factor (Condon, 1992); as the discount factor approaches 1, the decisions must be based on results that happen farther and farther into the future."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "However, the number of iterations required can grow exponentially inthe discount factor (Condon, 1992); as the discount factor approaches 1, the decisions mustbe based on results that happen farther and farther into the future."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6220440,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "08289b02cc13e35648f6b7c055992274c8967a87",
            "isKey": false,
            "numCitedBy": 509,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Complexity-of-Stochastic-Games-Condon",
            "title": {
                "fragments": [],
                "text": "The Complexity of Stochastic Games"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Comput."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2044910440"
                        ],
                        "name": "Ullrich R\u00fcde",
                        "slug": "Ullrich-R\u00fcde",
                        "structuredName": {
                            "firstName": "Ullrich",
                            "lastName": "R\u00fcde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ullrich R\u00fcde"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122782501,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51b916b805f401c378b4493ee8c1d70631dcf91d",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction. 1.1. Purpose and motivation 1.2. Notation 1.3. Basics and model problems 2. Multilevel Splittings. 2.1. Abstract stable splittings 2.2. Finite element spaces 2.2.1. Approximation property, inverse property, and stable bases 2.2.2. Besov norms 2.2.3. Upper bounds 2.2.4. Lower bounds 2.2.5 Main theorem 2.3. Stable bases 2.4. Induced splittings 2.5. Multilevel iterations 2.6. Multilevel error estimators 3. The Fully Adaptive Multigrid Method. 3.1. Adaptive relaxation 3.1.1. Sequential adaptive relaxation 3.1.2. Simultaneous adaptive relaxation 3.1.3. Evaluation 3.2. Algebraic structure 3.3. Application of the theory of multilevel splittings 3.4. Multilevel adaptive iteration 3.4.1. Tracing dependencies between levels 3.4.2. Cycling strategies 3.4.3. Selection of the critical tolerances 3.5. Analysis of the V-cycle 3.6. Hierarchical transformations 3.6.1. Hierarchical basis 3.6.2. Efficient implementation of the multilevel adaptive iteration by hierarchical transformations 3.7. Virtual global grids 3.8. Robustness 3.9. Parallelization 3.10. Numerical examples 3.10.1. Adaptive refinement at a re-entrant corner 3.10.2. Virtual global grids and multilevel adaptive solution 3.11. Perspectives 3.12. Historical remark 4. Data Structures. 4.1. Introduction 4.1.1. Overview 4.1.2. Relations 4.1.3. Specification of software 4.1.4. Efficiency and modularity 4.2. Finite element meshes 4.2.1. Classification 4.2.2. Triangulations 4.2.3. Topological structure of a finite element mesh 4.2.4. Geometric structure 4.2.5. Algebraic structure 4.3. Special cases 4.3.1. Uniform meshes 4.3.2. Quasi-uniform meshes 4.3.3. Piecewise uniform and quasi-discretionary unidiscretionary form meshes 4.3.4. Unstructured meshes 4.4. Adaptive techniques 4.4.1. A-priori adaptivity 4.4.2. Self-adaptive refinement 4.4.3. Algorithmic eequirements 4.4.4. Nested triangulations 4.4.5. Regular refinement 4.4.6. Refinement by bisection 4.4.7. Self-adaptive refinement with uniform patches 4.4.8. Virtual global grid refinement 4.5. Hierarchical meshes 4.5.1. Relations in the multilevel hierarchy 4.5.2. Element-based data structures 4.5.3. Node-based data structures 4.5.4. Classification of nodes 4.5.5. Semi-edges and connections 4.5.6. Partially refined meshes 4.5.7. Characteristic sets 4.6. Implementation using C++ 4.6.1. Node classes 4.6.2. Efficiency for special cases requiring limited mesh flexibility 4.6.3. Virtual overloaded functions 4.6.4. The role of C++ References Index."
            },
            "slug": "Mathematical-and-Computational-Techniques-for-R\u00fcde",
            "title": {
                "fragments": [],
                "text": "Mathematical and Computational Techniques for Multilevel Adaptive Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper presents the results of an evaluation of the implementation of the Fully Adaptive Multigrid Method, a method for efficient implementation of multilevel adaptive iteration by hierarchical transformations, using C++."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3069160"
                        ],
                        "name": "R. Stengel",
                        "slug": "R.-Stengel",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Stengel",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stengel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 87
                            }
                        ],
                        "text": "4 Reinforcement Learning and Adaptive ControlAdaptive control (Burghes & Graham, 1980; Stengel, 1986) is also concerned with algo-rithms for improving a sequence of decisions from experience."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121042954,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cf7961eb829c83af74313bbbcf533bef33477ab1",
            "isKey": false,
            "numCitedBy": 418,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Mathematics of Control and Estimation Optimal Trajectories and Neighboring-Optimal Solutions Optimal State Estimation Stochastic Optimal Control Linear Multivariable Control Epilogue Index."
            },
            "slug": "Stochastic-Optimal-Control:-Theory-and-Application-Stengel",
            "title": {
                "fragments": [],
                "text": "Stochastic Optimal Control: Theory and Application"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703072"
                        ],
                        "name": "G. Siouris",
                        "slug": "G.-Siouris",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Siouris",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Siouris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "The form of dynamic programming is known as linear-quadratic-regulator design [97]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "The form of dynamic programming isknown as linear-quadratic-regulator design (Sage & White, 1977).272\nReinforcement Learning: A Survey2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5313469,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "93ba16e5cd3d9231502b371528ed9d73987bf5ad",
            "isKey": false,
            "numCitedBy": 570,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimum systems control , Optimum systems control , \u0645\u0631\u06a9\u0632 \u0641\u0646\u0627\u0648\u0631\u06cc \u0627\u0637\u0644\u0627\u0639\u0627\u062a \u0648 \u0627\u0637\u0644\u0627\u0639 \u0631\u0633\u0627\u0646\u06cc \u06a9\u0634\u0627\u0648\u0631\u0632\u06cc"
            },
            "slug": "Optimum-systems-control-Siouris",
            "title": {
                "fragments": [],
                "text": "Optimum systems control"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145708111"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 207
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore [18] used local memory-based methods in conjunction with value iteration; Lin [59] used backpropagation networks for Q-learning; Watkins [128] used CMAC for Q-learning; Tesauro [118, 120] used backpropagation for learning the value function in backgammon (described in Section 8."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 142
                            }
                        ],
                        "text": "Many of the other hierarchical learning methods can be cast in this framework.6.3.1 Feudal Q-learningFeudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learningmodules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 18
                            }
                        ],
                        "text": "Feudal Q-learning [31, 128] involves a hierarchy of learning modules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 167
                            }
                        ],
                        "text": "If each action is executed in each state an in nite number of times on an in nite run and is decayed appropriately, the Q values will converge with probability 1 to Q [128, 125, 49]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 166
                            }
                        ],
                        "text": "If each action is executed ineach state an in nite number of times on an in nite run and is decayed appropriately, theQ values will converge with probability 1 to Q (Watkins, 1989; Tsitsiklis, 1994; Jaakkola,Jordan, & Singh, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 123
                            }
                        ],
                        "text": "2 Q-learning The work of the two components of AHC can be accomplished in a uni ed manner by Watkins' Q-learning algorithm [128, 129]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 259
                            }
                        ],
                        "text": "\u2026de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC can be accomplished in a uni ed manner byWatkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59809750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c8bb027eb65b6d250a22e9b6db22853a552ac81",
            "isKey": true,
            "numCitedBy": 2924,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-from-delayed-rewards-Watkins",
            "title": {
                "fragments": [],
                "text": "Learning from delayed rewards"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2067481107"
                        ],
                        "name": "R. Dunn",
                        "slug": "R.-Dunn",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Dunn",
                            "middleNames": [
                                "Minta"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dunn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 5
                            }
                        ],
                        "text": "CMAC (Albus, 1981), and local memory-based methods (Moore, Atkeson, & Schaal, 1995), such as generalizations of nearest neighbor methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 6
                            }
                        ],
                        "text": "CMAC (Albus, 1981), and local memory-based methods (Moore, Atkeson, & Schaal, 1995),such as generalizations of nearest neighbor methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "A very di erent kind of function approximator (CMAC (Albus, 1975)) that has weakgeneralization.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 223
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                }
            ],
            "corpusId": 38185336,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "ba0c649ff274730ab1f3e21f67ba5aa90080df17",
            "isKey": true,
            "numCitedBy": 85,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Brains,-behavior,-and-robotics-Dunn",
            "title": {
                "fragments": [],
                "text": "Brains, behavior, and robotics"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 142
                            }
                        ],
                        "text": "Many of the other hierarchical learning methods can be cast in this framework.6.3.1 Feudal Q-learningFeudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learningmodules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 166
                            }
                        ],
                        "text": "If each action is executed ineach state an in nite number of times on an in nite run and is decayed appropriately, theQ values will converge with probability 1 to Q (Watkins, 1989; Tsitsiklis, 1994; Jaakkola,Jordan, & Singh, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 191
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 259
                            }
                        ],
                        "text": "\u2026de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC can be accomplished in a uni ed manner byWatkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning from Delayed R ewards King's College"
            },
            "venue": {
                "fragments": [],
                "text": "Learning from Delayed R ewards King's College"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 142
                            }
                        ],
                        "text": "Many of the other hierarchical learning methods can be cast in this framework.6.3.1 Feudal Q-learningFeudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learningmodules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 166
                            }
                        ],
                        "text": "If each action is executed ineach state an in nite number of times on an in nite run and is decayed appropriately, theQ values will converge with probability 1 to Q (Watkins, 1989; Tsitsiklis, 1994; Jaakkola,Jordan, & Singh, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 191
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 259
                            }
                        ],
                        "text": "\u2026de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC can be accomplished in a uni ed manner byWatkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning from Delayed Rewards. PhD thesis, King's College"
            },
            "venue": {
                "fragments": [],
                "text": "Learning from Delayed Rewards. PhD thesis, King's College"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 6
                            }
                        ],
                        "text": "CMAC (Albus, 1981), and local memory-based methods (Moore, Atkeson, & Schaal, 1995),such as generalizations of nearest neighbor methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "A very di erent kind of function approximator (CMAC (Albus, 1975)) that has weakgeneralization.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 223
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "Popular methods include various neural-network methods [76], CMAC [2], and local memory-based methods [67], such as generalizations of nearest neighbor methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Brains, Behavior, and Robotics. BYTE Books, Subsidiary of McGraw-Hill,  Peterborough"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 142
                            }
                        ],
                        "text": "Many of the other hierarchical learning methods can be cast in this framework.6.3.1 Feudal Q-learningFeudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learningmodules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 166
                            }
                        ],
                        "text": "If each action is executed ineach state an in nite number of times on an in nite run and is decayed appropriately, theQ values will converge with probability 1 to Q (Watkins, 1989; Tsitsiklis, 1994; Jaakkola,Jordan, & Singh, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 191
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 259
                            }
                        ],
                        "text": "\u2026de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC can be accomplished in a uni ed manner byWatkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning from Delayed Rewards King's College"
            },
            "venue": {
                "fragments": [],
                "text": "Learning from Delayed Rewards King's College"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Jaakkola, Singh, and Jordan [50] have developed an algorithm for nding locally-optimal stochastic policies, but nding a globally optimal policy is still NP hard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Jaakkola, Jordan and Singh (1995) described an average-reward learning algorithmwith guaranteed convergence properties."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 56
                            }
                        ],
                        "text": "RTDP (real-time dynamic programming) (Barto, Bradtke, & Singh, 1995) is anothermodel-based method that uses Q-learning to concentrate computational e ort on the areasof the state-space that the agent is most likely to occupy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 14
                            }
                        ],
                        "text": "Jaakkola, T., Singh, S. P., & Jordan, M. I. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 68
                            }
                        ],
                        "text": "276\nReinforcement Learning: A SurveyBarto, A. G., Bradtke, S. J., & Singh, S. P. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Singh, S. P. (1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Singh, S. P. (1992b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 218
                            }
                        ],
                        "text": "If each action is executed ineach state an in nite number of times on an in nite run and is decayed appropriately, theQ values will converge with probability 1 to Q (Watkins, 1989; Tsitsiklis, 1994; Jaakkola,Jordan, & Singh, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Singh, S. P., & Sutton, R. S. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 152
                            }
                        ],
                        "text": "These problems can be ameliorated by programming a set of \\re exes\" that cause theagent to act initially in some way that is reasonable (Mataric, 1994; Singh, Barto,Grupen, & Connolly, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 59
                            }
                        ],
                        "text": "The MIT Press,Cambridge, MA.Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Singh, S. P., Barto, A. G., Grupen, R., & Connolly, C. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 31
                            }
                        ],
                        "text": "283\nKaelbling, Littman, & MooreSingh, S. P. (1992a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Jaakkola, Jordan and Singh [50] described an average-reward learning algorithmwith guaranteed convergence properties."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 188
                            }
                        ],
                        "text": "Therehas been some recent work on making the updates more e cient (Cichosz & Mulawka, 1995)and on changing the de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC can be accomplished in a uni ed manner byWatkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 0
                            }
                        ],
                        "text": "Jaakkola, Singh, and Jordan (1995)have developed an algorithm for nding locally-optimal stochastic policies, but nding aglobally optimal policy is still NP hard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 179
                            }
                        ],
                        "text": "The reinforcement functionsfor the individual behaviors (commands) are given, but learning takes place simultaneouslyat both the high and low levels.6.3.2 Compositional Q-learningSingh's compositional Q-learning (1992b, 1992a) (C-QL) consists of a hierarchy based onthe temporal sequencing of subgoals."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 28
                            }
                        ],
                        "text": "This type of sample backup (Singh, 1993) is criticalto the operation of the model-free methods discussed in the next section."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Monte-carlo reinforcement learn-  ing in non-Markovian decision problems"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 7,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 15
                            }
                        ],
                        "text": "More recently, Tesauro (1992, 1994, 1995) applied the temporal di erence algorithmto backgammon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 3
                            }
                        ],
                        "text": "In Tesauro, G., Touretzky, D. S., & Leen, T. K.(Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Tesauro, G. (1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 17
                            }
                        ],
                        "text": "In Cowan, J. D., Tesauro, G., & Alspector, J. (Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 23
                            }
                        ],
                        "text": "More recently, Tesauro [118, 119, 120] applied the temporal di erence algorithm to backgammon."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 3
                            }
                        ],
                        "text": "In Tesauro, G., Touretzky, D. S., &Leen, T. K. (Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 17
                            }
                        ],
                        "text": "In Cowan, J. D., Tesauro, G., & Alspector, J.(Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 275
                            }
                        ],
                        "text": "Kaelbling, Littman, & Moore Many reseachers have experimented with this approach: Boyan and Moore [18] used local memory-based methods in conjunction with value iteration; Lin [59] used backpropagation networks for Q-learning; Watkins [128] used CMAC for Q-learning; Tesauro [118, 120] used backpropagation for learning the value function in backgammon (described in Section 8."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 191
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 145
                            }
                        ],
                        "text": "\u2026methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 17
                            }
                        ],
                        "text": "In Cowan, J. D., Tesauro, G., & Alspector,J. (Eds.)"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Tesauro, G. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Tesauro, G. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 9
                            }
                        ],
                        "text": "Instead, Tesauro used a backpropagation-based three-layer270\nReinforcement Learning: A SurveyTrainingGames HiddenUnits ResultsBasic PoorTD 1.0 300,000 80 Lost by 13 points in 51gamesTD 2.0 800,000 40 Lost by 7 points in 38gamesTD 2.1 1,500,000 80 Lost by 1 point in 40gamesTable 2: TD-Gammon's performance in games against the top human professional players."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Temporal di erence learning and TD-Gammon.Communications of the ACM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 142
                            }
                        ],
                        "text": "Many of the other hierarchical learning methods can be cast in this framework.6.3.1 Feudal Q-learningFeudal Q-learning (Dayan & Hinton, 1993; Watkins, 1989) involves a hierarchy of learningmodules."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 166
                            }
                        ],
                        "text": "If each action is executed ineach state an in nite number of times on an in nite run and is decayed appropriately, theQ values will converge with probability 1 to Q (Watkins, 1989; Tsitsiklis, 1994; Jaakkola,Jordan, & Singh, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 150
                            }
                        ],
                        "text": "\u2026Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 202,
                                "start": 191
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 272,
                                "start": 259
                            }
                        ],
                        "text": "\u2026de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC can be accomplished in a uni ed manner byWatkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning from Delayed R ewards. PhD thesis, King's College"
            },
            "venue": {
                "fragments": [],
                "text": "Learning from Delayed R ewards. PhD thesis, King's College"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "Jaakkola, Singh, and Jordan [50] have developed an algorithm for nding locally-optimal stochastic policies, but nding a globally optimal policy is still NP hard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Jaakkola, Jordan and Singh (1995) described an average-reward learning algorithmwith guaranteed convergence properties."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 56
                            }
                        ],
                        "text": "RTDP (real-time dynamic programming) (Barto, Bradtke, & Singh, 1995) is anothermodel-based method that uses Q-learning to concentrate computational e ort on the areasof the state-space that the agent is most likely to occupy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 14
                            }
                        ],
                        "text": "Jaakkola, T., Singh, S. P., & Jordan, M. I. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 68
                            }
                        ],
                        "text": "276\nReinforcement Learning: A SurveyBarto, A. G., Bradtke, S. J., & Singh, S. P. (1995)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Singh, S. P. (1993)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Singh, S. P. (1992b)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 223,
                                "start": 218
                            }
                        ],
                        "text": "If each action is executed ineach state an in nite number of times on an in nite run and is decayed appropriately, theQ values will converge with probability 1 to Q (Watkins, 1989; Tsitsiklis, 1994; Jaakkola,Jordan, & Singh, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Singh, S. P., & Sutton, R. S. (1996)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 152
                            }
                        ],
                        "text": "These problems can be ameliorated by programming a set of \\re exes\" that cause theagent to act initially in some way that is reasonable (Mataric, 1994; Singh, Barto,Grupen, & Connolly, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 59
                            }
                        ],
                        "text": "The MIT Press,Cambridge, MA.Jaakkola, T., Jordan, M. I., & Singh, S. P. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "Singh, S. P., Barto, A. G., Grupen, R., & Connolly, C. (1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 31
                            }
                        ],
                        "text": "283\nKaelbling, Littman, & MooreSingh, S. P. (1992a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Jaakkola, Jordan and Singh [50] described an average-reward learning algorithmwith guaranteed convergence properties."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 188
                            }
                        ],
                        "text": "Therehas been some recent work on making the updates more e cient (Cichosz & Mulawka, 1995)and on changing the de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC can be accomplished in a uni ed manner byWatkins' Q-learning algorithm (Watkins, 1989; Watkins & Dayan, 1992)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 0
                            }
                        ],
                        "text": "Jaakkola, Singh, and Jordan (1995)have developed an algorithm for nding locally-optimal stochastic policies, but nding aglobally optimal policy is still NP hard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 179
                            }
                        ],
                        "text": "The reinforcement functionsfor the individual behaviors (commands) are given, but learning takes place simultaneouslyat both the high and low levels.6.3.2 Compositional Q-learningSingh's compositional Q-learning (1992b, 1992a) (C-QL) consists of a hierarchy based onthe temporal sequencing of subgoals."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 28
                            }
                        ],
                        "text": "This type of sample backup (Singh, 1993) is criticalto the operation of the model-free methods discussed in the next section."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Monte-carlo reinforcement learning in non-Markovian decision problems"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "McCallum (1995) suggests some related tree-structuredmethods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "McCallum (1995) describes the \\utile su x memory\" which learns a variable-width windowthat serves simultaneously as a model of the environment and a nite-memory policy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 99
                            }
                        ],
                        "text": "It outperformed Q-learning with backpropagation in a simple video-game environment and was used by McCallum (1995) (in conjunction with other techniquesfor dealing with partial observability) to learn behaviors in a complex driving-simulator."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "Thissystem has had excellent results in a very complex driving-simulation domain (McCallum,1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 63335792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62137dd1efd655b31045e9f759970a7939989f0a",
            "isKey": true,
            "numCitedBy": 40,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Instance-Based-Utile-Distinctions-for-Reinforcement-McCallum",
            "title": {
                "fragments": [],
                "text": "Instance-Based Utile Distinctions for Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70740667"
                        ],
                        "name": "C. Ernst",
                        "slug": "C.-Ernst",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Ernst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ernst"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 283
                            }
                        ],
                        "text": "\u2026way for all attainable belief statesis linear in the number of belief states times actions, and thus exponential in the horizon.2.1.2 Gittins Allocation IndicesGittins gives an \\allocation index\" method for nding the optimal choice of action at eachstep in k-armed bandit problems (Gittins, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 151
                            }
                        ],
                        "text": "2 Gittins Allocation Indices Gittins gives an \\allocation index\" method for nding the optimal choice of action at each step in k-armed bandit problems (Gittins, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62731396,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "a2d5e774e878dc36141e8b78cdebdf712bd0a321",
            "isKey": true,
            "numCitedBy": 774,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multi-armed-Bandit-Allocation-Indices-Ernst",
            "title": {
                "fragments": [],
                "text": "Multi-armed Bandit Allocation Indices"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 82
                            }
                        ],
                        "text": "Thissystem has had excellent results in a very complex driving-simulation domain (McCallum,1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 99
                            }
                        ],
                        "text": "It outperformed Q-learning with backpropagation in a simple video-game environment and was used by McCallum (1995) (in conjunction with other techniquesfor dealing with partial observability) to learn behaviors in a complex driving-simulator."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 82
                            }
                        ],
                        "text": "This system has had excellent results in a very complex driving-simulation domain (McCallum, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "McCallum (1995) suggests some related tree-structuredmethods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "McCallum (1995) describes the \\utile su x memory\" which learns a variable-width windowthat serves simultaneously as a model of the environment and a nite-memory policy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 60716402,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "5ee38bf9494a91ca8665f9fbe59830464c223b82",
            "isKey": true,
            "numCitedBy": 558,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reinforcement-learning-with-selective-perception-McCallum-Ballard",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning with selective perception and hidden state"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 67
                            }
                        ],
                        "text": "Therehas been some recent work on making the updates more e cient (Cichosz & Mulawka, 1995)and on changing the de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "There has been some recent work on making the updates more e cient [19] and on changing the de nition to make TD( ) more consistent with the certaintyequivalent method (discussed in Section 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast and e cient reinforcement learning with truncated  temporal di erences"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the  Twelfth International Conference on Machine Learning,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 6
                            }
                        ],
                        "text": "CMAC (Albus, 1981), and local memory-based methods (Moore, Atkeson, & Schaal, 1995),such as generalizations of nearest neighbor methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "A very di erent kind of function approximator (CMAC (Albus, 1975)) that has weakgeneralization.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 223
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Brains, Behavior, and Robotics. BYTE Books"
            },
            "venue": {
                "fragments": [],
                "text": "Brains, Behavior, and Robotics. BYTE Books"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 181
                            }
                        ],
                        "text": "\u2026networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A reinforcement learning approach to job-shop"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "Gittins gives an \\allocation index\" method for nding the optimal choice of action at each step in k-armed bandit problems [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 283
                            }
                        ],
                        "text": "\u2026way for all attainable belief statesis linear in the number of belief states times actions, and thus exponential in the horizon.2.1.2 Gittins Allocation IndicesGittins gives an \\allocation index\" method for nding the optimal choice of action at eachstep in k-armed bandit problems (Gittins, 1989)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Multi-armed Bandit Allocation Indices. Wiley-Interscience series in systems and optimization"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 53
                            }
                        ],
                        "text": "These problems are addressedby prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna (Peng & Williams,1993), which are two independently-developed but very similar techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 149
                            }
                        ],
                        "text": "\u2026ex-ploration bonus in Dyna (Sutton, 1990), curiosity-driven exploration (Schmidhuber, 1991a),and the exploration mechanism in prioritized sweeping (Moore & Atkeson, 1993).2.2.2 Randomized StrategiesAnother simple exploration strategy is to take the action with the best estimated expectedreward by\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Prioritized sweeping: Reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 64
                            }
                        ],
                        "text": "The value of a policy is learned using Sutton's TD(0) algorithm (Sutton, 1988) which uses the update rule V (s) := V (s) + (r + V (s0) V (s)) : Whenever a state s is visited, its estimated value is updated to be closer to r + V (s0), since r is the instantaneous reward received and V (s0) is the estimated value of the actually occurring next state."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 64
                            }
                        ],
                        "text": "This class of algorithmsis known as temporal di erence methods (Sutton, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 64
                            }
                        ],
                        "text": "The value of a policy is learned using Sutton's TD(0)algorithm (Sutton, 1988) which uses the update ruleV (s) := V (s) + (r + V (s0) V (s)) :Whenever a state s is visited, its estimated value is updated to be closer to r + V (s0),since r is the instantaneous reward received and V (s0) is the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 64
                            }
                        ],
                        "text": "This class of algorithms is known as temporal di erence methods (Sutton, 1988)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to predict by the method of temporal di erences"
            },
            "venue": {
                "fragments": [],
                "text": "Machine"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 0
                            }
                        ],
                        "text": "Puterman (1994) discusses another stoppingcriterion, based on the span semi-norm, which may result in earlier termination."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "Since there are at most jAjjSj distinct policies, and the sequence of policies improves at each step, this algorithm terminates in at most an exponential number of iterations [73]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 45
                            }
                        ],
                        "text": "There are many good references to MDP models [8, 11, 35, 73]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 90
                            }
                        ],
                        "text": "There are many good references toMDP models (Bellman, 1957; Bertsekas, 1987; Howard,1960; Puterman, 1994)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Markov Decision Processes|Discrete Stochastic Dynamic Program-  ming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 148
                            }
                        ],
                        "text": "\u2026the updates more e cient (Cichosz & Mulawka, 1995)and on changing the de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC can be accomplished in a uni ed manner\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning with replacing eligibility"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "Modi ed policy iteration [91] seeks a trade-o between cheap and e ective iterations and is preferred by some practictioners [96]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 47
                            }
                        ],
                        "text": "Puterman's modi ed policy iteration algorithm (Puterman & Shin, 1978)provides a method for trading iteration time for iteration improvement in a smoother way."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "Puterman's modi ed policy iteration algorithm [91] provides a method for trading iteration time for iteration improvement in a smoother way."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 26
                            }
                        ],
                        "text": "Modi ed policy iteration (Puterman & Shin,1978) seeks a trade-o between cheap and e ective iterations and is preferred by somepractictioners (Rust, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modi ed policy iteration algorithms for discounted  Markov decision processes"
            },
            "venue": {
                "fragments": [],
                "text": "Management Science,"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 67
                            }
                        ],
                        "text": "Therehas been some recent work on making the updates more e cient (Cichosz & Mulawka, 1995)and on changing the de nition to make TD( ) more consistent with the certainty-equivalentmethod (Singh & Sutton, 1996), which is discussed in Section 5.1.4.2 Q-learningThe work of the two components of AHC\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast and e cient reinforcement learning with trun"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 6
                            }
                        ],
                        "text": "CMAC (Albus, 1981), and local memory-based methods (Moore, Atkeson, & Schaal, 1995),such as generalizations of nearest neighbor methods."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "A very di erent kind of function approximator (CMAC (Albus, 1975)) that has weakgeneralization.3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 223
                            }
                        ],
                        "text": "Many reseachers have experimented with this approach: Boyan and Moore (1995) usedlocal memory-based methods in conjunction with value iteration; Lin (1991) used backprop-agation networks for Q-learning; Watkins (1989) used CMAC for Q-learning; Tesauro (1992,1995) used backpropagation for learning the value function in backgammon (described inSection 8.1); Zhang and Dietterich (1995) used backpropagation and TD( ) to learn goodstrategies for job-shop scheduling."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Brains, Behavior, and Robotics. BYTE Books, Subsidiary of McGraw-Hill"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 14
                            }
                        ],
                        "text": "It is NP-hard (Littman, 1994b) to nd this mapping, and even the best mapping can have very poor performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 104
                            }
                        ],
                        "text": "Nonetheless, reinforcement-learning algorithms can be adapted to work for avery general class of games (Littman, 1994a) and many researchers have used reinforcementlearning in these environments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 15
                            }
                        ],
                        "text": "It is NP-hard (Littman, 1994b) to nd this mapping, and even thebest mapping can have very poor performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 239
                            }
                        ],
                        "text": "POMDP Approach Another strategy consists of using hidden Markov model (HMM)techniques to learn a model of the environment, including the hidden state, then to use thatmodel to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994;Lovejoy, 1991; Monahan, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58402513,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4eb68f4c2ecfce149e631b909dad37b451d38dba",
            "isKey": false,
            "numCitedBy": 233,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Memoryless-policies:-theoretical-limitations-and-Littman",
            "title": {
                "fragments": [],
                "text": "Memoryless policies: theoretical limitations and practical results"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47387053"
                        ],
                        "name": "David K. Smith",
                        "slug": "David-K.-Smith",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Smith",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David K. Smith"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 52
                            }
                        ],
                        "text": "Reinforcement Learning: A Survey Linear programming [105] is an extremely general problem, and MDPs can be solved by generalpurpose linear-programming packages [35, 34, 46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 20
                            }
                        ],
                        "text": "Linear programming (Schrijver, 1986) is an extremely general problem, and MDPs canbe solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux,1963; Ho man & Karp, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 189867398,
            "fieldsOfStudy": [],
            "id": "dddeb270e322f34b571a3b6e3b297e04da8d67e3",
            "isKey": false,
            "numCitedBy": 2047,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theory-of-Linear-and-Integer-Programming-Smith",
            "title": {
                "fragments": [],
                "text": "Theory of Linear and Integer Programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98742251"
                        ],
                        "name": "D. Burghes",
                        "slug": "D.-Burghes",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Burghes",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burghes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144690230"
                        ],
                        "name": "A. Graham",
                        "slug": "A.-Graham",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Graham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graham"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 63
                            }
                        ],
                        "text": "4 Reinforcement Learning and Adaptive ControlAdaptive control (Burghes & Graham, 1980; Stengel, 1986) is also concerned with algo-rithms for improving a sequence of decisions from experience."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 63
                            }
                        ],
                        "text": "4 Reinforcement Learning and Adaptive Control Adaptive control [19, 112] is also concerned with algorithms for improving a sequence of decisions from experience."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 153203018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "035e591daa89e68563ee32798ef3bee72b508c87",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-Control-Theory,-Including-Optimal-Burghes-Graham",
            "title": {
                "fragments": [],
                "text": "Introduction to Control Theory, Including Optimal Control"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1412494839"
                        ],
                        "name": "Raymond J. Bandlow",
                        "slug": "Raymond-J.-Bandlow",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Bandlow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond J. Bandlow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 143293115,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "74f5e7bc41a629a3cef50d4d60cce0523ebb6bdf",
            "isKey": false,
            "numCitedBy": 288,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Theories-of-Learning,-4th-Edition.-By-Ernest-R.-and-Bandlow",
            "title": {
                "fragments": [],
                "text": "Theories of Learning, 4th Edition. By Ernest R. Hilgard and Gordon H. Bower. Englewood Cliffs, N.J.: Prentice-Hall, Inc., 1975"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4139651"
                        ],
                        "name": "R. Mortensen",
                        "slug": "R.-Mortensen",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Mortensen",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mortensen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 63
                            }
                        ],
                        "text": "4 Reinforcement Learning and Adaptive ControlAdaptive control (Burghes & Graham, 1980; Stengel, 1986) is also concerned with algo-rithms for improving a sequence of decisions from experience."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121732676,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "b60ed548a94feaae342dfa825a682b0ce70c28fc",
            "isKey": false,
            "numCitedBy": 1,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Introduction-to-Control-Theory,-Including-Optimal-Mortensen",
            "title": {
                "fragments": [],
                "text": "Introduction to Control Theory, Including Optimal Control (David Burghes and Alexander Graham)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3365230"
                        ],
                        "name": "C. Derman",
                        "slug": "C.-Derman",
                        "structuredName": {
                            "firstName": "Cyrus",
                            "lastName": "Derman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Derman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 140
                            }
                        ],
                        "text": "Linear programming (Schrijver, 1986) is an extremely general problem, and MDPs can be solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux, 1963; Ho man & Karp, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 140
                            }
                        ],
                        "text": "Linear programming (Schrijver, 1986) is an extremely general problem, and MDPs canbe solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux,1963; Ho man & Karp, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 117572511,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "167be8348ff60a37c96efdfc1e0e3f467a84a470",
            "isKey": false,
            "numCitedBy": 819,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Finite-State-Markovian-Decision-Processes-Derman",
            "title": {
                "fragments": [],
                "text": "Finite State Markovian Decision Processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1970
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27581584"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Howard",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 44
                            }
                        ],
                        "text": "There are many good references toMDP models (Bellman, 1957; Bertsekas, 1987; Howard, 1960; Puterman, 1994)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 77
                            }
                        ],
                        "text": "There are many good references toMDP models (Bellman, 1957; Bertsekas, 1987; Howard,1960; Puterman, 1994)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62124406,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c7d3e9a1dd86f9c96f709d0ddb76972862784231",
            "isKey": false,
            "numCitedBy": 2857,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dynamic-Programming-and-Markov-Processes-Howard",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming and Markov Processes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1960
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 85
                            }
                        ],
                        "text": "They can also begeneralized to real-valued reward through reward comparison methods (Sutton, 1984)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 228
                            }
                        ],
                        "text": "The second network is trained in a standardsupervised mode to estimate r as a function of the input state s.Variations of this approach have been used in a variety of applications (Anderson, 1986;Barto et al., 1983; Lin, 1993b; Sutton, 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 62
                            }
                        ],
                        "text": "ARC The associative reinforcement comparison (arc) algorithm (Sutton, 1984) is aninstance of the ahc architecture for the case of boolean actions, consisting of two feed-260\nReinforcement Learning: A Surveyforward networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60564875,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "22069cd4504656d3bb85748a4d43be7a4d7d5545",
            "isKey": false,
            "numCitedBy": 870,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Temporal-credit-assignment-in-reinforcement-Sutton",
            "title": {
                "fragments": [],
                "text": "Temporal credit assignment in reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153570946"
                        ],
                        "name": "M. Dorigo",
                        "slug": "M.-Dorigo",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Dorigo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dorigo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775694"
                        ],
                        "name": "H. Bersini",
                        "slug": "H.-Bersini",
                        "structuredName": {
                            "firstName": "Hugues",
                            "lastName": "Bersini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bersini"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 67
                            }
                        ],
                        "text": "Dorigo did a comparative study of Q-learning andclassi er systems (Dorigo & Bersini, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Dorigo did a comparative study of Q-learning and classi er systems [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60930935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2671db5204a4cffe82c09254495137066751ef8",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-comparison-of-Q-learning-and-classifier-systems-Dorigo-Bersini",
            "title": {
                "fragments": [],
                "text": "A comparison of Q-learning and classifier systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 60
                            }
                        ],
                        "text": "Variable Resolution Dynamic Programming The VRDP algorithm (Moore, 1991)enables conventional dynamic programming to be performed in real-valued multivariatestate-spaces where straightforward discretization would fall prey to the curse of dimension-ality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 35796418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07f9849396e9ae59a007c9008e5d7bd352a9433a",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Variable-Resolution-Dynamic-Programming-Moore",
            "title": {
                "fragments": [],
                "text": "Variable Resolution Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 150
                            }
                        ],
                        "text": "Dorigo and Colombetti applied classi er systems to a moderately complex problem oflearning robot behavior from immediate reinforcement (Dorigo, 1995; Dorigo & Colombetti,1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 16
                            }
                        ],
                        "text": "Lin (1993a) and Dorigo and Colombetti (1995,1994) both used this approach, rst training the behaviors and then training the gatingfunction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 152
                            }
                        ],
                        "text": "\u2026systems from the bottomup (Lin, 1991), and to alleviate problems of delayed reinforcement by decreasing thedelay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995).local reinforcement signals: Whenever possible, agents should be given reinforcementsignals that are local."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robot shaping: Developing autonomous agents"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 150
                            }
                        ],
                        "text": "Dorigo and Colombetti applied classi er systems to a moderately complex problem oflearning robot behavior from immediate reinforcement (Dorigo, 1995; Dorigo & Colombetti,1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 16
                            }
                        ],
                        "text": "Lin (1993a) and Dorigo and Colombetti (1995,1994) both used this approach, rst training the behaviors and then training the gatingfunction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 152
                            }
                        ],
                        "text": "\u2026systems from the bottomup (Lin, 1991), and to alleviate problems of delayed reinforcement by decreasing thedelay until the problem is well understood (Dorigo & Colombetti, 1994; Dorigo, 1995).local reinforcement signals: Whenever possible, agents should be given reinforcementsignals that are local."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robot shaping: Developing autonomous agents"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 21
                            }
                        ],
                        "text": "REINFORCE Algorithms Williams (1987, 1992) studied the problem of choosing ac-tions to maximize immedate reward."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A class of gradient-estimating algorithms for reinforcement l e a r n i n g in neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE First International Conference o n Neural Networks"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 52
                            }
                        ],
                        "text": "A very di erent kind of function approximator (CMAC (Albus, 1975)) that has weak generalization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 53
                            }
                        ],
                        "text": "A very di erent kind of function approximator (CMAC (Albus, 1975)) that has weakgeneralization.3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new approach to manipulator control: Cerebellar model articulation controller cmac"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Dynamic Systems, Measurement and Control"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 59
                            }
                        ],
                        "text": "Popular techniques include various neural-network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed P r ocessing: Explorations in the microstructures of cognition. Volume 1: Foundations"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Distributed P r ocessing: Explorations in the microstructures of cognition. Volume 1: Foundations"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Variable resolution dynamic programming : E \u000e ciently learning action mapsin multivariate real - valued spaces"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden state and short-term memory, 1993. Presentation at Reinforcement Learning Workshop"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning Conference"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 52
                            }
                        ],
                        "text": "A very di erent kind of function approximator (CMAC (Albus, 1975)) that has weak generalization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 53
                            }
                        ],
                        "text": "A very di erent kind of function approximator (CMAC (Albus, 1975)) that has weakgeneralization.3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new approach to manipulator control: Cerebellar model articulation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 58
                            }
                        ],
                        "text": "Q-learning has been used in an elevator dispatching task (Crites & Barto, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving elevator performance using reinforcement"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 104
                            }
                        ],
                        "text": "Nonetheless, reinforcement-learning algorithms can be adapted to work for avery general class of games (Littman, 1994a) and many researchers have used reinforcementlearning in these environments."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 15
                            }
                        ],
                        "text": "It is NP-hard (Littman, 1994b) to nd this mapping, and even thebest mapping can have very poor performance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 239
                            }
                        ],
                        "text": "POMDP Approach Another strategy consists of using hidden Markov model (HMM)techniques to learn a model of the environment, including the hidden state, then to use thatmodel to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994;Lovejoy, 1991; Monahan, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1994a). Markov games as a framework for multi-agent reinforcement"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 176
                            }
                        ],
                        "text": "Multigrid methods canbe used to quickly seed a good initial approximation to a high resolution value functionby initially performing value iteration at a coarser resolution (R ude, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mathematical and computational techniques for multilevel adaptive methods. Society for Industrial and Applied Mathematics, Philadelphia, Pennsylvania"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Temporal di erence learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Empirical Model-Building and Response"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 64
                            }
                        ],
                        "text": "This class of algorithmsis known as temporal di erence methods (Sutton, 1988)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 64
                            }
                        ],
                        "text": "The value of a policy is learned using Sutton's TD(0)algorithm (Sutton, 1988) which uses the update ruleV (s) := V (s) + (r + V (s0) V (s)) :Whenever a state s is visited, its estimated value is updated to be closer to r + V (s0),since r is the instantaneous reward received and V (s0) is the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to predict by the method of temporal diierences"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 0
                            }
                        ],
                        "text": "Jaakkola, Singh, and Jordan (1995)have developed an algorithm for nding locally-optimal stochastic policies, but nding aglobally optimal policy is still NP hard."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Monte-carlo reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 37
                            }
                        ],
                        "text": "Classi er Systems Classi er systems (Holland, 1975; Goldberg, 1989) were explicitlydeveloped to solve problems with delayed reward, including those requiring short-termmemory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptation in Natural and Artiicial Systems. U n i v ersity o f M i c higan Press"
            },
            "venue": {
                "fragments": [],
                "text": "Adaptation in Natural and Artiicial Systems. U n i v ersity o f M i c higan Press"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 53
                            }
                        ],
                        "text": "A very di erent kind of function approximator (CMAC (Albus, 1975)) that has weakgeneralization.3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A new approach to manipulator control : Cerebellar model articulationcontroller ( cmac )"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Dynamic Systems , Measurement and Control"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 63
                            }
                        ],
                        "text": "The environmentis partitioned (a priori, but more recent work (Ashar, 1994) addresses the case of learningthe partition) into a set of regions whose centers are known as \\landmarks.\""
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical learning in stochastic domains. Master's thesis"
            },
            "venue": {
                "fragments": [],
                "text": "Hierarchical learning in stochastic domains. Master's thesis"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Puterman and Moon Chirl Shin . Modi ed policy iteration algorithms for discountedMarkov decision processes"
            },
            "venue": {
                "fragments": [],
                "text": "Management Science"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 64
                            }
                        ],
                        "text": "CRBP The complementary reinforcement backpropagation algorithm (Ackley & Littman,1990) (crbp) consists of a feed-forward network mapping an encoding of the state to anencoding of the action."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization and scaling in reinforcement"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Ronny) Ashar. Hierarchical learning in stochastic domains. Master's thesis"
            },
            "venue": {
                "fragments": [],
                "text": "Rachita"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 64
                            }
                        ],
                        "text": "CRBP The complementary reinforcement backpropagation algorithm (Ackley & Littman,1990) (crbp) consists of a feed-forward network mapping an encoding of the state to anencoding of the action."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Generalization and scaling in reinforcement"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "The environment is partitioned (a priori, but more recent work [5] addresses the case of learning the partition) into a set of regions whose centers are known as \\landmarks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 63
                            }
                        ],
                        "text": "The environmentis partitioned (a priori, but more recent work (Ashar, 1994) addresses the case of learningthe partition) into a set of regions whose centers are known as \\landmarks.\""
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical learning in stochastic domains. Master's thesis, Brown  University, Providence"
            },
            "venue": {
                "fragments": [],
                "text": "Rhode Island,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 60
                            }
                        ],
                        "text": "Variable Resolution Dynamic Programming The VRDP algorithm (Moore, 1991)enables conventional dynamic programming to be performed in real-valued multivariatestate-spaces where straightforward discretization would fall prey to the curse of dimension-ality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 59
                            }
                        ],
                        "text": "Variable Resolution Dynamic Programming The VRDP algorithm (Moore, 1991) enables conventional dynamic programming to be performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimensionality."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Variable resolution dynamic programming: E ciently learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 23
                            }
                        ],
                        "text": "level classi er system (Wilson, 1995) and add one and two-bit memory registers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "Cli and Ross (1994) start with Wilson's zeroth-268\nReinforcement Learning: A Survey i\nb a SE \u03c0Figure 10: Structure of a POMDP agent.level classi er system (Wilson, 1995) and add one and two-bit memory registers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classi er tness based on accuracy"
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation, 3 (2),"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 21
                            }
                        ],
                        "text": "REINFORCE Algorithms Williams (1987, 1992) studied the problem of choosing ac-tions to maximize immedate reward."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A class of gradient-estimating algorithms for reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mataric . Reward functions for accelerated learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 67
                            }
                        ],
                        "text": "Dorigo did a comparative study of Q-learning andclassi er systems (Dorigo & Bersini, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "Dorigo did a comparative study of Q-learning and classi er systems [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A comparison of q-learning and classi er systems. In From Animals  to Animats"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Third International Conference on the Simulation of Adaptive"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 27
                            }
                        ],
                        "text": "The Plexus planning system [26, 42] exploits a similar intuition."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Planning with dead-  lines in stochastic domains"
            },
            "venue": {
                "fragments": [],
                "text": "In Proceedings of the Eleventh National Conference on Arti cial  Intelligence,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 89
                            }
                        ],
                        "text": "However, the number of iterations required can grow exponentially inthe discount factor (Condon, 1992); as the discount factor approaches 1, the decisions mustbe based on results that happen farther and farther into the future."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The complexity o f s t o c hastic games"
            },
            "venue": {
                "fragments": [],
                "text": "Information and Computation"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 100
                            }
                        ],
                        "text": "One method is to do a local gradient-ascent search on the action inorder to nd one with high value (Baird & Klopf, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning with high-dimensional"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 118
                            }
                        ],
                        "text": "This method is computationally intractable, but may serve as inspiration for methods that make further approximations (Cassandra et al., 1994; Littman, Cassandra, & Kaelbling, 1995a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Acting optimally in partially"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive aggregation for innnite horizon dynamic programming"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1 9 9 1 ) . A s u r v ey of algorithmic methods for partially observable Markov decision processes"
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Operations Research"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Williams . E \u000e cient learning and planning within the Dyna framework"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems , Man , and Cybernetics"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Puterman and Moon Chirl Shin . Modi ed policy iteration algorithms for discounted Markov decision processes"
            },
            "venue": {
                "fragments": [],
                "text": "Management Science"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 176
                            }
                        ],
                        "text": "Multigrid methods canbe used to quickly seed a good initial approximation to a high resolution value functionby initially performing value iteration at a coarser resolution (R ude, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mathematical and computational techniques for multilevel adaptive meth"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 25
                            }
                        ],
                        "text": "3 Prioritized Sweeping / Queue-DynaAlthough Dyna is a great improvement on previous methods, it su ers from being relativelyundirected."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "These problems are addressedby prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna (Peng & Williams,1993), which are two independently-developed but very similar techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "These problems are addressed by prioritized sweeping [66] and Queue-Dyna [70], which are two independently-developed but very similar techniques."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E cient learning and planning within the Dyna frame-  work"
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive Behavior,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sejnowski . Temporal di erence learning ofposition evaluation in the game of Go"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming and Optimal Control. A thena Scientiic"
            },
            "venue": {
                "fragments": [],
                "text": "Dynamic Programming and Optimal Control. A thena Scientiic"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 101
                            }
                        ],
                        "text": "Popular techniques include various neuralnetwork methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 103
                            }
                        ],
                        "text": "Popular techniques include various neural-network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arti cial neural networks and approximate reasoning for intelligent"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 101
                            }
                        ],
                        "text": "Popular techniques include various neuralnetwork methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 103
                            }
                        ],
                        "text": "Popular techniques include various neural-network methods (Rumelhart & McClelland, 1986), fuzzy logic (Berenji, 1991; Lee, 1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arti cial neural networks and approximate reasoning for intelligent"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 138
                            }
                        ],
                        "text": "The necessary bias can come in a variety of forms, including the following:shaping: The technique of shaping is used in training animals (Hilgard & Bower, 1975); ateacher presents very simple problems to solve rst, then gradually exposes the learnerto more complex problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 117
                            }
                        ],
                        "text": "An example, which stands among a set of algorithms independently developed in themathematical psychology literature (Hilgard & Bower, 1975), is the linear reward-inactionalgorithm."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Theories of Learning fourth edition"
            },
            "venue": {
                "fragments": [],
                "text": "Theories of Learning fourth edition"
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Memory approaches to reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E cient reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Seventh"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": ", a selfteaching backgammon program , achieves masterlevelplay"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modiied policy iteration algorithms for discounted Markov decision processes"
            },
            "venue": {
                "fragments": [],
                "text": "Management Science"
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 72
                            }
                        ],
                        "text": "There is no known tight worst-case bound availablefor policy iteration (Littman et al., 1995b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the complexity of solving"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 128
                            }
                        ],
                        "text": "State aggre-gation works by collapsing groups of states to a single meta-state solving the abstractedproblem (Bertsekas & Casta~non, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "Bertsekas, D. P., & Casta~non, D. A. (1989)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive aggregation for in nite horizon"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tsitsikilis . Asynchronous stochastic approximation and Qlearning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Atkeson . An investigation of memory - based functionapproximators for learning control"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 45
                            }
                        ],
                        "text": "as well as some more novel search techniques (Schmidhuber, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 141
                            }
                        ],
                        "text": "Kaelbling, Littman, & Moore a T\ns i\nr B I RFigure 1: The standard reinforcement-learning model.as well as some more novel search techniques (Schmidhuber, 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A general method for multi-agent learning and incremental self"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Planning with deadlines"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 216
                            }
                        ],
                        "text": "POMDP Approach Another strategy consists of using hidden Markov model (HMM) techniques to learn a model of the environment, including the hidden state, then to use that model to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994; Lovejoy, 1991; Monahan, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 253
                            }
                        ],
                        "text": "POMDP Approach Another strategy consists of using hidden Markov model (HMM)techniques to learn a model of the environment, including the hidden state, then to use thatmodel to construct a perfect memory controller (Cassandra, Kaelbling, & Littman, 1994;Lovejoy, 1991; Monahan, 1982)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A survey of algorithmic methods for partially observable"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 128
                            }
                        ],
                        "text": "State aggre-gation works by collapsing groups of states to a single meta-state solving the abstractedproblem (Bertsekas & Casta~non, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "Bertsekas, D. P., & Casta~non, D. A. (1989)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive aggregation for in nite horizon"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 160
                            }
                        ],
                        "text": "Small breaches of the Markov requirement are well handled by Q-learning, but it is possible to construct simple environments that cause Q-learning to oscillate [18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hidden state and short-term memory"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Williams . E \u000e cient learning and planning within the Dyna framework"
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive Behavior"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 90
                            }
                        ],
                        "text": "This method proved useful in an application to robotic manipulationwith immediate reward (Salganico & Ungar, 1995)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Active exploration and learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 171
                            }
                        ],
                        "text": "The juggling robot learned a world model from experience, which was generalizedto unvisited states by a function approximation scheme known as locally weightedregression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Locally weighted regression: An approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 185
                            }
                        ],
                        "text": "1E( 1h hXt=0 rt) :Such a policy is referred to as a gain optimal policy; it can be seen as the limiting case ofthe in nite-horizon discounted model as the discount factor approaches 1 (Bertsekas, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic Programming and Optimal Control. A thena Scientiic"
            },
            "venue": {
                "fragments": [],
                "text": "Dynamic Programming and Optimal Control. A thena Scientiic"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 45
                            }
                        ],
                        "text": "as well as some more novel search techniques (Schmidhuber, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 141
                            }
                        ],
                        "text": "Kaelbling, Littman, & Moore a T\ns i\nr B I RFigure 1: The standard reinforcement-learning model.as well as some more novel search techniques (Schmidhuber, 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A general method for multi-agent learning and incremental selfimprovement in unrestricted environments"
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation: Theory and Applications"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 129
                            }
                        ],
                        "text": "This approach has been taken by work in genetic algorithms and genetic programming, as well as some more novel search techniques [101]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 141
                            }
                        ],
                        "text": "Kaelbling, Littman, & Moore a T\ns i\nr B I RFigure 1: The standard reinforcement-learning model.as well as some more novel search techniques (Schmidhuber, 1996)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A general method for multi-agent learning and incremental self-improvement  in unrestricted environments"
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation: Theory and  Applications. Scienti c Publ. Co., Singapore,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "Other games that have been studied include Go [82] and Chess [96]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using the TD( ) algorithm  to learn an evaluation function for the game of Go"
            },
            "venue": {
                "fragments": [],
                "text": "In Advances in Neural Information  Processing Systems 6,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "( 1 9 9 4 ) . T emporal diierence learning of position evaluation in the game of Go"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 6"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning in Embedded S y s t e m s"
            },
            "venue": {
                "fragments": [],
                "text": "Learning in Embedded S y s t e m s"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 60
                            }
                        ],
                        "text": "Variable Resolution Dynamic Programming The VRDP algorithm (Moore, 1991)enables conventional dynamic programming to be performed in real-valued multivariatestate-spaces where straightforward discretization would fall prey to the curse of dimension-ality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "Variable Resolution Dynamic Programming The VRDP algorithm [63] enables conventional dynamic programming to be performed in real-valued multivariate state-spaces where straightforward discretization would fall prey to the curse of dimensionality."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Variable resolution dynamic programming: E ciently learning action  maps in multivariate real-valued spaces"
            },
            "venue": {
                "fragments": [],
                "text": "In Proc. Eighth International Machine Learning  Workshop,"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel and Distributed Computation: Numer"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 128
                            }
                        ],
                        "text": "State aggre-gation works by collapsing groups of states to a single meta-state solving the abstractedproblem (Bertsekas & Casta~non, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 20
                            }
                        ],
                        "text": "Bertsekas, D. P., & Casta~non, D. A. (1989)."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive aggregation for in nite horizon dynamic programming"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 197
                            }
                        ],
                        "text": "The juggling robot learned a world model from experience, which was generalizedto unvisited states by a function approximation scheme known as locally weightedregression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An investigation of memory-based function"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 72
                            }
                        ],
                        "text": "Variations of this approach have been used in a variety of applications (Anderson, 1986; Barto et al., 1983; Lin, 1993b; Sutton, 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 196
                            }
                        ],
                        "text": "The second network is trained in a standardsupervised mode to estimate r as a function of the input state s.Variations of this approach have been used in a variety of applications (Anderson, 1986;Barto et al., 1983; Lin, 1993b; Sutton, 1984)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neuronlike adaptive elements"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1983
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Schaal and Atkeson (1994) constructed a two-armed robot, shown in Figure 11, thatlearns to juggle a device known as a devil-stick."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robot juggling: An implementation of memory-based"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Mahadevan and Connell (1991a) discuss a task in which a mobile robot pushes largeboxes for extended periods of time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 0
                            }
                        ],
                        "text": "Mahadevan and Connell (1991b) used thedual approach: they xed the gating function, and supplied reinforcement functions for theindividual behaviors, which were learned."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Scaling reinforcement learning to robotics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 30
                            }
                        ],
                        "text": "REINFORCE Algorithms Williams [131, 132] studied the problem of choosing actions to maximize immedate reward."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 21
                            }
                        ],
                        "text": "REINFORCE Algorithms Williams (1987, 1992) studied the problem of choosing ac-tions to maximize immedate reward."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A class of gradient-estimating algorithms for reinforcement learning in neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE First International Conference o n Neural Networks"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "Cli and Ross (1994) start with Wilson's zeroth-268\nReinforcement Learning: A Survey i\nb a SE \u03c0Figure 10: Structure of a POMDP agent.level classi er system (Wilson, 1995) and add one and two-bit memory registers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classiier tness based on accuracy"
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 47
                            }
                        ],
                        "text": "Puterman's modi ed policy iteration algorithm (Puterman & Shin, 1978)provides a method for trading iteration time for iteration improvement in a smoother way."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 26
                            }
                        ],
                        "text": "Modi ed policy iteration (Puterman & Shin,1978) seeks a trade-o between cheap and e ective iterations and is preferred by somepractictioners (Rust, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modi ed policy iteration algorithms for discounted"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "State aggregation works by collapsing groups of states to a single meta-state solving the abstracted problem [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive aggregation for in nite horizon dynamic  programming"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control,"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive aggregation for innnite horizon dynamic programming"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Automatic Control"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 25
                            }
                        ],
                        "text": "3 Prioritized Sweeping / Queue-DynaAlthough Dyna is a great improvement on previous methods, it su ers from being relativelyundirected."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 92
                            }
                        ],
                        "text": "These problems are addressedby prioritized sweeping (Moore & Atkeson, 1993) and Queue-Dyna (Peng & Williams,1993), which are two independently-developed but very similar techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "E cient learning and planning within the Dyna"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 25
                            }
                        ],
                        "text": "TheG-learning algorithm (Chapman & Kaelbling, 1991), works as follows."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Input generalization in delayed reinforcement"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 72
                            }
                        ],
                        "text": "There is no known tight worst-case bound availablefor policy iteration (Littman et al., 1995b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning policies for partially"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 60
                            }
                        ],
                        "text": "Variable Resolution Dynamic Programming The VRDP algorithm (Moore, 1991)enables conventional dynamic programming to be performed in real-valued multivariatestate-spaces where straightforward discretization would fall prey to the curse of dimension-ality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Variable resolution dynamic programming: EEciently learning action maps in multivariate real-valued spaces"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Eighth International Machine Learning Workshop"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 63
                            }
                        ],
                        "text": "The environmentis partitioned (a priori, but more recent work (Ashar, 1994) addresses the case of learningthe partition) into a set of regions whose centers are known as \\landmarks.\""
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 63
                            }
                        ],
                        "text": "The environment is partitioned (a priori, but more recent work (Ashar, 1994) addresses the case of learning the partition) into a set of regions whose centers are known as \\landmarks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hierarchical learning in stochastic domains"
            },
            "venue": {
                "fragments": [],
                "text": "Master's thesis, Brown"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 173
                            }
                        ],
                        "text": "Linear programming (Schrijver, 1986) is an extremely general problem, and MDPs canbe solved by general-purpose linear-programming packages (Derman, 1970; D'Epenoux,1963; Ho man & Karp, 1966)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On nonterminating stochastic games. Management"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Temporal di erence learning and TDGammon"
            },
            "venue": {
                "fragments": [],
                "text": "Communications of the ACM"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 176
                            }
                        ],
                        "text": "Multigrid methods canbe used to quickly seed a good initial approximation to a high resolution value functionby initially performing value iteration at a coarser resolution (R ude, 1993)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Mathematical and computational techniques for multilevel adaptive methods . S o c i e t y for Industrial and Applied Mathematics"
            },
            "venue": {
                "fragments": [],
                "text": "Mathematical and computational techniques for multilevel adaptive methods . S o c i e t y for Industrial and Applied Mathematics"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Mahadevan (1996) surveyed model-based average-reward algorithms froma reinforcement-learning perspective and found several di culties with existing algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Average reward reinforcement learning: Foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 136
                            }
                        ],
                        "text": "It is computationally more expensive to execute the general TD( ), though it oftenconverges considerably faster for large (Dayan, 1992; Dayan & Sejnowski, 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "TD() c o n verges with probability 1"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 172
                            }
                        ],
                        "text": "The juggling robot learned a world model from experience, which was generalized to unvisited states by a function approximation scheme known as locally weighted regression [25, 82]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 197
                            }
                        ],
                        "text": "The juggling robot learned a world model from experience, which was generalizedto unvisited states by a function approximation scheme known as locally weightedregression (Cleveland & Delvin, 1988; Moore & Atkeson, 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An investigation of memory-based function approximators for learning control"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. rep"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using the T D() algorithm to learn an evaluation function for the game of Go"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 6"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Atkeson . Prioritized sweeping : Reinforcement learningwith less data and less real time"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1993
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 111,
            "methodology": 85,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 229,
        "totalPages": 23
    },
    "page_url": "https://www.semanticscholar.org/paper/Reinforcement-Learning:-A-Survey-Kaelbling-Littman/12d1d070a53d4084d88a77b8b143bad51c40c38f?sort=total-citations"
}