{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3335364"
                        ],
                        "name": "Dzmitry Bahdanau",
                        "slug": "Dzmitry-Bahdanau",
                        "structuredName": {
                            "firstName": "Dzmitry",
                            "lastName": "Bahdanau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dzmitry Bahdanau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 72
                            }
                        ],
                        "text": "We evaluate our tree-to-tree model against a sequence-to-sequence model [4, 31], a sequence-to-tree model [11], and a tree-to-sequence model [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "Recently, neural network approaches, such as sequence-to-sequencebased models, have achieved the state-of-the-art performance on machine translation [4, 9, 13, 14, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11212020,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa72afa9b2cbc8f0d7b05d52548906610ffbb9c5",
            "isKey": false,
            "numCitedBy": 19562,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural machine translation is a recently proposed approach to machine translation. Unlike the traditional statistical machine translation, the neural machine translation aims at building a single neural network that can be jointly tuned to maximize the translation performance. The models proposed recently for neural machine translation often belong to a family of encoder-decoders and consists of an encoder that encodes a source sentence into a fixed-length vector from which a decoder generates a translation. In this paper, we conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and propose to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly. With this new approach, we achieve a translation performance comparable to the existing state-of-the-art phrase-based system on the task of English-to-French translation. Furthermore, qualitative analysis reveals that the (soft-)alignments found by the model agree well with our intuition."
            },
            "slug": "Neural-Machine-Translation-by-Jointly-Learning-to-Bahdanau-Cho",
            "title": {
                "fragments": [],
                "text": "Neural Machine Translation by Jointly Learning to Align and Translate"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is conjecture that the use of a fixed-length vector is a bottleneck in improving the performance of this basic encoder-decoder architecture, and it is proposed to extend this by allowing a model to automatically (soft-)search for parts of a source sentence that are relevant to predicting a target word, without having to form these parts as a hard segment explicitly."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50123248"
                        ],
                        "name": "Akiko Eriguchi",
                        "slug": "Akiko-Eriguchi",
                        "structuredName": {
                            "firstName": "Akiko",
                            "lastName": "Eriguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Akiko Eriguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20851195"
                        ],
                        "name": "Kazuma Hashimoto",
                        "slug": "Kazuma-Hashimoto",
                        "structuredName": {
                            "firstName": "Kazuma",
                            "lastName": "Hashimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kazuma Hashimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143946906"
                        ],
                        "name": "Yoshimasa Tsuruoka",
                        "slug": "Yoshimasa-Tsuruoka",
                        "structuredName": {
                            "firstName": "Yoshimasa",
                            "lastName": "Tsuruoka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshimasa Tsuruoka"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "We evaluate our tree-to-tree model against a sequence-to-sequence model [4, 31], a sequence-to-tree model [11], and a tree-to-sequence model [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "Recently, neural network approaches, such as sequence-to-sequencebased models, have achieved the state-of-the-art performance on machine translation [4, 9, 13, 14, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12851711,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db02cd07726371790a825208cec377ec15f5b5f1",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Most of the existing Neural Machine Translation (NMT) models focus on the conversion of sequential data and do not directly use syntactic information. We propose a novel end-to-end syntactic NMT model, extending a sequence-to-sequence model with the source-side phrase structure. Our model has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence. Experimental results on the WAT'15 English-to-Japanese dataset demonstrate that our proposed model considerably outperforms sequence-to-sequence attentional NMT models and compares favorably with the state-of-the-art tree-to-string SMT system."
            },
            "slug": "Tree-to-Sequence-Attentional-Neural-Machine-Eriguchi-Hashimoto",
            "title": {
                "fragments": [],
                "text": "Tree-to-Sequence Attentional Neural Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work proposes a novel end-to-end syntactic NMT model, extending a sequence- to-sequence model with the source-side phrase structure, which has an attention mechanism that enables the decoder to generate a translated word while softly aligning it with phrases as well as words of the source sentence."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118484320"
                        ],
                        "name": "Chang Liu",
                        "slug": "Chang-Liu",
                        "structuredName": {
                            "firstName": "Chang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1425082935"
                        ],
                        "name": "Xinyun Chen",
                        "slug": "Xinyun-Chen",
                        "structuredName": {
                            "firstName": "Xinyun",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyun Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2802006"
                        ],
                        "name": "E. C. Shin",
                        "slug": "E.-C.-Shin",
                        "structuredName": {
                            "firstName": "Eui",
                            "lastName": "Shin",
                            "middleNames": [
                                "Chul",
                                "Richard"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. C. Shin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389257"
                        ],
                        "name": "Mingcheng Chen",
                        "slug": "Mingcheng-Chen",
                        "structuredName": {
                            "firstName": "Mingcheng",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingcheng Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143711382"
                        ],
                        "name": "D. Song",
                        "slug": "D.-Song",
                        "structuredName": {
                            "firstName": "Dawn",
                            "lastName": "Song",
                            "middleNames": [
                                "Xiaodong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Song"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6201712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "165f45f42a1418526c2fd5eeb0ebf6c147f4f507",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Automatic translation from natural language descriptions into programs is a long-standing challenging problem. In this work, we consider a simple yet important sub-problem: translation from textual descriptions to If-Then programs. We devise a novel neural network architecture for this task which we train end-to-end. Specifically, we introduce Latent Attention, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements. Our architecture reduces the error rate by 28.57% compared to prior art. We also propose a one-shot learning scenario of If-Then program synthesis and simulate it with our existing dataset. We demonstrate a variation on the training procedure for this scenario that outperforms the original procedure, significantly closing the gap to the model trained with all data."
            },
            "slug": "Latent-Attention-For-If-Then-Program-Synthesis-Liu-Chen",
            "title": {
                "fragments": [],
                "text": "Latent Attention For If-Then Program Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Latent Attention is introduced, which computes multiplicative weights for the words in the description in a two-stage process with the goal of better leveraging the natural language structures that indicate the relevant parts for predicting program elements."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1425082935"
                        ],
                        "name": "Xinyun Chen",
                        "slug": "Xinyun-Chen",
                        "structuredName": {
                            "firstName": "Xinyun",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyun Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118484320"
                        ],
                        "name": "Chang Liu",
                        "slug": "Chang-Liu",
                        "structuredName": {
                            "firstName": "Chang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143711382"
                        ],
                        "name": "D. Song",
                        "slug": "D.-Song",
                        "structuredName": {
                            "firstName": "Dawn",
                            "lastName": "Song",
                            "middleNames": [
                                "Xiaodong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Song"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 90
                            }
                        ],
                        "text": "Other work study using neural networks to generate parse trees from input-output examples (Dong & Lapata, 2016; Vinyals et al., 2015; Aharoni & Goldberg, 2017; Rabinovich et al., 2017; Yin & Neubig, 2017; Alvarez-Melis & Jaakkola, 2017; Dyer et al., 2016; Chen et al., 2017; 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 125773649,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7460b3a91e7347cdce0c0f9edf3c3b6d3eac12a0",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, deep learning techniques have been developed to improve the performance of program synthesis from input-output examples. Albeit its significant progress, the programs that can be synthesized by state-of-the-art approaches are still simple in terms of their complexity. In this work, we move a significant step forward along this direction by proposing a new class of challenging tasks in the domain of program synthesis from input-output examples: learning a context-free parser from pairs of input programs and their parse trees. We show that this class of tasks are much more challenging than previously studied tasks, and the test accuracy of existing approaches is almost 0%. We tackle the challenges by developing three novel techniques inspired by three novel observations, which reveal the key ingredients of using deep learning to synthesize a complex program. First, the use of a non-differentiable machine is the key to effectively restrict the search space. Thus our proposed approach learns a neural program operating a domain-specific non-differentiable machine. Second, recursion is the key to achieve generalizability. Thus, we bake-in the notion of recursion in the design of our non-differentiable machine. Third, reinforcement learning is the key to learn how to operate the non-differentiable machine, but it is also hard to train the model effectively with existing reinforcement learning algorithms from a cold boot. We develop a novel two-phase reinforcement learning-based search algorithm to overcome this issue. In our evaluation, we show that using our novel approach, neural parsing programs can be learned to achieve 100% test accuracy on test inputs that are 500x longer than the training samples."
            },
            "slug": "Learning-Neural-Programs-To-Parse-Programs-Chen-Liu",
            "title": {
                "fragments": [],
                "text": "Learning Neural Programs To Parse Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work proposes a new class of challenging tasks in the domain of program synthesis from input-output examples: learning a context-free parser from pairs of input programs and their parse trees and develops a novel two-phase reinforcement learning-based search algorithm to overcome this issue."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144659797"
                        ],
                        "name": "A. Nguyen",
                        "slug": "A.-Nguyen",
                        "structuredName": {
                            "firstName": "Anh",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Tuan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40538640"
                        ],
                        "name": "T. Nguyen",
                        "slug": "T.-Nguyen",
                        "structuredName": {
                            "firstName": "Tung",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Thanh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2327697"
                        ],
                        "name": "T. Nguyen",
                        "slug": "T.-Nguyen",
                        "structuredName": {
                            "firstName": "Tien",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Nhut"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nguyen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 96
                            }
                        ],
                        "text": "Some recent work have applied statistical machine translation techniques to program translation [2, 16, 22, 21, 23, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 149
                            }
                        ],
                        "text": "To test our hypothesis, we develop two novel program translation tasks, and employ a Java to C# benchmark used by existing program translation works [22, 21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 49
                            }
                        ],
                        "text": "Note that all existing program translation works [16, 22, 21] also study the problem under such an assumption."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": ", J2C# [15], 1pSMT [21], and mppSMT [22], on the real-world benchmark from Java to C#."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 106
                            }
                        ],
                        "text": "Some previous work propose to adapt phrase-based statistical machine translation (SMT) for code migration [21, 16, 22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 176
                            }
                        ],
                        "text": "For example, several works propose to adapt phrase-based statistical machine translation models and leverage grammatical structures of programming languages for code migration [16, 22, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15149849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5447a3b8701d59f3a2f1a7f7af030f687ba495c3",
            "isKey": true,
            "numCitedBy": 86,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Prior research has shown that source code also exhibits naturalness, i.e. it is written by humans and is likely to be repetitive. The researchers also showed that the n-gram language model is useful in predicting the next token in a source file given a large corpus of existing source code. In this paper, we investigate how well statistical machine translation (SMT) models for natural languages could help in migrating source code from one programming language to another. We treat source code as a sequence of lexical tokens and apply a phrase-based SMT model on the lexemes of those tokens. Our empirical evaluation on migrating two Java projects into C# showed that lexical, phrase-based SMT could achieve high lexical translation accuracy (BLEU from 81.3-82.6%). Users would have to manually edit only 11.9-15.8% of the total number of tokens in the resulting code to correct it. However, a high percentage of total translation methods (49.5-58.6%) is syntactically incorrect. Therefore, our result calls for a more program-oriented SMT model that is capable of better integrating the syntactic and semantic information of a program to support language migration."
            },
            "slug": "Lexical-statistical-machine-translation-for-Nguyen-Nguyen",
            "title": {
                "fragments": [],
                "text": "Lexical statistical machine translation for language migration"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper treats source code as a sequence of lexical tokens and applies a phrase-based SMT model on the lexemes of those tokens, which shows a high percentage of total translation methods is syntactically incorrect."
            },
            "venue": {
                "fragments": [],
                "text": "ESEC/FSE 2013"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144659797"
                        ],
                        "name": "A. Nguyen",
                        "slug": "A.-Nguyen",
                        "structuredName": {
                            "firstName": "Anh",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Tuan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40538640"
                        ],
                        "name": "T. Nguyen",
                        "slug": "T.-Nguyen",
                        "structuredName": {
                            "firstName": "Tung",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Thanh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2327697"
                        ],
                        "name": "T. Nguyen",
                        "slug": "T.-Nguyen",
                        "structuredName": {
                            "firstName": "Tien",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Nhut"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nguyen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "Note that the program accuracy is an underestimation of the true accuracy based on semantic equivalence, and this metric has been used in [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 149
                            }
                        ],
                        "text": "To test our hypothesis, we develop two novel program translation tasks, and employ a Java to C# benchmark used by existing program translation works [22, 21]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Same as in [22], we pair the methods in Java and C# based on their file names and method names."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 49
                            }
                        ],
                        "text": "Note that all existing program translation works [16, 22, 21] also study the problem under such an assumption."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "For the evaluation on Java to C#, we tried to contact the authors of [22] for their dataset, but our emails were not responded."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "For each project, we apply ten-fold validation on matched method pairs, as in [22]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Tree2tree J2C# 1pSMT mppSMT Reported in [22] Lucene 72."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 106
                            }
                        ],
                        "text": "Some previous work propose to adapt phrase-based statistical machine translation (SMT) for code migration [21, 16, 22]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Thus, we employ the same approach as in [22] to crawl several open-source projects, which have both a Java and a C# implementation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 108
                            }
                        ],
                        "text": "Due to the change of the versions of these projects, the concrete dataset in our evaluation may differ from [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13272216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "927aaa20318d18564bd331ed37428e05820d647e",
            "isKey": true,
            "numCitedBy": 45,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Prior research shows that directly applying phrase-based SMT on lexical tokens to migrate Java to C# produces much semantically incorrect code. A key limitation is the use of sequences in phrase-based SMT to model and translate source code with well-formed structures. We propose mppSMT, a divide-and-conquer technique to address that with novel training and migration algorithms using phrase-based SMT in three phases. First, mppSMT treats a program as a sequence of syntactic units and maps/translates such sequences in two languages to one another. Second, with a syntax-directed fashion, it deals with the tokens within syntactic units by encoding them with semantic symbols to represent their data and token types. This encoding via semantic symbols helps better migration of API usages. Third, the lexical tokens corresponding to each sememe are mapped or migrated. The resulting sequences of tokens are merged together to form the final migrated code. Such divide-and-conquer and syntax-direction strategies enable phrase-based SMT to adapt well to syntactical structures in source code, thus, improving migration accuracy. Our empirical evaluation on several real-world systems shows that 84.8 -- 97.9% and 70 -- 83% of the migrated methods are syntactically and semantically correct, respectively. 26.3 -- 51.2% of total migrated methods are exactly matched to the human-written C# code in the oracle. Compared to Java2CSharp, a rule-based migration tool, it achieves higher semantic accuracy from 6.6 -- 57.7% relatively. Importantly, it does not require manual labeling for training data or manual definition of rules."
            },
            "slug": "Divide-and-Conquer-Approach-for-Multi-phase-for-(T)-Nguyen-Nguyen",
            "title": {
                "fragments": [],
                "text": "Divide-and-Conquer Approach for Multi-phase Statistical Migration for Source Code (T)"
            },
            "venue": {
                "fragments": [],
                "text": "2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2335771"
                        ],
                        "name": "Roee Aharoni",
                        "slug": "Roee-Aharoni",
                        "structuredName": {
                            "firstName": "Roee",
                            "lastName": "Aharoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roee Aharoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089067"
                        ],
                        "name": "Yoav Goldberg",
                        "slug": "Yoav-Goldberg",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Goldberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 90
                            }
                        ],
                        "text": "Other work study using neural networks to generate parse trees from input-output examples [11, 31, 1, 26, 32, 3, 12, 8, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8078153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc8eb74acd25169672b3b85517da3d66eb0c47cf",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees. An experiment on the WMT16 German-English news translation task resulted in an improved BLEU score when compared to a syntax-agnostic NMT baseline trained on the same dataset. An analysis of the translations from the syntax-aware system shows that it performs more reordering during translation in comparison to the baseline. A small-scale human evaluation also showed an advantage to the syntax-aware system."
            },
            "slug": "Towards-String-To-Tree-Neural-Machine-Translation-Aharoni-Goldberg",
            "title": {
                "fragments": [],
                "text": "Towards String-To-Tree Neural Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A simple method to incorporate syntactic information about the target language in a neural machine translation system by translating into linearized, lexicalized constituency trees and shows that the syntax-aware system performs more reordering during translation in comparison to the baseline."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2608160"
                        ],
                        "name": "Svetoslav Karaivanov",
                        "slug": "Svetoslav-Karaivanov",
                        "structuredName": {
                            "firstName": "Svetoslav",
                            "lastName": "Karaivanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Svetoslav Karaivanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1967472"
                        ],
                        "name": "Veselin Raychev",
                        "slug": "Veselin-Raychev",
                        "structuredName": {
                            "firstName": "Veselin",
                            "lastName": "Raychev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Veselin Raychev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736447"
                        ],
                        "name": "Martin T. Vechev",
                        "slug": "Martin-T.-Vechev",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Vechev",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin T. Vechev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 49
                            }
                        ],
                        "text": "Note that all existing program translation works [16, 22, 21] also study the problem under such an assumption."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 176
                            }
                        ],
                        "text": "For example, several works propose to adapt phrase-based statistical machine translation models and leverage grammatical structures of programming languages for code migration [16, 22, 21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 96
                            }
                        ],
                        "text": "Some recent work have applied statistical machine translation techniques to program translation [2, 16, 22, 21, 23, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 106
                            }
                        ],
                        "text": "Some previous work propose to adapt phrase-based statistical machine translation (SMT) for code migration [21, 16, 22]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8810124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4755b856dc08ac024ae935e7a6f9df325b00ae53",
            "isKey": true,
            "numCitedBy": 88,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Phrase-based statistical machine translation approaches have been highly successful in translating between natural languages and are heavily used by commercial systems (e.g. Google Translate). The main objective of this work is to investigate the applicability of these approaches for translating between programming languages. Towards that, we investigated several variants of the phrase-based translation approach: i) a direct application of the approach to programming languages, ii) a novel modification of the approach to incorporate the grammatical structure of the target programming language (so to avoid generating target programs which do not parse), and iii) a combination of ii) with custom rules added to improve the quality of the translation. To experiment with the above systems, we investigated machine translation from C# to Java. For the training, which takes about 60 hours, we used a parallel corpus of 20,499 C#-to-Java method translations. We then evaluated each of the three systems above by translating 1,000 C# methods. Our experimental results indicate that with the most advanced system, about 60% of the translated methods compile (the top ranked) and out of a random sample of 50 correctly compiled methods, 68% (34 methods) were semantically equivalent to the reference solution."
            },
            "slug": "Phrase-Based-Statistical-Translation-of-Programming-Karaivanov-Raychev",
            "title": {
                "fragments": [],
                "text": "Phrase-Based Statistical Translation of Programming Languages"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This work investigated phrase-based statistical machine translation approaches from C# to Java, and found that with the most advanced system, about 60% of the translated methods compile and out of a random sample of 50 correctly compiled methods, 68% were semantically equivalent to the reference solution."
            },
            "venue": {
                "fragments": [],
                "text": "Onward!"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1425082935"
                        ],
                        "name": "Xinyun Chen",
                        "slug": "Xinyun-Chen",
                        "structuredName": {
                            "firstName": "Xinyun",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyun Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118484320"
                        ],
                        "name": "Chang Liu",
                        "slug": "Chang-Liu",
                        "structuredName": {
                            "firstName": "Chang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143711382"
                        ],
                        "name": "D. Song",
                        "slug": "D.-Song",
                        "structuredName": {
                            "firstName": "Dawn",
                            "lastName": "Song",
                            "middleNames": [
                                "Xiaodong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Song"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 90
                            }
                        ],
                        "text": "Other work study using neural networks to generate parse trees from input-output examples [11, 31, 1, 26, 32, 3, 12, 8, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 96267,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42dea2a24629bd4b1b56619536e263b078becddd",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, deep learning techniques have been developed to improve the performance of program synthesis from input-output examples. Albeit its significant progress, the programs that can be synthesized by state-of-the-art approaches are still simple in terms of their complexity. In this work, we move a significant step forward along this direction by proposing a new class of challenging tasks in the domain of program synthesis from input-output examples: learning a context-free parser from pairs of input programs and their parse trees. We show that this class of tasks are much more challenging than previously studied tasks, and the test accuracy of existing approaches is almost 0%. \nWe tackle the challenges by developing three novel techniques inspired by three novel observations, which reveal the key ingredients of using deep learning to synthesize a complex program. First, the use of a non-differentiable machine is the key to effectively restrict the search space. Thus our proposed approach learns a neural program operating a domain-specific non-differentiable machine. Second, recursion is the key to achieve generalizability. Thus, we bake-in the notion of recursion in the design of our non-differentiable machine. Third, reinforcement learning is the key to learn how to operate the non-differentiable machine, but it is also hard to train the model effectively with existing reinforcement learning algorithms from a cold boot. We develop a novel two-phase reinforcement learning-based search algorithm to overcome this issue. In our evaluation, we show that using our novel approach, neural parsing programs can be learned to achieve 100% test accuracy on test inputs that are 500x longer than the training samples."
            },
            "slug": "Towards-Synthesizing-Complex-Programs-From-Examples-Chen-Liu",
            "title": {
                "fragments": [],
                "text": "Towards Synthesizing Complex Programs From Input-Output Examples"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "This work proposes a new class of challenging tasks in the domain of program synthesis from input-output examples: learning a context-free parser from pairs of input programs and their parse trees and develops a novel two-phase reinforcement learning-based search algorithm to overcome this issue."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821711"
                        ],
                        "name": "Thang Luong",
                        "slug": "Thang-Luong",
                        "structuredName": {
                            "firstName": "Thang",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thang Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143950636"
                        ],
                        "name": "Hieu Pham",
                        "slug": "Hieu-Pham",
                        "structuredName": {
                            "firstName": "Hieu",
                            "lastName": "Pham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hieu Pham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "Notice that these formulas in their formats coincide with the input-feeding method for sequential neural networks [20], but their meanings are different."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1998416,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93499a7c7f699b6630a86fad964536f9423bb6d0",
            "isKey": false,
            "numCitedBy": 5954,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "An attentional mechanism has lately been used to improve neural machine translation (NMT) by selectively focusing on parts of the source sentence during translation. However, there has been little work exploring useful architectures for attention-based NMT. This paper examines two simple and effective classes of attentional mechanism: a global approach which always attends to all source words and a local one that only looks at a subset of source words at a time. We demonstrate the effectiveness of both approaches on the WMT translation tasks between English and German in both directions. With local attention, we achieve a significant gain of 5.0 BLEU points over non-attentional systems that already incorporate known techniques such as dropout. Our ensemble model using different attention architectures yields a new state-of-the-art result in the WMT\u201915 English to German translation task with 25.9 BLEU points, an improvement of 1.0 BLEU points over the existing best system backed by NMT and an n-gram reranker. 1"
            },
            "slug": "Effective-Approaches-to-Attention-based-Neural-Luong-Pham",
            "title": {
                "fragments": [],
                "text": "Effective Approaches to Attention-based Neural Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A global approach which always attends to all source words and a local one that only looks at a subset of source words at a time are examined, demonstrating the effectiveness of both approaches on the WMT translation tasks between English and German in both directions."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40348417"
                        ],
                        "name": "Ashish Vaswani",
                        "slug": "Ashish-Vaswani",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Vaswani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Vaswani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3877127"
                        ],
                        "name": "Niki Parmar",
                        "slug": "Niki-Parmar",
                        "structuredName": {
                            "firstName": "Niki",
                            "lastName": "Parmar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niki Parmar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39328010"
                        ],
                        "name": "Jakob Uszkoreit",
                        "slug": "Jakob-Uszkoreit",
                        "structuredName": {
                            "firstName": "Jakob",
                            "lastName": "Uszkoreit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jakob Uszkoreit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145024664"
                        ],
                        "name": "Llion Jones",
                        "slug": "Llion-Jones",
                        "structuredName": {
                            "firstName": "Llion",
                            "lastName": "Jones",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Llion Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "19177000"
                        ],
                        "name": "Aidan N. Gomez",
                        "slug": "Aidan-N.-Gomez",
                        "structuredName": {
                            "firstName": "Aidan",
                            "lastName": "Gomez",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aidan N. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3443442"
                        ],
                        "name": "Illia Polosukhin",
                        "slug": "Illia-Polosukhin",
                        "structuredName": {
                            "firstName": "Illia",
                            "lastName": "Polosukhin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Illia Polosukhin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "Recently, neural network approaches, such as sequence-to-sequencebased models, have achieved the state-of-the-art performance on machine translation [4, 9, 13, 14, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13756489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204e3073870fae3d05bcbc2f6a8e263d9b72e776",
            "isKey": false,
            "numCitedBy": 36491,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "The dominant sequence transduction models are based on complex recurrent or convolutional neural networks in an encoder-decoder configuration. The best performing models also connect the encoder and decoder through an attention mechanism. We propose a new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely. Experiments on two machine translation tasks show these models to be superior in quality while being more parallelizable and requiring significantly less time to train. Our model achieves 28.4 BLEU on the WMT 2014 English-to-German translation task, improving over the existing best results, including ensembles by over 2 BLEU. On the WMT 2014 English-to-French translation task, our model establishes a new single-model state-of-the-art BLEU score of 41.8 after training for 3.5 days on eight GPUs, a small fraction of the training costs of the best models from the literature. We show that the Transformer generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "slug": "Attention-is-All-you-Need-Vaswani-Shazeer",
            "title": {
                "fragments": [],
                "text": "Attention is All you Need"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new simple network architecture, the Transformer, based solely on attention mechanisms, dispensing with recurrence and convolutions entirely is proposed, which generalizes well to other tasks by applying it successfully to English constituency parsing both with large and limited training data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1854999"
                        ],
                        "name": "Xiao-Dan Zhu",
                        "slug": "Xiao-Dan-Zhu",
                        "structuredName": {
                            "firstName": "Xiao-Dan",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao-Dan Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690185"
                        ],
                        "name": "Parinaz Sobhani",
                        "slug": "Parinaz-Sobhani",
                        "structuredName": {
                            "firstName": "Parinaz",
                            "lastName": "Sobhani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Parinaz Sobhani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694050"
                        ],
                        "name": "Hongyu Guo",
                        "slug": "Hongyu-Guo",
                        "structuredName": {
                            "firstName": "Hongyu",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongyu Guo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17748586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f600f213dbbd70f06093438855f39022957b4bf",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The chain-structured long short-term memory (LSTM) has showed to be effective in a wide range of problems such as speech recognition and machine translation. In this paper, we propose to extend it to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process. We call the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies, e.g., language or image parse structures. We leverage the models for semantic composition to understand the meaning of text, a fundamental problem in natural language understanding, and show that it outperforms a state-of-the-art recursive model by replacing its composition layers with the S-LSTM memory blocks. We also show that utilizing the given structures is helpful in achieving a performance better than that without considering the structures."
            },
            "slug": "Long-Short-Term-Memory-Over-Recursive-Structures-Zhu-Sobhani",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory Over Recursive Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This paper proposes to extend chain-structured long short-term memory to tree structures, in which a memory cell can reflect the history memories of multiple child cells or multiple descendant cells in a recursive process, and calls the model S-LSTM, which provides a principled way of considering long-distance interaction over hierarchies."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1956459"
                        ],
                        "name": "Matej Balog",
                        "slug": "Matej-Balog",
                        "structuredName": {
                            "firstName": "Matej",
                            "lastName": "Balog",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matej Balog"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35058304"
                        ],
                        "name": "Alexander L. Gaunt",
                        "slug": "Alexander-L.-Gaunt",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Gaunt",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander L. Gaunt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107692"
                        ],
                        "name": "M. Brockschmidt",
                        "slug": "M.-Brockschmidt",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Brockschmidt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brockschmidt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388416"
                        ],
                        "name": "S. Nowozin",
                        "slug": "S.-Nowozin",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Nowozin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowozin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725299"
                        ],
                        "name": "Daniel Tarlow",
                        "slug": "Daniel-Tarlow",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Tarlow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Tarlow"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 74
                            }
                        ],
                        "text": "A recent line of research study using neural networks for code generation [5, 10, 25, 19, 26, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2906360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a25c9403d8a0e2fb8ca362a1b26262afd57417f",
            "isKey": false,
            "numCitedBy": 394,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a first line of attack for solving programming competition-style problems from input-output examples using deep learning. The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs. We use the neural network\u2019s predictions to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver. Empirically, we show that our approach leads to an order of magnitude speedup over the strong non-augmented baselines and a Recurrent Neural Network approach, and that we are able to solve problems of difficulty comparable to the simplest problems on programming competition websites."
            },
            "slug": "DeepCoder:-Learning-to-Write-Programs-Balog-Gaunt",
            "title": {
                "fragments": [],
                "text": "DeepCoder: Learning to Write Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The approach is to train a neural network to predict properties of the program that generated the outputs from the inputs to augment search techniques from the programming languages community, including enumerative search and an SMT-based solver."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40518045"
                        ],
                        "name": "James Bradbury",
                        "slug": "James-Bradbury",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bradbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Bradbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Another work [6] proposes to use a tree-structured encoder-decoder architecture for natural language"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Another work [6] proposes a tree-based attentional encoder-decoder architecture for natural language translation, but their model performs even worse than the attentional sequence-to-sequence baseline model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9683221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc1407cfaadfa2d3a2ac5d174ac4aa4f3b7edf84",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Building models that take advantage of the hierarchical structure of language without a priori annotation is a longstanding goal in natural language processing. We introduce such a model for the task of machine translation, pairing a recurrent neural network grammar encoder with a novel attentional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target. When trained on character-level datasets with no explicit segmentation or parse annotation, the model learns a plausible segmentation and shallow parse, obtaining performance close to an attentional baseline."
            },
            "slug": "Towards-Neural-Machine-Translation-with-Latent-Tree-Bradbury-Socher",
            "title": {
                "fragments": [],
                "text": "Towards Neural Machine Translation with Latent Tree Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work introduces a model for the task of machine translation, pairing a recurrent neural network grammar encoder with a novel attentional RNNG decoder and applying policy gradient reinforcement learning to induce unsupervised tree structures on both the source and target."
            },
            "venue": {
                "fragments": [],
                "text": "SPNLP@EMNLP"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37409910"
                        ],
                        "name": "Xingxing Zhang",
                        "slug": "Xingxing-Zhang",
                        "structuredName": {
                            "firstName": "Xingxing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xingxing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145705748"
                        ],
                        "name": "Liang Lu",
                        "slug": "Liang-Lu",
                        "structuredName": {
                            "firstName": "Liang",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 214079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5f0762b9cfd07f88608f5502ed4467a8b5546cb",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 73,
            "paperAbstract": {
                "fragments": [],
                "text": "Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have been successfully applied to a variety of sequence modeling tasks. In this paper we develop Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence. TreeLSTM defines the probability of a sentence by estimating the generation probability of its dependency tree. At each time step, a node is generated based on the representation of the generated sub-tree. We further enhance the modeling power of TreeLSTM by explicitly representing the correlations between left and right dependents. Application of our model to the MSR sentence completion challenge achieves results beyond the current state of the art. We also report results on dependency parsing reranking achieving competitive performance."
            },
            "slug": "Top-down-Tree-Long-Short-Term-Memory-Networks-Zhang-Lu",
            "title": {
                "fragments": [],
                "text": "Top-down Tree Long Short-Term Memory Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper develops Tree Long Short-Term Memory (TreeLSTM), a neural network model based on LSTM, which is designed to predict a tree rather than a linear sequence, and reports results on dependency parsing reranking achieving competitive performance."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152857609"
                        ],
                        "name": "S\u00e9bastien Jean",
                        "slug": "S\u00e9bastien-Jean",
                        "structuredName": {
                            "firstName": "S\u00e9bastien",
                            "lastName": "Jean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S\u00e9bastien Jean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710604"
                        ],
                        "name": "R. Memisevic",
                        "slug": "R.-Memisevic",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Memisevic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Memisevic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "Recently, neural network approaches, such as sequence-to-sequencebased models, have achieved the state-of-the-art performance on machine translation [4, 9, 13, 14, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2863491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1938624bb9b0f999536dcc8d8f519810bb4e1b3b",
            "isKey": false,
            "numCitedBy": 864,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural machine translation, a recently proposed approach to machine translation based purely on neural networks, has shown promising results compared to the existing approaches such as phrase-based statistical machine translation. Despite its recent success, neural machine translation has its limitation in handling a larger vocabulary, as training complexity as well as decoding complexity increase proportionally to the number of target words. In this paper, we propose a method based on importance sampling that allows us to use a very large target vocabulary without increasing training complexity. We show that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary. The models trained by the proposed approach are empirically found to outperform the baseline models with a small vocabulary as well as the LSTM-based neural machine translation models. Furthermore, when we use the ensemble of a few models with very large target vocabularies, we achieve the state-of-the-art translation performance (measured by BLEU) on the English!German translation and almost as high performance as state-of-the-art English!French translation system."
            },
            "slug": "On-Using-Very-Large-Target-Vocabulary-for-Neural-Jean-Cho",
            "title": {
                "fragments": [],
                "text": "On Using Very Large Target Vocabulary for Neural Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that decoding can be efficiently done even with the model having a very large target vocabulary by selecting only a small subset of the whole target vocabulary."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9960452"
                        ],
                        "name": "Jonathan Uesato",
                        "slug": "Jonathan-Uesato",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Uesato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Uesato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9692128"
                        ],
                        "name": "Surya Bhupatiraju",
                        "slug": "Surya-Bhupatiraju",
                        "structuredName": {
                            "firstName": "Surya",
                            "lastName": "Bhupatiraju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Surya Bhupatiraju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50631599"
                        ],
                        "name": "Rishabh Singh",
                        "slug": "Rishabh-Singh",
                        "structuredName": {
                            "firstName": "Rishabh",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rishabh Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 74
                            }
                        ],
                        "text": "A recent line of research study using neural networks for code generation [5, 10, 25, 19, 26, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6933074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ff0af64279929a952ee340e645256b7e0580f65",
            "isKey": false,
            "numCitedBy": 281,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of automatically generating a computer program from some specification has been studied since the early days of AI. Recently, two competing approaches for automatic program learning have received significant attention: (1) neural program synthesis, where a neural network is conditioned on input/output (I/O) examples and learns to generate a program, and (2) neural program induction, where a neural network generates new outputs directly using a latent program representation. Here, for the first time, we directly compare both approaches on a large-scale, real-world learning task and we additionally contrast to rule-based program synthesis, which uses hand-crafted semantics to guide the program generation. Our neural models use a modified attention RNN to allow encoding of variable-sized sets of I/O pairs, which achieve 92% accuracy on a real-world test set, compared to the 34% accuracy of the previous best neural synthesis approach. The synthesis model also outperforms a comparable induction model on this task, but we more importantly demonstrate that the strength of each approach is highly dependent on the evaluation metric and end-user application. Finally, we show that we can train our neural models to remain very robust to the type of noise expected in real-world data (e.g., typos), while a highly-engineered rule-based system fails entirely."
            },
            "slug": "RobustFill:-Neural-Program-Learning-under-Noisy-I/O-Devlin-Uesato",
            "title": {
                "fragments": [],
                "text": "RobustFill: Neural Program Learning under Noisy I/O"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work directly compares both approaches for automatic program learning on a large-scale, real-world learning task and demonstrates that the strength of each approach is highly dependent on the evaluation metric and end-user application."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391126980"
                        ],
                        "name": "Di He",
                        "slug": "Di-He",
                        "structuredName": {
                            "firstName": "Di",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Di He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2794096"
                        ],
                        "name": "Yingce Xia",
                        "slug": "Yingce-Xia",
                        "structuredName": {
                            "firstName": "Yingce",
                            "lastName": "Xia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yingce Xia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143826491"
                        ],
                        "name": "Tao Qin",
                        "slug": "Tao-Qin",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Qin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Qin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24952249"
                        ],
                        "name": "Liwei Wang",
                        "slug": "Liwei-Wang",
                        "structuredName": {
                            "firstName": "Liwei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liwei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708598"
                        ],
                        "name": "Nenghai Yu",
                        "slug": "Nenghai-Yu",
                        "structuredName": {
                            "firstName": "Nenghai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nenghai Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110264337"
                        ],
                        "name": "Tie-Yan Liu",
                        "slug": "Tie-Yan-Liu",
                        "structuredName": {
                            "firstName": "Tie-Yan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tie-Yan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712167"
                        ],
                        "name": "Wei-Ying Ma",
                        "slug": "Wei-Ying-Ma",
                        "structuredName": {
                            "firstName": "Wei-Ying",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Ying Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "Several techniques for NMT have been proposed to handle this issue, such as dual learning [14], which have the potential to be extended for the program translation task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 149
                            }
                        ],
                        "text": "Recently, neural network approaches, such as sequence-to-sequencebased models, have achieved the state-of-the-art performance on machine translation [4, 9, 13, 14, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5758868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23b559b5ab27f2fca6f56c0a7b6478bcf69db509",
            "isKey": false,
            "numCitedBy": 651,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "While neural machine translation (NMT) is making good progress in the past two years, tens of millions of bilingual sentence pairs are needed for its training. However, human labeling is very costly. To tackle this training data bottleneck, we develop a dual-learning mechanism, which can enable an NMT system to automatically learn from unlabeled data through a dual-learning game. This mechanism is inspired by the following observation: any machine translation task has a dual task, e.g., English-to-French translation (primal) versus French-to-English translation (dual); the primal and dual tasks can form a closed loop, and generate informative feedback signals to train the translation models, even if without the involvement of a human labeler. In the dual-learning mechanism, we use one agent to represent the model for the primal task and the other agent to represent the model for the dual task, then ask them to teach each other through a reinforcement learning process. Based on the feedback signals generated during this process (e.g., the language-model likelihood of the output of a model, and the reconstruction error of the original sentence after the primal and dual translations), we can iteratively update the two models until convergence (e.g., using the policy gradient methods). We call the corresponding approach to neural machine translation dual-NMT. Experiments show that dual-NMT works very well on English \u2194 French translation; especially, by learning from monolingual data (with 10% bilingual data for warm start), it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task."
            },
            "slug": "Dual-Learning-for-Machine-Translation-He-Xia",
            "title": {
                "fragments": [],
                "text": "Dual Learning for Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experiments show that dual-NMT works very well on English \u2194 French translation; especially, by learning from monolingual data, it achieves a comparable accuracy to NMT trained from the full bilingual data for the French-to-English translation task."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060101052"
                        ],
                        "name": "Terry Koo",
                        "slug": "Terry-Koo",
                        "structuredName": {
                            "firstName": "Terry",
                            "lastName": "Koo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Terry Koo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754497"
                        ],
                        "name": "Slav Petrov",
                        "slug": "Slav-Petrov",
                        "structuredName": {
                            "firstName": "Slav",
                            "lastName": "Petrov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Slav Petrov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 72
                            }
                        ],
                        "text": "We evaluate our tree-to-tree model against a sequence-to-sequence model [4, 31], a sequence-to-tree model [11], and a tree-to-sequence model [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 90
                            }
                        ],
                        "text": "Our serialization of a tree follows its depth-first traversal order, which is the same as [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 90
                            }
                        ],
                        "text": "Other work study using neural networks to generate parse trees from input-output examples [11, 31, 1, 26, 32, 3, 12, 8, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47570e7f63e296f224a0e7f9a0d08b0de3cbaf40",
            "isKey": false,
            "numCitedBy": 846,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Syntactic constituency parsing is a fundamental problem in natural language processing and has been the subject of intensive research and engineering for decades. As a result, the most accurate parsers are domain specific, complex, and inefficient. In this paper we show that the domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers. It also matches the performance of standard parsers when trained only on a small human-annotated dataset, which shows that this model is highly data-efficient, in contrast to sequence-to-sequence models without the attention mechanism. Our parser is also fast, processing over a hundred sentences per second with an unoptimized CPU implementation."
            },
            "slug": "Grammar-as-a-Foreign-Language-Vinyals-Kaiser",
            "title": {
                "fragments": [],
                "text": "Grammar as a Foreign Language"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The domain agnostic attention-enhanced sequence-to-sequence model achieves state-of-the-art results on the most widely used syntactic constituency parsing dataset, when trained on a large synthetic corpus that was annotated using existing parsers."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143800179"
                        ],
                        "name": "Yusuke Oda",
                        "slug": "Yusuke-Oda",
                        "structuredName": {
                            "firstName": "Yusuke",
                            "lastName": "Oda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yusuke Oda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2281695"
                        ],
                        "name": "Hiroyuki Fudaba",
                        "slug": "Hiroyuki-Fudaba",
                        "structuredName": {
                            "firstName": "Hiroyuki",
                            "lastName": "Fudaba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroyuki Fudaba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700325"
                        ],
                        "name": "Graham Neubig",
                        "slug": "Graham-Neubig",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Neubig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graham Neubig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145050815"
                        ],
                        "name": "Hideaki Hata",
                        "slug": "Hideaki-Hata",
                        "structuredName": {
                            "firstName": "Hideaki",
                            "lastName": "Hata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hideaki Hata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783949"
                        ],
                        "name": "S. Sakti",
                        "slug": "S.-Sakti",
                        "structuredName": {
                            "firstName": "Sakriani",
                            "lastName": "Sakti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sakti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726559"
                        ],
                        "name": "T. Toda",
                        "slug": "T.-Toda",
                        "structuredName": {
                            "firstName": "Tomoki",
                            "lastName": "Toda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Toda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145223960"
                        ],
                        "name": "Satoshi Nakamura",
                        "slug": "Satoshi-Nakamura",
                        "structuredName": {
                            "firstName": "Satoshi",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satoshi Nakamura"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 96
                            }
                        ],
                        "text": "Some recent work have applied statistical machine translation techniques to program translation [2, 16, 22, 21, 23, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15979705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27e1dbe9f7c71cd6cc1b0357f49aef497e572d09",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Pseudo-code written in natural language can aid the comprehension of source code in unfamiliar programming languages. However, the great majority of source code has no corresponding pseudo-code, because pseudo-code is redundant and laborious to create. If pseudo-code could be generated automatically and instantly from given source code, we could allow for on-demand production of pseudo-code without human effort. In this paper, we propose a method to automatically generate pseudo-code from source code, specifically adopting the statistical machine translation (SMT) framework. SMT, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort. In experiments, we generated English or Japanese pseudo-code from Python statements using SMT, and find that the generated pseudo-code is largely accurate, and aids code understanding."
            },
            "slug": "Learning-to-Generate-Pseudo-Code-from-Source-Code-Oda-Fudaba",
            "title": {
                "fragments": [],
                "text": "Learning to Generate Pseudo-Code from Source Code Using Statistical Machine Translation (T)"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "SMT, which was originally designed to translate between two natural languages, allows us to automatically learn the relationship between source code/pseudo-code pairs, making it possible to create a pseudo-code generator with less human effort."
            },
            "venue": {
                "fragments": [],
                "text": "2015 30th IEEE/ACM International Conference on Automated Software Engineering (ASE)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40354648"
                        ],
                        "name": "Maxim Rabinovich",
                        "slug": "Maxim-Rabinovich",
                        "structuredName": {
                            "firstName": "Maxim",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maxim Rabinovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144872294"
                        ],
                        "name": "Mitchell Stern",
                        "slug": "Mitchell-Stern",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Stern",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mitchell Stern"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 3
                            }
                        ],
                        "text": "In [19, 26, 32], they study generating code in a DSL from inputs in natural language or in another DSL."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 74
                            }
                        ],
                        "text": "A recent line of research study using neural networks for code generation [5, 10, 25, 19, 26, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 90
                            }
                        ],
                        "text": "Other work study using neural networks to generate parse trees from input-output examples [11, 31, 1, 26, 32, 3, 12, 8, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "Some other work incorporate the knowledge of the grammar into the architecture design [32, 26] to achieve better performance on specific tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13529592,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c1e874c3b67510a3215e535f5646b362de5bc89",
            "isKey": true,
            "numCitedBy": 259,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Tasks like code generation and semantic parsing require mapping unstructured (or partially structured) inputs to well-formed, executable outputs. We introduce abstract syntax networks, a modeling framework for these problems. The outputs are represented as abstract syntax trees (ASTs) and constructed by a decoder with a dynamically-determined modular structure paralleling the structure of the output tree. On the benchmark Hearthstone dataset for code generation, our model obtains 79.2 BLEU and 22.7% exact match accuracy, compared to previous state-of-the-art values of 67.1 and 6.1%. Furthermore, we perform competitively on the Atis, Jobs, and Geo semantic parsing datasets with no task-specific engineering."
            },
            "slug": "Abstract-Syntax-Networks-for-Code-Generation-and-Rabinovich-Stern",
            "title": {
                "fragments": [],
                "text": "Abstract Syntax Networks for Code Generation and Semantic Parsing"
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1379953252"
                        ],
                        "name": "Wang Ling",
                        "slug": "Wang-Ling",
                        "structuredName": {
                            "firstName": "Wang",
                            "lastName": "Ling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wang Ling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685771"
                        ],
                        "name": "P. Blunsom",
                        "slug": "P.-Blunsom",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Blunsom",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Blunsom"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1864353"
                        ],
                        "name": "Edward Grefenstette",
                        "slug": "Edward-Grefenstette",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Grefenstette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Grefenstette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2910877"
                        ],
                        "name": "K. Hermann",
                        "slug": "K.-Hermann",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Hermann",
                            "middleNames": [
                                "Moritz"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hermann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367821"
                        ],
                        "name": "Tom\u00e1s Kocisk\u00fd",
                        "slug": "Tom\u00e1s-Kocisk\u00fd",
                        "structuredName": {
                            "firstName": "Tom\u00e1s",
                            "lastName": "Kocisk\u00fd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom\u00e1s Kocisk\u00fd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115096249"
                        ],
                        "name": "Fumin Wang",
                        "slug": "Fumin-Wang",
                        "structuredName": {
                            "firstName": "Fumin",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fumin Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 74
                            }
                        ],
                        "text": "A recent line of research study using neural networks for code generation [5, 10, 25, 19, 26, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 3
                            }
                        ],
                        "text": "In [19, 26, 32], they study generating code in a DSL from inputs in natural language or in another DSL."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14434979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e957747f4f8600940be4c5bb001aa70c84e53a53",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Many language generation tasks require the production of text conditioned on both structured and unstructured inputs. We present a novel neural network architecture which generates an output sequence conditioned on an arbitrary number of input functions. Crucially, our approach allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training. Using this framework, we address the problem of generating programming code from a mixed natural language and structured specification. We create two new data sets for this paradigm derived from the collectible trading card games Magic the Gathering and Hearthstone. On these, and a third preexisting corpus, we demonstrate that marginalising multiple predictors allows our model to outperform strong benchmarks."
            },
            "slug": "Latent-Predictor-Networks-for-Code-Generation-Ling-Blunsom",
            "title": {
                "fragments": [],
                "text": "Latent Predictor Networks for Code Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "A novel neural network architecture is presented which generates an output sequence conditioned on an arbitrary number of input functions and allows both the choice of conditioning context and the granularity of generation, for example characters or tokens, to be marginalised, thus permitting scalable and effective training."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3166516"
                        ],
                        "name": "Emilio Parisotto",
                        "slug": "Emilio-Parisotto",
                        "structuredName": {
                            "firstName": "Emilio",
                            "lastName": "Parisotto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emilio Parisotto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40360972"
                        ],
                        "name": "Abdel-rahman Mohamed",
                        "slug": "Abdel-rahman-Mohamed",
                        "structuredName": {
                            "firstName": "Abdel-rahman",
                            "lastName": "Mohamed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdel-rahman Mohamed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50631599"
                        ],
                        "name": "Rishabh Singh",
                        "slug": "Rishabh-Singh",
                        "structuredName": {
                            "firstName": "Rishabh",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rishabh Singh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47681372"
                        ],
                        "name": "Lihong Li",
                        "slug": "Lihong-Li",
                        "structuredName": {
                            "firstName": "Lihong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lihong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24982365"
                        ],
                        "name": "Dengyong Zhou",
                        "slug": "Dengyong-Zhou",
                        "structuredName": {
                            "firstName": "Dengyong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dengyong Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143967473"
                        ],
                        "name": "Pushmeet Kohli",
                        "slug": "Pushmeet-Kohli",
                        "structuredName": {
                            "firstName": "Pushmeet",
                            "lastName": "Kohli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pushmeet Kohli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 74
                            }
                        ],
                        "text": "A recent line of research study using neural networks for code generation [5, 10, 25, 19, 26, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15904815,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "644ca74f80463415613847ab01cff067fb58f0ad",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent years have seen the proposal of a number of neural architectures for the problem of Program Induction. Given a set of input-output examples, these architectures are able to learn mappings that generalize to new test inputs. While achieving impressive results, these approaches have a number of important limitations: (a) they are computationally expensive and hard to train, (b) a model has to be trained for each task (program) separately, and (c) it is hard to interpret or verify the correctness of the learnt mapping (as it is defined by a neural network). In this paper, we propose a novel technique, Neuro-Symbolic Program Synthesis, to overcome the above-mentioned problems. Once trained, our approach can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time. Our method is based on two novel neural modules. The first module, called the cross correlation I/O network, given a set of input-output examples, produces a continuous representation of the set of I/O examples. The second module, the Recursive-Reverse-Recursive Neural Network (R3NN), given the continuous representation of the examples, synthesizes a program by incrementally expanding partial programs. We demonstrate the effectiveness of our approach by applying it to the rich and complex domain of regular expression based string transformations. Experiments show that the R3NN model is not only able to construct programs from new input-output examples, but it is also able to construct new programs for tasks that it had never observed before during training."
            },
            "slug": "Neuro-Symbolic-Program-Synthesis-Parisotto-Mohamed",
            "title": {
                "fragments": [],
                "text": "Neuro-Symbolic Program Synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes a novel technique, Neuro-Symbolic Program Synthesis, that can automatically construct computer programs in a domain-specific language that are consistent with a set of input-output examples provided at test time and demonstrates the effectiveness of the approach by applying it to the rich and complex domain of regular expression based string transformations."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3400339"
                        ],
                        "name": "Trong Duc Nguyen",
                        "slug": "Trong-Duc-Nguyen",
                        "structuredName": {
                            "firstName": "Trong",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Duc"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trong Duc Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144659797"
                        ],
                        "name": "A. Nguyen",
                        "slug": "A.-Nguyen",
                        "structuredName": {
                            "firstName": "Anh",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Tuan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2327697"
                        ],
                        "name": "T. Nguyen",
                        "slug": "T.-Nguyen",
                        "structuredName": {
                            "firstName": "Tien",
                            "lastName": "Nguyen",
                            "middleNames": [
                                "Nhut"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Nguyen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 96
                            }
                        ],
                        "text": "Some recent work have applied statistical machine translation techniques to program translation [2, 16, 22, 21, 23, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18434647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "68d4a1d4d059ec8efa1650a715cf4935d886758d",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Mapping API elements has a significant role in software development, especially in code migration. A manual process of defining the migration is tedious and error-prone while recent approaches to automatically mine API mappings are limited to discover the mappings with textually similar APIs\u2019 names. This leads to the low accuracy in existing migration tools.We propose an approach to automatically mine API mappings which overcomes the lexical mismatch problem. We represent an API by its usages instead of its name.To characterize an API with its context consisting of surrounding APIs in its usages, we take advantage of Word2Vec model to project the APIs of Java JDK and C# .NET into corresponding continuous vector spaces. The semantic relations among APIs will be observed in those continuous space as the geometric arrangements between their representation vectors in two vector spaces.We use a learning approach to derive the linear (e.g., rotating and scaling) transformation function between two vector spaces. Transformation function is trained from human-defined pairs of API mappings from Java to C#. To find the C# API mapping with a given Java API, we use the learned function to compute its transformed vector in the C# vector space. Then, the C# API which has the most similar vector with the transformed vector is considered as the result. Our experiment shows that for just one suggestion, we are able to correctly derive the API in C# in almost 43% of the cases. With 5 suggestions, we can correctly suggest the correct C# API in almost 3 out of 4 cases (73.2%)."
            },
            "slug": "Mapping-API-Elements-for-Code-Migration-with-Vector-Nguyen-Nguyen",
            "title": {
                "fragments": [],
                "text": "Mapping API Elements for Code Migration with Vector Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An approach to automatically mine API mappings which overcomes the lexical mismatch problem is proposed and is able to correctly derive the API in C# in almost 43% of the cases."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE/ACM 38th International Conference on Software Engineering Companion (ICSE-C)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390096054"
                        ],
                        "name": "David Alvarez-Melis",
                        "slug": "David-Alvarez-Melis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Alvarez-Melis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Alvarez-Melis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 90
                            }
                        ],
                        "text": "Other work study using neural networks to generate parse trees from input-output examples [11, 31, 1, 26, 32, 3, 12, 8, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56763307,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c69926bdb72912725d20a55af7147f86bed01ae",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel neural network architecture specifically tailored to treestructured decoding, which: \u2022 maintains separate depth and width recurrent states and combines them to obtain hidden states for every node in the tree. \u2022 has a mechanism to predict tree topology explicitly (as opposed to implicitly by adding nodes with special tokens). Our experiments show that this architecture \u2022 is capable of recovering trees from encoded representations \u2022 achieves state-of-the-art performance in a task consisting of mapping sentences to simple functional programs \u2022 exhibits desirable invariance properties over sequential architectures"
            },
            "slug": "Tree-structured-decoding-with-doubly-recurrent-Alvarez-Melis-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Tree-structured decoding with doubly-recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "A novel neural network architecture specifically tailored to treestructured decoding, which maintains separate depth and width recurrent states and combines them to obtain hidden states for every node in the tree, and exhibits desirable invariance properties over sequential architectures."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8421815"
                        ],
                        "name": "Kai Sheng Tai",
                        "slug": "Kai-Sheng-Tai",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Tai",
                            "middleNames": [
                                "Sheng"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Sheng Tai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 22
                            }
                        ],
                        "text": "The encoder employs a Tree-LSTM [35] to compute embeddings for both the entire source tree and each of its sub-tree."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "The encoder employs a Tree-LSTM [29] to compute embeddings for both the entire source tree and each of its sub-tree."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3033526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32de44f01a96d4473d21099d15e25bc2b9f08e2f",
            "isKey": false,
            "numCitedBy": 2533,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Because of their superior ability to preserve sequence information over time, Long Short-Term Memory (LSTM) networks, a type of recurrent neural network with a more complex computational unit, have obtained strong results on a variety of sequence modeling tasks. The only underlying LSTM structure that has been explored so far is a linear chain. However, natural language exhibits syntactic properties that would naturally combine words to phrases. We introduce the Tree-LSTM, a generalization of LSTMs to tree-structured network topologies. Tree-LSTMs outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences (SemEval 2014, Task 1) and sentiment classification (Stanford Sentiment Treebank)."
            },
            "slug": "Improved-Semantic-Representations-From-Long-Memory-Tai-Socher",
            "title": {
                "fragments": [],
                "text": "Improved Semantic Representations From Tree-Structured Long Short-Term Memory Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Tree-LSTM is introduced, a generalization of LSTMs to tree-structured network topologies that outperform all existing systems and strong LSTM baselines on two tasks: predicting the semantic relatedness of two sentences and sentiment classification."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145307652"
                        ],
                        "name": "Li Dong",
                        "slug": "Li-Dong",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Dong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Dong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747893"
                        ],
                        "name": "Mirella Lapata",
                        "slug": "Mirella-Lapata",
                        "structuredName": {
                            "firstName": "Mirella",
                            "lastName": "Lapata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirella Lapata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 106
                            }
                        ],
                        "text": "We evaluate our tree-to-tree model against a sequence-to-sequence model [4, 31], a sequence-to-tree model [11], and a tree-to-sequence model [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "employ a tree-based decoder to separate the two tasks [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "In particular, the decoder in [11] leverages the tree structural information to (1) generate the nodes at the same depth of the parse tree using an LSTM decoder; and (2) expand a non-terminal and generate its children in the parse tree."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 90
                            }
                        ],
                        "text": "Other work study using neural networks to generate parse trees from input-output examples [11, 31, 1, 26, 32, 3, 12, 8, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15412473,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "558ac446dc26bee9789d660a251b75728cb6eeb2",
            "isKey": true,
            "numCitedBy": 559,
            "numCiting": 64,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic parsing aims at mapping natural language to machine interpretable meaning representations. Traditional approaches rely on high-quality lexicons, manually-built templates, and linguistic features which are either domain- or representation-specific. In this paper we present a general method based on an attention-enhanced encoder-decoder model. We encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors. Experimental results on four datasets show that our approach performs competitively without using hand-engineered features and is easy to adapt across domains and meaning representations."
            },
            "slug": "Language-to-Logical-Form-with-Neural-Attention-Dong-Lapata",
            "title": {
                "fragments": [],
                "text": "Language to Logical Form with Neural Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper presents a general method based on an attention-enhanced encoder-decoder model that encode input utterances into vector representations, and generate their logical forms by conditioning the output sequences or trees on the encoding vectors."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354728"
                        ],
                        "name": "A. Karpathy",
                        "slug": "A.-Karpathy",
                        "structuredName": {
                            "firstName": "Andrej",
                            "lastName": "Karpathy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Karpathy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115231104"
                        ],
                        "name": "Justin Johnson",
                        "slug": "Justin-Johnson",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48004138"
                        ],
                        "name": "Li Fei-Fei",
                        "slug": "Li-Fei-Fei",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Fei-Fei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Fei-Fei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "It has been demonstrated that it is very hard for an RNN-based sequence generator to generate syntactically correct programs when the lengths grow large [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 988348,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "40be3888daa5c2e5af4d36ae22f690bcc8caf600",
            "isKey": false,
            "numCitedBy": 917,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Neural Networks (RNNs), and specifically a variant with Long Short-Term Memory (LSTM), are enjoying renewed interest as a result of successful applications in a wide range of machine learning problems that involve sequential data. However, while LSTMs provide exceptional results in practice, the source of their performance and their limitations remain rather poorly understood. Using character-level language models as an interpretable testbed, we aim to bridge this gap by providing an analysis of their representations, predictions and error types. In particular, our experiments reveal the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets. Moreover, our comparative analysis with finite horizon n-gram models traces the source of the LSTM improvements to long-range structural dependencies. Finally, we provide analysis of the remaining errors and suggests areas for further study."
            },
            "slug": "Visualizing-and-Understanding-Recurrent-Networks-Karpathy-Johnson",
            "title": {
                "fragments": [],
                "text": "Visualizing and Understanding Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work uses character-level language models as an interpretable testbed to provide an analysis of LSTM representations, predictions and error types, and reveals the existence of interpretable cells that keep track of long-range dependencies such as line lengths, quotes and brackets."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3216345"
                        ],
                        "name": "Miltiadis Allamanis",
                        "slug": "Miltiadis-Allamanis",
                        "structuredName": {
                            "firstName": "Miltiadis",
                            "lastName": "Allamanis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miltiadis Allamanis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1757975"
                        ],
                        "name": "Earl T. Barr",
                        "slug": "Earl-T.-Barr",
                        "structuredName": {
                            "firstName": "Earl",
                            "lastName": "Barr",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Earl T. Barr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730296"
                        ],
                        "name": "Premkumar T. Devanbu",
                        "slug": "Premkumar-T.-Devanbu",
                        "structuredName": {
                            "firstName": "Premkumar",
                            "lastName": "Devanbu",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Premkumar T. Devanbu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37210858"
                        ],
                        "name": "Charles Sutton",
                        "slug": "Charles-Sutton",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Sutton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 96
                            }
                        ],
                        "text": "Some recent work have applied statistical machine translation techniques to program translation [2, 16, 22, 21, 23, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207591052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "960ba564e9e598d864dff38d2f3d0bad1b319ead",
            "isKey": false,
            "numCitedBy": 493,
            "numCiting": 315,
            "paperAbstract": {
                "fragments": [],
                "text": "Research at the intersection of machine learning, programming languages, and software engineering has recently taken important steps in proposing learnable probabilistic models of source code that exploit the abundance of patterns of code. In this article, we survey this work. We contrast programming languages against natural languages and discuss how these similarities and differences drive the design of probabilistic models. We present a taxonomy based on the underlying design principles of each model and use it to navigate the literature. Then, we review how researchers have adapted these models to application areas and discuss cross-cutting and application-specific challenges and opportunities."
            },
            "slug": "A-Survey-of-Machine-Learning-for-Big-Code-and-Allamanis-Barr",
            "title": {
                "fragments": [],
                "text": "A Survey of Machine Learning for Big Code and Naturalness"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article presents a taxonomy based on the underlying design principles of each model and uses it to navigate the literature and discuss cross-cutting and application-specific challenges and opportunities."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Comput. Surv."
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2585821"
                        ],
                        "name": "Cliff Chiung-Yu Lin",
                        "slug": "Cliff-Chiung-Yu-Lin",
                        "structuredName": {
                            "firstName": "Cliff",
                            "lastName": "Lin",
                            "middleNames": [
                                "Chiung-Yu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cliff Chiung-Yu Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18690358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c0ddf74f87d154db88d79c640578c1610451eec",
            "isKey": false,
            "numCitedBy": 1331,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Recursive structure is commonly found in the inputs of different modalities such as natural scene images or natural language sentences. Discovering this recursive structure helps us to not only identify the units that an image or sentence contains but also how they interact to form a whole. We introduce a max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences. The same algorithm can be used both to provide a competitive syntactic parser for natural language sentences from the Penn Treebank and to outperform alternative approaches for semantic scene segmentation, annotation and classification. For segmentation and annotation our algorithm obtains a new level of state-of-the-art performance on the Stanford background dataset (78.1%). The features from the image parse tree outperform Gist descriptors for scene classification by 4%."
            },
            "slug": "Parsing-Natural-Scenes-and-Natural-Language-with-Socher-Lin",
            "title": {
                "fragments": [],
                "text": "Parsing Natural Scenes and Natural Language with Recursive Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A max-margin structure prediction architecture based on recursive neural networks that can successfully recover such structure both in complex scene images as well as sentences is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940272"
                        ],
                        "name": "Matt J. Kusner",
                        "slug": "Matt-J.-Kusner",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Kusner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt J. Kusner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2885717"
                        ],
                        "name": "Brooks Paige",
                        "slug": "Brooks-Paige",
                        "structuredName": {
                            "firstName": "Brooks",
                            "lastName": "Paige",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brooks Paige"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388574431"
                        ],
                        "name": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
                        "slug": "Jos\u00e9-Miguel-Hern\u00e1ndez-Lobato",
                        "structuredName": {
                            "firstName": "Jos\u00e9 Miguel",
                            "lastName": "Hern\u00e1ndez-Lobato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "Some existing work [28, 18] propose tree-based autoencoder architectures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 18
                            }
                        ],
                        "text": "In particular, in [28, 18], they propose tree-structured autoencoders to learn vector representations of trees, and show better performance on tree reconstruction and other tasks such as sentiment analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7648414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "222928303a72d1389b0add8032a31abccbba41b3",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep generative models have been wildly successful at learning coherent latent representations for continuous data such as video and audio. However, generative modeling of discrete data such as arithmetic expressions and molecular structures still poses significant challenges. Crucially, state-of-the-art methods often produce outputs that are not valid. We make the key observation that frequently, discrete data can be represented as a parse tree from a context-free grammar. We propose a variational autoencoder which encodes and decodes directly to and from these parse trees, ensuring the generated outputs are always valid. Surprisingly, we show that not only does our model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs. We demonstrate the effectiveness of our learned models by showing their improved performance in Bayesian optimization for symbolic regression and molecular synthesis."
            },
            "slug": "Grammar-Variational-Autoencoder-Kusner-Paige",
            "title": {
                "fragments": [],
                "text": "Grammar Variational Autoencoder"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Surprisingly, it is shown that not only does the model more often generate valid outputs, it also learns a more coherent latent space in which nearby points decode to similar discrete outputs."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745899"
                        ],
                        "name": "Chris Dyer",
                        "slug": "Chris-Dyer",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Dyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Dyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3376845"
                        ],
                        "name": "A. Kuncoro",
                        "slug": "A.-Kuncoro",
                        "structuredName": {
                            "firstName": "Adhiguna",
                            "lastName": "Kuncoro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kuncoro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143668305"
                        ],
                        "name": "Miguel Ballesteros",
                        "slug": "Miguel-Ballesteros",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Ballesteros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miguel Ballesteros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "translation, where both the encoder and the decoder are variants of the RNNG model [12]; however, the performance of their model is slightly worse than the sequence-to-sequence model with attention, which is mainly due to the fact that their attention mechanism can not condition the future attention weights on previously computed ones."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 90
                            }
                        ],
                        "text": "Other work study using neural networks to generate parse trees from input-output examples [11, 31, 1, 26, 32, 3, 12, 8, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1949831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7345843e87c81e24e42264859b214d26042f8d51",
            "isKey": false,
            "numCitedBy": 428,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce recurrent neural network grammars, probabilistic models of sentences with explicit phrase structure. We explain efficient inference procedures that allow application to both parsing and language modeling. Experiments show that they provide better parsing in English than any single previously published supervised generative model and better language modeling than state-of-the-art sequential RNNs in English and Chinese."
            },
            "slug": "Recurrent-Neural-Network-Grammars-Dyer-Kuncoro",
            "title": {
                "fragments": [],
                "text": "Recurrent Neural Network Grammars"
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2166511"
                        ],
                        "name": "R. Socher",
                        "slug": "R.-Socher",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Socher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Socher"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143845796"
                        ],
                        "name": "Jeffrey Pennington",
                        "slug": "Jeffrey-Pennington",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Pennington",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeffrey Pennington"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40150953"
                        ],
                        "name": "E. Huang",
                        "slug": "E.-Huang",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Huang",
                            "middleNames": [
                                "Hsin-Chun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "Some existing work [28, 18] propose tree-based autoencoder architectures."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 18
                            }
                        ],
                        "text": "In particular, in [28, 18], they propose tree-structured autoencoders to learn vector representations of trees, and show better performance on tree reconstruction and other tasks such as sentiment analysis."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3116311,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfa2646776405d50533055ceb1b7f050e9014dcb",
            "isKey": false,
            "numCitedBy": 1253,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions. Our method learns vector space representations for multi-word phrases. In sentiment prediction tasks these representations outperform other state-of-the-art approaches on commonly used datasets, such as movie reviews, without using any pre-defined sentiment lexica or polarity shifting rules. We also evaluate the model's ability to predict sentiment distributions on a new dataset based on confessions from the experience project. The dataset consists of personal user stories annotated with multiple labels which, when aggregated, form a multinomial distribution that captures emotional reactions. Our algorithm can more accurately predict distributions over such labels compared to several competitive baselines."
            },
            "slug": "Semi-Supervised-Recursive-Autoencoders-for-Socher-Pennington",
            "title": {
                "fragments": [],
                "text": "Semi-Supervised Recursive Autoencoders for Predicting Sentiment Distributions"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A novel machine learning framework based on recursive autoencoders for sentence-level prediction of sentiment label distributions that outperform other state-of-the-art approaches on commonly used datasets, without using any pre-defined sentiment lexica or polarity shifting rules."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2011
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 123
                            }
                        ],
                        "text": "Recently, various neural networks with tree structures have been proposed to employ the structural information of the data [11, 26, 25, 32, 3, 29, 34, 27, 13, 33, 28, 18, 6]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 3
                            }
                        ],
                        "text": "In [19, 26, 32], they study generating code in a DSL from inputs in natural language or in another DSL."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 74
                            }
                        ],
                        "text": "A recent line of research study using neural networks for code generation [5, 10, 25, 19, 26, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 90
                            }
                        ],
                        "text": "Other work study using neural networks to generate parse trees from input-output examples [11, 31, 1, 26, 32, 3, 12, 8, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 86
                            }
                        ],
                        "text": "Some other work incorporate the knowledge of the grammar into the architecture design [32, 26] to achieve better performance on specific tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A syntactic neural model for general-purpose code generation"
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Treestructured decoding with doubly-recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "In ICLR,"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Latent attention for ifthen program synthesis"
            },
            "venue": {
                "fragments": [],
                "text": "In Advances in Neural Information Processing Systems,"
            },
            "year": 2016
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 25,
            "methodology": 23
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 36,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Tree-to-tree-Neural-Networks-for-Program-Chen-Liu/6c6170ffb39cdc8cfffbeda9c7a2259eda5875f2?sort=total-citations"
}