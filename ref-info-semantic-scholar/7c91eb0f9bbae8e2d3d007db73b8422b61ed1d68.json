{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020514"
                        ],
                        "name": "Maurice Peemen",
                        "slug": "Maurice-Peemen",
                        "structuredName": {
                            "firstName": "Maurice",
                            "lastName": "Peemen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maurice Peemen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145101451"
                        ],
                        "name": "A. Setio",
                        "slug": "A.-Setio",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Setio",
                            "middleNames": [
                                "Arindra",
                                "Adiyoso"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Setio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145248324"
                        ],
                        "name": "B. Mesman",
                        "slug": "B.-Mesman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Mesman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Mesman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684335"
                        ],
                        "name": "H. Corporaal",
                        "slug": "H.-Corporaal",
                        "structuredName": {
                            "firstName": "Henk",
                            "lastName": "Corporaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Corporaal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 250
                            }
                        ],
                        "text": "Among these approaches, FPGA based accelerators have attracted more and more attention of researchers because they have advantages of good performance, high energy efficiency, fast development round, and capability of reconfiguration [1] [2] [3] [6] [12] [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Second, work [12], considering CNN\u2019s communication issue, chooses to maximize date reuse and reduce bandwidth requirement to the minimum."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "ICCD2013 [12] ASAP2009 [14] FPL2009 [6] FPL2009 [6] PACT2010 [2] ISCA2010 [3] Our Impl."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Second, previous study [12] accelerates CNN applications by reducing external data access with delicate data reuse."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 48
                            }
                        ],
                        "text": "Table 5: Comparison to previous implementations\nICCD2013 [12] ASAP2009 [14] FPL2009 [6] FPL2009 [6] PACT2010 [2]\nISCA2010 [3] Our Impl."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2398978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b71de48a1c7a5d7a7850bbe33a2bb055ea594ef",
            "isKey": true,
            "numCitedBy": 255,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In the near future, cameras will be used everywhere as flexible sensors for numerous applications. For mobility and privacy reasons, the required image processing should be local on embedded computer platforms with performance requirements and energy constraints. Dedicated acceleration of Convolutional Neural Networks (CNN) can achieve these targets with enough flexibility to perform multiple vision tasks. A challenging problem for the design of efficient accelerators is the limited amount of external memory bandwidth. We show that the effects of the memory bottleneck can be reduced by a flexible memory hierarchy that supports the complex data access patterns in CNN workload. The efficiency of the on-chip memories is maximized by our scheduler that uses tiling to optimize for data locality. Our design flow ensures that on-chip memory size is minimized, which reduces area and energy usage. The design flow is evaluated by a High Level Synthesis implementation on a Virtex 6 FPGA board. Compared to accelerators with standard scratchpad memories the FPGA resources can be reduced up to 13\u00d7 while maintaining the same performance. Alternatively, when the same amount of FPGA resources is used our accelerators are up to 11\u00d7 faster."
            },
            "slug": "Memory-centric-accelerator-design-for-Convolutional-Peemen-Setio",
            "title": {
                "fragments": [],
                "text": "Memory-centric accelerator design for Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the effects of the memory bottleneck can be reduced by a flexible memory hierarchy that supports the complex data access patterns in CNN workload and ensures that on-chip memory size is minimized, which reduces area and energy usage."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE 31st International Conference on Computer Design (ICCD)"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256269"
                        ],
                        "name": "C. Farabet",
                        "slug": "C.-Farabet",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Farabet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Farabet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35130633"
                        ],
                        "name": "Cyril Poulet",
                        "slug": "Cyril-Poulet",
                        "structuredName": {
                            "firstName": "Cyril",
                            "lastName": "Poulet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cyril Poulet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2183554"
                        ],
                        "name": "Jefferson Y. Han",
                        "slug": "Jefferson-Y.-Han",
                        "structuredName": {
                            "firstName": "Jefferson",
                            "lastName": "Han",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jefferson Y. Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5339694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "07956c7cf9bf4267b86d52aa4143c17a4aa5d0d6",
            "isKey": false,
            "numCitedBy": 328,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional Networks (ConvNets) are biologicallyinspired hierarchical architectures that can be trained to perform a variety of detection, recognition and segmentation tasks. ConvNets have a feed-forward architecture consisting of multiple linear convolution filters interspersed with pointwise non-linear squashing functions. This paper presents an efficient implementation of ConvNets on a low-end DSPoriented Field Programmable Gate Array (FPGA). The implementation exploits the inherent parallelism of ConvNets and takes full advantage of multiple hardware multiplyaccumulate units on the FPGA. The entire system uses a single FPGA with an external memory module, and no extra parts. A network compiler software was implemented, which takes a description of a trained ConvNet and compiles it into a sequence of instructions for the ConvNet Processor (CNP). A ConvNet face detection system was implemented and tested. Face detection on a 512 \u00d7 384 frame takes 100ms (10 frames per second), which corresponds to an average performance of 3.4\u00d7109 connections per second for this 340 million connection network. The design can be used for low-power, lightweight embedded vision systems for micro-UAVs and other small robots."
            },
            "slug": "CNP:-An-FPGA-based-processor-for-Convolutional-Farabet-Poulet",
            "title": {
                "fragments": [],
                "text": "CNP: An FPGA-based processor for Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The implementation exploits the inherent parallelism of ConvNets and takes full advantage of multiple hardware multiplyaccumulate units on the FPGA and can be used for low-power, lightweight embedded vision systems for micro-UAVs and other small robots."
            },
            "venue": {
                "fragments": [],
                "text": "2009 International Conference on Field Programmable Logic and Applications"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144049725"
                        ],
                        "name": "Tianshi Chen",
                        "slug": "Tianshi-Chen",
                        "structuredName": {
                            "firstName": "Tianshi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianshi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678776"
                        ],
                        "name": "Zidong Du",
                        "slug": "Zidong-Du",
                        "structuredName": {
                            "firstName": "Zidong",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zidong Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145550877"
                        ],
                        "name": "Ninghui Sun",
                        "slug": "Ninghui-Sun",
                        "structuredName": {
                            "firstName": "Ninghui",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ninghui Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110368816"
                        ],
                        "name": "Jia Wang",
                        "slug": "Jia-Wang",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7514065"
                        ],
                        "name": "Chengyong Wu",
                        "slug": "Chengyong-Wu",
                        "structuredName": {
                            "firstName": "Chengyong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengyong Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7377735"
                        ],
                        "name": "Yunji Chen",
                        "slug": "Yunji-Chen",
                        "structuredName": {
                            "firstName": "Yunji",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunji Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731764"
                        ],
                        "name": "O. Temam",
                        "slug": "O.-Temam",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Temam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Temam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 207209696,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd",
            "isKey": false,
            "numCitedBy": 1235,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope. Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy. We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications."
            },
            "slug": "DianNao:-a-small-footprint-high-throughput-for-Chen-Du",
            "title": {
                "fragments": [],
                "text": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This study designs an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy, and shows that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s in a small footprint."
            },
            "venue": {
                "fragments": [],
                "text": "ASPLOS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329060"
                        ],
                        "name": "M. Sankaradass",
                        "slug": "M.-Sankaradass",
                        "structuredName": {
                            "firstName": "Murugan",
                            "lastName": "Sankaradass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sankaradass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101580"
                        ],
                        "name": "V. Jakkula",
                        "slug": "V.-Jakkula",
                        "structuredName": {
                            "firstName": "Venkata",
                            "lastName": "Jakkula",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Jakkula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2468150"
                        ],
                        "name": "S. Cadambi",
                        "slug": "S.-Cadambi",
                        "structuredName": {
                            "firstName": "Srihari",
                            "lastName": "Cadambi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cadambi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752242"
                        ],
                        "name": "S. Chakradhar",
                        "slug": "S.-Chakradhar",
                        "structuredName": {
                            "firstName": "Srimat",
                            "lastName": "Chakradhar",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chakradhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844230"
                        ],
                        "name": "Igor Durdanovic",
                        "slug": "Igor-Durdanovic",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Durdanovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Igor Durdanovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3165487"
                        ],
                        "name": "E. Cosatto",
                        "slug": "E.-Cosatto",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Cosatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cosatto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775043"
                        ],
                        "name": "H. Graf",
                        "slug": "H.-Graf",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Graf",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Graf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 255
                            }
                        ],
                        "text": "Among these approaches, FPGA based accelerators have attracted more and more attention of researchers because they have advantages of good performance, high energy efficiency, fast development round, and capability of reconfiguration [1] [2] [3] [6] [12] [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 284
                            }
                        ],
                        "text": "Convolutional neural network (CNN), a well-known deep learning architecture extended from artificial neural network, has been extensively adopted in various applications, which include video surveillance, mobile robot vision, image search engine in data centers, etc [6] [7] [8] [10] [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "ICCD2013 [12] ASAP2009 [14] FPL2009 [6] FPL2009 [6] PACT2010 [2] ISCA2010 [3] Our Impl."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "Work [14] and [2] mainly uses the parallelism within feature maps and convolution kernel."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "First, work [1] [2] [3] [6] [14] mainly focused on computation engine optimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Implementations [6], [14] and [3] are three representatives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 62
                            }
                        ],
                        "text": "Table 5: Comparison to previous implementations\nICCD2013 [12] ASAP2009 [14] FPL2009 [6] FPL2009 [6] PACT2010 [2]\nISCA2010 [3] Our Impl."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "Implementations in work [14], [2] and [3] implement complete CNN applications on FPGA but exploits different parallelism opportunities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9736053,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f40ea0248653d4ffb6ef4857cd23f0f713d8c69",
            "isKey": true,
            "numCitedBy": 206,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a massively parallel coprocessor for accelerating Convolutional Neural Networks (CNNs), a class of important machine learning algorithms. The coprocessor functional units, consisting of parallel 2D convolution primitives and programmable units performing sub-sampling and non-linear functions specific to CNNs, implement a \u201cmeta-operator\u201d to which a CNN may be compiled to. The coprocessor is serviced by distributed off-chip memory banks with large data bandwidth. As a key feature, we use low precision data and further increase the effective memory bandwidth by packing multiple words in every memory operation, and leverage the algorithm\u2019s simple data access patterns to use off-chip memory as a scratchpad for intermediate data, critical for CNNs. A CNN is mapped to the coprocessor hardware primitives with instructions to transfer data between the memory and coprocessor. We have implemented a prototype of the CNN coprocessor on an off-the-shelf PCI FPGA card with a single Xilinx Virtex5 LX330T FPGA and 4 DDR2 memory banks totaling 1GB. The coprocessor prototype can process at the rate of 3.4 billion multiply accumulates per second (GMACs) for CNN forward propagation, a speed that is 31x faster than a software implementation on a 2.2 GHz AMD Opteron processor. For a complete face recognition application with the CNN on the coprocessor and the rest of the image processing tasks on the host, the prototype is 6-10x faster, depending on the host-coprocessor bandwidth."
            },
            "slug": "A-Massively-Parallel-Coprocessor-for-Convolutional-Sankaradass-Jakkula",
            "title": {
                "fragments": [],
                "text": "A Massively Parallel Coprocessor for Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A massively parallel coprocessor for accelerating Convolutional Neural Networks (CNNs), a class of important machine learning algorithms, is presented, which uses low precision data and further increase the effective memory bandwidth by packing multiple words in every memory operation."
            },
            "venue": {
                "fragments": [],
                "text": "2009 20th IEEE International Conference on Application-specific Systems, Architectures and Processors"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259796"
                        ],
                        "name": "J. Cong",
                        "slug": "J.-Cong",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Cong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082437233"
                        ],
                        "name": "Bing-Yu Xiao",
                        "slug": "Bing-Yu-Xiao",
                        "structuredName": {
                            "firstName": "Bing-Yu",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing-Yu Xiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "In the feedforward computation perspective, a previous study [5] proved that convolution operations will occupy over 90% of the computation time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16946782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5f1beada9e269b2a7faed8dfe936919ac0c2397",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional Neural Networks (CNNs) have been successfully used for many computer vision applications. It would be beneficial to these applications if the computational workload of CNNs could be reduced. In this work we analyze the linear algebraic properties of CNNs and propose an algorithmic modification to reduce their computational workload. An up to a 47% reduction can be achieved without any change in the image recognition results or the addition of any hardware accelerators."
            },
            "slug": "Minimizing-Computation-in-Convolutional-Neural-Cong-Xiao",
            "title": {
                "fragments": [],
                "text": "Minimizing Computation in Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work analyzes the linear algebraic properties of CNNs and proposes an algorithmic modification to reduce their computational workload, achieving up to a 47% reduction."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793611"
                        ],
                        "name": "L. Pouchet",
                        "slug": "L.-Pouchet",
                        "structuredName": {
                            "firstName": "Louis-No\u00ebl",
                            "lastName": "Pouchet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pouchet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145702907"
                        ],
                        "name": "Peng Zhang",
                        "slug": "Peng-Zhang",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293384"
                        ],
                        "name": "P. Sadayappan",
                        "slug": "P.-Sadayappan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Sadayappan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sadayappan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259796"
                        ],
                        "name": "J. Cong",
                        "slug": "J.-Cong",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Cong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "In this section, we use standard polyhedral-based data dependence analysis [13] to derive a series of legal design variants of equivalently CNN implementations through loop scheduling and loop tile size enumeration."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Local memory promotion [13] can be used to reduce the redundant operations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "The polyhedral annotations used here can be found in [13]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12671526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c29d0ea9ee2540630c078f4a382ebc5317d459fc",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Many applications, such as medical imaging, generate intensive data traffic between the FPGA and off-chip memory. Significant improvements in the execution time can be achieved with effective utilization of on-chip (scratchpad) memories, associated with careful software-based data reuse and communication scheduling techniques. We present a fully automated C-to-FPGA framework to address this problem. Our framework effectively implements data reuse through aggressive loop transformation-based program restructuring. In addition, our proposed framework automatically implements critical optimizations for performance such as task-level parallelization, loop pipelining, and data prefetching.\n We leverage the power and expressiveness of the polyhedral compilation model to develop a multi-objective optimization system for off-chip communications management. Our technique can satisfy hardware resource constraints (scratchpad size) while still aggressively exploiting data reuse. Our approach can also be used to reduce the on-chip buffer size subject to bandwidth constraint. We also implement a fast design space exploration technique for effective optimization of program performance using the Xilinx high-level synthesis tool."
            },
            "slug": "Polyhedral-based-data-reuse-optimization-for-Pouchet-Zhang",
            "title": {
                "fragments": [],
                "text": "Polyhedral-based data reuse optimization for configurable computing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work uses the power and expressiveness of the polyhedral compilation model to develop a multi-objective optimization system for off-chip communications management and implements a fast design space exploration technique for effective optimization of program performance using the Xilinx high-level synthesis tool."
            },
            "venue": {
                "fragments": [],
                "text": "FPGA '13"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752242"
                        ],
                        "name": "S. Chakradhar",
                        "slug": "S.-Chakradhar",
                        "structuredName": {
                            "firstName": "Srimat",
                            "lastName": "Chakradhar",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chakradhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329060"
                        ],
                        "name": "M. Sankaradass",
                        "slug": "M.-Sankaradass",
                        "structuredName": {
                            "firstName": "Murugan",
                            "lastName": "Sankaradass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sankaradass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101580"
                        ],
                        "name": "V. Jakkula",
                        "slug": "V.-Jakkula",
                        "structuredName": {
                            "firstName": "Venkata",
                            "lastName": "Jakkula",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Jakkula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2468150"
                        ],
                        "name": "S. Cadambi",
                        "slug": "S.-Cadambi",
                        "structuredName": {
                            "firstName": "Srihari",
                            "lastName": "Cadambi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cadambi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 242
                            }
                        ],
                        "text": "Among these approaches, FPGA based accelerators have attracted more and more attention of researchers because they have advantages of good performance, high energy efficiency, fast development round, and capability of reconfiguration [1] [2] [3] [6] [12] [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "ICCD2013 [12] ASAP2009 [14] FPL2009 [6] FPL2009 [6] PACT2010 [2] ISCA2010 [3] Our Impl."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "Work [3] uses \u201cinter-output\u201d and \u201cintra-output\u201d parallelism."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "First, work [1] [2] [3] [6] [14] mainly focused on computation engine optimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "Implementations [6], [14] and [3] are three representatives."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 113
                            }
                        ],
                        "text": "Table 5: Comparison to previous implementations\nICCD2013 [12] ASAP2009 [14] FPL2009 [6] FPL2009 [6] PACT2010 [2]\nISCA2010 [3] Our Impl."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "Implementations in work [14], [2] and [3] implement complete CNN applications on FPGA but exploits different parallelism opportunities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 134
                            }
                        ],
                        "text": "Thus, various accelerators based on FPGA, GPU, and even ASIC design have been proposed recently to improve performance of CNN designs [3] [4] [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3350152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1c4e2fa071046569a05e9cfdf13496d094025dd",
            "isKey": true,
            "numCitedBy": 350,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional neural networks (CNN) applications range from recognition and reasoning (such as handwriting recognition, facial expression recognition and video surveillance) to intelligent text applications such as semantic text analysis and natural language processing applications. Two key observations drive the design of a new architecture for CNN. First, CNN workloads exhibit a widely varying mix of three types of parallelism: parallelism within a convolution operation, intra-output parallelism where multiple input sources (features) are combined to create a single output, and inter-output parallelism where multiple, independent outputs (features) are computed simultaneously. Workloads differ significantly across different CNN applications, and across different layers of a CNN. Second, the number of processing elements in an architecture continues to scale (as per Moore's law) much faster than off-chip memory bandwidth (or pin-count) of chips. Based on these two observations, we show that for a given number of processing elements and off-chip memory bandwidth, a new CNN hardware architecture that dynamically configures the hardware on-the-fly to match the specific mix of parallelism in a given workload gives the best throughput performance. Our CNN compiler automatically translates high abstraction network specification into a parallel microprogram (a sequence of low-level VLIW instructions) that is mapped, scheduled and executed by the coprocessor. Compared to a 2.3 GHz quad-core, dual socket Intel Xeon, 1.35 GHz C870 GPU, and a 200 MHz FPGA implementation, our 120 MHz dynamically configurable architecture is 4x to 8x faster. This is the first CNN architecture to achieve real-time video stream processing (25 to 30 frames per second) on a wide range of object detection and recognition tasks."
            },
            "slug": "A-dynamically-configurable-coprocessor-for-neural-Chakradhar-Sankaradass",
            "title": {
                "fragments": [],
                "text": "A dynamically configurable coprocessor for convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This is the first CNN architecture to achieve real-time video stream processing (25 to 30 frames per second) on a wide range of object detection and recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ISCA"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2468150"
                        ],
                        "name": "S. Cadambi",
                        "slug": "S.-Cadambi",
                        "structuredName": {
                            "firstName": "Srihari",
                            "lastName": "Cadambi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cadambi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080400"
                        ],
                        "name": "Abhinandan Majumdar",
                        "slug": "Abhinandan-Majumdar",
                        "structuredName": {
                            "firstName": "Abhinandan",
                            "lastName": "Majumdar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abhinandan Majumdar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2948456"
                        ],
                        "name": "M. Becchi",
                        "slug": "M.-Becchi",
                        "structuredName": {
                            "firstName": "Michela",
                            "lastName": "Becchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Becchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752242"
                        ],
                        "name": "S. Chakradhar",
                        "slug": "S.-Chakradhar",
                        "structuredName": {
                            "firstName": "Srimat",
                            "lastName": "Chakradhar",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chakradhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775043"
                        ],
                        "name": "H. Graf",
                        "slug": "H.-Graf",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Graf",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Graf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 241,
                                "start": 238
                            }
                        ],
                        "text": "Among these approaches, FPGA based accelerators have attracted more and more attention of researchers because they have advantages of good performance, high energy efficiency, fast development round, and capability of reconfiguration [1] [2] [3] [6] [12] [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "ICCD2013 [12] ASAP2009 [14] FPL2009 [6] FPL2009 [6] PACT2010 [2] ISCA2010 [3] Our Impl."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "Work [14] and [2] mainly uses the parallelism within feature maps and convolution kernel."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "First, work [1] [2] [3] [6] [14] mainly focused on computation engine optimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 100
                            }
                        ],
                        "text": "Table 5: Comparison to previous implementations\nICCD2013 [12] ASAP2009 [14] FPL2009 [6] FPL2009 [6] PACT2010 [2]\nISCA2010 [3] Our Impl."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "Implementations in work [14], [2] and [3] implement complete CNN applications on FPGA but exploits different parallelism opportunities."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2332389,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04fd80d4d6c6ac5ef94350e3142580f54a66c093",
            "isKey": true,
            "numCitedBy": 94,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "For learning and classification workloads that operate on large amounts of unstructured data with stringent performance constraints, general purpose processor performance scales poorly with data size. In this paper, we present a programmable accelerator for this workload domain. To architect the accelerator, we profile five representative workloads, and find that their computationally intensive portions can be formulated as matrix or vector operations generating large amounts of intermediate data, which are then reduced by a secondary operation such as array ranking, finding max/min and aggregation. The proposed accelerator, called MAPLE, has hundreds of simple processing elements (PEs) laid out in a two-dimensional grid, with two key features. First, it uses in-memory processing where on-chip memory blocks perform the secondary reduction operations. By doing so, the intermediate data are dynamically processed and never stored or sent off-chip. Second, MAPLE uses banked off-chip memory, and organizes its PEs into independent groups each with its own off-chip memory bank. These two features together allow MAPLE to scale its performance with data size. This paper describes the MAPLE architecture, explores its design space with a simulator, and illustrates how to automatically map application kernels to the hardware. We also implement a 512-PE FPGA prototype of MAPLE and find that it is 1.5\u201310x faster than a 2.5 GHz quad-core Xeon processor despite running at a modest 125 MHz."
            },
            "slug": "A-programmable-parallel-accelerator-for-learning-Cadambi-Majumdar",
            "title": {
                "fragments": [],
                "text": "A programmable parallel accelerator for learning and classification"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The MAPLE architecture is described, its design space is explored with a simulator, and how to automatically map application kernels to the hardware is illustrated, finding that it is 1.5\u201310x faster than a 2.5 GHz quad-core Xeon processor despite running at a modest 125 MHz."
            },
            "venue": {
                "fragments": [],
                "text": "2010 19th International Conference on Parallel Architectures and Compilation Techniques (PACT)"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25168273"
                        ],
                        "name": "Waseem Rawat",
                        "slug": "Waseem-Rawat",
                        "structuredName": {
                            "firstName": "Waseem",
                            "lastName": "Rawat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Waseem Rawat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47197274"
                        ],
                        "name": "Zenghui Wang",
                        "slug": "Zenghui-Wang",
                        "structuredName": {
                            "firstName": "Zenghui",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zenghui Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3290651,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1f05b723e53ac4eb1133249b445c0011d42ca79",
            "isKey": false,
            "numCitedBy": 1534,
            "numCiting": 328,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional neural networks (CNNs) have been applied to visual tasks since the late 1980s. However, despite a few scattered applications, they were dormant until the mid-2000s when developments in computing power and the advent of large amounts of labeled data, supplemented by improved algorithms, contributed to their advancement and brought them to the forefront of a neural network renaissance that has seen rapid progression since 2012. In this review, which focuses on the application of CNNs to image classification tasks, we cover their development, from their predecessors up to recent state-of-the-art deep learning systems. Along the way, we analyze (1) their early successes, (2) their role in the deep learning renaissance, (3) selected symbolic works that have contributed to their recent popularity, and (4) several improvement attempts by reviewing contributions and challenges of over 300 publications. We also introduce some of their current trends and remaining challenges."
            },
            "slug": "Deep-Convolutional-Neural-Networks-for-Image-A-Rawat-Wang",
            "title": {
                "fragments": [],
                "text": "Deep Convolutional Neural Networks for Image Classification: A Comprehensive Review"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This review, which focuses on the application of CNNs to image classification tasks, covers their development, from their predecessors up to recent state-of-the-art deep learning systems."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 55
                            }
                        ],
                        "text": "Figure 2 shows a real-life CNN application, taken from [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 142
                            }
                        ],
                        "text": "Thus, various accelerators based on FPGA, GPU, and even ASIC design have been proposed recently to improve performance of CNN designs [3] [4] [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 61
                            }
                        ],
                        "text": "Figure 2: A real-life CNN that won the ImageNet 2012 contest [9]"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 82053,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143983894"
                        ],
                        "name": "Wei Zuo",
                        "slug": "Wei-Zuo",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zuo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zuo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144884036"
                        ],
                        "name": "Yun Liang",
                        "slug": "Yun-Liang",
                        "structuredName": {
                            "firstName": "Yun",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yun Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149247466"
                        ],
                        "name": "Peng Li",
                        "slug": "Peng-Li",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144904704"
                        ],
                        "name": "K. Rupnow",
                        "slug": "K.-Rupnow",
                        "structuredName": {
                            "firstName": "Kyle",
                            "lastName": "Rupnow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rupnow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713644"
                        ],
                        "name": "Deming Chen",
                        "slug": "Deming-Chen",
                        "structuredName": {
                            "firstName": "Deming",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deming Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259796"
                        ],
                        "name": "J. Cong",
                        "slug": "J.-Cong",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Cong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Polyhedral-based optimization framework [16] can be used to perform automatic loop transformation to permute the parallel loop levels to the innermost levels to avoid loop carried dependence."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5623885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7158027f20a65a11146a20310073de32a3717b8f",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "High level synthesis (HLS) is an important enabling technology for the adoption of hardware accelerator technologies. It promises the performance and energy efficiency of hardware designs with a lower barrier to entry in design expertise, and shorter design time. State-of-the-art high level synthesis now includes a wide variety of powerful optimizations that implement efficient hardware. These optimizations can implement some of the most important features generally performed in manual designs including parallel hardware units, pipelining of execution both within a hardware unit and between units, and fine-grained data communication. We may generally classify the optimizations as those that optimize hardware implementation within a code block (intra-block) and those that optimize communication and pipelining between code blocks (inter-block). However, both optimizations are in practice difficult to apply. Real-world applications contain data-dependent blocks of code and communicate through complex data access patterns. Existing high level synthesis tools cannot apply these powerful optimizations unless the code is inherently compatible, severely limiting the optimization opportunity. In this paper we present an integrated framework to model and enable both intra- and inter-block optimizations. This integrated technique substantially improves the opportunity to use the powerful HLS optimizations that implement parallelism, pipelining, and fine-grained communication. Our polyhedral model-based technique systematically defines a set of data access patterns, identifies effective data access patterns, and performs the loop transformations to enable the intra- and inter-block optimizations. Our framework automatically explores transformation options, performs code transformations, and inserts the appropriate HLS directives to implement the HLS optimizations. Furthermore, our framework can automatically generate the optimized communication blocks for fine-grained communication between hardware blocks. Experimental evaluation demonstrates that we can achieve an average of 6.04X speedup over the high level synthesis solution without our transformations to enable intra- and inter-block optimizations."
            },
            "slug": "Improving-high-level-synthesis-optimization-through-Zuo-Liang",
            "title": {
                "fragments": [],
                "text": "Improving high level synthesis optimization opportunity through polyhedral transformations"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An integrated framework to model and enable both intra- and inter-block HLS optimizations that implement parallelism, pipelining, and fine-grained communication is presented."
            },
            "venue": {
                "fragments": [],
                "text": "FPGA '13"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "The algorithm receives three 224x224 input images that are from an original 256x256 three-channel RGB image."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35624,
            "numCiting": 246,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743600"
                        ],
                        "name": "Shuiwang Ji",
                        "slug": "Shuiwang-Ji",
                        "structuredName": {
                            "firstName": "Shuiwang",
                            "lastName": "Ji",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuiwang Ji"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143836295"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41216159"
                        ],
                        "name": "Ming Yang",
                        "slug": "Ming-Yang",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782042"
                        ],
                        "name": "Kai Yu",
                        "slug": "Kai-Yu",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 278,
                                "start": 275
                            }
                        ],
                        "text": "Convolutional neural network (CNN), a well-known deep learning architecture extended from artificial neural network, has been extensively adopted in various applications, which include video surveillance, mobile robot vision, image search engine in data centers, etc [6] [7] [8] [10] [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1923924,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80bfcf1be2bf1b95cc6f36d229665cdf22d76190",
            "isKey": false,
            "numCitedBy": 4241,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the automated recognition of human actions in surveillance videos. Most current methods build classifiers based on complex handcrafted features computed from the raw inputs. Convolutional neural networks (CNNs) are a type of deep model that can act directly on the raw inputs. However, such models are currently limited to handling 2D inputs. In this paper, we develop a novel 3D CNN model for action recognition. This model extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames. The developed model generates multiple channels of information from the input frames, and the final feature representation combines information from all channels. To further boost the performance, we propose regularizing the outputs with high-level features and combining the predictions of a variety of different models. We apply the developed models to recognize human actions in the real-world environment of airport surveillance videos, and they achieve superior performance in comparison to baseline methods."
            },
            "slug": "3D-Convolutional-Neural-Networks-for-Human-Action-Ji-Xu",
            "title": {
                "fragments": [],
                "text": "3D Convolutional Neural Networks for Human Action Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel 3D CNN model for action recognition that extracts features from both the spatial and the temporal dimensions by performing 3D convolutions, thereby capturing the motion information encoded in multiple adjacent frames."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32837403"
                        ],
                        "name": "J. Bergstra",
                        "slug": "J.-Bergstra",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Bergstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bergstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 279
                            }
                        ],
                        "text": "Convolutional neural network (CNN), a well-known deep learning architecture extended from artificial neural network, has been extensively adopted in various applications, which include video surveillance, mobile robot vision, image search engine in data centers, etc [6] [7] [8] [10] [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14805281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8012351bc5ebce4a4b3039bbbba3ce393bc3315",
            "isKey": false,
            "numCitedBy": 983,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, several learning algorithms relying on models with deep architectures have been proposed. Though they have demonstrated impressive performance, to date, they have only been evaluated on relatively simple problems such as digit recognition in a controlled environment, for which many machine learning algorithms already report reasonable results. Here, we present a series of experiments which indicate that these models show promise in solving harder learning problems that exhibit many factors of variation. These models are compared with well-established algorithms such as Support Vector Machines and single hidden-layer feed-forward neural networks."
            },
            "slug": "An-empirical-evaluation-of-deep-architectures-on-of-Larochelle-Erhan",
            "title": {
                "fragments": [],
                "text": "An empirical evaluation of deep architectures on problems with many factors of variation"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A series of experiments indicate that these models with deep architectures show promise in solving harder learning problems that exhibit many factors of variation."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111143430"
                        ],
                        "name": "Samuel Williams",
                        "slug": "Samuel-Williams",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145492285"
                        ],
                        "name": "Andrew Waterman",
                        "slug": "Andrew-Waterman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Waterman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Waterman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052996765"
                        ],
                        "name": "David A. Patterson",
                        "slug": "David-A.-Patterson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Patterson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Patterson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5703612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "092217c2267f6e0673590aa151d811e579ff7760",
            "isKey": false,
            "numCitedBy": 1897,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "The Roofline model offers insight on how to improve the performance of software and hardware."
            },
            "slug": "Roofline:-an-insightful-visual-performance-model-Williams-Waterman",
            "title": {
                "fragments": [],
                "text": "Roofline: an insightful visual performance model for multicore architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "The Roofline model offers insight on how to improve the performance of software and hardware in the rapidly changing world of connected devices."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. of the"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 234
                            }
                        ],
                        "text": "Among these approaches, FPGA based accelerators have attracted more and more attention of researchers because they have advantages of good performance, high energy efficiency, fast development round, and capability of reconfiguration [1] [2] [3] [6] [12] [14]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "First, work [1] [2] [3] [6] [14] mainly focused on computation engine optimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Accelerating deep neural networks on mobile processor with embedded programmable logic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "As a classical supervised learning algorithm, CNN employs a feedforward process for recognition and a backward path for training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving photo search: A step across the semantic gap. http://googleresearch.blogspot.com/ 2013/06/improving-photo-search-step-across.html"
            },
            "venue": {
                "fragments": [],
                "text": "Improving photo search: A step across the semantic gap. http://googleresearch.blogspot.com/ 2013/06/improving-photo-search-step-across.html"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Improving photo search: A step across the semantic gap"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 8
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 19,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Optimizing-FPGA-based-Accelerator-Design-for-Deep-Zhang-Li/7c91eb0f9bbae8e2d3d007db73b8422b61ed1d68?sort=total-citations"
}