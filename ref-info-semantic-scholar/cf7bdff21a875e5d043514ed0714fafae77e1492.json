{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066182179"
                        ],
                        "name": "Peter D\u00fcrr",
                        "slug": "Peter-D\u00fcrr",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "D\u00fcrr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D\u00fcrr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2288013"
                        ],
                        "name": "C. Mattiussi",
                        "slug": "C.-Mattiussi",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Mattiussi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mattiussi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "When compared to NEAT, AGE reported equal performance on a non-Markovian double-pole balancing problem [ 16 ], while both algorithms performed better than a developmental encoding (CE) and a coevolution method (ESP)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2.3) outperformed ESP on these benchmarks [ 16 , 79]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 382293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bbea267e80e64b2282a1e14ac7dbafd4f3e1f0e3",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The evolution of artificial neural networks (ANNs) is often used to tackle difficult control problems. There are different approaches to the encoding of neural networks in artificial genomes. Analog Genetic Encoding (AGE) is a new implicit method derived from the observation of biological genetic regulatory networks. This paper shows how AGE can be used to simultaneously evolve the topology and the weights of ANNs for complex control systems. AGE is applied to a standard benchmark problem and we show that its performance is equivalent or superior to some of the most powerful algorithms for neuroevolution in the literature."
            },
            "slug": "Neuroevolution-with-Analog-Genetic-Encoding-D\u00fcrr-Mattiussi",
            "title": {
                "fragments": [],
                "text": "Neuroevolution with Analog Genetic Encoding"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper shows how AGE can be used to simultaneously evolve the topology and the weights of ANNs for complex control systems and shows that its performance is equivalent or superior to some of the most powerful algorithms for neuroevolution in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "PPSN"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Baxter [ 6 ] encoded both the architecture and whether a synapse could be modified by a simple Hebb rule (the rule was predetermined)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15237898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b332d94a2ea0c9ff6507f074b83c58e9da4d2439",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigate a neural network model in which weights between computational nodes are modiied according to a local learning rule. To determine whether local learning rules are suucient for learning, we encode the network architectures and learning dynamics genetically and then apply selection pressure to evolve networks capable of learning the four boolean functions of one variable. The successful networks are analysed and we show how learning behaviour emerges as a distributed property of the entire network. Finally the utility of genetic algorithms as a tool of discovery is discussed."
            },
            "slug": "The-evolution-of-learning-algorithms-for-artificial-Baxter",
            "title": {
                "fragments": [],
                "text": "The evolution of learning algorithms for artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A neural network model in which weights between computational nodes are modiied according to a local learning rule is investigated and it is shown how learning behaviour emerges as a distributed property of the entire network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723475"
                        ],
                        "name": "D. Montana",
                        "slug": "D.-Montana",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Montana",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Montana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144286518"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In a classic study, Montana and Davis [ 51 ] compared the performance of synaptic weight evolution with a discrete direct representation with that of the back-propagation"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6336712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09920800fa7841c84a551d70c6101d9510e6fcc8",
            "isKey": false,
            "numCitedBy": 1119,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayered feedforward neural networks possess a number of properties which make them particularly suited to complex pattern classification problems. However, their application to some realworld problems has been hampered by the lack of a training algonthm which reliably finds a nearly globally optimal set of weights in a relatively short time. Genetic algorithms are a class of optimization procedures which are good at exploring a large and complex space in an intelligent way to find values close to the global optimum. Hence, they are well suited to the problem of training feedforward networks. In this paper, we describe a set of experiments performed on data from a sonar image classification problem. These experiments both 1) illustrate the improvements gained by using a genetic algorithm rather than backpropagation and 2) chronicle the evolution of the performance of the genetic algorithm as we added more and more domain-specific knowledge into it."
            },
            "slug": "Training-Feedforward-Neural-Networks-Using-Genetic-Montana-Davis",
            "title": {
                "fragments": [],
                "text": "Training Feedforward Neural Networks Using Genetic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A set of experiments performed on data from a sonar image classification problem are described to illustrate the improvements gained by using a genetic algorithm rather than backpropagation and chronicle the evolution of the performance of the genetic algorithm as it added more and more domain-specific knowledge into it."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748824"
                        ],
                        "name": "C. Igel",
                        "slug": "C.-Igel",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Igel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Igel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 216
                            }
                        ],
                        "text": "Recent benchmark experiments with Evolution Strategies [62], which use a floating-point representation of the synaptic weights, have reported excellent performance with direct encoding of a small, fixed architecture [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15808719,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "04377cba52732168d3e55e229bae2ee1ad4d43a2",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply the CMA-ES, an evolution strategy which efficiently adapts the covariance matrix of the mutation distribution, to the optimization of the weights of neural networks for solving reinforcement learning problems. It turns out that the topology of the networks considerably influences the time to find a suitable control strategy. Still, our results with fixed network topologies are significantly better than those reported for the best evolutionary method so far, which adapts both the weights and the structure of the networks."
            },
            "slug": "Neuroevolution-for-reinforcement-learning-using-Igel",
            "title": {
                "fragments": [],
                "text": "Neuroevolution for reinforcement learning using evolution strategies"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "The CMA-ES is applied to the optimization of the weights of neural networks for solving reinforcement learning problems, and results with fixed network topologies are significantly better than those reported for the best evolutionary method so far."
            },
            "venue": {
                "fragments": [],
                "text": "The 2003 Congress on Evolutionary Computation, 2003. CEC '03."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859405"
                        ],
                        "name": "David E. Moriarty",
                        "slug": "David-E.-Moriarty",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Moriarty",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David E. Moriarty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 116
                            }
                        ],
                        "text": "The fitness of each neuron was defined as the average fitness of all the networks it had participated in. Gomez and Miikkulainen [27] extended this approach by segregating the neurons in subpopulations with a method they called enforced subpopulations (ESP)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "To overcome these problems, Moriarty and Miikkulainen [52] suggested to evolve individual neurons to cooperate in networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 14
                            }
                        ],
                        "text": "Reisinger and Miikkulainen [65] showed that an implicit encoding very similar to AGE outperforms NEAT on a complex board-game task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Redrawn from Moriarty and Miikkulainen [52] Evol."
                    },
                    "intents": []
                }
            ],
            "corpusId": 608769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb01aa49fdd9b59127b8651b36fa187095097799",
            "isKey": true,
            "numCitedBy": 197,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a new reinforcement learning method called SANE (Symbiotic, Adaptive Neuro-Evolution), which evolves a population of neurons through genetic algorithms to form a neural network capable of performing a task. Symbiotic evolution promotes both cooperation and specialization, which results in a fast, efficient genetic search and discourages convergence to suboptimal solutions. In the inverted pendulum problem, SANE formed effective networks 9 to 16 times faster than the Adaptive Heuristic Critic and 2 times faster thanQ-learning and the GENITOR neuro-evolution approach without loss of generalization. Such efficient learning, combined with few domain assumptions, make SANE a promising approach to a broad range of reinforcement learning problems, including many real-world applications."
            },
            "slug": "Efficient-reinforcement-learning-through-symbiotic-Moriarty-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Efficient reinforcement learning through symbiotic evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A new reinforcement learning method called SANE (Symbiotic, Adaptive Neuro-Evolution), which evolves a population of neurons through genetic algorithms to form a neural network capable of performing a task, is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846883"
                        ],
                        "name": "Kenneth O. Stanley",
                        "slug": "Kenneth-O.-Stanley",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Stanley",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth O. Stanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Redrawn from Stanley and Miikkulainen [79] 50 Evol."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "Neuro-evolution of augmenting topologies (NEAT) is a method for genetically encoding and evolving the architecture and the weights of neural networks [79]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "Neuro-evolution of augmenting topologies (NEAT) has been applied to many problems such as pole balancing [79], robot control [80], computer games [66, 82] or an automobile crash warning system [81]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 40
                            }
                        ],
                        "text": "3) outperformed ESP on these benchmarks [16, 79]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 14
                            }
                        ],
                        "text": "Reisinger and Miikkulainen [65] showed that an implicit encoding very similar to AGE outperforms NEAT on a complex board-game task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "To overcome these problems, Moriarty and Miikkulainen [52] suggested to evolve individual neurons to cooperate in networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 116
                            }
                        ],
                        "text": "The fitness of each neuron was defined as the average fitness of all the networks it had participated in. Gomez and Miikkulainen [27] extended this approach by segregating the neurons in subpopulations with a method they called enforced subpopulations (ESP)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 498161,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d03c916d49268d48fde3b76a68e64af7761835e7",
            "isKey": true,
            "numCitedBy": 2994,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signicantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution."
            },
            "slug": "Evolving-Neural-Networks-through-Augmenting-Stanley-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Evolving Neural Networks through Augmenting Topologies"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A method is presented, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task and shows how it is possible for evolution to both optimize and complexify solutions simultaneously."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2937141"
                        ],
                        "name": "Brian Yamauchi",
                        "slug": "Brian-Yamauchi",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Yamauchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Yamauchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760775"
                        ],
                        "name": "R. Beer",
                        "slug": "R.-Beer",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Beer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Beer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "For example, Yamauchi and Beer [90] evolved CTRNNs capable of generating and learning short sequences without synaptic plasticity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1167765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45eeee4e13639f33b6655795ac4929732faa9352",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This article explores the use of a real-valued modular genetic algorithm to evolve continuous-time recurrent neural networks capable of sequential behavior and learning. We evolve networks that can generate a fixed sequence of outputs in response to an external trigger occurring at varying intervals of time. We also evolve networks that can learn to generate one of a set of possible sequences based on reinforcement from the environment. Finally, we utilize concepts from dynamical systems theory to understand the operation of some of these evolved networks. A novel feature of our approach is that we assume neither an a priori discretization of states or time nor an a priori learning algorithm that explicitly modifies network parameters during learning. Rather, we merely expose dynamical neural networks to tasks that require sequential behavior and learning and allow the genetic algorithm to evolve network dynamics capable of accomplishing these tasks."
            },
            "slug": "Sequential-Behavior-and-Learning-in-Evolved-Neural-Yamauchi-Beer",
            "title": {
                "fragments": [],
                "text": "Sequential Behavior and Learning in Evolved Dynamical Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This article explores the use of a real-valued modular genetic algorithm to evolve continuous-time recurrent neural networks capable of sequential behavior and learning and utilizes concepts from dynamical systems theory to understand the operation of some of these evolved networks."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072252"
                        ],
                        "name": "D. Chalmers",
                        "slug": "D.-Chalmers",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Chalmers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Chalmers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Chalmers [11] suggested to describe this function as a linear combination of the products between the variables weighted by constants."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10538501,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "74fd7e2750614b8b405a9e45d639331c3ed9d811",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Evolution-of-Learning:-An-Experiment-in-Genetic-Chalmers",
            "title": {
                "fragments": [],
                "text": "The Evolution of Learning: An Experiment in Genetic Connectionism"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760775"
                        ],
                        "name": "R. Beer",
                        "slug": "R.-Beer",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Beer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Beer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34741198"
                        ],
                        "name": "J. Gallagher",
                        "slug": "J.-Gallagher",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Gallagher",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gallagher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 13
                            }
                        ],
                        "text": "Evolution of CTRNNs has been applied to difficult problems such as robot control [86]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 44
                            }
                        ],
                        "text": "For example, Yamauchi and Beer [90] evolved CTRNNs capable of generating and learning short sequences without synaptic plasticity."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 151
                            }
                        ],
                        "text": "3.1 Continuous-time recurrent neural networks\nOne of the most widely-used dynamic neuron models is called continuous-time recurrent neural network, or CTRNN [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 189
                            }
                        ],
                        "text": "As evolution favored rules that led to succesful behavior, the set of rules adapted to the requirements of the task.\ntested networks, only the GasNets achieved a cyclic locomotion, but the CTRNNs achieved a higher average fitness."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "CTRNNs display rich dynamics and represent a first approximation of the timedependent processes that occur at the membrane of biological neurons."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "A simple way of implementing a spiking neuron is to take the dynamic model of a CTRNN neuron and substitute the output function with an element that compares the neuron activation with its threshold followed by a pulse generator that takes the form of a Dirac function (see Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 27
                            }
                        ],
                        "text": "An interesting property of CTRNNs is that they can display learning behavior without synaptic plasticity because they can store intermediate states in the activation function of internal neurons."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 111
                            }
                        ],
                        "text": "One of the most widely-used dynamic neuron models is called continuous-time recurrent neural network, or CTRNN [7]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 76
                            }
                        ],
                        "text": "In another set of experiments, McHale and Husbands [48] compared GasNets to CTRNNs and conventional ANNs with Hebbian learning in a robot locomotion control tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 78
                            }
                        ],
                        "text": "While being one of the simplest nonlinear, continuous dynamic network models, CTRNNs are universal dynamic approximators [24]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 42196865,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b03528ebfa23050a744da5a80fe85b7552e37a4d",
            "isKey": true,
            "numCitedBy": 624,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We would like the behavior of the artificial agents that we construct to be as well-adapted to their environments as natural animals are to theirs. Unfortunately, designing controllers with these properties is a very difficult task. In this article, we demonstrate that continuous-time recurrent neural networks are a viable mechanism for adaptive agent control and that the genetic algorithm can be used to evolve effective neural controllers. A significant advantage of this approach is that one need specify only a measure of an agent's overall performance rather than the precise motor output trajectories by which it is achieved. By manipulating the performance evaluation, one can place selective pressure on the development of controllers with desired properties. Several novel controllers have been evolved, including a chemotaxis controller that switches between different strategies depending on environmental conditions, and a locomotion controller that takes advantage of sensory feedback if available but that can operate in its absence if necessary."
            },
            "slug": "Evolving-Dynamical-Neural-Networks-for-Adaptive-Beer-Gallagher",
            "title": {
                "fragments": [],
                "text": "Evolving Dynamical Neural Networks for Adaptive Behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that continuous-time recurrent neural networks are a viable mechanism for adaptive agent control and that the genetic algorithm can be used to evolve effective neural controllers."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35004790"
                        ],
                        "name": "D. Federici",
                        "slug": "D.-Federici",
                        "structuredName": {
                            "firstName": "Diego",
                            "lastName": "Federici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Federici"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Federici [17] combined an integrate and fire neuron model with a correlation-based synaptic plasticity model and a developmental encoding."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 23835420,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "548817cb254a00c4ac53e422af8cb0a7c984b43a",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Indirect encoding strategies aim at higher evolvability by reducing the dimensionality of the search space. If on one hand scalability is often improved for specific tasks, on the other the generality of these methods can be limited. In previous work, a development system was introduced and tested in the evolution of specific 2D morphologies of various size and complexity. Here the same model is used to instead specify the structure and properties of neuro-controllers for simulated Khepera robots. In this paper, we introduce a plastic spiking neural network model, particularly suited for evolution and development, testing its performance against direct encoding. Compared to previous work, the new task implies the solution of a functional problem. Nevertheless, results show similar conclusions regarding the improved scalability of the development system and its connection to regularity"
            },
            "slug": "Evolving-developing-spiking-neural-networks-Federici",
            "title": {
                "fragments": [],
                "text": "Evolving developing spiking neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A plastic spiking neural network model, particularly suited for evolution and development, is introduced, testing its performance against direct encoding and results show similar conclusions regarding the improved scalability of the development system and its connection to regularity."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Congress on Evolutionary Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682655"
                        ],
                        "name": "R. Belew",
                        "slug": "R.-Belew",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Belew",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Belew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49268728"
                        ],
                        "name": "J. McInerney",
                        "slug": "J.-McInerney",
                        "structuredName": {
                            "firstName": "John G.",
                            "lastName": "McInerney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McInerney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] found that the best evolved networks employed learning rates ten times higher than values suggested before (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60923273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5378dbf09e75d133cafdcb388bb5546f91b2a02",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "It is appealing to consider hybrids of neural-network learning algorithms with evolutionary search procedures, simply because Nature has so successfully done so. In fact, computational models of learning and evolution ooer theoretical biology new tools for addressing questions about Nature that have dogged that eld since Darwin Belew, 1990]. The concern of this paper, however, is strictly artiicial: Can hybrids of connectionist learning algorithms and genetic algorithms produce more eecient and eeective algorithms than either technique applied in isolation? The paper begins with a survey of recent work (by us and others) that combines Holland's Genetic Algorithm (GA) with con-nectionist techniques and delineates some of the basic design problems these hybrids share. This analysis suggests the dangers of overly literal representations of the network on the genome (e.g., encoding each weight explicitly). A preliminary set of experiments that use the GA to nd unusual but successful values for BP parameters (learning rate, momentum) are also reported. The focus of the report is a series of experiments that use the GA to explore the space of initial weight values , from which two diierent gradient techniques (conjugate gradient and back propagation) are then allowed to optimize. We nd that use of the GA provides much greater conndence in the face of the stochas-tic variation that can plague gradient techniques, and can also allow training times to be reduced by as much as two orders of magnitude. Computational trade-oos between BP and the GA are considered, including discussion of a software facility that exploits the parallelism inherent in GA/BP hybrids. This evidence leads us to conclude that the GA's global sampling characteristics compliment connectionist local search techniques well, leading to eecient and reliable hybrids."
            },
            "slug": "Evolving-networks:-using-the-genetic-algorithm-with-Belew-McInerney",
            "title": {
                "fragments": [],
                "text": "Evolving networks: using the genetic algorithm with connectionist learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A survey of recent work that combines Holland's Genetic Algorithm with con-nectionist techniques and delineates some of the basic design problems these hybrids share concludes that the GA's global sampling characteristics compliment connectionist local search techniques well, leading to eecient and reliable hybrids."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745219"
                        ],
                        "name": "S. Schaal",
                        "slug": "S.-Schaal",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Schaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schaal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8038419"
                        ],
                        "name": "A. Ijspeert",
                        "slug": "A.-Ijspeert",
                        "structuredName": {
                            "firstName": "Auke",
                            "lastName": "Ijspeert",
                            "middleNames": [
                                "Jan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ijspeert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807928"
                        ],
                        "name": "A. Billard",
                        "slug": "A.-Billard",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Billard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Billard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144575699"
                        ],
                        "name": "S. Vijayakumar",
                        "slug": "S.-Vijayakumar",
                        "structuredName": {
                            "firstName": "Sethu",
                            "lastName": "Vijayakumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vijayakumar"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 76
                            }
                        ],
                        "text": "Another approach to evolving neuromodulatory architectures was suggested by Husbands et al. [37]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "Husbands et al. [36] proposed a similar method where the connections grew according to a set of differential equations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "In another set of experiments, McHale and Husbands [48] compared GasNets to CTRNNs and conventional ANNs with Hebbian learning in a robot locomotion control tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64236162,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "45094847ab9ae017adc4a9de2511a177ec8649af",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary robotics relies upon techniques involving the evolution of artificial neural networks to synthesize sensorimotor control systems for actual or physically simulated robots. This paper is a comparative study of three principal types of artificial neural networks; the Continuous Time Recurrent Neural Network (CTRNN), the Plastic Neural Network (PNN) and the GasNet. An attempt is made to evolve networks capable of achieving locomotion with a physically simulated biped. Of the 14 distinct networks tested, GasNets were the only network to achieve cyclical locomotion, although CTRNNs were able to attain a higher level of average fitness."
            },
            "slug": "GasNets-and-other-Evolvable-Neural-Networks-applied-Schaal-Ijspeert",
            "title": {
                "fragments": [],
                "text": "GasNets and other Evolvable Neural Networks applied to Bipedal Locomotion"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Of the 14 distinct networks tested, GasNets were the only network to achieve cyclical locomotion, although CTRNNs were able to attain a higher level of average fitness."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727799"
                        ],
                        "name": "F. Mondada",
                        "slug": "F.-Mondada",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Mondada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Mondada"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Floreano and Mondada [20] allowed evolution to choose among four Hebbian learning rules for each synaptic connection and evaluated the approach for a mobile robot requested to solve a sequential task involving multiple\nsensory modalities and environmental landmarks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Floreano and Mondada [20] allowed evolution to choose among four Hebbian learning rules for each synaptic connection and evaluated the approach for a mobile robot requested to solve a sequential task involving multiple sensory modalities and environmental landmarks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15172960,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f9d515d0416805e75125260a42c43e323ec7af89",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigate a novel approach to the evolutionary development of autonomous situated agents based on the assumption that the neural mechanisms underlying ontogenetic learning are themselves developed and shaped by evolutionary process. A genetic algorithm is used to evolve neural structures that can be continuously modified during life according to the mechanisms specified in the genotype. The evolutionary process is carried out on a real mobile robot. The analysis of one of the best evolved individuals shows rapid development of stable behavior mediated by fast-changing synapses which are dynamically stable."
            },
            "slug": "Evolution-of-Plastic-Neurocontrollers-for-Situated-Floreano-Mondada",
            "title": {
                "fragments": [],
                "text": "Evolution of Plastic Neurocontrollers for Situated Agents"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A novel approach to the evolutionary development of autonomous situated agents based on the assumption that the neural mechanisms underlying ontogenetic learning are themselves developed and shaped by evolutionary process is investigated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015062"
                        ],
                        "name": "S. Nolfi",
                        "slug": "S.-Nolfi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Nolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nolfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762202"
                        ],
                        "name": "D. Parisi",
                        "slug": "D.-Parisi",
                        "structuredName": {
                            "firstName": "Domenico",
                            "lastName": "Parisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Parisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Nolfi and Parisi [56] evolved a neural controller for a mobile robot whose output layer included two \u2018\u2018teaching neurons\u2019\u2019 that were used to modify the connection weights from the sensory neurons to the motor neurons with backpropagation learning during the robot\u2019s lifetime."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "This predisposition to learn involved several aspects such as a tendency to move so as to experience useful learning experiences and a tendency to acquire useful adaptive characters through learning [56]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2003070,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "0256dcd57230253bd24cd98f387f45cd00de063e",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "To study learning as an adaptive process, one must take into consideration the role of evolution, which is the primary adaptive process. In addition, learning should be studied in (artificial) organisms that live in an independent physical environment in such a way that the input from the environment can be at least partially controlled by the organisms' behavior. To explore these issues, we used a genetic algorithm to simulate the evolution of a population of neural networks, each controlling the behavior of a small mobile robot that must explore efficiently an environment surrounded by walls. Because the environment changes from one generation to the next, each network must learn during its life to adapt to the particular environment into which it happens to be born. We found that evolved networks incorporate a genetically inherited predisposition to learn that can be described as (1) the presence of initial conditions that tend to canalize learning in the right directions; (2) the tendency to behave in a way that enhances the perceived differences between different environments and determines input stimuli that facilitate the learning of adaptive changes; and (3) the ability to reach desirable stable states."
            },
            "slug": "Learning-to-Adapt-to-Changing-Environments-in-Nolfi-Parisi",
            "title": {
                "fragments": [],
                "text": "Learning to Adapt to Changing Environments in Evolving Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A genetic algorithm is used to simulate the evolution of a population of neural networks, each controlling the behavior of a small mobile robot that must explore efficiently an environment surrounded by walls, and found that evolved networks incorporate a genetically inherited predisposition to learn."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "Among others, (see Yao [91] for more examples), Gruau [28] proposed a genetic encoding scheme for neural networks based on a cellular duplication and differentiation process, i.e., a cellular encoding (CE)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Among others, (see Yao [91] for more examples), Gruau [28] proposed a genetic encoding scheme for neural net-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13958007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ac303258fd7f522fd3e4f172b97bb17eb888598",
            "isKey": false,
            "numCitedBy": 1175,
            "numCiting": 363,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANN\u2019s) in recent years. This paper: 1) reviews different combinations between ANN\u2019s and evolutionary algorithms (EA\u2019s), including using EA\u2019s to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EA\u2019s; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANN\u2019s and EA\u2019s can lead to significantly better intelligent systems than relying on ANN\u2019s or EA\u2019s alone."
            },
            "slug": "Evolving-Artificial-Neural-Networks-Yao",
            "title": {
                "fragments": [],
                "text": "Evolving Artificial Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown, through a considerably large literature review, that combinations between ANN\u2019s and EA\u2019\u2019 can lead to significantly better intelligent systems than relying on ANNs or EA\u201ds alone."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015062"
                        ],
                        "name": "S. Nolfi",
                        "slug": "S.-Nolfi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Nolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nolfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2267481"
                        ],
                        "name": "O. Miglino",
                        "slug": "O.-Miglino",
                        "structuredName": {
                            "firstName": "Orazio",
                            "lastName": "Miglino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Miglino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762202"
                        ],
                        "name": "D. Parisi",
                        "slug": "D.-Parisi",
                        "structuredName": {
                            "firstName": "Domenico",
                            "lastName": "Parisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Parisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14019289,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "8a38793a02d5d87af001ffd0105815ed610ea020",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model based on genetic algorithm and neural networks. The neural networks develop on the basis of an inherited genotype but they show phenotypic plasticity, i.e. they develop in ways that are adapted to the specific environment The genotype-to-phenotype mapping is not abstractly conceived as taking place in a single instant but is a temporal process that takes a substantial portion of an individual's lifetime to complete and is sensitive to the particular environment in which the individual happens to develop. Furthermore, the respective roles of the genotype and of the environment are not decided a priori but are part of what evolves. We show how such a model is able to evolve control systems for autonomous robots that can adapt to different types of environments."
            },
            "slug": "Phenotypic-plasticity-in-evolving-neural-networks-Nolfi-Miglino",
            "title": {
                "fragments": [],
                "text": "Phenotypic plasticity in evolving neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "It is shown how a model based on genetic algorithm and neural networks is able to evolve control systems for autonomous robots that can adapt to different types of environments."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of PerAc '94. From Perception to Action"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694198"
                        ],
                        "name": "J. Urzelai",
                        "slug": "J.-Urzelai",
                        "structuredName": {
                            "firstName": "Joseba",
                            "lastName": "Urzelai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Urzelai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 221,
                                "start": 217
                            }
                        ],
                        "text": "results indicated that the evolved robots had the ability to adapt on-the-fly to changed environmental conditions (including spatial reorganization, textures and even robot morphologies) without incremental evolution [87]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13546513,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0562b5dc160f9f4469a3b9d9d5748516cac0be04",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper is concerned with adaptation capabilities of evolved neural controllers. We propose to evolve mechanisms for parameter self-organization instead of evolving the parameters themselves. The method consists of encoding a set of local adaptation rules that synapses follow while the robot freely moves in the environment. In the experiments presented here, the performance of the robot is measured in environments that are different in significant ways from those used during evolution. The results show that evolutionary adaptive controllers solve the task much faster and better than evolutionary standard fixed-weight controllers, that the method scales up well to large architectures, and that evolutionary adaptive controllers can adapt to environmental changes that involve new sensory characteristics (including transfer from simulation to reality and across different robotic platforms) and new spatial relationships."
            },
            "slug": "Evolution-of-Adaptive-Synapses:-Robots-with-Fast-in-Urzelai-Floreano",
            "title": {
                "fragments": [],
                "text": "Evolution of Adaptive Synapses: Robots with Fast Adaptive Behavior in New Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The results show that evolutionary adaptive controllers solve the task much faster and better than evolutionary standard fixed-weight controllers, that the method scales up well to large architectures, and that they can adapt to environmental changes that involve new sensory characteristics and new spatial relationships."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2729994"
                        ],
                        "name": "Elio Tuci",
                        "slug": "Elio-Tuci",
                        "structuredName": {
                            "firstName": "Elio",
                            "lastName": "Tuci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elio Tuci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121100119"
                        ],
                        "name": "Matt Quinn",
                        "slug": "Matt-Quinn",
                        "structuredName": {
                            "firstName": "Matt",
                            "lastName": "Quinn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matt Quinn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145472053"
                        ],
                        "name": "I. Harvey",
                        "slug": "I.-Harvey",
                        "structuredName": {
                            "firstName": "Inman",
                            "lastName": "Harvey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Harvey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "Evolution of CTRNNs has been applied to difficult problems such as robot control [86]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 415147,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "409c51120a48d87a8a240309e19c43b57ace62f2",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "We are interested in the construction of ecological models of the evolution of learning behavior using methodological tools developed in the field of evolutionary robotics. In this article, we explore the applicability of integrated (i.e., nonmodular) neural networks with fixed connection weights and simple 'leaky-integrator' neurons as controllers for autonomous learning robots. In contrast to Yamauchi and Beer (1994a), we show that such a control system is capable of integrating reactive and learned behaviour without explicitly needing hand-designed modules, dedicated to a particular behavior, or an externally introduced reinforcement signal. In our model, evolutionary and ecological contingencies structure the controller and the behavioral responses of the robot. This allows us to concentrate on examining the conditions under which learning behavior evolves."
            },
            "slug": "An-Evolutionary-Ecological-Approach-to-the-Study-of-Tuci-Quinn",
            "title": {
                "fragments": [],
                "text": "An Evolutionary Ecological Approach to the Study of Learning Behavior Using a Robot-Based Model"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This article explores the applicability of integrated neural networks with fixed connection weights and simple 'leaky-integrator' neurons as controllers for autonomous learning robots and shows that such a control system is capable of integrating reactive and learned behaviour without explicitly needing hand-designed modules, dedicated to a particular behavior, or an externally introduced reinforcement signal."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846883"
                        ],
                        "name": "Kenneth O. Stanley",
                        "slug": "Kenneth-O.-Stanley",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Stanley",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth O. Stanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "Neuro-evolution of augmenting topologies (NEAT) has been applied to many problems such as pole balancing [79], robot control [80], computer games [66, 82] or an automobile crash warning system [81]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11881625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "73f7aa7d1b21ce637e25ffa4314e8ff877f29abd",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Two major goals in machine learning are the discovery and improvement of solutions to complex problems. In this paper, we argue that complexification, i.e. the incremental elaboration of solutions through adding new structure, achieves both these goals. We demonstrate the power of complexification through the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves increasingly complex neural network architectures. NEAT is applied to an open-ended coevolutionary robot duel domain where robot controllers compete head to head. Because the robot duel domain supports a wide range of strategies, and because coevolution benefits from an escalating arms race, it serves as a suitable testbed for studying complexification. When compared to the evolution of networks with fixed structure, complexifying evolution discovers significantly more sophisticated strategies. The results suggest that in order to discover and improve complex solutions, evolution, and search in general, should be allowed to complexify as well as optimize."
            },
            "slug": "Competitive-Coevolution-through-Evolutionary-Stanley-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Competitive Coevolution through Evolutionary Complexification"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "It is argued that complexification, i.e. the incremental elaboration of solutions through adding new structure, achieves both these goals and is demonstrated through the NeuroEvolution of Augmenting Topologies (NEAT) method, which evolves increasingly complex neural network architectures."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917876"
                        ],
                        "name": "P. Husbands",
                        "slug": "P.-Husbands",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Husbands",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Husbands"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108718870"
                        ],
                        "name": "Tom Smith",
                        "slug": "Tom-Smith",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2429322"
                        ],
                        "name": "N. Jakobi",
                        "slug": "N.-Jakobi",
                        "structuredName": {
                            "firstName": "Nick",
                            "lastName": "Jakobi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Jakobi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401010208"
                        ],
                        "name": "M. O\u2019Shea",
                        "slug": "M.-O\u2019Shea",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "O\u2019Shea",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. O\u2019Shea"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7583158,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b79c5b30e833f232d91d6783ab462fd4ff2dd622",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new type of artificial neural network (GasNets) and shows that it is possible to use evolutionary computing techniques to find robot controllers based on them. The controllers are built from networks inspired by the modulatory effects of freely diffusing gases, especially nitric oxide, in real neuronal networks. Evolutionary robotics techniques were used to develop control networks and visual morphologies to enable a robot to achieve a target discrimination task under very noisy lighting conditions. A series of evolutionary runs with and without the gas modulation active demonstrated that networks incorporating modulation by diffusing gases evolved to produce successful controllers considerably faster than networks without this mechanism. GasNets also consistently achieved evolutionary success in far fewer evaluations than were needed when using more conventional connectionist style networks."
            },
            "slug": "Better-Living-Through-Chemistry:-Evolving-GasNets-Husbands-Smith",
            "title": {
                "fragments": [],
                "text": "Better Living Through Chemistry: Evolving GasNets for Robot Control"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A new type of artificial neural network (GasNets) is introduced and shows that it is possible to use evolutionary computing techniques to find robot controllers based on them and consistently achieved evolutionary success in far fewer evaluations than were needed when using more conventional connectionist style networks."
            },
            "venue": {
                "fragments": [],
                "text": "Connect. Sci."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917876"
                        ],
                        "name": "P. Husbands",
                        "slug": "P.-Husbands",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Husbands",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Husbands"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32813862"
                        ],
                        "name": "G. McHale",
                        "slug": "G.-McHale",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "McHale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. McHale"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60058404,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "98397060385539c50c1dc8f7078f752c67c28a96",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary robotics relies upon techniques involving the evolution of artificial neural networks to synthesize sensorimotor control systems for actual or physically simulated robots. This paper is a comparative study of three principal types of artificial neural networks; the Continuous Time Recurrent Neural Network (CTRNN), the Plastic Neural Network (PNN) and the GasNet. An attempt is made to evolve networks capable of achieving locomotion with a physically simulated biped. Of the 14 distinct networks tested, GasNets were the only network to achieve cyclical locomotion, although CTRNNs were able to attain a higher level of average fitness."
            },
            "slug": "GasNets-and-other-evovalble-neural-networks-applied-Husbands-McHale",
            "title": {
                "fragments": [],
                "text": "GasNets and other evovalble neural networks applied to bipedal locomotion"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Of the 14 distinct networks tested, GasNets were the only network to achieve cyclical locomotion, although CTRNNs were able to attain a higher level of average fitness."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2288013"
                        ],
                        "name": "C. Mattiussi",
                        "slug": "C.-Mattiussi",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Mattiussi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mattiussi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12780567,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "7d5593f8935306c7ff39fa1f75ed64fa25543c30",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a set of preliminary experiments to evolve spiking neural controllers for a vision-based mobile robot. All the evolutionary experiments are carried out on physical robots without human intervention. After discussing how to implement and interface these neurons with a physical robot, we show that evolution finds relatively quickly functional spiking controllers capable of navigating in irregularly textured environments without hitting obstacles using a very simple genetic encoding and fitness function. Neuroethological analysis of the network activity let us understand the functioning of evolved controllers and tell the relative importance of single neurons independently of their observed firing rate. Finally, a number of systematic lesion experiments indicate that evolved spiking controllers are very robust to synaptic strength decay that typically occurs in hardware implementations of spiking circuits."
            },
            "slug": "Evolution-of-Spiking-Neural-Controllers-for-Robots-Floreano-Mattiussi",
            "title": {
                "fragments": [],
                "text": "Evolution of Spiking Neural Controllers for Autonomous Vision-Based Robots"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that evolution finds relatively quickly functional spiking controllers capable of navigating in irregularly textured environments without hitting obstacles using a very simple genetic encoding and fitness function."
            },
            "venue": {
                "fragments": [],
                "text": "EvoRobots"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145285040"
                        ],
                        "name": "L. D. Whitley",
                        "slug": "L.-D.-Whitley",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Whitley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Whitley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209955"
                        ],
                        "name": "T. Starkweather",
                        "slug": "T.-Starkweather",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Starkweather",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starkweather"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143882377"
                        ],
                        "name": "C. Bogart",
                        "slug": "C.-Bogart",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bogart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bogart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "for a different classification task by other authors [88]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6273216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d52b087713f79b7b1d3fe2112e9cfa3bad221bb",
            "isKey": false,
            "numCitedBy": 742,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Genetic-algorithms-and-neural-networks:-optimizing-Whitley-Starkweather",
            "title": {
                "fragments": [],
                "text": "Genetic algorithms and neural networks: optimizing connections and connectivity"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Comput."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694724"
                        ],
                        "name": "F. Gruau",
                        "slug": "F.-Gruau",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Gruau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gruau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "Cellular encodings have been applied to different control problems such as pole balancing [29] or the control of a hexapod robot [28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 48
                            }
                        ],
                        "text": "Among others, (see Yao [91] for more examples), Gruau [28] proposed a genetic encoding scheme for neural networks based on a cellular duplication and differentiation process, i.e., a cellular encoding (CE)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 189
                            }
                        ],
                        "text": "This mechanism could potentially allow the emergence of phenotypes with repeated structures formed by re-expression of the same genetic instructions, similarly to the approach described by Gruau."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 73
                            }
                        ],
                        "text": "Inspired by Koza\u2019s work on automatic discovery of reusable programs [42], Gruau also considered the case of genotypes formed by many trees where the terminal nodes of a tree may point to other trees."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 122
                            }
                        ],
                        "text": "Since the identification of substructures that are read more than once is an emergent result of the evolutionary process, Gruau called this cellular encoding automatic definition of neural subnetworks (ADNS)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 3
                            }
                        ],
                        "text": "In Gruau\u2019s model, connection links are established during the cellular duplication process."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Among others, (see Yao [91] for more examples), Gruau [28] proposed a genetic encoding scheme for neural net-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16828119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97b3386dd7c06841a1addc5903f6e251575264c2",
            "isKey": true,
            "numCitedBy": 313,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This article illustrates an artificial developmental system that is a computationally efficient technique for the automatic generation of complex artificial neural networks (ANNs). The artificial developmental system can develop a graph grammar into a modular ANN made of a combination of simpler subnetworks. A genetic algorithm is used to evolve coded grammars that generate ANNs for controlling six-legged robot locomotion. A mechanism for the automatic definition of neural subnetworks is incorporated Using this mechanism, the genetic algorithm can automatically decompose a problem into subproblems, generate a subANN for solving the subproblem, and instantiate copies of this subANN to build a higher-level ANN that solves the problem. We report some simulation results showing that the same problem cannot be solved if the mechanism for automatic definition of subnetworks is suppressed. We support our argument with pictures that describe the steps of development, how ANN structures are evolved, and how the ANNs compute."
            },
            "slug": "Automatic-Definition-of-Modular-Neural-Networks-Gruau",
            "title": {
                "fragments": [],
                "text": "Automatic Definition of Modular Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An artificial developmental system that is a computationally efficient technique for the automatic generation of complex artificial neural networks (ANNs) and some simulation results showing that the same problem cannot be solved if the mechanism for automatic definition of subnetworks is suppressed."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2818113"
                        ],
                        "name": "A. Soltoggio",
                        "slug": "A.-Soltoggio",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Soltoggio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Soltoggio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066182179"
                        ],
                        "name": "Peter D\u00fcrr",
                        "slug": "Peter-D\u00fcrr",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "D\u00fcrr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D\u00fcrr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2288013"
                        ],
                        "name": "C. Mattiussi",
                        "slug": "C.-Mattiussi",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Mattiussi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mattiussi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 116
                            }
                        ],
                        "text": "solution during evolution, outperforming the results obtained with hand-designed value-based learning architectures [78]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 7809369,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "35d1d73beddf9fbc14ae3fdd7a6215267c0b8450",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Environments with varying reward contingencies constitute a challenge to many living creatures. In such conditions, animals capable of adaptation and learning derive an advantage. Recent studies suggest that neuromodulatory dynamics are a key factor in regulating learning and adaptivity when reward conditions are subject to variability. In biological neural networks, specific circuits generate modulatory signals, particularly in situations that involve learning cues such as a reward or novel stimuli. Modulatory signals are then broadcast and applied onto target synapses to activate or regulate synaptic plasticity. Artificial neural models that include modulatory dynamics could prove their potential in uncertain environments when online learning is required. However, a topology that synthesises and delivers modulatory signals to target synapses must be devised. So far, only handcrafted architectures of such kind have been attempted. Here we show that modulatory topologies can be designed autonomously by artificial evolution and achieve superior learning capabilities than traditional fixed-weight or Hebbian networks. In our experiments, we show that simulated bees autonomously evolved a modulatory network to maximise the reward in a reinforcement learning-like environment."
            },
            "slug": "Evolving-neuromodulatory-topologies-for-problems-Soltoggio-D\u00fcrr",
            "title": {
                "fragments": [],
                "text": "Evolving neuromodulatory topologies for reinforcement learning-like problems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that modulatory topologies can be designed autonomously by artificial evolution and achieve superior learning capabilities than traditional fixed-weight or Hebbian networks."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Congress on Evolutionary Computation"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 106
                            }
                        ],
                        "text": "The fitness of each neuron was defined as the average fitness of all the networks it had participated in. Gomez and Miikkulainen [27] extended this approach by segregating the neurons in subpopulations with a method they called enforced subpopulations (ESP)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "To overcome these problems, Moriarty and Miikkulainen [52] suggested to evolve individual neurons to cooperate in networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Gomez and Miikkulainen [27] extended this approach by segregating the neurons in subpopulations with a method they called enforced subpopulations (ESP)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 14
                            }
                        ],
                        "text": "Reisinger and Miikkulainen [65] showed that an implicit encoding very similar to AGE outperforms NEAT on a complex board-game task."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2209222,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8939e9298eedb90aa4f040ab8e9f16872089a495",
            "isKey": true,
            "numCitedBy": 505,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Several researchers have demonstrated how complex action sequences can be learned through neuroevolution (i.e., evolving neural networks with genetic algorithms). However, complex general behavior such as evading predators or avoiding obstacles, which is not tied to specific environments, turns out to be very difficult to evolve. Often the system discovers mechanical strategies, such as moving back and forth, that help the agent cope but are not very effective, do not appear believable, and do not generalize to new environments. The problem is that a general strategy is too difficult for the evolution system to discover directly. This article proposes an approach wherein such complex general behavior is learned incrementally, by starting with simpler behavior and gradually making the task more challenging and general. The task transitions are implemented through successive stages of Delta coding (i.e., evolving modifications), which allows even converged populations to adapt to the new task. The method is tested in the stochastic, dynamic task of prey capture and is compared with direct evolution. The incremental approach evolves more effective and more general behavior and should also scale up to harder tasks."
            },
            "slug": "Incremental-Evolution-of-Complex-General-Behavior-Gomez-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Incremental Evolution of Complex General Behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article proposes an approach wherein complex general behavior is learned incrementally, by starting with simpler behavior and gradually making the task more challenging and general, which evolves more effective and more general behavior."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015062"
                        ],
                        "name": "S. Nolfi",
                        "slug": "S.-Nolfi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Nolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nolfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "with some form of learning [55]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13537139,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1c6ce972dd397c7410ff7811721ecd962e1732c9",
            "isKey": false,
            "numCitedBy": 203,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last few years several researchers have resorted to artificial evolution (e.g., genetic algorithms) and learning techniques (e.g., neural networks) for studying the interaction between learning and evolution. These studies have been conducted for two different purposes: (a) looking at the performance advantages obtained by combining these two adaptive techniques; (b) understanding the role of the interaction between learning and evolution in biological organisms. In this paper we describe some of the most representative experiments conducted in this area and point out their implications for both perspectives outlined above. Understanding the interaction between learning and evolution is probably one of the best examples in which computational studies have shed light on problems that are difficult to study with the research tools employed by evolutionary biology and biology in general. From an engineering point of view, the most relevant results are those showing that adaptation in dynamic environments gains a significant advantage by the combination of evolution and learning. These studies also show that the interaction between learning and evolution deeply alters the evolutionary and the learning process themselves, offering new perspectives from a biological point of view. The study of learning within an evolutionary perspective is still in its infancy and in the forthcoming years it will produce an enormous impact on our understanding of how learning and evolution operate."
            },
            "slug": "Learning-and-Evolution-Nolfi-Floreano",
            "title": {
                "fragments": [],
                "text": "Learning and Evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Some of the most representative experiments conducted in this area are described and show that the interaction between learning and evolution deeply alters the evolutionary and the learning process themselves, offering new perspectives from a biological point of view."
            },
            "venue": {
                "fragments": [],
                "text": "Auton. Robots"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2432655"
                        ],
                        "name": "J. Reisinger",
                        "slug": "J.-Reisinger",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Reisinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Reisinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Reisinger and Miikkulainen [ 65 ] showed that an implicit encoding very similar to AGE outperforms NEAT on a complex board-game task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9084748,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "87c70927282f201908a60673e8169fbef3f2a26b",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive representations allow evolution to explore the space of phenotypes by choosing the most suitable set of genotypic parameters. Although such an approach is believed to be efficient on complex problems, few empirical studieshave been conducted in such domains. In this paper, three neural network representations, a direct encoding, a complexifying encoding, and an implicit encoding capable of adapting the genotype-phenotype mapping are compared on Nothello, a complex game playing domain from the AAAI General Game Playing Competition. Implicit encoding makes the search more efficient and uses several times fewer parameters. Random mutation leads to highly structured phenotypic variation that is acquired during the course of evolution rather than built into the representation itself. Thus, adaptive representations learn to become evolvable, and furthermore do so in a way that makes search efficient on difficult coevolutionary problems."
            },
            "slug": "Acquiring-evolvability-through-adaptive-Reisinger-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Acquiring evolvability through adaptive representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Three neural network representations, a direct encoding, a complexifying encoding, and an implicit encoding capable of adapting the genotype-phenotype mapping are compared on Nothello, acomplex game playing domain from the AAAI General Game Playing Competition."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694198"
                        ],
                        "name": "J. Urzelai",
                        "slug": "J.-Urzelai",
                        "structuredName": {
                            "firstName": "Joseba",
                            "lastName": "Urzelai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Urzelai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Floreano and Urzelai [ 22 ] also showed that a morphogenetic approach can greatly benefit from co-evolution of synaptic plasticity because the strengths of the growing connections are developed by learning rules that are coevolved with the developmental rules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6732036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d9bbdb9f7287f74dd366afefd52f5344365c37f",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary Robotics is a powerful method to generate efficient controllers with minimal human intervention, but its applicability to real-world problems remains a challenge because the method takes long time and it requires software simulations that do not necessarily transfer smoothly to physical robots. In this paper we describe a method that overcomes these limitations by evolving robots for the ability to adapt on-line in few seconds. Experiments show that this method require less generations and smaller populations to evolve, that evolved robots adapt in a few seconds to unpredictable change-including transfers from simulations to physical robots- and display non-trivial behaviors. Robots evolved with this method can be dispatched to other planets and to our homes where they will autonomously and quickly adapt to the specific properties of their environments if and when necessary."
            },
            "slug": "Evolution-of-Plastic-Control-Networks-Floreano-Urzelai",
            "title": {
                "fragments": [],
                "text": "Evolution of Plastic Control Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Experiments show that this method require less generations and smaller populations to evolve, that evolved robots adapt in a few seconds to unpredictable change-including transfers from simulations to physical robots- and display non-trivial behaviors."
            },
            "venue": {
                "fragments": [],
                "text": "Auton. Robots"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2185842"
                        ],
                        "name": "Jesper Blynel",
                        "slug": "Jesper-Blynel",
                        "structuredName": {
                            "firstName": "Jesper",
                            "lastName": "Blynel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesper Blynel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5576451,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fc84cf42f04d6474a9fc92117957ef695f976030",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper explores the capabilities of continuous time recurrent neural networks (CTRNNs) to display reinforcement learning-like abilities on a set of T-Maze and double T-Maze navigation tasks, where the robot has to locate and \"remember\" the position of a reward-zone. The \"learning\" comes about without modifications of synapse strengths, but simply from internal network dynamics, as proposed by [12]. Neural controllers are evolved in simulation and in the simple case evaluated on a real robot. The evolved controllers are analyzed and the results obtained are discussed."
            },
            "slug": "Exploring-the-T-Maze:-Evolving-Learning-Like-Robot-Blynel-Floreano",
            "title": {
                "fragments": [],
                "text": "Exploring the T-Maze: Evolving Learning-Like Robot Behaviors Using CTRNNs"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "The capabilities of continuous time recurrent neural networks (CTRNNs) to display reinforcement learning-like abilities on a set of T-Maze and double T-maze navigation tasks, where the robot has to locate and \"remember\" the position of a reward-zone."
            },
            "venue": {
                "fragments": [],
                "text": "EvoWorkshops"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 19
                            }
                        ],
                        "text": "Ackley and Littman [1] for instance claimed that in artificial evolution, where inherited characters can easily be coded into the genotype given that the mapping between genotype and phenotype is generally quite simple, it is preferable to use Lamarckian evolution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61096220,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "675be3c1f8a57015a91be5cd191a8d262a9061fb",
            "isKey": false,
            "numCitedBy": 504,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial Life (AL) test bed Results Decision Making Project Applicability Motivation This combines two completely different time scales Learning is a individual behaviour Evolution operates at the level of populations over extended periods of time \" An entire life of learning is but one tick of the clock for evolution. \" What is the motivation to create a system that combines genetic programming with reinforcement learning? Motivation Extremely difficult to study the interaction between evolution and learning in the \" real world \" fossils provide information about evolution but provide very little data about day-today life can study evolution of some life forms (viruses, bacteria, flies) that have a short enough life span to study a significant number of generations, but unfortunately these have little ability to learn Motivation"
            },
            "slug": "Interactions-between-learning-and-evolution-Ackley-Littman",
            "title": {
                "fragments": [],
                "text": "Interactions between learning and evolution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47148221"
                        ],
                        "name": "Jacob Hurst",
                        "slug": "Jacob-Hurst",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Hurst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Hurst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144925000"
                        ],
                        "name": "L. Bull",
                        "slug": "L.-Bull",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Bull",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bull"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "For example, Hurst and Bull [35] addressed the control of a simulated robot in a maze task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7198147,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f555e87a3172e03e977fca65b0a672cb00ab5fb",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "For artificial entities to achieve true autonomy and display complex lifelike behavior, they will need to exploit appropriate adaptable learning algorithms. In this context adaptability implies flexibility guided by the environment at any given time and an open-ended ability to learn appropriate behaviors. This article examines the use of constructivism-inspired mechanisms within a neural learning classifier system architecture that exploits parameter self-adaptation as an approach to realize such behavior. The system uses a rule structure in which each rule is represented by an artificial neural network. It is shown that appropriate internal rule complexity emerges during learning at a rate controlled by the learner and that the structure indicates underlying features of the task. Results are presented in simulated mazes before moving to a mobile robot platform."
            },
            "slug": "A-Neural-Learning-Classifier-System-with-for-Mobile-Hurst-Bull",
            "title": {
                "fragments": [],
                "text": "A Neural Learning Classifier System with Self-Adaptive Constructivism for Mobile Robot Control"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This article examines the use of constructivism-inspired mechanisms within a neural learning classifier system architecture that exploits parameter self-adaptation as an approach to realize such behavior."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Life"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694724"
                        ],
                        "name": "F. Gruau",
                        "slug": "F.-Gruau",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Gruau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gruau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145285040"
                        ],
                        "name": "L. D. Whitley",
                        "slug": "L.-D.-Whitley",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Whitley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Whitley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2800737"
                        ],
                        "name": "Larry D. Pyeatt",
                        "slug": "Larry-D.-Pyeatt",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Pyeatt",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Larry D. Pyeatt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60742600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20c8c77d469575a077d6a15f70b7b0ee590e5bd3",
            "isKey": false,
            "numCitedBy": 331,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper compares the efficiency of two encoding schemes for Artificial Neural Networks optimized by evolutionary algorithms. Direct Encoding encodes the weights for an a priori fixed neural network architecture. Cellular Encoding encodes both weights and the architecture of the neural network. In previous studies, Direct Encoding and Cellular Encoding have been used to create neural networks for balancing 1 and 2 poles attached to a cart on a fixed track. The poles are balanced by a controller that pushes the cart to the left or the right. In some cases velocity information about the pole and cart is provided as an input; in other cases the network must learn to balance a single pole without velocity information. A careful study of the behavior of these systems suggests that it is possible to balance a single pole with velocity information as an input and without learning to compute the velocity. A new fitness function is introduced that forces the neural network to compute the velocity. By using this new fitness function and tuning the syntactic constraints used with cellular encoding, we achieve a tenfold speedup over our previous study and solve a more difficult problem: balancing two poles when no information about the velocity is provided as input."
            },
            "slug": "A-comparison-between-cellular-encoding-and-direct-Gruau-Whitley",
            "title": {
                "fragments": [],
                "text": "A comparison between cellular encoding and direct encoding for genetic neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "This paper compares the efficiency of two encoding schemes for Artificial Neural Networks optimized by evolutionary algorithms and solves a more difficult problem: balancing two poles when no information about the velocity is provided as input."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3190105"
                        ],
                        "name": "Keren Saggie",
                        "slug": "Keren-Saggie",
                        "structuredName": {
                            "firstName": "Keren",
                            "lastName": "Saggie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keren Saggie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2188015"
                        ],
                        "name": "A. Keinan",
                        "slug": "A.-Keinan",
                        "structuredName": {
                            "firstName": "Alon",
                            "lastName": "Keinan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Keinan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779370"
                        ],
                        "name": "E. Ruppin",
                        "slug": "E.-Ruppin",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Ruppin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ruppin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[70] compared spiking neural networks with conventional static ANNs in a simple setup where an agent had to search food and avoid poison in a simulated grid world."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 96848,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "ff3a26d3fdcc5a1c066de09bfbead5c48ad65e37",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spikes-that-count:-rethinking-spikiness-in-neurally-Saggie-Keinan",
            "title": {
                "fragments": [],
                "text": "Spikes that count: rethinking spikiness in neurally embedded systems"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122185897"
                        ],
                        "name": "A. Siddiqi",
                        "slug": "A.-Siddiqi",
                        "structuredName": {
                            "firstName": "Ansar",
                            "lastName": "Siddiqi",
                            "middleNames": [
                                "Ahmed"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Siddiqi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "However, Siddiqi and Lucas [76] showed that the inferior performance of the direct representation in Kitano\u2019s work did not result from the difference in the network representations but from different initial populations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15788622,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e11ef91575eddff818ae26b600dbea2116d7bf5",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The intuitive expectation is that the scheme used to encode the neural network in the chromosome should be critical to the success of evolving neural networks to solve difficult problems. In 1990 Kitano published an encoding scheme based on context-free parallel matrix rewriting. The method allowed compact, finite, chromosomes to grow neural networks of potentially infinite size. Results were presented that demonstrated superior evolutionary properties of the matrix rewriting method compared to a simple direct encoding. The authors present results that contradict those findings, and demonstrate that a genetic algorithm (GA) using a direct encoding can find good individuals just as efficiently as a GA using matrix rewriting."
            },
            "slug": "A-comparison-of-matrix-rewriting-versus-direct-for-Siddiqi-Lucas",
            "title": {
                "fragments": [],
                "text": "A comparison of matrix rewriting versus direct encoding for evolving neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The authors present results that contradict findings, and demonstrate that a genetic algorithm (GA) using a direct encoding can find good individuals just as efficiently as a GA using matrix rewriting."
            },
            "venue": {
                "fragments": [],
                "text": "1998 IEEE International Conference on Evolutionary Computation Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98TH8360)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694198"
                        ],
                        "name": "J. Urzelai",
                        "slug": "J.-Urzelai",
                        "structuredName": {
                            "firstName": "Joseba",
                            "lastName": "Urzelai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Urzelai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Finally, Floreano and Urzelai [ 21 ] showed that dynamic environments favor the genetic expression of plastic connections over static connections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16171996,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "bad220bf5119f515d931090ec0f3c5bca88f7772",
            "isKey": false,
            "numCitedBy": 266,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Evolutionary-robots-with-on-line-self-organization-Floreano-Urzelai",
            "title": {
                "fragments": [],
                "text": "Evolutionary robots with on-line self-organization and behavioral fitness"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 287
                            }
                        ],
                        "text": "Reinforcement learning is a class of learning algorithms that attempts to estimate explicitly or implicitly the value of the states experienced by the agents in order to favor the choice of those actions that maximize the amount of positive reinforcement received by the agent over time [84]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9166388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97efafdb4a3942ab3efba53ded7413199f79c054",
            "isKey": false,
            "numCitedBy": 33129,
            "numCiting": 636,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning, one of the most active research areas in artificial intelligence, is a computational approach to learning whereby an agent tries to maximize the total amount of reward it receives when interacting with a complex, uncertain environment. In Reinforcement Learning, Richard Sutton and Andrew Barto provide a clear and simple account of the key ideas and algorithms of reinforcement learning. Their discussion ranges from the history of the field's intellectual foundations to the most recent developments and applications. The only necessary mathematical background is familiarity with elementary concepts of probability. The book is divided into three parts. Part I defines the reinforcement learning problem in terms of Markov decision processes. Part II provides basic solution methods: dynamic programming, Monte Carlo methods, and temporal-difference learning. Part III presents a unified view of the solution methods and incorporates artificial neural networks, eligibility traces, and planning; the two final chapters present case studies and consider the future of reinforcement learning."
            },
            "slug": "Reinforcement-Learning:-An-Introduction-Sutton-Barto",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning: An Introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This book provides a clear and simple account of the key ideas and algorithms of reinforcement learning, which ranges from the history of the field's intellectual foundations to the most recent developments and applications."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Neural Networks"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145738991"
                        ],
                        "name": "J. D. Schaffer",
                        "slug": "J.-D.-Schaffer",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Schaffer",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Schaffer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145285040"
                        ],
                        "name": "L. D. Whitley",
                        "slug": "L.-D.-Whitley",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Whitley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Whitley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3035326"
                        ],
                        "name": "L. Eshelman",
                        "slug": "L.-Eshelman",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Eshelman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Eshelman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "It has been argued [72, 61] that evolving neural networks may not be trivial because the population may include individuals with competing conventions (see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60670877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afdf48aaf69a520ed6d5b4a50189facc0ebf4e37",
            "isKey": false,
            "numCitedBy": 621,
            "numCiting": 150,
            "paperAbstract": {
                "fragments": [],
                "text": "Various schemes for combining genetic algorithms and neural networks have been proposed and tested in recent years, but the literature is scattered among a variety of journals, proceedings and technical reports. Activity in this area is clearly increasing. The authors provide an overview of this body of literature drawing out common themes and providing, where possible, the emerging wisdom about what seems to work and what does not.<<ETX>>"
            },
            "slug": "Combinations-of-genetic-algorithms-and-neural-a-of-Schaffer-Whitley",
            "title": {
                "fragments": [],
                "text": "Combinations of genetic algorithms and neural networks: a survey of the state of the art"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An overview of this body of literature drawing out common themes and providing, where possible, the emerging wisdom about what seems to work and what does not is provided."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723120"
                        ],
                        "name": "Kemal Oflazer",
                        "slug": "Kemal-Oflazer",
                        "structuredName": {
                            "firstName": "Kemal",
                            "lastName": "Oflazer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kemal Oflazer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Dasdan and Oflazer [14] employed a similar encoding strategy as Chalmers to evolve unsupervised learning rules for classification tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Dasdan and Oflazer [14] employed a similar encoding"
                    },
                    "intents": []
                }
            ],
            "corpusId": 59659031,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f736031bf5fe00e1a3e7c44635e2c134ab795ce",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents new unsupervised learning algorithms that have been synthesized using a genetic approach. A set of such learning algorithms has been compared with the classical Kohonen's Algorithm on the Self-Organizing Map and has been found to provide a better performance measure. This study indicates that there exist many unsupervised learning algorithms that lead to an organization similar to that of Kohonen's Algorithm, and that genetic algorithms can be used to search for optimal algorithms and optimal architectures for the unsupervised learning."
            },
            "slug": "Genetic-Synthesis-of-Unsupervised-Learning-Oflazer",
            "title": {
                "fragments": [],
                "text": "Genetic Synthesis of Unsupervised Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This study indicates that there exist many unsupervised learning algorithms that lead to an organization similar to that of Kohonen's Algorithm, and that genetic algorithms can be used to search for optimal algorithms and optimal architectures for the unsuper supervised learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61908236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "693a614718a96d61968ec573b2932a3301092c9a",
            "isKey": false,
            "numCitedBy": 23520,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book represents the most comprehensive treatment available of neural networks from an engineering perspective. Thorough, well-organized, and completely up to date, it examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks. Written in a concise and fluid manner, by a foremost engineering textbook author, to make the material more accessible, this book is ideal for professional engineers and graduate students entering this exciting field. Computer experiments, problems, worked examples, a bibliography, photographs, and illustrations reinforce key concepts."
            },
            "slug": "Neural-Networks:-A-Comprehensive-Foundation-Haykin",
            "title": {
                "fragments": [],
                "text": "Neural Networks: A Comprehensive Foundation"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "Thorough, well-organized, and completely up to date, this book examines all the important aspects of this emerging technology, including the learning process, back-propagation learning, radial-basis function networks, self-organizing systems, modular networks, temporal processing and neurodynamics, and VLSI implementation of neural networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35988382"
                        ],
                        "name": "A. Chandra",
                        "slug": "A.-Chandra",
                        "structuredName": {
                            "firstName": "Arjun",
                            "lastName": "Chandra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Chandra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Evolved neural networks with direct encoding have also been applied to a variety of other problems such as game playing [13], data classification [ 12 ] or the control of robot swarms [85]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207175852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3cb1936482925b40e9859d2da4fe961c96ff5a7",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-objective evolutionary algorithms for the construction of neural ensembles is a relatively new area of research. We recently proposed an ensemble learning algorithm called DIVACE (DIVerse and ACcurate Ensemble learning algorithm). It was shown that DIVACE tries to find an optimal trade-off between diversity and accuracy as it searches for an ensemble for some particular pattern recognition task by treating these two objectives explicitly separately. A detailed discussion of DIVACE together with further experimental studies form the essence of this paper. A new diversity measure which we call Pairwise Failure Crediting (PFC) is proposed. This measure forms one of the two evolutionary pressures being exerted explicitly in DIVACE. Experiments with this diversity measure as well as comparisons with previously studied approaches are hence considered. Detailed analysis of the results show that DIVACE, as a concept, has promise."
            },
            "slug": "Ensemble-Learning-Using-Multi-Objective-Algorithms-Chandra-Yao",
            "title": {
                "fragments": [],
                "text": "Ensemble Learning Using Multi-Objective Evolutionary Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "A new diversity measure which is called Pairwise Failure Crediting (PFC) is proposed, which forms one of the two evolutionary pressures being exerted explicitly in DIVACE."
            },
            "venue": {
                "fragments": [],
                "text": "J. Math. Model. Algorithms"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9796712"
                        ],
                        "name": "Y. Niv",
                        "slug": "Y.-Niv",
                        "structuredName": {
                            "firstName": "Yael",
                            "lastName": "Niv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Niv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145141118"
                        ],
                        "name": "D. Joel",
                        "slug": "D.-Joel",
                        "structuredName": {
                            "firstName": "Daphna",
                            "lastName": "Joel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Joel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1904578"
                        ],
                        "name": "I. Meilijson",
                        "slug": "I.-Meilijson",
                        "structuredName": {
                            "firstName": "Isaac",
                            "lastName": "Meilijson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Meilijson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779370"
                        ],
                        "name": "E. Ruppin",
                        "slug": "E.-Ruppin",
                        "structuredName": {
                            "firstName": "Eytan",
                            "lastName": "Ruppin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ruppin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5842616,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5b319c5e4478cc44e60fde5bc2fe77b21631caa5",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning is a fundamental process by which organisms learn to achieve goals from their interactions with the environment. Using evolutionary computation techniques we evolve (near-)optimal neuronal learning rules in a simple neural network model of reinforcement learning in bumblebees foraging for nectar. The resulting neural networks exhibit efficient reinforcement learning, allowing the bees to respond rapidly to changes in reward contingencies. The evolved synaptic plasticity dynamics give rise to varying exploration/exploitation levels and to the well-documented choice strategies of risk aversion and probability matching. Additionally, risk aversion is shown to emerge even when bees are evolved in a completely risk-less environment. In contrast to existing theories in economics and game theory, risk-averse behavior is shown to be a direct consequence of (near-)optimal reinforcement learning, without requiring additional assumptions such as the existence of a nonlinear subjective utility function for rewards. Our results are corroborated by a rigorous mathematical analysis, and their robustness in real-world situations is supported by experiments in a mobile robot. Thus we provide a biologically founded, parsimonious, and novel explanation for risk aversion and probability matching."
            },
            "slug": "Evolution-of-Reinforcement-Learning-in-Uncertain-A-Niv-Joel",
            "title": {
                "fragments": [],
                "text": "Evolution of Reinforcement Learning in Uncertain Environments: A Simple Explanation for Complex Foraging Behaviors"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work provides a biologically founded, parsimonious, and novel explanation for risk aversion and probability matching in bumblebees, and shows risk aversion to emerge even when bees are evolved in a completely risk-less environment."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917876"
                        ],
                        "name": "P. Husbands",
                        "slug": "P.-Husbands",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Husbands",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Husbands"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145472053"
                        ],
                        "name": "I. Harvey",
                        "slug": "I.-Harvey",
                        "structuredName": {
                            "firstName": "Inman",
                            "lastName": "Harvey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Harvey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145676014"
                        ],
                        "name": "D. Cliff",
                        "slug": "D.-Cliff",
                        "structuredName": {
                            "firstName": "Dave",
                            "lastName": "Cliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cliff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114424160"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Husbands et al. [ 36 ] proposed a similar method where the connections grew according to a set of differential equations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14950099,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eede1a8e761c73e33020b141cd9ae8db5e216793",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a high-level review of current and recent work in the use of genetic algorithm-based techniques to develop sensorimotor control systems for autonomous agents. It focuses on network-based controllers and genetic encoding issues associated with them. The paper closes with a discussion of the possibility of using artificial evolutionary techniques to help tackle more specifically scientific questions about natural sensorimotor systems."
            },
            "slug": "The-use-of-genetic-algorithms-for-the-development-Husbands-Harvey",
            "title": {
                "fragments": [],
                "text": "The use of genetic algorithms for the development of sensorimotor control systems"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A high-level review of current and recent work in the use of genetic algorithm-based techniques to develop sensorimotor control systems for autonomous agents focuses on network-based controllers and genetic encoding issues associated with them."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of PerAc '94. From Perception to Action"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110618745"
                        ],
                        "name": "Takahiro Sasaki",
                        "slug": "Takahiro-Sasaki",
                        "structuredName": {
                            "firstName": "Takahiro",
                            "lastName": "Sasaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takahiro Sasaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747029"
                        ],
                        "name": "M. Tokoro",
                        "slug": "M.-Tokoro",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Tokoro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tokoro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12161981,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "250c34b9fc88c8acaeeb86a37f2a0668970537bc",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "The processes of adaptation in a multi-agent system consist of two complementary phases: 1) learning, occurring within each agent's individual lifetime, and 2) evolution, occurring over successive generations of the population. In this paper, we observe the dynamics of such adaptive processes in a simple abstract model, where each neural network is regarded as an individual capable of learning, and genetic algorithms are applied as the evolutionary processes for the population of such agents. By evaluating the characteristics of two di erent mechanisms of genetic inheritance, i.e Darwinian and Lamarckian, we show the following results. While the Lamarckian mechanism is far more e ective than the Darwinian one under static environments, it is found to be unstable and performs quite poorly under dynamic environments. In contrast, even under dynamic environments, a Darwinian population is not only more stable than a Lamarckian one, but also maintains its adaptability with respect to such dynamic environments."
            },
            "slug": "Adaptation-toward-Changing-Environments:-Why-in-Sasaki-Tokoro",
            "title": {
                "fragments": [],
                "text": "Adaptation toward Changing Environments: Why Darwinian in Nature?"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The dynamics of such adaptive processes in a simple abstract model, where each neural network is regarded as an individual capable of learning, and genetic algorithms are applied as the evolutionary processes for the population of such agents, show the following results."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20461,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734883"
                        ],
                        "name": "E. D. Paolo",
                        "slug": "E.-D.-Paolo",
                        "structuredName": {
                            "firstName": "Ezequiel",
                            "lastName": "Paolo",
                            "middleNames": [
                                "A.",
                                "Di"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. D. Paolo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14230131,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "073352b702373d179874e6567bf70e6d311bac20",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Single\u2013trial learning is studied in an evolved robot model of synaptic spike\u2013timing\u2013dependent plasticity (STDP). Robots must perform positive phototaxis but must learn to perform negative phototaxis in the presence of a short\u2013lived aversive sound stimulus. STDP acts at the millisecond range and depends asymmetrically on the relative timing of pre\u2013 and post\u2013synaptic spikes. Although it has been involved in learning models of input prediction, these models require the iterated presentation of the input pattern, and it is hard to see how this mechanism could sustain single\u2013trial learning over a time\u2013scale of tens of seconds. An incremental evolutionary approach is used to answer this question. The evolved robots succeed in learning the appropriate behaviour, but learning does not depend on achieving the right synaptic configuration but rather the right pattern of neural activity. Robot performance during positive phototaxis is quite robust to loss of spike\u2013timing information, but in contrast, this loss is catastrophic for learning negative phototaxis where entrained firing is common. Tests show that the final weight configuration carries no information about whether a robot is performing one behaviour or the other. Fixing weights, however, has the effect of degrading performance, thus demonstrating that plasticity is used to sustain the neural activity corresponding both to the normal phototaxis condition and to the learned behaviour. The implications and limitations of this result are discussed."
            },
            "slug": "Evolving-spike-timing-dependent-plasticity-for-in-Paolo",
            "title": {
                "fragments": [],
                "text": "Evolving spike-timing-dependent plasticity for single-trial learning in robots"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Experimental single\u2013trial learning in an evolved robot model of synaptic spike\u2013timing\u2013dependent plasticity shows that plasticity is used to sustain the neural activity corresponding both to the normal phototaxis condition and to the learned behaviour."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917876"
                        ],
                        "name": "P. Husbands",
                        "slug": "P.-Husbands",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Husbands",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Husbands"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015062"
                        ],
                        "name": "S. Nolfi",
                        "slug": "S.-Nolfi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Nolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nolfi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6034149,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "90fe9cfa664a4f875e175ecd8d59a0ec8d98127a",
            "isKey": false,
            "numCitedBy": 534,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary robotics combines evolutionary computing with robotics [1, 2, 4, 7\u201310]. It is a field that \u2018\u2018aims to apply evolutionary computation techniques to evolve the overall design or controllers, or both, for real and simulated autonomous robots\u2019\u2019 [9]. It is a useful approach \u2018\u2018both for investigating the design space of robotic applications and for testing scientific hypotheses of biological mechanisms and processes\u2019\u2019 [4]. Over the last fifteen years, evolutionary robotics has developed from the novel field described in Nolfi and Floreano\u2019s seminal book [7] to a more mature discipline. Evolutionary robotics is now also used to tackle problems involving complex dynamical systems, such as swarm robotics, modular robotics, flying and underwater robotics. Major research challenges have been identified and are currently being investigated by the community, ranging from the impact of selection pressure to embodied evolution and lifelong learning. From the perspective of evolutionary computing, evolutionary robotics is a particular application area that is different from, say, combinatorial optimisation. Somewhat oversimplifying, the main challenge in solving optimisation problems with evolutionary algorithms is the ruggedness of the fitness landscape defined by the objective function. Evolutionary robotics applications face additional problems: one is the very indirect link between controllable design details and the target feature(s). Another particular issue is the great variety of conditions and requirements under which a solution has to hold. Unlike candidate solutions in \u2018simple\u2019 optimisation, robot phenotypes (controller, morphology, or both) cannot be directly evaluated. Rather, it is the robot\u2019s behaviour that needs to be observed and assessed. Thus, regular evolutionary computing implies a three levels of organization: genotype\u2014phenotype\u2014fitness, while in evolutionary robotics the process comprises of four levels of organization: genotype\u2014phenotype\u2014behaviour\u2014fitness. Additionally, the behavior exhibited by a robot is not only the product of the robot morphology and controller but is the emergent result of the robot/environmental interactions [6]. Thus, the evaluation of a candidate solution necessarily require the evaluation of the behavior that arise by robots that are situated in the external environment and are allowed to \u2019\u2019live\u2018\u2018 long enough to experience a large variety of environmental conditions. Last but not least, desirable robot behaviour is almost never defined by one single skill (except for pure research purposes). Consequently, the design of the fitness function is far from trivial [5]. E. Haasdijk A. E. Eiben VU University Amsterdam, Amsterdam, The Netherlands e-mail: e.haasdijk@vu.nl"
            },
            "slug": "Evolutionary-robotics-Floreano-Husbands",
            "title": {
                "fragments": [],
                "text": "Evolutionary Robotics"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2288013"
                        ],
                        "name": "C. Mattiussi",
                        "slug": "C.-Mattiussi",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Mattiussi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mattiussi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2990531"
                        ],
                        "name": "D. Marbach",
                        "slug": "D.-Marbach",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Marbach",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Marbach"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066182179"
                        ],
                        "name": "Peter D\u00fcrr",
                        "slug": "Peter-D\u00fcrr",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "D\u00fcrr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D\u00fcrr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 658636,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3abf4a7a717d7b5430ed9272e22bc6ea5aa5c381",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "A large class of systems of biological and technological relevance can be described as analog networks, that is, collections of dynamical devices interconnected by links of varying strength. Some examples of analog networks are genetic regulatory networks, metabolic networks, neural networks, analog electronic circuits, and control systems. Analog networks are typically complex systems which include nonlinear feedback loops and possess temporal dynamics at different time scales. Both the synthesis and reverse engineering of analog networks are recognized as knowledge-intensive activities, for which few systematic techniques exist. In this paper we will discuss the general relevance of the analog network concept and describe an evolutionary approach to the automatic synthesis and the reverse engineering of analog networks. The proposed approach is called analog genetic encoding (AGE) and realizes an implicit genetic encoding of analog networks. AGE permits the evolution of human-competitive solutions to real-world analog network design and identification problems. This is illustrated by some examples of application to the design of electronic circuits, control systems, learning neural architectures, and the reverse engineering of biological networks."
            },
            "slug": "The-Age-of-Analog-Networks-Mattiussi-Marbach",
            "title": {
                "fragments": [],
                "text": "The Age of Analog Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The proposed approach is called analog genetic encoding (AGE) and realizes an implicit genetic encoding of analog networks that permits the evolution of human-competitive solutions to real-world analog network design and identification problems."
            },
            "venue": {
                "fragments": [],
                "text": "AI Mag."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2236299"
                        ],
                        "name": "M. Korkin",
                        "slug": "M.-Korkin",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Korkin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Korkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3148012"
                        ],
                        "name": "N. Nawa",
                        "slug": "N.-Nawa",
                        "structuredName": {
                            "firstName": "Norberto",
                            "lastName": "Nawa",
                            "middleNames": [
                                "Eiji"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797302"
                        ],
                        "name": "H. D. Garis",
                        "slug": "H.-D.-Garis",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Garis",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. D. Garis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16220029,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b8164efd532d790f9d964d111e79245eec7c287",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports on ongoing attempts to find an efficient and effective representation for the binary signaling of ATR\u2019s CAM-Brain Machine (CBM), using the so-called \u201dCoDi-1Bit\u201d model. The CBM is an Field Programmable Gate Array (FPGA) based hardware accelerator which updates 3D cellular automata (CA) cells at the rate of 100 billion a second, allowing a complete run of a genetic algorithm with tens of thousands of CA based neural net circuit growths and hardware compiled fitness evaluations, all in about 1 second. It is hoped that using such a device, it will become practical to evolve 10,000s of neural net modules and then to assemble them into humanly defined RAM based artificial brain architectures which can be run by the CBM in real time to control robots, e.g. a robot kitten. Before large numbers of modules can be assembled together, it is essential that the individual modules have a good functionality and evolvability. The \u201dCoDi-1Bit\u201d CA based neural network model uses 1 bit binary signaling, so a representation needs to be chosen based on this fact. This paper introduces and discusses the merits and demerits of a representation that we call \u201dSpike Interval Information Coding\u201d (SIIC). Simulation results using the SIIC representation method to evolve time dependent waveforms and simple functional modules are presented. The results indicate the suitability of the SIIC representation method to decode the bit streams generated by the CA based neural networks."
            },
            "slug": "A-\"Spike-Interval-Information-Coding\"-for-ATR's-Korkin-Nawa",
            "title": {
                "fragments": [],
                "text": "A \"Spike Interval Information Coding\" Representation for ATR's CAM-Brain Machine (CBM)"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Simulation results indicate the suitability of the SIIC representation method to decode the bit streams generated by the CA based neural networks, and the merits and demerits of a representation that the authors call \u201dSpike Interval Information Coding\u201d (SIIC)."
            },
            "venue": {
                "fragments": [],
                "text": "ICES"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145893752"
                        ],
                        "name": "J. Fellous",
                        "slug": "J.-Fellous",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Fellous",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Fellous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2018795"
                        ],
                        "name": "C. Linster",
                        "slug": "C.-Linster",
                        "structuredName": {
                            "firstName": "Christiane",
                            "lastName": "Linster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Linster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The existing evidence points to the combined action of evolved value systems [58] and neuromodulatory effects [2, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8272654,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "c66917a0a255ba939b633b306b24029e7c7301bd",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 117,
            "paperAbstract": {
                "fragments": [],
                "text": "Computational modeling of neural substrates provides an excellent theoretical framework for the understanding of the computational roles of neuromodulation. In this review, we illustrate, with a large number of modeling studies, the specific computations performed by neuromodulation in the context of various neural models of invertebrate and vertebrate preparations. We base our characterization of neuromodulations on their computational and functional roles rather than on anatomical or chemical criteria. We review the main framework in which neuromodulation has been studied theoretically (central pattern generation and oscillations, sensory processing, memory and information integration). Finally, we present a detailed mathematical overview of how neuromodulation has been implemented at the single cell and network levels in modeling studies. Overall, neuromodulation is found to increase and control computational complexity."
            },
            "slug": "Computational-Models-of-Neuromodulation-Fellous-Linster",
            "title": {
                "fragments": [],
                "text": "Computational Models of Neuromodulation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This review illustrates, with a large number of modeling studies, the specific computations performed by neuromodulation in the context of various neural models of invertebrate and vertebrate preparations."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899177"
                        ],
                        "name": "Ken-ichi Funahashi",
                        "slug": "Ken-ichi-Funahashi",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Funahashi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Funahashi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109993561"
                        ],
                        "name": "Yuichi Nakamura",
                        "slug": "Yuichi-Nakamura",
                        "structuredName": {
                            "firstName": "Yuichi",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuichi Nakamura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "While being one of the simplest nonlinear, continuous dynamic network models, CTRNNs are universal dynamic approximators [ 24 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10352852,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1ba05d8e1c97a92295105101dba54d3b680c1f70",
            "isKey": false,
            "numCitedBy": 840,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Approximation-of-dynamical-systems-by-continuous-Funahashi-Nakamura",
            "title": {
                "fragments": [],
                "text": "Approximation of dynamical systems by continuous time recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846883"
                        ],
                        "name": "Kenneth O. Stanley",
                        "slug": "Kenneth-O.-Stanley",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Stanley",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth O. Stanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2686687"
                        ],
                        "name": "Nate Kohl",
                        "slug": "Nate-Kohl",
                        "structuredName": {
                            "firstName": "Nate",
                            "lastName": "Kohl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nate Kohl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2204677"
                        ],
                        "name": "Rini Sherony",
                        "slug": "Rini-Sherony",
                        "structuredName": {
                            "firstName": "Rini",
                            "lastName": "Sherony",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rini Sherony"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 193
                            }
                        ],
                        "text": "Neuro-evolution of augmenting topologies (NEAT) has been applied to many problems such as pole balancing [79], robot control [80], computer games [66, 82] or an automobile crash warning system [81]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 377561,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4598962fd18744c78844a626da41b73ff07fbb2c",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Many serious automobile accidents could be avoided if drivers were warned of impending crashes before they occurred. In this paper, a vehicle warning system is evolved to predict such crashes in the RARS driving simulator. The NeuroEvolution of Augmenting Topologies (NEAT) method is first used to evolve a neural network driver that can autonomously navigate a track without crashing. The network is subsequently impaired, resulting in a driver that occasionally makes mistakes and crashes. Using this impaired driver, a crash predictor is evolved that can predict how far in the future a crash is going to occur, information that can be used to generate an appropriate warning level. The main result is that NEAT can successfully evolve a warning system that takes into account the recent history of inputs and outputs, and therefore makes few errors. Experiments were also run to compare training offline from previously collected data with training online in the simulator. While both methods result in successful warning systems, offline training is both faster and more accurate. Thus, the results in this paper set the stage for developing crash predictors that are both accurate and able to adapt online, which may someday save lives in real vehicles."
            },
            "slug": "Neuroevolution-of-an-automobile-crash-warning-Stanley-Kohl",
            "title": {
                "fragments": [],
                "text": "Neuroevolution of an automobile crash warning system"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A vehicle warning system is evolved to predict such crashes in the RARS driving simulator to set the stage for developing crash predictors that are both accurate and able to adapt online, which may someday save lives in real vehicles."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7373730"
                        ],
                        "name": "J. Bongard",
                        "slug": "J.-Bongard",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "Bongard",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bongard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 179
                            }
                        ],
                        "text": "Although these early computational GRN models were not directly aimed at the exploitation of the evolutionary properties of such a system, they inspired other researchers such as Bongard\n[10] to use GRN models to evolve autonomous agents."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 86
                            }
                        ],
                        "text": "The so-called implicit encoding is frequently used as a representation for GRN models [10]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Bongard used a GRN model that relied on the diffusion of chemicals, so-called transcription factors, which could either directly influence the phenotype of the agent or regulate the expression of genes."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "Although these early computational GRN models were not directly aimed at the exploitation of the evolutionary properties of such a system, they inspired other researchers such as Bongard [10] to use GRN models to evolve autonomous agents."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14906701,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5b2c126dd209d619a6482f58d76541aab90916a1",
            "isKey": true,
            "numCitedBy": 236,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a system that combines ontogenetic development and artificial evolution to automatically design robots in a physics-based, virtual environment. Through lesion experiments on the evolved agents, we demonstrate that the evolved genetic regulatory networks from successful evolutionary runs are more modular than those obtained from unsuccessful runs."
            },
            "slug": "Evolving-modular-genetic-regulatory-networks-Bongard",
            "title": {
                "fragments": [],
                "text": "Evolving modular genetic regulatory networks"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A system that combines ontogenetic development and artificial evolution to automatically design robots in a physics-based, virtual environment is introduced and it is demonstrated that the evolved genetic regulatory networks from successful evolutionary runs are more modular than those obtained from unsuccessful runs."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2002 Congress on Evolutionary Computation. CEC'02 (Cat. No.02TH8600)"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47663824"
                        ],
                        "name": "E. Mizutani",
                        "slug": "E.-Mizutani",
                        "structuredName": {
                            "firstName": "Eiji",
                            "lastName": "Mizutani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Mizutani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2547681"
                        ],
                        "name": "S. Dreyfus",
                        "slug": "S.-Dreyfus",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Dreyfus",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dreyfus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Fig. 12 The Actor\u2010Critic architecture for reinforcement learning. Redrawn from Mizutani and Dreyfus [ 49 ] mod"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2533654,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2b0645a06716ae96fa8a9b76bf301c0f0fea255",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe how an actor-critic reinforcement learning agent in a non-Markovian domain finds an optimal sequence of actions in a totally model-free fashion; that is, the agent neither learns transitional probabilities and associated rewards, nor by how much the state space should be augmented so that the Markov property holds. In particular, we employ an Elman-type recurrent neural network to solve non-Markovian problems since an Elman-type network is able to implicitly and automatically render the process Markovian. A standard \"actor-critic\" neural network model has two separate components: the action (actor) network and the value (critic) network. In animal brains, however, those two presumably may not be distinct, but rather somehow entwined. We thus construct one Elman network with two output nodes: actor node and critic node, and a portion of the shared hidden layer is fed back as the context layer, which functions as a history memory to produce sensitivity to non-Markovian dependencies. The agent explores small-scale three and four-stage triangular path-networks to learn an optimal sequence of actions that maximizes total value (or reward) associated with its transition from vertex to vertex. The posed problem has deterministic transition and reward associated with each allowable action (although either could be stochastic) and is rendered non-Markovian by the reward being dependent on an earlier transition. Due to the nature of neural model-free learning, the agent needs many iterations to find the optimal actions even in small-scale path problems."
            },
            "slug": "Totally-model-free-reinforcement-learning-by-Elman-Mizutani-Dreyfus",
            "title": {
                "fragments": [],
                "text": "Totally model-free reinforcement learning by actor-critic Elman networks in non-Markovian domains"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This work describes how an actor-critic reinforcement learning agent in a non-Markovian domain finds an optimal sequence of actions in a totally model-free fashion; that is, the agent neither learns transitional probabilities and associated rewards, nor by how much the state space should be augmented so that the Markov property holds."
            },
            "venue": {
                "fragments": [],
                "text": "1998 IEEE International Joint Conference on Neural Networks Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98CH36227)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Hinton and Nowlan [33] proposed a simple computational model that shows how learning might facilitate and guide evolution."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Hinton and Nowlan\u2019s results suggest that the addition of learning produces a smoothing of the fitness surface area around the good combination of genes (weights), which can be discovered and easily climbed by the genetic algorithm (see Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 31
                            }
                        ],
                        "text": "One of the main limitations of Hinton and Nowlan\u2019s model is that the learning space and the evolutionary space are completely correlated.3 By systematically varying the cost of learning and the correlation between the learning space and the evolutionary space, Mayley [47] showed that: (1) the adaptive advantage of learning is proportional to the correlation between the two search spaces; (2) the assimilation of characters first acquired through learning is proportional to the correlation between the two search spaces and to the cost of learning (i.e., to the fitness lost during the first part of the lifetime in which individuals have suboptimal performance); (3) in certain situations learning costs may exceed learning benefits."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Hinton and Nowlan [33] proposed a simple computational model that shows how"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2937027,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f9197ff9fdabd2b78bfe0602365011c6699b0d66",
            "isKey": true,
            "numCitedBy": 1188,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The assumption that acquired character istics are not in\u00ad herited is ofte n taken to imply t hat t he adaptations t hat an organism learns dur ing its lifeti me cannot guide t he course of evolut ion . This infere nce is incor rec t (2). Learni ng alt ers the shape of t he search space in which evolu tio n operates and thereby pro vides good evolut ion ar y paths towa rds sets of co-adapted alleles. We demonst r at e t hat th is effect allows learning organisms to evolve much faster than their 000 \u00ad learning equivalents, even though the characteris tics acquired by t he phenotype are not communicated to the genotype."
            },
            "slug": "How-Learning-Can-Guide-Evolution-Hinton-Nowlan",
            "title": {
                "fragments": [],
                "text": "How Learning Can Guide Evolution"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214740"
                        ],
                        "name": "Torsten Reil",
                        "slug": "Torsten-Reil",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Reil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Torsten Reil"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13642057,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f77f0d3643cf2a8b43e4f17ea0b780c899980c82",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An artificial genome with biologically plausible properties was developed and the dynamics of gene expression were studied. The model differs from previous approaches, such as Random Boolean Nets [1], in that it is entirely based on template matching in a nucleotide-like sequence. Genes activate or inhibit other genes by binding to their regulatory sequences. \n \nThe results of the experiments suggest that many features of real-life development, such as cyclic gene activity, differentiation into multiple cell types, and robusteness may be inherent properties of a template-matching system rather than necessarily designed from scratch by Natural Selection. Moreover, the system may provide a new hypothesis about the role of junk DNA in real genomes. In addition to these biological implications, the approach used here is thought to provide a flexible basis for future simulations of morphogenesis."
            },
            "slug": "Dynamics-of-Gene-Expression-in-an-Artificial-Genome-Reil",
            "title": {
                "fragments": [],
                "text": "Dynamics of Gene Expression in an Artificial Genome - Implications for Biological and Artificial Ontogeny"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results of the experiments suggest that many features of real-life development, such as cyclic gene activity, differentiation into multiple cell types, and robusteness may be inherent properties of a template-matching system rather than necessarily designed from scratch by Natural Selection."
            },
            "venue": {
                "fragments": [],
                "text": "ECAL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145576002"
                        ],
                        "name": "R. Pfeifer",
                        "slug": "R.-Pfeifer",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Pfeifer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Pfeifer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3302790"
                        ],
                        "name": "C. Scheier",
                        "slug": "C.-Scheier",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Scheier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Scheier"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "The existing evidence points to the combined action of evolved value systems [58] and neuromodulatory effects [2, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52838522,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "c4141164bd0b9a0a9bdeb4ea793fc297c796d0d9",
            "isKey": false,
            "numCitedBy": 1670,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nResearchers now agree that intelligence always manifests itself in behavior - thus it is behavior that we must understand. An exciting new field has grown around the study of behavior-based intelligence, also known as embodied cognitive science, \"new AI,\" and \"behavior-based AI.\". \"Rolf Pfeifer and Christian Scheier provide a systematic introduction to this new way of thinking about intelligence and computers. After discussing concepts and approaches such as subsumption architecture, Braitenberg vehicles, evolutionary robotics, artificial life, self-organization, and learning, the authors derive a set of principles and a coherent framework for the study of naturally and artificially intelligent systems, or autonomous agents. This framework is based on a synthetic methodology whose goal is understanding by designing and building."
            },
            "slug": "Understanding-intelligence-Pfeifer-Scheier",
            "title": {
                "fragments": [],
                "text": "Understanding intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Rolf Pfeifer and Christian Scheier provide a systematic introduction to this new way of thinking about intelligence and computers and derive a set of principles and a coherent framework for the study of naturally and artificially intelligent systems, or autonomous agents."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056565"
                        ],
                        "name": "Giles Mayley",
                        "slug": "Giles-Mayley",
                        "structuredName": {
                            "firstName": "Giles",
                            "lastName": "Mayley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Giles Mayley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 260
                            }
                        ],
                        "text": "One of the main limitations of Hinton and Nowlan\u2019s model is that the learning space and the evolutionary space are completely correlated.3 By systematically varying the cost of learning and the correlation between the learning space and the evolutionary space, Mayley [47] showed that: (1) the adaptive advantage of learning is proportional to the correlation between the two search spaces; (2) the assimilation of characters first acquired through learning is proportional to the correlation between the two search spaces and to the cost of learning (i.e., to the fitness lost during the first part of the lifetime in which individuals have suboptimal performance); (3) in certain situations learning costs may exceed learning benefits."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "By systematically varying the cost of learning and the correlation between the learning space and the evolutionary space, Mayley [47] showed that: (1) the adaptive advantage of learning is proportional to the correlation between the two search spaces; (2) the assimilation of characters first acquired through learning is proportional to the correlation between the two search spaces and to the cost of learning (i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6361572,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "bfe5d6ece68c746148da9949364517c2d4539e71",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The evolution of a population can be guided by phenotypic traits acquired by members of that population during their lifetime. This phenomenon, known as the Baldwin effect, can speed the evolutionary process as traits that are initially acquired become genetically specified in later generations. This paper presents conditions under which this genetic assimilation can take place. As well as the benefits that lifetime adaptation can give a population, there may be a cost to be paid for that adaptive ability. It is the evolutionary trade-off between these costs and benefits that provides the selection pressure for acquired traits to become genetically specified. It is also noted that genotypic space, in which evolution operates, and phenotypic space, in which adaptive processes (such as learning) operate, are, in general, of a different nature. To guarantee that an acquired characteristic can become genetically specified, these spaces must have the property of neighborhood correlation, which means that a small distance between two individuals in phenotypic space implies that there is a small distance between the same two individuals in genotypic space."
            },
            "slug": "Landscapes,-Learning-Costs,-and-Genetic-Mayley",
            "title": {
                "fragments": [],
                "text": "Landscapes, Learning Costs, and Genetic Assimilation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Conditions under which genetic assimilation can take place in the evolutionary process as traits that are initially acquired become genetically specified in later generations are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2653849"
                        ],
                        "name": "F. Rieke",
                        "slug": "F.-Rieke",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Rieke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Rieke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72300681"
                        ],
                        "name": "D. Warland",
                        "slug": "D.-Warland",
                        "structuredName": {
                            "firstName": "Davd",
                            "lastName": "Warland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Warland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2991348"
                        ],
                        "name": "R. Steveninck",
                        "slug": "R.-Steveninck",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Steveninck",
                            "middleNames": [
                                "R.",
                                "de",
                                "Ruyter",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Steveninck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762240"
                        ],
                        "name": "W. Bialek",
                        "slug": "W.-Bialek",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Bialek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Bialek"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 206
                            }
                        ],
                        "text": "This hints at the fact that spiking neurons may use other ways to efficiently encode information, such as the firing time of single spikes or the temporal coincidence of spikes coming from multiple sources [77, 67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62646232,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "87378cc06871a3812a0543be46455597eb108443",
            "isKey": false,
            "numCitedBy": 2824,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Our perception of the world is driven by input from the sensory nerves. This input arrives encoded as sequences of identical spikes. Much of neural computation involves processing these spike trains. What does it mean to say that a certain set of spikes is the right answer to a computational problem? In what sense does a spike train convey information about the sensory world? Spikes begins by providing precise formulations of these and related questions about the representation of sensory signals in neural spike trains. The answers to these questions are then pursued in experiments on sensory neurons.The authors invite the reader to play the role of a hypothetical observer inside the brain who makes decisions based on the incoming spike trains. Rather than asking how a neuron responds to a given stimulus, the authors ask how the brain could make inferences about an unknown stimulus from a given neural response. The flavor of some problems faced by the organism is captured by analyzing the way in which the observer can make a running reconstruction of the sensory stimulus as it evolves in time. These ideas are illustrated by examples from experiments on several biological systems. Intended for neurobiologists with an interest in mathematical analysis of neural data as well as the growing number of physicists and mathematicians interested in information processing by \"real\" nervous systems, Spikes provides a self-contained review of relevant concepts in information theory and statistical decision theory. A quantitative framework is used to pose precise questions about the structure of the neural code. These questions in turn influence both the design and analysis of experiments on sensory neurons."
            },
            "slug": "Spikes:-Exploring-the-Neural-Code-Rieke-Warland",
            "title": {
                "fragments": [],
                "text": "Spikes: Exploring the Neural Code"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Spikes provides a self-contained review of relevant concepts in information theory and statistical decision theory about the representation of sensory signals in neural spike trains and a quantitative framework is used to pose precise questions about the structure of the neural code."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2926859"
                        ],
                        "name": "S. Quartz",
                        "slug": "S.-Quartz",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Quartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Quartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5818342,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "f61b125715cdfda589109b6349cd7861ee6a00e2",
            "isKey": false,
            "numCitedBy": 773,
            "numCiting": 590,
            "paperAbstract": {
                "fragments": [],
                "text": "How do minds emerge from developing brains? According to \u201cneural constructivism,\u201d the representational features of cortex are built from the dynamic interaction between neural growth mechanisms and environmentally derived neural activity. Contrary to popular selectionist models that emphasize regressive mechanisms, the neurobiological evidence suggests that this growth is a progressive increase in the representational properties of cortex. The interaction between the environment and neural growth results in a flexible type of learning: \u201cconstructive learning\u201d minimizes the need for prespecification in accordance with recent neurobiological evidence that the developing cerebral cortex is largely free of domain-specific structure. Instead, the representational properties of cortex are built by the nature of the problem domain confronting it. This uniquely powerful and general learning strategy undermines the central assumption of classical learnability theory, that the learning properties of a system can be deduced from a fixed computational architecture. Neural constructivism suggests that the evolutionary emergence of neocortex in mammals is a progression toward more flexible representational structures, in contrast to the popular view of cortical evolution as an increase in innate, specialized circuits. Human cortical postnatal development is also more extensive and protracted than generally supposed, suggesting that cortex has evolved so as to maximize the capacity of environmental structure to shape its structure and function through constructive learning."
            },
            "slug": "The-neural-basis-of-cognitive-development:-A-Quartz-Sejnowski",
            "title": {
                "fragments": [],
                "text": "The neural basis of cognitive development: A constructivist manifesto"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Neural constructivism suggests that the evolutionary emergence of neocortex in mammals is a progression toward more flexible representational structures, in contrast to the popular view of cortical evolution as an increase in innate, specialized circuits."
            },
            "venue": {
                "fragments": [],
                "text": "Behavioral and Brain Sciences"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2432655"
                        ],
                        "name": "J. Reisinger",
                        "slug": "J.-Reisinger",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Reisinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Reisinger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49732612"
                        ],
                        "name": "E. Bah\u00e7eci",
                        "slug": "E.-Bah\u00e7eci",
                        "structuredName": {
                            "firstName": "Erkin",
                            "lastName": "Bah\u00e7eci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Bah\u00e7eci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50355738"
                        ],
                        "name": "Igor Karpov",
                        "slug": "Igor-Karpov",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Karpov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Igor Karpov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Neuro-evolution of augmenting topologies (NEAT) has been applied to many problems such as pole balancing [79], robot control [80], computer games [ 66 , 82 ]o r an automobile crash warning system [81]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9916444,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a39c1411b6e68457ece391b057dcc26142c23a7",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The General Game Playing Competition (Genesereth et al., 2005) poses a unique challenge for artificial intelligence. To be successful, a player must learn to play well in a limited number of example games encoded in first-order logic and then generalize its game play to previously unseen games with entirely different rules. Because good opponents are usually not available, learning algorithms must come up with plausible opponent strategies in order to benchmark performance. One approach to simultaneously learning all player strategies is coevolution. This paper presents a coevolutionary approach using neuroevolution of augmenting topologies to evolve populations of game state evaluators. This approach is tested on a sample of games from the General Game Playing Competition and shown to be effective: It allows the algorithm designer to minimize the amount of domain knowledge built into the system, which leads to more general game play and allows modeling opponent strategies efficiently. Furthermore, the general game playing domain proves to be a powerful tool for developing and testing coevolutionary methods"
            },
            "slug": "Coevolving-Strategies-for-General-Game-Playing-Reisinger-Bah\u00e7eci",
            "title": {
                "fragments": [],
                "text": "Coevolving Strategies for General Game Playing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A coevolutionary approach using neuroevolution of augmenting topologies to evolve populations of game state evaluators is presented, which allows the algorithm designer to minimize the amount of domain knowledge built into the system, which leads to more general game play and allows modeling opponent strategies efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE Symposium on Computational Intelligence and Games"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2288013"
                        ],
                        "name": "C. Mattiussi",
                        "slug": "C.-Mattiussi",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Mattiussi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mattiussi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "(AGE) is such a representation which has been applied to the evolution of ANNs [15,  44 , 46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15472709,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "13c72ee2dcc035ac0b6268cf1bbabd5e1401634b",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce and apply a genetic representation for analog electronic circuits based on the association of character strings extracted from the genome with the terminals and parameters of components, and the use of local string alignment to generate the connection between components. The representation produces a variable genome length structure that tolerates the execution of major genome reorganization operators such as duplication and transposition, along with less disruptive ones such as character insertion, deletion and substitution. The representation can be applied also to other analog networks such as artificial neural networks, control systems, and genetic regulatory networks."
            },
            "slug": "Evolution-of-analog-networks-using-local-string-on-Mattiussi-Floreano",
            "title": {
                "fragments": [],
                "text": "Evolution of analog networks using local string alignment on highly reorganizable genomes"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A genetic representation for analog electronic circuits based on the association of character strings extracted from the genome with the terminals and parameters of components, and the use of local string alignment to generate the connection between components is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 2004 NASA/DoD Conference on Evolvable Hardware, 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2632932"
                        ],
                        "name": "S. Magg",
                        "slug": "S.-Magg",
                        "structuredName": {
                            "firstName": "Sven",
                            "lastName": "Magg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Magg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143801577"
                        ],
                        "name": "A. Philippides",
                        "slug": "A.-Philippides",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Philippides",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Philippides"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This is in line with Magg and Philippides [ 43 ] whose results indicate that GasNets perform extremely well on tasks that require some kind of neural pattern generator or timer, while performing worse on tasks which do not require different time scales in the network."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 3113482,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ccffb0e8cf643ca3559c2b6c817592c9646081bf",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last few years a lot of work has been done to discover why GasNets outperform other network types in terms of evolvability In this work GasNets are again compared to CTRNNs on a shape discrimination task This task is used as to solve it, or gain an advantage, a controller does not need timers or pattern generators We show that GasNets are outperformed by CTRNNs in terms of evolvability on this task and possible reasons for the disadvantages of GasNets are investigated It is shown that, on a simple task where there is no necessity for a timer or pattern generator, there may be other issues which are better tackled by CTRNNs."
            },
            "slug": "GasNets-and-CTRNNs-A-Comparison-in-Terms-of-Magg-Philippides",
            "title": {
                "fragments": [],
                "text": "GasNets and CTRNNs - A Comparison in Terms of Evolvability"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that, on a simple task where there is no necessity for a timer or pattern generator, there may be other issues which are better tackled by CTRNNs."
            },
            "venue": {
                "fragments": [],
                "text": "SAB"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67024781"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Sejnowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 97
                            }
                        ],
                        "text": "learning mechanisms that result in behaviors similar to those produced by Reinforcement learning [50, 74]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 224172,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "2811534b332206223188687e214ebbde0f2f294d",
            "isKey": false,
            "numCitedBy": 1847,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop a theoretical framework that shows how mesencephalic dopamine systems could distribute to their targets a signal that represents information about future expectations. In particular, we show how activity in the cerebral cortex can make predictions about future receipt of reward and how fluctuations in the activity levels of neurons in diffuse dopamine systems above and below baseline levels would represent errors in these predictions that are delivered to cortical and subcortical targets. We present a model for how such errors could be constructed in a real brain that is consistent with physiological results for a subset of dopaminergic neurons located in the ventral tegmental area and surrounding dopaminergic neurons. The theory also makes testable predictions about human choice behavior on a simple decision-making task. Furthermore, we show that, through a simple influence on synaptic plasticity, fluctuations in dopamine release can act to change the predictions in an appropriate manner."
            },
            "slug": "A-framework-for-mesencephalic-dopamine-systems-on-Montague-Dayan",
            "title": {
                "fragments": [],
                "text": "A framework for mesencephalic dopamine systems based on predictive Hebbian learning"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A theoretical framework is developed that shows how mesencephalic dopamine systems could distribute to their targets a signal that represents information about future expectations and shows that, through a simple influence on synaptic plasticity, fluctuations in dopamine release can act to change the predictions in an appropriate manner."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of neuroscience : the official journal of the Society for Neuroscience"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15291527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff2c2e3e83d1e8828695484728393c76ee07a101",
            "isKey": false,
            "numCitedBy": 15736,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system."
            },
            "slug": "Parallel-distributed-processing:-explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809791"
                        ],
                        "name": "K. Chellapilla",
                        "slug": "K.-Chellapilla",
                        "structuredName": {
                            "firstName": "Kumar",
                            "lastName": "Chellapilla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chellapilla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "playing [13], data classification [12] or the control of robot swarms [85]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6252984,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27df56624d9e0f8570e2624d6226201338b7df7f",
            "isKey": false,
            "numCitedBy": 209,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "An evolutionary algorithm has taught itself how to play the game of checkers without using features that would normally require human expertise. Using only the raw positions of pieces on the board and the piece differential, the evolutionary program optimized artificial neural networks to evaluate alternative positions in the game. Over the course of several hundred generations, the program taught itself to play at a level that is competitive with human experts (one level below human masters). This was verified by playing the best evolved neural network against 165 human players on an Internet gaming zone. The neural network's performance earned a rating that was better than 99.61% of all registered players at the Website. Control experiments between the best evolved neural network and a program that relies on material advantage indicate the superiority of the neural network both at equal levels of look ahead and CPU time. The results suggest that the principles of Darwinian evolution may he usefully applied to solving problems that have not yet been solved by human expertise."
            },
            "slug": "Evolving-an-expert-checkers-playing-program-without-Chellapilla-Fogel",
            "title": {
                "fragments": [],
                "text": "Evolving an expert checkers playing program without using human expertise"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Control experiments between the best evolved neural network and a program that relies on material advantage indicate the superiority of the neural network both at equal levels of look ahead and CPU time."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Evol. Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145401965"
                        ],
                        "name": "W. Schultz",
                        "slug": "W.-Schultz",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schultz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790646"
                        ],
                        "name": "P. Dayan",
                        "slug": "P.-Dayan",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Dayan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dayan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895942"
                        ],
                        "name": "P. Montague",
                        "slug": "P.-Montague",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Montague",
                            "middleNames": [
                                "Read"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Montague"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 97
                            }
                        ],
                        "text": "learning mechanisms that result in behaviors similar to those produced by Reinforcement learning [50, 74]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 220093382,
            "fieldsOfStudy": [
                "Psychology",
                "Biology"
            ],
            "id": "12b9019f99a315a137400389ee7c6faa4cceef35",
            "isKey": false,
            "numCitedBy": 7635,
            "numCiting": 114,
            "paperAbstract": {
                "fragments": [],
                "text": "The capacity to predict future events permits a creature to detect, model, and manipulate the causal structure of its interactions with its environment. Behavioral experiments suggest that learning is driven by changes in the expectations about future salient events such as rewards and punishments. Physiological work has recently complemented these studies by identifying dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events. Taken together, these findings can be understood through quantitative theories of adaptive optimizing control."
            },
            "slug": "A-Neural-Substrate-of-Prediction-and-Reward-Schultz-Dayan",
            "title": {
                "fragments": [],
                "text": "A Neural Substrate of Prediction and Reward"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Findings in this work indicate that dopaminergic neurons in the primate whose fluctuating output apparently signals changes or errors in the predictions of future salient and rewarding events can be understood through quantitative theories of adaptive optimizing control."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144539890"
                        ],
                        "name": "N. Hansen",
                        "slug": "N.-Hansen",
                        "structuredName": {
                            "firstName": "Nikolaus",
                            "lastName": "Hansen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Hansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069398"
                        ],
                        "name": "A. Ostermeier",
                        "slug": "A.-Ostermeier",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ostermeier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ostermeier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7524826,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f1bdebedf07fd444628c955568f0d51e1a26835e",
            "isKey": false,
            "numCitedBy": 3374,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation. Principle shortcomings of the concept of mutative strategy parameter control and two levels of derandomization are reviewed. Basic demands on the self-adaptation of arbitrary (normal) mutation distributions are developed. Applying arbitrary, normal mutation distributions is equiv-alent to applying a general, linear problem encoding. The underlying objective of mutative strategy parameter control is roughly to favor previously selected mutation steps in the future. If this objective is pursued rigor-ously, a completely derandomized self-adaptation scheme results, which adapts arbitrary normal mutation distributions. This scheme, called covariance matrix adaptation (CMA), meets the previously stated demands. It can still be considerably improved by cumulation - utilizing an evolution path rather than single search steps. Simulations on various test functions reveal local and global search properties of the evolution strategy with and without covariance matrix adaptation. Their performances are comparable only on perfectly scaled functions. On badly scaled, non-separable functions usually a speed up factor of several orders of magnitude is ob-served. On moderately mis-scaled functions a speed up factor of three to ten can be expected."
            },
            "slug": "Completely-Derandomized-Self-Adaptation-in-Hansen-Ostermeier",
            "title": {
                "fragments": [],
                "text": "Completely Derandomized Self-Adaptation in Evolution Strategies"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This paper puts forward two useful methods for self-adaptation of the mutation distribution - the concepts of derandomization and cumulation and reveals local and global search properties of the evolution strategy with and without covariance matrix adaptation."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398192342"
                        ],
                        "name": "Una-May O\u2019Reilly",
                        "slug": "Una-May-O\u2019Reilly",
                        "structuredName": {
                            "firstName": "Una-May",
                            "lastName": "O\u2019Reilly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Una-May O\u2019Reilly"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "Inspired by Koza\u2019s work on automatic discovery of reusable programs [42], Gruau also considered the case of genotypes formed by many trees"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31795221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "465418b258a94b8cd44fc255654103b824714f2d",
            "isKey": false,
            "numCitedBy": 1705,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Reading Genetic Programming IE Automatic Discovery ofReusable Programs (GPII) in its entirety is not a task for the weak-willed because the book without appendices is about 650 pages. An entire previous book by the same author [1] is devoted to describing Genetic Programming (GP), while this book is a sequel extolling an extension called Automatically Defined Functions (ADFs). The author, John R. Koza, argues that ADFs can be used in conjunction with GP to improve its efficacy on large problems. \"An automatically defined function (ADF) is a function (i.e., subroutine, procedure, module) that is dynamically evolved during a run of genetic programming and which may be called by a calling program (e.g., a main program) that is simultaneously being evolved\" (p. 1). Dr. Koza recommends adding the ADF technique to the \"GP toolkit.\" The book presents evidence that it is possible to interpret GP with ADFs as performing either a top-down process of problem decomposition or a bottom-up process of representational change to exploit identified regularities. This is stated as Main Point 1. Main Point 2 states that ADFs work by exploiting inherent regularities, symmetries, patterns, modularities, and homogeneities within a problem, though perhaps in ways that are very different from the style of programmers. Main Points 3 to 7 are appropriately qualified statements to the effect that, with a variety of problems, ADFs pay off be-"
            },
            "slug": "Genetic-Programming-II:-Automatic-Discovery-of-O\u2019Reilly",
            "title": {
                "fragments": [],
                "text": "Genetic Programming II: Automatic Discovery of Reusable Programs."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This book presents evidence that it is possible to interpret GP with ADFs as performing either a top-down process of problem decomposition or a bottom-up process of representational change to exploit identified regularities."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Life"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758662"
                        ],
                        "name": "N. Geard",
                        "slug": "N.-Geard",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Geard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Geard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716264"
                        ],
                        "name": "Janet Wiles",
                        "slug": "Janet-Wiles",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Wiles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Janet Wiles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "Geard and Wiles [25] refined this model by complexifying the map that transforms a gene into a regulator protein, adding a further level of regulation which mimics the action of small RNA regulation in real genomes and defining regulation in terms of a weighted sum of regulator proteins effects."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14095939,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f55ee1afb933292b86b0ab53a45d36007b4c839d",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "As advances in molecular biology continue to reveal additional layers of complexity in gene regulation, computational models need to incorporate additional features to explore the implications of new theories and hypotheses. It has recently been suggested that eukaryotic organisms owe their phenotypic complexity and diversity to the exploitation of small RNAs as signalling molecules. Previous models of genetic systems are, for several reasons, inadequate to investigate this theory. In this study, we present an artificial genome model of genetic regulatory networks based upon previous work by Torsten Reil, and demonstrate how this model generates networks with biologically plausible structural and dynamic properties. We also extend the model to explore the implications of incorporating regulation by small RNA molecules in a gene network. We demonstrate how, using these signals, highly connected networks can display dynamics that are more stable than expected given their level of connectivity."
            },
            "slug": "Structure-and-dynamics-of-a-gene-network-model-RNAs-Geard-Wiles",
            "title": {
                "fragments": [],
                "text": "Structure and dynamics of a gene network model incorporating small RNAs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This study presents an artificial genome model of genetic regulatory networks based upon previous work by Torsten Reil, and demonstrates how this model generates networks with biologically plausible structural and dynamic properties and extends the model to explore the implications of incorporating regulation by small RNA molecules in a gene network."
            },
            "venue": {
                "fragments": [],
                "text": "The 2003 Congress on Evolutionary Computation, 2003. CEC '03."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2566982"
                        ],
                        "name": "J. Shapiro",
                        "slug": "J.-Shapiro",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Shapiro",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shapiro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, gene duplication and the insertion of fragments of genome of foreign organisms are deemed to be crucial mechanism for the evolutionary increase of complexity of GRNs [ 75 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18172576,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e3788f1325e56050373ad40546d018ee10462b0a",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 115,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-21st-century-view-of-evolution:-genome-system-and-Shapiro",
            "title": {
                "fragments": [],
                "text": "A 21st century view of evolution: genome system architecture, repetitive DNA, and natural genetic engineering."
            },
            "venue": {
                "fragments": [],
                "text": "Gene"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The actor-critic model [5,  83 ] for example, is a simple neural architecture that implements reinforcement learning with two modules, the Actor and the Critic shown in Fig. 12. Both modules receive information on the current sensory state."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3349598,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a91635f8d0e7fb804efd1c38d9c24ee952ba7076",
            "isKey": false,
            "numCitedBy": 3572,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "This article introduces a class of incremental learning procedures specialized for prediction \u2013 that is, for using past experience with an incompletely known system to predict its future behavior. Whereas conventional prediction-learning methods assign credit by means of the difference between predicted and actual outcomes, the new methods assign credit by means of the difference between temporally successive predictions. Although such temporal-difference methods have been used in Samuel's checker player, Holland's bucket brigade, and the author's Adaptive Heuristic Critic, they have remained poorly understood. Here we prove their convergence and optimality for special cases and relate them to supervised-learning methods. For most real-world prediction problems, temporal-difference methods require less memory and less peak computation than conventional methods and they produce more accurate predictions. We argue that most problems to which supervised learning is currently applied are really prediction problems of the sort to which temporal-difference methods can be applied to advantage."
            },
            "slug": "Learning-to-Predict-by-the-Methods-of-Temporal-Sutton",
            "title": {
                "fragments": [],
                "text": "Learning to Predict by the Methods of Temporal Differences"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This article introduces a class of incremental learning procedures specialized for prediction \u2013 that is, for using past experience with an incompletely known system to predict its future behavior \u2013 and proves their convergence and optimality for special cases and relation to supervised-learning methods."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2805781"
                        ],
                        "name": "V. Trianni",
                        "slug": "V.-Trianni",
                        "structuredName": {
                            "firstName": "V.",
                            "lastName": "Trianni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Trianni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083089"
                        ],
                        "name": "C. Ampatzis",
                        "slug": "C.-Ampatzis",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Ampatzis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ampatzis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32232754"
                        ],
                        "name": "A. Christensen",
                        "slug": "A.-Christensen",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Christensen",
                            "middleNames": [
                                "Lyhne"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Christensen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2729994"
                        ],
                        "name": "Elio Tuci",
                        "slug": "Elio-Tuci",
                        "structuredName": {
                            "firstName": "Elio",
                            "lastName": "Tuci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Elio Tuci"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153570946"
                        ],
                        "name": "M. Dorigo",
                        "slug": "M.-Dorigo",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Dorigo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dorigo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015062"
                        ],
                        "name": "S. Nolfi",
                        "slug": "S.-Nolfi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Nolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nolfi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12395694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "613ac74e3d28b6432bf44de26c381df46f3bb123",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "In a social scenario, establishing whether a collaboration is required to achieve a certain goal is a complex problem that requires decision making capabilities and coordination among the members of the group. Depending on the environmental contingencies, solitary actions may result more efficient than collective ones and vice versa. In robotics, it may be difficult to estimate the utility of engaging in collaboration versus remaining solitary, especially if the robots have only limited knowledge about the environment. In this paper, we use artificial evolution to synthesise neural controllers that let a homogeneous group of robots decide when to switch from solitary to collective actions based on the information gathered through time. However, being in a social scenario, the decision taken by a robot can influence--and is influenced itself--by the status of the other robots that are taking their own decisions at the same time. We show that the simultaneous presence of robots trying to decide whether to engage in a collective action or not can lead to cooperation in the decision making process itself."
            },
            "slug": "From-Solitary-to-Collective-Behaviours:-Decision-Trianni-Ampatzis",
            "title": {
                "fragments": [],
                "text": "From Solitary to Collective Behaviours: Decision Making and Cooperation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Artificial evolution is used to synthesise neural controllers that let a homogeneous group of robots decide when to switch from solitary to collective actions based on the information gathered through time and shows that the simultaneous presence of robots trying to decide whether to engage in a collective action or not can lead to cooperation in the decision making process itself."
            },
            "venue": {
                "fragments": [],
                "text": "ECAL"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2507766"
                        ],
                        "name": "W. Banzhaf",
                        "slug": "W.-Banzhaf",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Banzhaf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Banzhaf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2191107"
                        ],
                        "name": "F. Francone",
                        "slug": "F.-Francone",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Francone",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Francone"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105602651"
                        ],
                        "name": "Robert E. Keller",
                        "slug": "Robert-E.-Keller",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Keller",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert E. Keller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1835888"
                        ],
                        "name": "P. Nordin",
                        "slug": "P.-Nordin",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Nordin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nordin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 57323334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f374bdf3b57e62944e5d99f8589b187624cf3951",
            "isKey": false,
            "numCitedBy": 1811,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Genetic Programming as Machine Learning 2 Genetic Programming and Biology 3 Computer Science and Mathematical Basics 4 Genetic Programming as Evolutionary Computation 5 Basic ConceptsThe Foundation 6 CrossoverThe Center of the Storm 7 Genetic Programming and Emergent Order 8 AnalysisImproving Genetic Programming with Statistics 9 Different Varieties of Genetic Programming 10 Advanced Genetic Programming 11 ImplementationMaking Genetic Programming Work 12 Applications of Genetic Programming 13 Summary and Perspectives A Printed and Recorded Resources B Information Available on the Internet C GP Software D Events"
            },
            "slug": "Genetic-programming-An-Introduction:-On-the-of-and-Banzhaf-Francone",
            "title": {
                "fragments": [],
                "text": "Genetic programming - An Introduction: On the Automatic Evolution of Computer Programs and Its Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book presents a meta-modelling framework for genetic programming that automates the very labor-intensive and therefore time-heavy and expensive process of designing and implementing genetic algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2288013"
                        ],
                        "name": "C. Mattiussi",
                        "slug": "C.-Mattiussi",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Mattiussi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mattiussi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066182179"
                        ],
                        "name": "Peter D\u00fcrr",
                        "slug": "Peter-D\u00fcrr",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "D\u00fcrr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter D\u00fcrr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[45], where the characters of the string are interpreted as a system of particles whose center of mass determines the encoded value."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6369324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c763a1abcec49d50cc4b34960991ac087d4da2f8",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a new class of representations for real-valued parameters called Center of Mass Encoding (CoME). CoME is based on variable length strings, it is self-adaptive, and it permits the choice of the degree of redundancy of the genotype-to-phenotype map and the choice of the distribution of the redundancy over the space of phenotypes. We first describe CoME and then proceed to test its performance and compare it with other representations and with a state-of-the-art evolution strategy. We show that CoME performs well on a large set of test functions. Furthermore, we show how CoME adapts the granularity of its discretization on functions defined over nonuniformly scaled domains."
            },
            "slug": "Center-of-mass-encoding:-a-self-adaptive-with-for-Mattiussi-D\u00fcrr",
            "title": {
                "fragments": [],
                "text": "Center of mass encoding: a self-adaptive representation with adjustable redundancy for real-valued parameters"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "This paper describes a new class of representations for real-valued parameters called Center of Mass Encoding (CoME), based on variable length strings, which is self-adaptive, and permits the choice of the degree of redundancy of the genotype-to-phenotype map and the distribution of the redundancy over the space of phenotypes."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846883"
                        ],
                        "name": "Kenneth O. Stanley",
                        "slug": "Kenneth-O.-Stanley",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Stanley",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth O. Stanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070100066"
                        ],
                        "name": "Ryan Cornelius",
                        "slug": "Ryan-Cornelius",
                        "structuredName": {
                            "firstName": "Ryan",
                            "lastName": "Cornelius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ryan Cornelius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Neuro-evolution of augmenting topologies (NEAT) has been applied to many problems such as pole balancing [79], robot control [80], computer games [66,  82  ]o r an automobile crash warning system [81]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 30892256,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "5ae4e964f12ccf011a9174df87ae1b313a4834a7",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "If game characters could learn through interacting with the player, behavior could improve as the game is played, keeping it interesting. The real-time NeuroEvolution of Augmenting Topologies (rtNEAT) method, which can evolve increasingly complex artificial neural networks in real time as a game is being played, will be presented. The rtNEAT method makes possible an entirely new genre of video games in which the player trains a team of agents through a series of customized exercises. In order to demonstrate this concept, the NeuroEvolving Robotic Operatives (NERO) game was built based on rtNEAT. In NERO, the player trains a team of virtual robots for combat against other players\u2019 teams. The live demo will show how agents in NERO adapt in real time as they interact with the player. In the future, rtNEAT may allow new kinds of educational and training applications through interactive and adapting games."
            },
            "slug": "Real-time-Learning-in-the-NERO-Video-Game-Stanley-Cornelius",
            "title": {
                "fragments": [],
                "text": "Real-time Learning in the NERO Video Game"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The real-time NeuroEvolution of Augmenting Topologies (rtNEAT) method, which can evolve increasingly complex artificial neural networks in real time as a game is being played, will be presented."
            },
            "venue": {
                "fragments": [],
                "text": "AIIDE"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109967142"
                        ],
                        "name": "Sanjeev Kumar",
                        "slug": "Sanjeev-Kumar",
                        "structuredName": {
                            "firstName": "Sanjeev",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjeev Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145074286"
                        ],
                        "name": "P. Bentley",
                        "slug": "P.-Bentley",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Bentley",
                            "middleNames": [
                                "John"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Bentley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59959636,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "7f6c9f353dbc720b7b260d2eb2339e6605a8b3fc",
            "isKey": false,
            "numCitedBy": 134,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-growth,-form-and-computers-Kumar-Bentley",
            "title": {
                "fragments": [],
                "text": "On growth, form and computers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144895766"
                        ],
                        "name": "C. H. Bailey",
                        "slug": "C.-H.-Bailey",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Bailey",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. H. Bailey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4611602"
                        ],
                        "name": "M. Giustetto",
                        "slug": "M.-Giustetto",
                        "structuredName": {
                            "firstName": "Maurizio",
                            "lastName": "Giustetto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Giustetto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100975718"
                        ],
                        "name": "Yan-You Huang",
                        "slug": "Yan-You-Huang",
                        "structuredName": {
                            "firstName": "Yan-You",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan-You Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057580732"
                        ],
                        "name": "R. Hawkins",
                        "slug": "R.-Hawkins",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hawkins",
                            "middleNames": [
                                "X.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hawkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5177815"
                        ],
                        "name": "E. Kandel",
                        "slug": "E.-Kandel",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Kandel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kandel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 110
                            }
                        ],
                        "text": "The existing evidence points to the combined action of evolved value systems [58] and neuromodulatory effects [2, 18]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205009448,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "69ca313aa99bf30feb3801a1ae19825b80678b39",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 159,
            "paperAbstract": {
                "fragments": [],
                "text": "In 1894, Ram\u00f3n y Cajal first proposed that memory is stored as an anatomical change in the strength of neuronal connections. For the following 60 years, little evidence was recruited in support of this idea. This situation changed in the middle of the twentieth century with the development of cellular techniques for the study of synaptic connections and the emergence of new formulations of synaptic plasticity that redefined Ram\u00f3n y Cajal's idea, making it more suitable for testing. These formulations defined two categories of plasticity, referred to as homosynaptic or Hebbian activity-dependent, and heterosynaptic or modulatory input-dependent. Here we suggest that Hebbian mechanisms are used primarily for learning and for short-term memory but often cannot, by themselves, recruit the events required to maintain a long-term memory. In contrast, heterosynaptic plasticity commonly recruits long-term memory mechanisms that lead to transcription and to synaptic growth. When jointly recruited, homosynaptic mechanisms assure that learning is effectively established and heterosynaptic mechanisms ensure that memory is maintained."
            },
            "slug": "Is-Heterosynaptic-modulation-essential-for-hebbian-Bailey-Giustetto",
            "title": {
                "fragments": [],
                "text": "Is Heterosynaptic modulation essential for stabilizing hebbian plasiticity and memory"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is suggested that Hebbian mechanisms are used primarily for learning and for short-term memory but often cannot, by themselves, recruit the events required to maintain a long-termMemory, while heterosynaptic plasticity commonly recruits long- term memory mechanisms that lead to transcription and to synaptic growth."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Reviews Neuroscience"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144999249"
                        ],
                        "name": "G. Kane",
                        "slug": "G.-Kane",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Kane",
                            "middleNames": [
                                "Stanley"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kane"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 73
                            }
                        ],
                        "text": "For example, gradient-based learning algorithms such as back-propagation [68, 69] are sensitive to the initial weight values, which may Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 71267845,
            "fieldsOfStudy": [
                "Computer Science",
                "History"
            ],
            "id": "78f6f0ac3d501cb0073a7d94edde5267044a59ae",
            "isKey": false,
            "numCitedBy": 2719,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural Computing: Theory and Practice , by Philip Wasserman, 230 pp, $41.95, with illus, ISBN 0-442-20743-3, New York, NY, Van Nostrand Reinhold, 1989. Neural Networks: A Tutorial , by Michael Chester, 182 pp, $38, with illus, ISBN 0-13-368903-4, Englewood Cliffs, NJ, Prentice Hall, 1993. Neural Networks: Algorithms, Applications, and Programming Techniques , by James Freeman and David Skapura, 401 pp, $50.50, with illus, ISBN 0-201-51376-5, Reading, Mass, Addison-Wesley, 1991. Understanding Neural Networks: Computer Explorations , vol 1: Basic Networks , vol 2: Advanced Networks , 309, 367 pp, by Maureen Caudill and Charles Butler, paper, with illus, spiral-bound, with 1 diskette/vol, $39.95/vol, vol 1: ISBN0-262-53102-X (Macintosh), 0-262-53099-6 (IBM), vol 2: ISBN 0-262-53103-8 (Macintosh), 0-262-53100-3 (IBM), Cambridge, Mass, The MIT Press, 1992. Artificial neural network research began in the early 1940s, advancing in fits and starts, until the late 1960s when Minsky and Papert published Perceptrons , in which they proved that neural networks, as then conceived, can"
            },
            "slug": "Parallel-Distributed-Processing:-Explorations-in-of-Kane",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the Microstructure of Cognition, vol 1: Foundations, vol 2: Psychological and Biological Models"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Artificial neural network research began in the early 1940s, advancing in fits and starts, until the late 1960s when Minsky and Papert published Perceptrons, in which they proved that neural networks, as then conceived, can be proved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108275637"
                        ],
                        "name": "J. Baldwin",
                        "slug": "J.-Baldwin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Baldwin",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baldwin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7059820,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "0c1386d88cb54eb17c6a2745cd4bf15dbb3f2b09",
            "isKey": false,
            "numCitedBy": 1624,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In several recent publications I have developed, from different points of view, some considerations which tend to bring out a certain influence at work in organic evolutionwhich I venture to call \u201ca new factor.\u201d I give below a list of references1 to these publications and shall refer to them by number as this paper proceeds. The object of the present paper is to gather into one sketch an outline of the view of the process of development which these different publications have hinged upon. The problems involved in a theory of organic development may be gathered up under three great heads: Ontogeny, Phylogeny, Heredity. The general consideration, the \u201cfactor\u201d which I propose to bring out, is operative in the first instance, in the field of Ontogeny; I shall consequently speak first of the problem of Ontogeny, then of that of Phylogeny, in so far as the topic dealt with makes it necessary, then of that of Heredity, under the same limitation, and finally, give some definitions and conclusions."
            },
            "slug": "A-New-Factor-in-Evolution-Baldwin",
            "title": {
                "fragments": [],
                "text": "A New Factor in Evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The object of the present paper is to gather into one sketch an outline of the view of the process of development which these different publications have hinged upon."
            },
            "venue": {
                "fragments": [],
                "text": "The American Naturalist"
            },
            "year": 1896
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712899"
                        ],
                        "name": "W. Singer",
                        "slug": "W.-Singer",
                        "structuredName": {
                            "firstName": "Wolf",
                            "lastName": "Singer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145941291"
                        ],
                        "name": "C. Gray",
                        "slug": "C.-Gray",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gray",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gray"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 206
                            }
                        ],
                        "text": "This hints at the fact that spiking neurons may use other ways to efficiently encode information, such as the firing time of single spikes or the temporal coincidence of spikes coming from multiple sources [77, 67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13493427,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c90c3c10cba3c4700298ab2883c2bfecd7401fae",
            "isKey": false,
            "numCitedBy": 3221,
            "numCiting": 139,
            "paperAbstract": {
                "fragments": [],
                "text": "The mammalian visual system is endowed with a nearly infinite capacity for the recognition of patterns and objects. To have acquired this capability the visual system must have solved what is a fundamentally combinatorial prob\u00ad lem. Any given image consists of a collection of features, consisting of local contrast borders of luminance and wavelength, distributed across the visual field. For one to detect and recognize an object within a scene, the features comprising the object must be identified and segregated from those comprising other objects. This problem is inherently difficult to solve because of the combinatorial nature of visual images. To appreciate this point, consider a simple local feature such as a small vertically oriented line segment placed within a fixed location of the visual field. When combined with other line segments, this feature can form a nearly infinite number of geometrical objects. Any one of these objects may coexist with an equally large number of other"
            },
            "slug": "Visual-feature-integration-and-the-temporal-Singer-Gray",
            "title": {
                "fragments": [],
                "text": "Visual feature integration and the temporal correlation hypothesis."
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The mammalian visual system is endowed with a nearly infinite capacity for the recognition of patterns and objects, but to have acquired this capability the visual system must have solved what is a fundamentally combinatorial prob\u00ad lem."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of neuroscience"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1863874"
                        ],
                        "name": "O. Hikosaka",
                        "slug": "O.-Hikosaka",
                        "structuredName": {
                            "firstName": "Okihide",
                            "lastName": "Hikosaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Hikosaka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53173484,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "2ad8afe073e03c928839a3f41fc9294b57b97c3e",
            "isKey": false,
            "numCitedBy": 192,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Models-of-information-processing-in-the-basal-by-C.-Hikosaka",
            "title": {
                "fragments": [],
                "text": "Models of information processing in the basal Ganglia \n edited by James C. Houk, Joel L. Davis and David G. Beiser, The MIT Press, 1995. $60.00 (400 pp) ISBN 0 262 08234 9\n"
            },
            "venue": {
                "fragments": [],
                "text": "Trends in Neurosciences"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46677293"
                        ],
                        "name": "W. Banzhaf",
                        "slug": "W.-Banzhaf",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Banzhaf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Banzhaf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 126
                            }
                        ],
                        "text": "The instructions contained in the genotype are represented with a binary tree structure and evolved using genetic programming [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 44843430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0ecdf9e186ce54645e12c9e40e32432acc3480d4",
            "isKey": false,
            "numCitedBy": 1316,
            "numCiting": 263,
            "paperAbstract": {
                "fragments": [],
                "text": "\u0089 Four appendices summarize valuable resources available for the reader: Appendix A contains printed and recorded resources, Appendix B suggests web-related resources, Appendix C discusses GP software tools, including Discipulus, the GP software developed by the authors, and Appendix D mentions events most closely related to the field of genetic programming. URLs can be found online at http://mkp.com/GPIntro."
            },
            "slug": "Genetic-Programming:-An-Introduction-Banzhaf",
            "title": {
                "fragments": [],
                "text": "Genetic Programming: An Introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The authors discuss GP software tools, including Discipulus, the GP software developed by the authors, and Appendix D mentions events most closely related to the field of genetic programming."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144203470"
                        ],
                        "name": "P. Moscato",
                        "slug": "P.-Moscato",
                        "structuredName": {
                            "firstName": "Pablo",
                            "lastName": "Moscato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Moscato"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "2 Algorithms which combine evolutionary search with some kinds of local search are sometimes called memetic algorithms [53]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1264156,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8b9a748ae77f9235396e04301b82143feb1167fe",
            "isKey": false,
            "numCitedBy": 1731,
            "numCiting": 239,
            "paperAbstract": {
                "fragments": [],
                "text": "Short abstract, isn't it?"
            },
            "slug": "On-Evolution,-Search,-Optimization,-Genetic-and-:-Moscato",
            "title": {
                "fragments": [],
                "text": "On Evolution, Search, Optimization, Genetic Algorithms and Martial Arts : Towards Memetic Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2225570"
                        ],
                        "name": "A. Hodgkin",
                        "slug": "A.-Hodgkin",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Hodgkin",
                            "middleNames": [
                                "Lloyd"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hodgkin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4211391"
                        ],
                        "name": "A. Huxley",
                        "slug": "A.-Huxley",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Huxley",
                            "middleNames": [
                                "Fielding"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Huxley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "describe in detail the electrochemical processes that produce spiking events by means of differential equations [34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20873334,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0d499ac0de809d38210140ab6e21c2e399838987",
            "isKey": false,
            "numCitedBy": 15305,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This article concludes a series of papers concerned with the flow of electric current through the surface membrane of a giant nerve fibre (Hodgkinet al., 1952,J. Physiol.116, 424\u2013448; Hodgkin and Huxley, 1952,J. Physiol.116, 449\u2013566). Its general object is to discuss the results of the preceding papers (Section 1), to put them into mathematical form (Section 2) and to show that they will account for conduction and excitation in quantitative terms (Sections 3\u20136)."
            },
            "slug": "A-quantitative-description-of-membrane-current-and-Hodgkin-Huxley",
            "title": {
                "fragments": [],
                "text": "A quantitative description of membrane current and its application to conduction and excitation in nerve."
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This article concludes a series of papers concerned with the flow of electric current through the surface membrane of a giant nerve fibre by putting them into mathematical form and showing that they will account for conduction and excitation in quantitative terms."
            },
            "venue": {
                "fragments": [],
                "text": "The Journal of physiology"
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808919"
                        ],
                        "name": "N. Radcliffe",
                        "slug": "N.-Radcliffe",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Radcliffe",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Radcliffe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "It has been argued [72, 61] that evolving neural networks may not be trivial because the population may include individuals with competing conventions (see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13935950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c665f8ff092e339fa7b3328f34a1f417b1dfbc80",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Intrinsic parallelism is shown to have application beyond schemata and o-schemata. More general objects called formae are introduced and general operators which manipulate these are introduced and discussed. These include random, respectful recombination. The extended formalism is applied to various common representations and standard operators are analysed in the light of the formalism."
            },
            "slug": "Forma-Analysis-and-Random-Respectful-Recombination-Radcliffe",
            "title": {
                "fragments": [],
                "text": "Forma Analysis and Random Respectful Recombination"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "Intrinsic parallelism is shown to have application beyond schemata and o-schemata and more general objects called formae are introduced and general operators which manipulate these are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682655"
                        ],
                        "name": "R. Belew",
                        "slug": "R.-Belew",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Belew",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Belew"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 14
                            }
                        ],
                        "text": "In this case, Belew et al. [8] found that the best evolved networks employed learning rates ten times higher than values suggested before (i.e., much less than 1.0), but this result may depend on several factors, such as the order of presentation of the patterns, the number of learning cycles allowed before computing the fitness, and the initial weight values."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "In order to make the mapping more adaptive, Schraudolph and Belew [73] suggested a dynamic encoding, where the bits allocated for each weight are used to encode the most significant part of the binary representation until the population has converged to a satisfactory solution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5802285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ad676f89f45037bbd1293f6b2b0570f24c7a1d9",
            "isKey": true,
            "numCitedBy": 62,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The common use of static binary place-value codes for real-valued parameters of the phenotype in Holland's genetic algorithm (GA) forces either the sacrifice of representational precision for efficiency of search or vice versa.Dynamic Parameter Encoding (DPE) is a mechanism that avoids this dilemma by using convergence statistics derived from the GA population to adaptively control the mapping from fixed-length binary genes to real values. DPE is shown to be empirically effective and amenable to analysis; we explore the problem ofpremature convergence in GAs through two convergence models."
            },
            "slug": "Dynamic-Parameter-Encoding-for-genetic-algorithms-Schraudolph-Belew",
            "title": {
                "fragments": [],
                "text": "Dynamic Parameter Encoding for genetic algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Dynamic Parameter Encoding is shown to be empirically effective and amenable to analysis; the problem of premature convergence in GAs is explored through two convergence models."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118969901"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145993114"
                        ],
                        "name": "R. William",
                        "slug": "R.-William",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "William",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. William"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 151380665,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "749ce8ccd9453d1b34901143cddf5f9bee2977cf",
            "isKey": false,
            "numCitedBy": 1339,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-representations-by-back-propagation-nature-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagation errors, nature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145887784"
                        ],
                        "name": "J. Nazuno",
                        "slug": "J.-Nazuno",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Nazuno",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nazuno"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 147456412,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "ca91cd1e67cb558dd2b7d94d37c3dba208b7cd40",
            "isKey": false,
            "numCitedBy": 3263,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Haykin,-Simon.-Neural-networks:-A-comprehensive-Nazuno",
            "title": {
                "fragments": [],
                "text": "Haykin, Simon. Neural networks: A comprehensive foundation, Prentice Hall, Inc. Segunda Edici\u00f3n, 1999"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "83922335"
                        ],
                        "name": "K. Gunderson",
                        "slug": "K.-Gunderson",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Gunderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Gunderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 146137377,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "f1bdd1ac644591083ec161929c04e48dc3fda537",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Paranoia-concerning-program-resistant-aspects-of-on-Gunderson",
            "title": {
                "fragments": [],
                "text": "Paranoia concerning program-resistant aspects of the mind - and let's drop rocks on Turing's toes again"
            },
            "venue": {
                "fragments": [],
                "text": "Behavioral and Brain Sciences"
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "104458796"
                        ],
                        "name": "De Rumerhart",
                        "slug": "De-Rumerhart",
                        "structuredName": {
                            "firstName": "De",
                            "lastName": "Rumerhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "De Rumerhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 73
                            }
                        ],
                        "text": "For example, gradient-based learning algorithms such as back-propagation [68, 69] are sensitive to the initial weight values, which may Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 125302131,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0900df8d62877a68f052b41cadb3ded6e142785",
            "isKey": false,
            "numCitedBy": 507,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-representations-of-back-propagation-errors-Rumelhart-Rumerhart",
            "title": {
                "fragments": [],
                "text": "Learning representations of back-propagation errors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214740"
                        ],
                        "name": "Torsten Reil",
                        "slug": "Torsten-Reil",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Reil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Torsten Reil"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 89474010,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9a1a74b869dcd337b6ca013dba5479239591ccc5",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "14-\u2013-Artificial-genomes-as-models-of-gene-Reil",
            "title": {
                "fragments": [],
                "text": "14 \u2013 Artificial genomes as models of gene regulation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 215822104,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "0e3c001c3b89d35006512d1e168d82636d58a067",
            "isKey": false,
            "numCitedBy": 2461,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Reinforcement-learning-Barto",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88108619"
                        ],
                        "name": "W. Vent",
                        "slug": "W.-Vent",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Vent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Vent"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 85086435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0acf50ce5c4e1268742f31e98ed294b8c967b829",
            "isKey": false,
            "numCitedBy": 1327,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Rechenberg,-Ingo,-Evolutionsstrategie-\u2014-Optimierung-Vent",
            "title": {
                "fragments": [],
                "text": "Rechenberg, Ingo, Evolutionsstrategie \u2014 Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. 170 S. mit 36 Abb. Frommann\u2010Holzboog\u2010Verlag. Stuttgart 1973. Broschiert"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2572845"
                        ],
                        "name": "P. Katz",
                        "slug": "P.-Katz",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Katz",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Katz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63608570,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "9973bb6975204c20edbdb2e08fb3f80cc3cb14ea",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "What-are-we-talking-about-Modes-of-neuronal-Katz",
            "title": {
                "fragments": [],
                "text": "What are we talking about? Modes of neuronal communication"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708945"
                        ],
                        "name": "W. Gerstner",
                        "slug": "W.-Gerstner",
                        "structuredName": {
                            "firstName": "Wulfram",
                            "lastName": "Gerstner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Gerstner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 64006532,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "5c129f56b872c5c540f6d72b443c08b29f571c4c",
            "isKey": false,
            "numCitedBy": 172,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spiking-neurons-Gerstner",
            "title": {
                "fragments": [],
                "text": "Spiking neurons"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71606688"
                        ],
                        "name": "S. Carlos",
                        "slug": "S.-Carlos",
                        "structuredName": {
                            "firstName": "Sio",
                            "lastName": "Carlos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carlos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "Similar results were obtained by Fontanari and Meir [23]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 61219842,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3edf872e2d4cefdca46591d0c7b3727cb10a6e0e",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Evolving-a-learning-algorithm-for-the-binary-Carlos",
            "title": {
                "fragments": [],
                "text": "Evolving a learning algorithm for the binary perceptron"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69040387"
                        ],
                        "name": "I. Rechenberg",
                        "slug": "I.-Rechenberg",
                        "structuredName": {
                            "firstName": "Ingo",
                            "lastName": "Rechenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Rechenberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "Recent benchmark experiments with Evolution Strategies [62], which use a floating-point representation of the synaptic weights, have reported excellent performance with direct encoding of a small, fixed architecture [38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60975248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d04942a086f9cafbb1c6453b64ba188beeb03823",
            "isKey": false,
            "numCitedBy": 3173,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Evolutionsstrategie-:-Optimierung-technischer-nach-Rechenberg",
            "title": {
                "fragments": [],
                "text": "Evolutionsstrategie : Optimierung technischer Systeme nach Prinzipien der biologischen Evolution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247053"
                        ],
                        "name": "W. Maass",
                        "slug": "W.-Maass",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Maass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Maass"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45716162,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1e01eec1bebffa6a252e7b034255fda981a3242b",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Spiking-Neurons-Maass",
            "title": {
                "fragments": [],
                "text": "Spiking Neurons"
            },
            "venue": {
                "fragments": [],
                "text": "NC"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699459"
                        ],
                        "name": "R. Nagpal",
                        "slug": "R.-Nagpal",
                        "structuredName": {
                            "firstName": "Radhika",
                            "lastName": "Nagpal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nagpal"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6933185,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53a034cf3712e7d708c007679c8dadc70faa95a5",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-Growth,-Form-and-Computers-Nagpal",
            "title": {
                "fragments": [],
                "text": "On Growth, Form and Computers"
            },
            "venue": {
                "fragments": [],
                "text": "Genetic Programming and Evolvable Machines"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143953756"
                        ],
                        "name": "J. Kennedy",
                        "slug": "J.-Kennedy",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Kennedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kennedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30901372,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "537a14a25f9fb508930a1ac5587e1b57a1b0ee19",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-1998-IEEE-International-on-[Book-Kennedy",
            "title": {
                "fragments": [],
                "text": "Proceedings of the 1998 IEEE International Conference on Evolutionary Computation [Book Review]"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Evolutionary Computation"
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evol. Intel"
            },
            "venue": {
                "fragments": [],
                "text": "Evol. Intel"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 109
                            }
                        ],
                        "text": "Indeed, it has been shown that the maturation process is affected by the activity patterns of single neurons [59, 60]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural activity in the growth of the brain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "When compared to NEAT, AGE reported equal performance on a non-Markovian double-pole balancing problem [16], while both algorithms performed better than a developmental encoding (CE) and a coevolution method (ESP)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 40
                            }
                        ],
                        "text": "3) outperformed ESP on these benchmarks [16, 79]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neuroevolution with Analog Genetic Encoding. In: Parallel problem solving from nature\u2014PPSN iX"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 5
                            }
                        ],
                        "text": "Reil [63, 64] studied the structural properties of such gene networks with a simple computational model."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On growth, form and computers. In: Artificial genomes as models of gene regulation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 6
                            }
                        ],
                        "text": "model [5, 83] for example, is a simple neural architecture that implements reinforcement learning with two modules,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to predict by the method of temporal difference"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learn"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 20
                            }
                        ],
                        "text": "In a classic study, Montana and Davis [51] compared the performance of synaptic weight evolution with a discrete direct representation with that of the back-propagation\nalgorithm on a problem of sonar data classification."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "In a classic study, Montana and Davis [51] compared the performance of synaptic weight evolution with a discrete direct representation with that of the back-propagation Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training feed forward neural networks using genetic algorithms. In: Proceedings of the 11th international joint conference on artificial intelligence"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "In a seminal paper, [40] suggested a developmental encoding based on a set of rewriting rules encoded in the genotype (see Fig."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Designing neural networks by genetic algorithms using graph generation system"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst J"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 226,
                                "start": 222
                            }
                        ],
                        "text": "CMA-ES is an evolutionary algorithm that generates new candidate solutions by sampling from a multivariate normal distribution over the search space and changes this mutation distribution by adapting its covariance matrix [30]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Completely derandomized selfadaptation in evolution strategies"
            },
            "venue": {
                "fragments": [],
                "text": "Evol Comput"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural networks. a comprehensive foundation Upper Saddle River 32. Hebb DO (1949) The organisation of behavior How learning can guide evolution"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 146
                            }
                        ],
                        "text": "Neuro-evolution of augmenting topologies (NEAT) has been applied to many problems such as pole balancing [79], robot control [80], computer games [66, 82] or an automobile crash warning system [81]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gold A (2005b) Real-time learning in the nero video game. In: Proceedings of the artificial intelligence and interactive digital entertainment conference (AIIDE 2005) demo papers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 6
                            }
                        ],
                        "text": "model [5, 83] for example, is a simple neural architecture that implements reinforcement learning with two modules,"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive critic and the basal ganglia"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training feed forward neural networks using genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Morgan Kaufmann , San Mateo"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "equivalent to that describing the voltage difference of a capacitor, where the time constant of the exponential and synaptic weights can be approximated by a set of resistors [31]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural networks. a comprehensive foundation, 2nd edn"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 116
                            }
                        ],
                        "text": "The fitness of each neuron was defined as the average fitness of all the networks it had participated in. Gomez and Miikkulainen [27] extended this approach by segregating the neurons in subpopulations with a method they called enforced subpopulations (ESP)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "To overcome these problems, Moriarty and Miikkulainen [52] suggested to evolve individual neurons to cooperate in networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Reisinger and Miikkulainen [65] showed that an implicit encoding very similar to AGE outperforms NEAT on a complex board-game task."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Acquiring evolvability through adaptive representations. In: Proceedings of genetic and evolutionary computation conference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spikes. Exploring the neural code Learning representations by back-propagation of errors"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolving developing spiking neural networks Computational models of neuromodulation"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of CEC 2005 IEEE congress on evolutionary computation 18"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[36] proposed a similar method where the connections grew according to a set of differential"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The use of genetic algorithms for the development of sensorimotor control systems. In: Gaussier P, Nicoud J-D (eds) From perceptin to action"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 9
                            }
                        ],
                        "text": "Finally, Floreano and Urzelai [21] showed that dynamic environments favor the genetic expression of plastic connections over static connections."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Floreano and Urzelai [22] also showed that a morphogenetic approach can greatly benefit from co-evolution of synaptic plasticity because the strengths of the growing connections are developed by learning rules that are coevolved with the developmental rules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolution of plastic control networks. Autonom Robots"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 50,
            "methodology": 22,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 119,
        "totalPages": 12
    },
    "page_url": "https://www.semanticscholar.org/paper/Neuroevolution:-from-architectures-to-learning-Floreano-D\u00fcrr/cf7bdff21a875e5d043514ed0714fafae77e1492?sort=total-citations"
}