{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 100
                            }
                        ],
                        "text": "Weighting the extra tasks this way yields better performance than the simpler approach presented in (Caruana 1993), which combined task gains by averaging them; recursive splitting algorithms often suffer when the data becomes sparse low in the tree, so it is important early splits are sensitive to performance on the main task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 83
                            }
                        ],
                        "text": "Sometimes this same effect is observed with simpler, fully connected MTL nets, too (Caruana 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18522085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9464d15f4f8d578f93332db4aa1c9c182fd51735",
            "isKey": false,
            "numCitedBy": 739,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Multitask-Learning:-A-Knowledge-Based-Source-of-Caruana",
            "title": {
                "fragments": [],
                "text": "Multitask Learning: A Knowledge-Based Source of Inductive Bias"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395754302"
                        ],
                        "name": "Joseph O'Sullivan",
                        "slug": "Joseph-O'Sullivan",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "O'Sullivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph O'Sullivan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10420876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f42a55da3a222184eee20c67d374a9134b77bdc",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, there has been an increased interest in \u201clifelong\u201d machine learning methods, that transfer knowledge across multiple learning tasks. Such methods have repeatedly been found to outperform conventional, single-task learning algorithms when the learning tasks are appropriately related. To increase robustness of such approaches, methods are desirable that can reason about the relatedness of individual learning tasks, in order to avoid the danger arising from tasks that are unrelated and thus potentially misleading. This paper describes the task-clustering (TC) algorithm. TC clusters learning tasks into classes of mutually related tasks. When facing a new learning task, TC first determines the most related task cluster, then exploits information selectively from this task cluster only. An empirical study carried out in a mobile robot domain shows that TC outperforms its non-selective counterpart in situations where only a small number of tasks is relevant."
            },
            "slug": "Discovering-Structure-in-Multiple-Learning-Tasks:-Thrun-O'Sullivan",
            "title": {
                "fragments": [],
                "text": "Discovering Structure in Multiple Learning Tasks: The TC Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The task-clustering algorithm TC clusters learning tasks into classes of mutually related tasks, and outperforms its non-selective counterpart in situations where only a small number of tasks is relevant."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 211
                            }
                        ],
                        "text": "Sharing what is learned by different tasks while tasks are trained in parallel is the central idea in multitask learning [Suddarth & Kergosien 1990; Dietterich, Hild & Bakiri 1990, 1995; Suddarth & Holden 1991; Caruana 1993a, 1993b, 1994, 1995; Baxter 1994, 1995, 1996; Caruana & de Sa 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9667898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "210da45e57f86a50c04bdd7b37d498c8ecc288da",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Hinton [6] proposed that generalization in artificial neural nets should improve if nets learn to represent the domain's underlying regularities. Abu-Mustafa's hints work [1] shows that the outputs of a backprop net can be used as inputs through which domain-specific information can be given to the net. We extend these ideas by showing that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better. We identify five mechanisms by which multitask backprop improves generalization and give empirical evidence that multitask backprop generalizes better in real domains."
            },
            "slug": "Learning-Many-Related-Tasks-at-the-Same-Time-with-Caruana",
            "title": {
                "fragments": [],
                "text": "Learning Many Related Tasks at the Same Time with Backpropagation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work shows that a backprop net learning many related tasks at the same time can use these tasks as inductive bias for each other and thus learn better and give empirical evidence that multitask backprop generalizes better in real domains."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242194"
                        ],
                        "name": "Ghulum Bakiri",
                        "slug": "Ghulum-Bakiri",
                        "structuredName": {
                            "firstName": "Ghulum",
                            "lastName": "Bakiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ghulum Bakiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 47109072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d221bbcbd20c7157e4500f942de8ceec490f8936",
            "isKey": false,
            "numCitedBy": 2855,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "Multiclass learning problems involve finding a definition for an unknown function f(x) whose range is a discrete set containing k > 2 values (i.e., k \"classes\"). The definition is acquired by studying collections of training examples of the form (xi, f(xi)). Existing approaches to multiclass learning problems include direct application of multiclass algorithms such as the decision-tree algorithms C4.5 and CART, application of binary concept learning algorithms to learn individual binary functions for each of the k classes, and application of binary concept learning algorithms with distributed output representations. This paper compares these three approaches to a new technique in which error-correcting codes are employed as a distributed output representation. We show that these output representations improve the generalization performance of both C4.5 and backpropagation on a wide range of multiclass learning tasks. We also demonstrate that this approach is robust with respect to changes in the size of the training sample, the assignment of distributed representations to particular classes, and the application of overfitting avoidance techniques such as decision-tree pruning. Finally, we show that--like the other methods--the error-correcting code technique can provide reliable class probability estimates. Taken together, these results demonstrate that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "slug": "Solving-Multiclass-Learning-Problems-via-Output-Dietterich-Bakiri",
            "title": {
                "fragments": [],
                "text": "Solving Multiclass Learning Problems via Error-Correcting Output Codes"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is demonstrated that error-correcting output codes provide a general-purpose method for improving the performance of inductive learning programs on multiclass problems."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 5
                            }
                        ],
                        "text": "One of these problems is a real-world problem created by researchers other than the author who did not consider using MTL when they collected the data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 245
                            }
                        ],
                        "text": "Sharing what is learned by different tasks while tasks are trained in parallel is the central idea in multitask learning [Suddarth & Kergosien 1990; Dietterich, Hild & Bakiri 1990, 1995; Suddarth & Holden 1991; Caruana 1993a, 1993b, 1994, 1995; Baxter 1994, 1995, 1996; Caruana & de Sa 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6211302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a24508e65e599b5b20c33af96dbe7017d5caca37",
            "isKey": false,
            "numCitedBy": 344,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Probably the most important problem in machine learning is the preliminary biasing of a learner's hypothesis space so that it is small enough to ensure good generalisation from reasonable training sets, yet large enough that it contains a good solution to the problem being learnt. In this paper a mechanism for {\\em automatically} learning or biasing the learner's hypothesis space is introduced. It works by first learning an appropriate {\\em internal representation} for a learning environment and then using that representation to bias the learner's hypothesis space for the learning of future tasks drawn from the same environment. \nAn internal representation must be learnt by sampling from {\\em many similar tasks}, not just a single task as occurs in ordinary machine learning. It is proved that the number of examples $m$ {\\em per task} required to ensure good generalisation from a representation learner obeys $m = O(a+b/n)$ where $n$ is the number of tasks being learnt and $a$ and $b$ are constants. If the tasks are learnt independently ({\\em i.e.} without a common representation) then $m=O(a+b)$. It is argued that for learning environments such as speech and character recognition $b\\gg a$ and hence representation learning in these environments can potentially yield a drastic reduction in the number of examples required per task. It is also proved that if $n = O(b)$ (with $m=O(a+b/n)$) then the representation learnt will be good for learning novel tasks from the same environment, and that the number of examples required to generalise well on a novel task will be reduced to $O(a)$ (as opposed to $O(a+b)$ if no representation is used). \nIt is shown that gradient descent can be used to train neural network representations and experiment results are reported providing strong qualitative support for the theoretical results."
            },
            "slug": "Learning-internal-representations-Baxter",
            "title": {
                "fragments": [],
                "text": "Learning internal representations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is proved that the number of examples required to ensure good generalisation from a representation learner obeys and that gradient descent can be used to train neural network representations and experiment results are reported providing strong qualitative support for the theoretical results."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145900862"
                        ],
                        "name": "Joel D. Martin",
                        "slug": "Joel-D.-Martin",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Martin",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel D. Martin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15580264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9f212f40a831d7751d89ed53f4ddb039cb6d8fd",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents DP 1, an incremental clustering algorithm that accepts a description of the expected performance task-the goal of learning-and uses that description to alter its learning bias. With different goals DP1 addresses a wide range of empirical learning tasks from supervised to unsupervised learning. At one extreme, DP1 performs the same task as does ID3, and at the other, it performs the same task as does COBW~.B. A learning system's performance goals and the way those goals interrelate can significantly influence learning and subsequent performance. In complex domains, those that have many probabilistic relationships between variables, a learner can waste valuable time inducing relationships that are irrelevant for performance. However, if they bias their learning according to a description of the expected performance task, they can attend primarily to relevant relationships. The contrast between focused and unfocused learning is a traditional distinction between supervised and unsupervised learning (Duda & Hart, 1973). The supervised learner is told that one target variable will be important at performance, and the learner can safely ignore irrelevant variables or relationships between such variables. On the other hand, the unsupervised learner has no such guidance and learns all predictive structure in the domain assuming that all will be useful. The predictive structure of a domain is the set of informative conditional probabilities between subsets of variables. If some subsets of variable values provide probabilistic information about the values of other variables, the domain has predictive structure. If there is no predictive structure , the domain is random. In this paper, we present an integrated algorithm that smoothly varies its learning bias depending directly upon a description of the anticipated performance tasks. This description is represented as a distribution of prediction tests. In a prediction test the learner encounters some variable values and must predict the value of another variable. By specifying the probability that a variable will be available and the probability that it will be tested, DP1 can be made to address the same task as ID3 A performance task is supervised if a particular variable has a special status. At prediction, the learner expects to know the values for all the variables except the special one, and expects to have to predict the value of that target variable. More specifically, the probability that the learner will know the value of a variable at prediction time is 1.0 for non-target variables and 0.0 \u2026"
            },
            "slug": "Goal-directed-clustering-Martin",
            "title": {
                "fragments": [],
                "text": "Goal-directed clustering"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper presents DP 1, an incremental clustering algorithm that accepts a description of the expected performance task-the goal of learning-and uses that description to alter its learning bias based on a distribution of prediction tests."
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 39
                            }
                        ],
                        "text": "It might seem that sequential transfer (Pratt & Mostow 1991; Pratt 1992; Sharkey & Sharkey 1992; Thrun & Mitchell 1994; Thrun 1995) would be easier."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21e66a6c0feab04c9603196968c906ae2ff30ff1",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 88,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Machine learning has not yet succeeded in the design of robust learning algorithms that generalize well from very small datasets. In contrast, humans often generalize correctly from only a single training example, even if the number of potentially relevant features is large. To do so, they successfully exploit knowledge acquired in previous learning tasks, to bias subsequent learning. This paper investigates learning in a lifelong context. Lifelong learning addresses situations where a learner faces a stream of learning tasks. Such scenarios provide the opportunity for synergetic effects that arise if knowledge is transferred across multiple learning tasks. To study the utility of transfer, several approaches to lifelong learning are proposed and evaluated in an object recognition domain. It is shown that all these algorithms generalize consistently more accurately from scarce training data than comparable \"single-task\" approaches."
            },
            "slug": "Lifelong-Learning:-A-Case-Study.-Thrun",
            "title": {
                "fragments": [],
                "text": "Lifelong Learning: A Case Study."
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper investigates learning in a lifelong context where a learner faces a stream of learning tasks and proposes and evaluates several approaches to lifelong learning that generalize consistently more accurately from scarce training data than comparable \"single-task\" approaches."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 18
                            }
                        ],
                        "text": "For example, EBNN [Thrun & Mitchell 1994; Thrun 1996] requires that the domain theory be differentiable, but the MTL approach to sequential transfer does not."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 151
                            }
                        ],
                        "text": "Some serial transfer mechanisms have explicit mechanisms for reducing transfer when prior learning does not appear to be accurate for the task at hand [Thrun & Mitchell 1994; Thrun 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60488206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "939d7319b058d8f4d413daa7a153e25e798b9372",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter introduces the major learning approach studied in this book: the explanation-based neural network learning algorithm (EBNN). EBNN approaches the meta-level learning problem by learning a theory of the domain. This domain theory is domain-specific. It characterizes, for example, the relevance of individual features, their cross-dependencies, or certain invariant properties of the domain that apply to all learning tasks within the domain. Obviously, when the learner has a model of such regularities, there is an opportunity to generalize more accurately or, alternatively, learn from less training data. This is because without knowledge about these regularities the learner has to learn them from scratch, which necessarily requires more training data. EBNN transfers previously learned knowledge by explaining and analyzing training examples in terms of the domain theory. The result of this analysis is a set of domain-specific shape constraints for the function to be learned at the base-level. Thus, these constraints guide the base-level learning of new functions in a knowledgeable, domain-specific way."
            },
            "slug": "Explanation-based-neural-network-learning-Thrun",
            "title": {
                "fragments": [],
                "text": "Explanation-based neural network learning"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "This chapter introduces the major learning approach studied in this book: the explanation-based neural network learning algorithm (EBNN), which approaches the meta-level learning problem by learning a theory of the domain that characterizes the relevance of individual features, their cross-dependencies, or certain invariant properties of the Domain."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6046155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b65b99e772727dadc1b5e50a9d83367a892ec28",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Most research on machine learning has focused on scenarios in which a learner faces a single, isolated learning task. The lifelong learning framework assumes instead that the learner encounters a multitude of related learning tasks over its lifetime, providing the opportunity for the transfer of knowledge. This paper studies lifelong learning in the context of binary classification. It presents the invariance approach, in which knowledge is transferred via a learned model of the invariances of the domain. Results on learning to recognize objects from color images demonstrate superior generalization capabilities if invariances are learned and used to bias subsequent learning."
            },
            "slug": "Learning-One-More-Thing-Thrun-Mitchell",
            "title": {
                "fragments": [],
                "text": "Learning One More Thing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Results on learning to recognize objects from color images demonstrate superior generalization capabilities if invariances are learned and used to bias subsequent learning."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 245
                            }
                        ],
                        "text": "Sharing what is learned by different tasks while tasks are trained in parallel is the central idea in multitask learning [Suddarth & Kergosien 1990; Dietterich, Hild & Bakiri 1990, 1995; Suddarth & Holden 1991; Caruana 1993a, 1993b, 1994, 1995; Baxter 1994, 1995, 1996; Caruana & de Sa 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14605454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0ee1219f78e5b53938718e9a8f140491cef1523",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper the problem of learning appropriate bias for an environment of related tasks is examined from a Bayesian perspective. The environment of related tasks is shown to be naturally modelled by the concept of an {\\em objective} prior distribution. Sampling from the objective prior corresponds to sampling different learning tasks from the environment. It is argued that for many common machine learning problems, although we don't know the true (objective) prior for the problem, we do have some idea of a set of possible priors to which the true prior belongs. It is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by sampling from the objective prior. Bounds are given on the amount of information required to learn a task when it is simultaneously learnt with several other tasks. The bounds show that if the learner has little knowledge of the true prior, and the dimensionality of the true prior is small, then sampling multiple tasks is highly advantageous."
            },
            "slug": "A-Bayesian/information-theoretic-model-of-bias-Baxter",
            "title": {
                "fragments": [],
                "text": "A Bayesian/information theoretic model of bias learning"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "In this paper the problem of learning appropriate bias for an environment of related tasks is examined from a Bayesian perspective and it is shown that under these circumstances a learner can use Bayesian inference to learn the true prior by sampling from the objective prior."
            },
            "venue": {
                "fragments": [],
                "text": "COLT '96"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1016169,
            "fieldsOfStudy": [
                "Computer Science",
                "Education"
            ],
            "id": "371c9dc680e916f79d9c78fcf6c894a2dd299095",
            "isKey": false,
            "numCitedBy": 687,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates learning in a lifelong context. Lifelong learning addresses situations in which a learner faces a whole stream of learning tasks. Such scenarios provide the opportunity to transfer knowledge across multiple learning tasks, in order to generalize more accurately from less training data. In this paper, several different approaches to lifelong learning are described, and applied in an object recognition domain. It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks."
            },
            "slug": "Is-Learning-The-n-th-Thing-Any-Easier-Than-Learning-Thrun",
            "title": {
                "fragments": [],
                "text": "Is Learning The n-th Thing Any Easier Than Learning The First?"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that across the board, lifelong learning approaches generalize consistently more accurately from less training data, by their ability to transfer knowledge across learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 18
                            }
                        ],
                        "text": "For example, EBNN [Thrun & Mitchell 1994; Thrun 1996] requires that the domain theory be differentiable, but the MTL approach to sequential transfer does not."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 151
                            }
                        ],
                        "text": "Some serial transfer mechanisms have explicit mechanisms for reducing transfer when prior learning does not appear to be accurate for the task at hand [Thrun & Mitchell 1994; Thrun 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46398994,
            "fieldsOfStudy": [
                "Computer Science",
                "Education",
                "Psychology"
            ],
            "id": "75e50717070e82cdf3945265a75def6960b55a9d",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nLifelong learning addresses situations in which a learner faces a series of different learning tasks providing the opportunity for synergy among them. Explanation-based neural network learning (EBNN) is a machine learning algorithm that transfers knowledge across multiple learning tasks. When faced with a new learning task, EBNN exploits domain knowledge accumulated in previous learning tasks to guide generalization in the new one. As a result, EBNN generalizes more accurately from less data than comparable methods. Explanation-Based Neural Network Learning: A Lifelong Learning Approach describes the basic EBNN paradigm and investigates it in the context of supervised learning, reinforcement learning, robotics, and chess."
            },
            "slug": "Explanation-based-neural-network-learning-a-Thrun",
            "title": {
                "fragments": [],
                "text": "Explanation-based neural network learning a lifelong learning approach"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398965769"
                        ],
                        "name": "Y. Abu-Mostafa",
                        "slug": "Y.-Abu-Mostafa",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Abu-Mostafa",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Abu-Mostafa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 91
                            }
                        ],
                        "text": "Attempts have been made to develop theories of parallel transfer in artificial neural nets (Abu-Mostafa 1993; Baxter 1994, 1995, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207701407,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed0c5a44d2e1f9a3a284ee0e6bcff4978c8654bc",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning from hints is a generalization of learning from examples that allows for a variety of information about the unknown function to be used in the learning process. In this paper, we use the VC dimension, an established tool for analyzing learning from examples, to analyze learning from hints. In particular, we show how the VC dimension is affected by the introduction of a hint. We also derive a new quantity that defines a VC dimension for the hint itself. This quantity is used to estimate the number of examples needed to \"absorb\" the hint. We carry out the analysis for two types of hints, invariances and catalysts. We also describe how the same method can be applied to other types of hints."
            },
            "slug": "Hints-and-the-VC-Dimension-Abu-Mostafa",
            "title": {
                "fragments": [],
                "text": "Hints and the VC Dimension"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The VC dimension is shown to be affected by the introduction of a hint, and a new quantity is derived that defines a VC dimension for the hint itself, used to estimate the number of examples needed to \"absorb\" the hint."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2403454"
                        ],
                        "name": "E. Baum",
                        "slug": "E.-Baum",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Baum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Baum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733689"
                        ],
                        "name": "D. Haussler",
                        "slug": "D.-Haussler",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Haussler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Haussler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15659829,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "25406e6733a698bfc4ac836f8e74f458e75dad4f",
            "isKey": false,
            "numCitedBy": 1699,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the question of when a network can be expected to generalize from m random training examples chosen from some arbitrary probability distribution, assuming that future test examples are drawn from the same distribution. Among our results are the following bounds on appropriate sample vs. network size. Assume 0 < \u220a 1/8. We show that if m O(W/\u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 1 \u220a of future test examples drawn from the same distribution. Conversely, for fully-connected feedforward nets with one hidden layer, any learning algorithm using fewer than (W/\u220a) random training examples will, for some distributions of examples consistent with an appropriate weight choice, fail at least some fixed fraction of the time to find a weight choice that will correctly classify more than a 1 \u220a fraction of the future test examples."
            },
            "slug": "What-Size-Net-Gives-Valid-Generalization-Baum-Haussler",
            "title": {
                "fragments": [],
                "text": "What Size Net Gives Valid Generalization?"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that if m O(W/ \u220a log N/\u220a) random examples can be loaded on a feedforward network of linear threshold functions with N nodes and W weights, so that at least a fraction 1 \u220a/2 of the examples are correctly classified, then one has confidence approaching certainty that the network will correctly classify a fraction 2 \u220a of future test examples drawn from the same distribution."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 63
                            }
                        ],
                        "text": "11 Combining MTL and Boosting Bagging [Breiman 1994], Boosting [Schapire 1990][Freund 1995], Error-Correcting Codes [Dietterich & Bakiri 1995], and other voting schemes that combine multiple predictions for a task from di erent learned models are an exciting recent advance in machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6207294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35702d642c5f1b7950d088497837486b4ca682a3",
            "isKey": false,
            "numCitedBy": 2728,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the problem of improving the accuracy of an hypothesis output by a learning algorithm in the distribution-free (PAC) learning model. A concept class is learnable (or strongly learnable) if, given access to a source of examples of the unknown concept, the learner with high probability is able to output an hypothesis that is correct on all but an arbitrarily small fraction of the instances. The concept class is weakly learnable if the learner can produce an hypothesis that performs only slightly better than random guessing. In this paper, it is shown that these two notions of learnability are equivalent.A method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy. This construction may have practical applications as a tool for efficiently converting a mediocre learning algorithm into one that performs extremely well. In addition, the construction has some interesting theoretical consequences, including a set of general upper bounds on the complexity of any strong learning algorithm as a function of the allowed error \u2208."
            },
            "slug": "The-Strength-of-Weak-Learnability-Schapire",
            "title": {
                "fragments": [],
                "text": "The Strength of Weak Learnability"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "In this paper, a method is described for converting a weak learning algorithm into one that achieves arbitrarily high accuracy, and it is shown that these two notions of learnability are equivalent."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 84618,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "37bad2daf9b5d26a2d4c0e99c412751e95d76c38",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A patient visits the doctor; the doctor reviews the patient's history, asks questions, makes basic measurements (blood pressure, ...), and prescribes tests or treatment. The prescribed course of action is based on an assessment of patient risk--patients at higher risk are given more and faster attention. It is also sequential--it is too expensive to immediately order all tests which might later be of value. This paper presents two methods that together improve the accuracy of backprop nets on a pneumonia risk assessment problem by 10-50%. Rankprop improves on backpropagation with sum of squares error in ranking patients by risk. Multitask learning takes advantage of future lab tests available in the training set, but not available in practice when predictions must be made. Both methods are broadly applicable."
            },
            "slug": "Using-the-Future-to-Sort-Out-the-Present:-Rankprop-Caruana-Baluja",
            "title": {
                "fragments": [],
                "text": "Using the Future to Sort Out the Present: Rankprop and Multitask Learning for Medical Risk Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Two methods that together improve the accuracy of backprop nets on a pneumonia risk assessment problem by 10-50%."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1980953"
                        ],
                        "name": "S. Suddarth",
                        "slug": "S.-Suddarth",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Suddarth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Suddarth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4449657"
                        ],
                        "name": "Y. Kergosien",
                        "slug": "Y.-Kergosien",
                        "structuredName": {
                            "firstName": "Yannick",
                            "lastName": "Kergosien",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kergosien"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 122
                            }
                        ],
                        "text": "Sharing what is learned by different tasks while tasks are trained in parallel is the central idea in multitask learning [Suddarth & Kergosien 1990; Dietterich, Hild & Bakiri 1990, 1995; Suddarth & Holden 1991; Caruana 1993a, 1993b, 1994, 1995; Baxter 1994, 1995, 1996; Caruana & de Sa 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27857819,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ffb199e36de4f34ea233a30d392fdcf0c3b25a14",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks can be given \u201chints\u201d by increasing the number of parameters learned to include parameters related to the original relationship. The effect of this hint, whether applied to back-propagation learning or to more general types of pattern associators is to reduce training time and improve generalization performance. A detailed vector field analysis of a hinted back-propagation network solving the XOR problem, shows that the hint is capable of eliminating pathological local minima. A set-theory/functional entropy analysis shows that the hint can be applied to any learning mechanism that has an internal (\u201chidden\u201d) layer of processing. These analyses and tests conducted on a variety of problems using different types of networks demonstrate the potential of the hint as a method of controlling training in order to predictably train systems to effectively model data."
            },
            "slug": "Rule-Injection-Hints-as-a-Means-of-Improving-and-Suddarth-Kergosien",
            "title": {
                "fragments": [],
                "text": "Rule-Injection Hints as a Means of Improving Network Performance and Learning Time"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The potential of the hint, whether applied to back-propagation learning or to more general types of pattern associators is to reduce training time and improve generalization performance."
            },
            "venue": {
                "fragments": [],
                "text": "EURASIP Workshop"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50333420"
                        ],
                        "name": "S. Becker",
                        "slug": "S.-Becker",
                        "structuredName": {
                            "firstName": "Suzanna",
                            "lastName": "Becker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Becker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4332326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c85b7fe70dda0adbbd7630e2a341a904c74fbd2",
            "isKey": false,
            "numCitedBy": 411,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "THE standard form of back-propagation learning1 is implausible as a model of perceptual learning because it requires an external teacher to specify the desired output of the network. We show how the external teacher can be replaced by internally derived teaching signals. These signals are generated by using the assumption that different parts of the perceptual input have common causes in the external world. Small modules that look at separate but related parts of the perceptual input discover these common causes by striving to produce outputs that agree with each other (Fig. la). The modules may look at different modalities (such as vision and touch), or the same modality at different times (for example, the consecutive two-dimensional views of a rotating three-dimensional object), or even spatially adjacent parts of the same image. Our simulations show that when our learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discover depth in random dot stereograms of curved surfaces."
            },
            "slug": "Self-organizing-neural-network-that-discovers-in-Becker-Hinton",
            "title": {
                "fragments": [],
                "text": "Self-organizing neural network that discovers surfaces in random-dot stereograms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The authors' simulations show that when the learning procedure is applied to adjacent patches of two-dimensional images, it allows a neural network that has no prior knowledge of the third dimension to discover depth in random dot stereograms of curved surfaces."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080632378"
                        ],
                        "name": "B. Victorri",
                        "slug": "B.-Victorri",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Victorri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Victorri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2184474,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff32cebbdb8a436ccd8ae797647428615ae32d74",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "In many machine learning applications, one has access, not only to training data, but also to some high-level a priori knowledge about the desired behavior of the system. For example, it is known in advance that the output of a character recognizer should be invariant with respect to small spatial distortions of the input images (translations, rotations, scale changes, etcetera). \n \nWe have implemented a scheme that allows a network to learn the derivative of its outputs with respect to distortion operators of our choosing. This not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations we wish the network to perform."
            },
            "slug": "Tangent-Prop-A-Formalism-for-Specifying-Selected-in-Simard-Victorri",
            "title": {
                "fragments": [],
                "text": "Tangent Prop - A Formalism for Specifying Selected Invariances in an Adaptive Network"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A scheme is implemented that allows a network to learn the derivative of its outputs with respect to distortion operators of their choosing, which not only reduces the learning time and the amount of training data, but also provides a powerful language for specifying what generalizations the authors wish the network to perform."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267334"
                        ],
                        "name": "H. Hild",
                        "slug": "H.-Hild",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Hild",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hild"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242194"
                        ],
                        "name": "Ghulum Bakiri",
                        "slug": "Ghulum-Bakiri",
                        "structuredName": {
                            "firstName": "Ghulum",
                            "lastName": "Bakiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ghulum Bakiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13339009,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77dcffce3fb186fb971ac48a3bd2a9cc364e4a07",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Comparative-Study-of-ID3-and-Backpropagation-for-Dietterich-Hild",
            "title": {
                "fragments": [],
                "text": "A Comparative Study of ID3 and Backpropagation for English Text-to-Speech Mapping"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780015"
                        ],
                        "name": "V. D. Sa",
                        "slug": "V.-D.-Sa",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "Sa",
                            "middleNames": [
                                "R.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. D. Sa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 270
                            }
                        ],
                        "text": "Sharing what is learned by different tasks while tasks are trained in parallel is the central idea in multitask learning [Suddarth & Kergosien 1990; Dietterich, Hild & Bakiri 1990, 1995; Suddarth & Holden 1991; Caruana 1993a, 1993b, 1994, 1995; Baxter 1994, 1995, 1996; Caruana & de Sa 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 24334306,
            "fieldsOfStudy": [
                "Economics",
                "Computer Science"
            ],
            "id": "73efa0c00cf2709cd574af68664fd08673006d44",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In supervised learning there is usually a clear distinction between inputs and outputs - inputs are what you will measure, outputs are what you will predict from those measurements. This paper shows that the distinction between inputs and outputs is not this simple. Some features are more useful as extra outputs than as inputs. By using a feature as an output we get more than just the case values but can learn a mapping from the other inputs to that feature. For many features this mapping may be more useful than the feature value itself. We present two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra outputs instead. This result is surprising since a feature used as an output is not used during testing."
            },
            "slug": "Promoting-Poor-Features-to-Supervisors:-Some-Inputs-Caruana-Sa",
            "title": {
                "fragments": [],
                "text": "Promoting Poor Features to Supervisors: Some Inputs Work Better as Outputs"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents two regression problems and one classification problem where performance improves if features that could have been used as inputs are used as extra outputs instead and is surprising since a feature used as an output is not used during testing."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205001834,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052b1d8ce63b07fec3de9dbb583772d860b7c769",
            "isKey": false,
            "numCitedBy": 20383,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a new learning procedure, back-propagation, for networks of neurone-like units. The procedure repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector. As a result of the weight adjustments, internal \u2018hidden\u2019 units which are not part of the input or output come to represent important features of the task domain, and the regularities in the task are captured by the interactions of these units. The ability to create useful new features distinguishes back-propagation from earlier, simpler methods such as the perceptron-convergence procedure1."
            },
            "slug": "Learning-representations-by-back-propagating-errors-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagating errors"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "Back-propagation repeatedly adjusts the weights of the connections in the network so as to minimize a measure of the difference between the actual output vector of the net and the desired output vector, which helps to represent important features of the task domain."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094648"
                        ],
                        "name": "P. Munro",
                        "slug": "P.-Munro",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Munro",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Munro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2395812"
                        ],
                        "name": "B. Parmanto",
                        "slug": "B.-Parmanto",
                        "structuredName": {
                            "firstName": "Bambang",
                            "lastName": "Parmanto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Parmanto"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2883482,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd0bf48e0e1c2e9aeee53afa401c2eeb3e687cf8",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The separation of generalization error into two types, bias and variance (Geman, Bienenstock, Doursat, 1992), leads to the notion of error reduction by averaging over a \"committee\" of classifiers (Perrone, 1993). Committee performance decreases with both the average error of the constituent classifiers and increases with the degree to which the misclassifications are correlated across the committee. Here, a method for reducing correlations is introduced, that uses a winner-take-all procedure similar to competitive learning to drive the individual networks to different minima in weight space with respect to the training set, such that correlations in generalization performance will be reduced, thereby reducing committee error."
            },
            "slug": "Competition-Among-Networks-Improves-Committee-Munro-Parmanto",
            "title": {
                "fragments": [],
                "text": "Competition Among Networks Improves Committee Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A method for reducing correlations is introduced, that uses a winner-take-all procedure similar to competitive learning to drive the individual networks to different minima in weight space with respect to the training set, such that correlations in generalization performance will be reduced, thereby reducing committee error."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5262555,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "807c1f19047f96083e13614f7ce20f2ac98c239a",
            "isKey": false,
            "numCitedBy": 21909,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nClassifier systems play a major role in machine learning and knowledge-based systems, and Ross Quinlan's work on ID3 and C4.5 is widely acknowledged to have made some of the most significant contributions to their development. This book is a complete guide to the C4.5 system as implemented in C for the UNIX environment. It contains a comprehensive guide to the system's use , the source code (about 8,800 lines), and implementation notes. The source code and sample datasets are also available on a 3.5-inch floppy diskette for a Sun workstation. \n \nC4.5 starts with large sets of cases belonging to known classes. The cases, described by any mixture of nominal and numeric properties, are scrutinized for patterns that allow the classes to be reliably discriminated. These patterns are then expressed as models, in the form of decision trees or sets of if-then rules, that can be used to classify new cases, with emphasis on making the models understandable as well as accurate. The system has been applied successfully to tasks involving tens of thousands of cases described by hundreds of properties. The book starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting. Advantages and disadvantages of the C4.5 approach are discussed and illustrated with several case studies. \n \nThis book and software should be of interest to developers of classification-based intelligent systems and to students in machine learning and expert systems courses."
            },
            "slug": "C4.5:-Programs-for-Machine-Learning-Quinlan",
            "title": {
                "fragments": [],
                "text": "C4.5: Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A complete guide to the C4.5 system as implemented in C for the UNIX environment, which starts from simple core learning methods and shows how they can be elaborated and extended to deal with typical problems such as missing data and over hitting."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11015273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0f3ff5eaa7bf18fb95203a9c3ba73bc600708a7",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "FTP: CMU-CS-95-143.ps In many real-world tasks, the ability to focus attention on the important features of the input is crucial for good performance. In this paper a mechanism for achieving task-specific focus of attention is presented. A saliency map, which is based upon a computed expectation of the contents of the inputs at the next time step, indicates which regions of the input retina are important for performing the task. The saliency map can be used to accentuate the features which are important, and de-emphasize those which are not. The performance of this method is demonstrated on a real-world robotics task: autonomous road following. The applicability of this method is also demonstrated in a non-visual domain. Architectural and algorithmic details are provided, as well as empirical results."
            },
            "slug": "Using-the-Representation-in-a-Neural-Network's-for-Baluja-Pomerleau",
            "title": {
                "fragments": [],
                "text": "Using the Representation in a Neural Network's Hidden Layer for Task-Specific Focus of Attention"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A mechanism for achieving task-specific focus of attention is presented, based upon a computed expectation of the contents of the inputs at the next time step, which indicates which regions of the input retina are important for performing the task."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34056376"
                        ],
                        "name": "Mark W. Craven",
                        "slug": "Mark-W.-Craven",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Craven",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark W. Craven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734317"
                        ],
                        "name": "J. Shavlik",
                        "slug": "J.-Shavlik",
                        "structuredName": {
                            "firstName": "Jude",
                            "lastName": "Shavlik",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shavlik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2961336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba176454d1ade52e6eec74e3f9eed5f61179761a",
            "isKey": false,
            "numCitedBy": 340,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Using-Sampling-and-Queries-to-Extract-Rules-from-Craven-Shavlik",
            "title": {
                "fragments": [],
                "text": "Using Sampling and Queries to Extract Rules from Trained Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38746648"
                        ],
                        "name": "Huan Liu",
                        "slug": "Huan-Liu",
                        "structuredName": {
                            "firstName": "Huan",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huan Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697083"
                        ],
                        "name": "R. Setiono",
                        "slug": "R.-Setiono",
                        "structuredName": {
                            "firstName": "Rudy",
                            "lastName": "Setiono",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Setiono"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17123515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7285ee82aa0cde847fafb8b1109dd19dbdc04e35",
            "isKey": false,
            "numCitedBy": 767,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Feature selection can be de ned as a problem of nding a minimum set of M relevant at tributes that describes the dataset as well as the original N attributes do where M N After examining the problems with both the exhaustive and the heuristic approach to fea ture selection this paper proposes a proba bilistic approach The theoretic analysis and the experimental study show that the pro posed approach is simple to implement and guaranteed to nd the optimal if resources permit It is also fast in obtaining results and e ective in selecting features that im prove the performance of a learning algo rithm An on site application involving huge datasets has been conducted independently It proves the e ectiveness and scalability of the proposed algorithm Discussed also are various aspects and applications of this fea ture selection algorithm"
            },
            "slug": "A-Probabilistic-Approach-to-Feature-Selection-A-Liu-Setiono",
            "title": {
                "fragments": [],
                "text": "A Probabilistic Approach to Feature Selection - A Filter Solution"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The theoretic analysis and the experimental study show that the proposed proba bilistic approach is simple to implement and guaranteed to be the optimal if resources permit."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398965769"
                        ],
                        "name": "Y. Abu-Mostafa",
                        "slug": "Y.-Abu-Mostafa",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Abu-Mostafa",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Abu-Mostafa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 221400641,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8769dd708707779afad9133015886f00cf454a5",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "The systematic use of hints in the learning-from-examples paradigm is the subject of this review. Hints are the properties of the target function that are known to us independently of the training examples. The use of hints is tantamount to combining rules and data in learning, and is compatible with different learning models, optimization techniques, and regularization techniques. The hints are represented to the learning process by virtual examples, and the training examples of the target function are treated on equal footing with the rest of the hints. A balance is achieved between the information provided by the different hints through the choice of objective functions and learning schedules. The Adaptive Minimization algorithm achieves this balance by relating the performance on each hint to the overall performance. The application of hints in forecasting the very noisy foreign-exchange markets is illustrated. On the theoretical side, the information value of hints is contrasted to the complexity value and related to the VC dimension."
            },
            "slug": "Hints-Abu-Mostafa",
            "title": {
                "fragments": [],
                "text": "Hints"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The systematic use of hints in the learning-from-examples paradigm, which is tantamount to combining rules and data in learning, is compatible with different learning models, optimization techniques, and regularization techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115945547"
                        ],
                        "name": "H. Sawai",
                        "slug": "H.-Sawai",
                        "structuredName": {
                            "firstName": "Hidefumi",
                            "lastName": "Sawai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Sawai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9243990"
                        ],
                        "name": "K. Shikano",
                        "slug": "K.-Shikano",
                        "structuredName": {
                            "firstName": "Kiyohiro",
                            "lastName": "Shikano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shikano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18990685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f5966eba6336438ade570ea57e2e682fa0b5985",
            "isKey": false,
            "numCitedBy": 186,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors train several small time-delay neural networks aimed at all phonemic subcategories (nasals, fricatives, etc.) and report excellent fine phonemic discrimination performance for all cases. Exploiting the hidden structure of these small phonemic subcategory networks, they propose several technique that make it possible to grow larger nets in an incremental and modular fashion without loss in recognition performance and without the need for excessive training time or additional data. The techniques include class discriminatory learning, connectionist glue, selective/partial learning, and all-net fine tuning. A set of experiments shows that stop consonant networks (BDGPTK) constructed from subcomponent BDG- and PTK-nets achieved up to 98.6% correct recognition compared to 98.3 and 98.7% correct for the BDG- and PTK-nets. Similarly, an incrementally trained network aimed at all consonants achieved recognition scores of about 96% correct. These results are comparable to the performance of the subcomponent networks and significantly better than that of several alternative speech recognition methods. >"
            },
            "slug": "Modularity-and-scaling-in-large-phonemic-neural-Waibel-Sawai",
            "title": {
                "fragments": [],
                "text": "Modularity and scaling in large phonemic neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The authors train several small time-delay neural networks aimed at all phonemic subcategories and report excellent fine phonemic discrimination performance for all cases and propose several technique that make it possible to grow larger nets in an incremental and modular fashion without loss in recognition performance and without the need for excessive training time or additional data."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Acoust. Speech Signal Process."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1980953"
                        ],
                        "name": "S. Suddarth",
                        "slug": "S.-Suddarth",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Suddarth",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Suddarth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "24934793"
                        ],
                        "name": "A. Holden",
                        "slug": "A.-Holden",
                        "structuredName": {
                            "firstName": "Alistair",
                            "lastName": "Holden",
                            "middleNames": [
                                "D.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Holden"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41080002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dfe7dbbd6e8d5d3720f58c2c9a0b9bec040a8ef8",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Symbolic-Neural-Systems-and-the-Use-of-Hints-for-Suddarth-Holden",
            "title": {
                "fragments": [],
                "text": "Symbolic-Neural Systems and the Use of Hints for Developing Complex Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Man Mach. Stud."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144299726"
                        ],
                        "name": "Thomas G. Dietterich",
                        "slug": "Thomas-G.-Dietterich",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dietterich",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas G. Dietterich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3267334"
                        ],
                        "name": "H. Hild",
                        "slug": "H.-Hild",
                        "structuredName": {
                            "firstName": "Hermann",
                            "lastName": "Hild",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hild"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242194"
                        ],
                        "name": "Ghulum Bakiri",
                        "slug": "Ghulum-Bakiri",
                        "structuredName": {
                            "firstName": "Ghulum",
                            "lastName": "Bakiri",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ghulum Bakiri"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17881832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88bcac4326acb3384fc44fa053f3df4833394550",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The performance of the error backpropagation (BP) and ID3 learning algorithms was compared on the task of mapping English text to phonemes and stresses. Under the distributed output code developed by Sejnowski and Rosenberg, it is shown that BP consistently out-performs ID3 on this task by several percentage points. Three hypotheses explaining this difference were explored: (a) ID3 is overfitting the training data, (b) BP is able to share hidden units across several output units and hence can learn the output units better, and (c) BP captures statistical information that ID3 does not. We conclude that only hypothesis (c) is correct. By augmenting ID3 with a simple statistical learning procedure, the performance of BP can be closely matched. More complex statistical procedures can improve the performance of both BP and ID3 substantially in this domain."
            },
            "slug": "A-Comparison-of-ID3-and-Backpropagation-for-English-Dietterich-Hild",
            "title": {
                "fragments": [],
                "text": "A Comparison of ID3 and Backpropagation for English Text-To-Speech Mapping"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "The performance of the error backpropagation (BP) and ID3 learning algorithms was compared on the task of mapping English text to phonemes and stresses and it is shown that BP consistently out-performs ID3 on this task by several percentage points."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2864754"
                        ],
                        "name": "J. Ghosn",
                        "slug": "J.-Ghosn",
                        "structuredName": {
                            "firstName": "Joumana",
                            "lastName": "Ghosn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ghosn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5501409,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e5d26994a2ed3b7657dbefde26fc7eb929d62b16",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial Neural Networks can be used to predict future returns of stocks in order to take financial decisions. Should one build a separate network for each stock or share the same network for all the stocks? In this paper we also explore other alternatives, in which some layers are shared and others are not shared. When the prediction of future returns for different stocks are viewed as different tasks, sharing some parameters across stocks is a form of multi-task learning. In a series of experiments with Canadian stocks, we obtain yearly returns that are more than 14% above various benchmarks."
            },
            "slug": "Multi-Task-Learning-for-Stock-Selection-Ghosn-Bengio",
            "title": {
                "fragments": [],
                "text": "Multi-Task Learning for Stock Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper obtains yearly returns that are more than 14% above various benchmarks with Canadian stocks and explores other alternatives, in which some layers are shared and others are not shared."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17585310"
                        ],
                        "name": "M. I. Jordan",
                        "slug": "M.-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 67000854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6d8a7fc2e2d53923832f9404376512068ca2a57",
            "isKey": false,
            "numCitedBy": 2173,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tree-structured architecture for supervised learning. The statistical model underlying the architecture is a hierarchical mixture model in which both the mixture coefficients and the mixture components are generalized linear models (GLIM's). Learning is treated as a maximum likelihood problem; in particular, we present an Expectation-Maximization (EM) algorithm for adjusting the parameters of the architecture. We also develop an on-line learning algorithm in which the parameters are updated incrementally. Comparative simulation results are presented in the robot dynamics domain."
            },
            "slug": "Hierarchical-Mixtures-of-Experts-and-the-EM-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Hierarchical Mixtures of Experts and the EM Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "An Expectation-Maximization (EM) algorithm for adjusting the parameters of the tree-structured architecture for supervised learning and an on-line learning algorithm in which the parameters are updated incrementally."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398965769"
                        ],
                        "name": "Y. Abu-Mostafa",
                        "slug": "Y.-Abu-Mostafa",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Abu-Mostafa",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Abu-Mostafa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 88
                            }
                        ],
                        "text": ", the net is being told explicitly to learn a feature F that is useful to the main task (Abu-Mostafa 1990)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 176
                            }
                        ],
                        "text": "The simplest case of eavesdropping is what Abu-Mostafa calls catalytic hints where , i.e., the net is being told explicitly to learn a feature that is useful to the main task [Abu-Mostafa 1990]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 319536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3cd36c092abd65d6ac8e648f3468eeee90ee1fc",
            "isKey": false,
            "numCitedBy": 261,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-from-hints-in-neural-networks-Abu-Mostafa",
            "title": {
                "fragments": [],
                "text": "Learning from hints in neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "J. Complex."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 136
                            }
                        ],
                        "text": "1D-ALVINN uses a road image simulator first developed by Pomerleau to permit rapid testing of learning ideas for road-following domains (Pomerleau 1992)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 110954972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c67f0f00eaab360db1b9bd377e783c27c922dc86",
            "isKey": false,
            "numCitedBy": 597,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nVision based mobile robot guidance has proven difficult for classical machine vision methods because of the diversity and real time constraints inherent in the task. This book describes a connectionist system called ALVINN (Autonomous Land Vehicle In a Neural Network) that overcomes these difficulties. ALVINN learns to guide mobile robots using the back-propagation training algorithm. Because of its ability to learn from example, ALVINN can adapt to new situations and therefore cope with the diversity of the autonomous navigation task. But real world problems like vision based mobile robot guidance present a different set of challenges for the connectionist paradigm. Among them are: how to develop a general representation from a limited amount of real training data, how to understand the internal representations developed by artificial neural networks, how to estimate the reliability of individual networks, how to combine multiple networks trained for different situations into a single system, and how to combine connectionist perception with symbolic reasoning. Neural Network Perception for Mobile Robot Guidance presents novel solutions to each of these problems. Using these techniques, the ALVINN system can learn to control an autonomous van in under 5 minutes by watching a person drive. Once trained, individual ALVINN networks can drive in a variety of circumstances, including single-lane paved and unpaved roads, and multi-lane lined and unlined roads, at speeds of up to 55 miles per hour. The techniques also are shown to generalize to the task of controlling the precise foot placement of a walking robot."
            },
            "slug": "Neural-Network-Perception-for-Mobile-Robot-Guidance-Pomerleau",
            "title": {
                "fragments": [],
                "text": "Neural Network Perception for Mobile Robot Guidance"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book describes a connectionist system called ALVINN (Autonomous Land Vehicle In a Neural Network) that overcomes difficulties and can learn to control an autonomous van in under 5 minutes by watching a person drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744109"
                        ],
                        "name": "S. Salzberg",
                        "slug": "S.-Salzberg",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Salzberg",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Salzberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800681"
                        ],
                        "name": "Alberto Maria Segre",
                        "slug": "Alberto-Maria-Segre",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Segre",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto Maria Segre"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60499165,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7feb0fc888cd55360949554db032d7d1cba9e947",
            "isKey": false,
            "numCitedBy": 7076,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for constructing decision trees are among the most well known and widely used of all machine learning methods. Among decision tree algorithms, J. Ross Quinlan's ID3 and its successor, C4.5, are probably the most popular in the machine learning community. These algorithms and variations on them have been the subject of numerous research papers since Quinlan introduced ID3. Until recently, most researchers looking for an introduction to decision trees turned to Quinlan's seminal 1986 Machine Learning journal article [Quinlan, 1986]. In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments. As such, this book will be a welcome addition to the library of many researchers and students."
            },
            "slug": "Programs-for-Machine-Learning-Salzberg-Segre",
            "title": {
                "fragments": [],
                "text": "Programs for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "In his new book, C4.5: Programs for Machine Learning, Quinlan has put together a definitive, much needed description of his complete system, including the latest developments, which will be a welcome addition to the library of many researchers and students."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144442133"
                        ],
                        "name": "L. Pratt",
                        "slug": "L.-Pratt",
                        "structuredName": {
                            "firstName": "Lorien",
                            "lastName": "Pratt",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pratt"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15804736,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b89d8c17beea5d480fe6156098324782566feba6",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, neural networks have been used for a wide variety of applications, from medical screening [ Rutenberg, 1992, Weber, 1990 ] to municipal power grid security [ Atlas et al., 1990c ] . Furthermore, comparisons between neural networks and more traditional techniques have shown that neural networks often produce competitive, and sometimes superior, results ( [ Weiss and Kulikowski, 1991, Shavlik et al., 1991, Thrun et al., 1991, Atlas et al., 1990b, Atlas et al., 1990c, Cole et al., 1990, Atlas et al., 1990a, Dietterich et al., 1990, Fisher and McKusick, 1989, Mooney et al., 1989, Pratt, 1990 ] ). Neural network training techniques still have room for improvement, however. Though they eventually achieve good performance levels, neural networks often require more computing time than competing methods (cf. [ Maren et al., 1990, Page 92 ] , [ Waibel et al., 1989 ] , [ Hertz et al., 1991, Page 120 ] ). One source of power which, if properly utilized, may help to alleviate these problems is networks that have been trained on tasks that are related to the one at hand. For example, a network trained for heart disease diagnosis in Hungary might be used to initialize one that's to be trained for heart disease diagnosis of Swiss patients. Or a speech recognition network trained on only British speakers might be used to facilitate training on a network for American speakers. This paper explores this issue of transfer of information from a trained neural network to a new learning task. It presents an algorithm for modifying network weights during transfer from a source to a target problem. Empirical results on benchmark problems from several domains illustrate that this algorithm can improve learning speed. This work was partially supported by DOE #DE-FG02-91ER61129, through subcontract #097P753 from the University of Wisconsin."
            },
            "slug": "Non-literal-Transfer-Among-Neural-Network-Learners-Pratt",
            "title": {
                "fragments": [],
                "text": "Non-literal Transfer Among Neural Network Learners"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents an algorithm for modifying network weights during transfer from a source to a target problem and empirical results on benchmark problems from several domains illustrate that this algorithm can improve learning speed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 38
                            }
                        ],
                        "text": "11 Combining MTL and Boosting Bagging [Breiman 1994], Boosting [Schapire 1990][Freund 1995], Error-Correcting Codes [Dietterich & Bakiri 1995], and other voting schemes that combine multiple predictions for a task from di erent learned models are an exciting recent advance in machine learning."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 47328136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1ee87290fa827f1217b8fa2bccb3485da1a300e",
            "isKey": false,
            "numCitedBy": 15251,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Bagging predictors is a method for generating multiple versions of a predictor and using these to get an aggregated predictor. The aggregation averages over the versions when predicting a numerical outcome and does a plurality vote when predicting a class. The multiple versions are formed by making bootstrap replicates of the learning set and using these as new learning sets. Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy. The vital element is the instability of the prediction method. If perturbing the learning set can cause significant changes in the predictor constructed, then bagging can improve accuracy."
            },
            "slug": "Bagging-predictors-Breiman",
            "title": {
                "fragments": [],
                "text": "Bagging predictors"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Tests on real and simulated data sets using classification and regression trees and subset selection in linear regression show that bagging can give substantial gains in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41312633,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a8e8f3c8d4418c8d62e306538c9c1292635e9d27",
            "isKey": false,
            "numCitedBy": 7869,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of learning networks to generalize can be greatly enhanced by providing constraints from the task domain. This paper demonstrates how such constraints can be integrated into a backpropagation network through the architecture of the network. This approach has been successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service. A single network learns the entire recognition operation, going from the normalized image of the character to the final classification."
            },
            "slug": "Backpropagation-Applied-to-Handwritten-Zip-Code-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Backpropagation Applied to Handwritten Zip Code Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This paper demonstrates how constraints from the task domain can be integrated into a backpropagation network through the architecture of the network, successfully applied to the recognition of handwritten zip code digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2125286015"
                        ],
                        "name": "David E. Rumelhari",
                        "slug": "David-E.-Rumelhari",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhari",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David E. Rumelhari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2125288353"
                        ],
                        "name": "Geoffrey E. Hintont",
                        "slug": "Geoffrey-E.-Hintont",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hintont",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hintont"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "82961593"
                        ],
                        "name": "Ronald",
                        "slug": "Ronald",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Ronald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2070444145"
                        ],
                        "name": "J.",
                        "slug": "J.",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "J.",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J."
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058683052"
                        ],
                        "name": "Williams",
                        "slug": "Williams",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 237368852,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "ae3fe34be9230c98b04d68b4621c89b7dbc2d717",
            "isKey": false,
            "numCitedBy": 1053,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "delineating the absolute indigeneity of amino acids in fossils. As AMS iechniques are refined to handle smaller samples, it may also become possible to date individual amino acid enantiomers by the \u00b0C method. If one enantiomer is entirely derived from the other by racemization during diagenesis, the individual Dp. and L-enantiomers for a given amino acid should have identical \u201cC ages. Older, more poorly preserved fossils may not always prove amenable to the determination of amino acid indigeneity by the stable isotope method, as the prospects for complete replacement of indigenous amino acids with non-indigenous amino acids increases with time. As non-indigenous amino acids undergo racemization, the enantiomers may have identical isotopic compositions and still not be related to the original organisms. Such a circumstance may, however, become easier to recognize as more information becomes available concerning the distribution and stable isotopic composition of the amino acid constituents of modern representatives of fossil organisms. Also, AMS dates on individual amino acid enantiomers may, in some cases, help to clarify indigeneity problems, in particular when stratigraphic controls can be used to estimate a general age range for the fossil in question. Finally, the development of techniques for determining the stable isotopic compasition of amino acid enantiomers may enable us to establish whether non-racemic amino acids in some carbonaceous meteorites\u201d are indigenous, or result in part from terrestrial contamination. M.H.E. thanks the NSF, Division of Earth Sciences (grant | EAR-8352085) and the folowing contributors to his Presidential Young Investigator Award for partial support of this research: LETTERSTONATURE 533"
            },
            "slug": "Learning-representations-by-backpropagating-errors-Rumelhari-Hintont",
            "title": {
                "fragments": [],
                "text": "Learning representations by backpropagating errors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341779"
                        ],
                        "name": "J. R. Quinlan",
                        "slug": "J.-R.-Quinlan",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Quinlan",
                            "middleNames": [
                                "Ross"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. R. Quinlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13252401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcee7c85d237b79491a773ef51e746bbbcf48e35",
            "isKey": false,
            "numCitedBy": 13546,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "The technology for building knowledge-based systems by inductive inference from examples has been demonstrated successfully in several practical applications. This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail. Results from recent studies show ways in which the methodology can be modified to deal with information that is noisy and/or incomplete. A reported shortcoming of the basic algorithm is discussed and two means of overcoming it are compared. The paper concludes with illustrations of current research directions."
            },
            "slug": "Induction-of-Decision-Trees-Quinlan",
            "title": {
                "fragments": [],
                "text": "Induction of Decision Trees"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper summarizes an approach to synthesizing decision trees that has been used in a variety of systems, and it describes one such system, ID3, in detail, which is described in detail."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143648560"
                        ],
                        "name": "P. Spirtes",
                        "slug": "P.-Spirtes",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Spirtes",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Spirtes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3058012"
                        ],
                        "name": "C. Glymour",
                        "slug": "C.-Glymour",
                        "structuredName": {
                            "firstName": "Clark",
                            "lastName": "Glymour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Glymour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2758080"
                        ],
                        "name": "R. Scheines",
                        "slug": "R.-Scheines",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Scheines",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Scheines"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 117765107,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b3b9fe128e6849d59a26d7ceda57baad2524815",
            "isKey": false,
            "numCitedBy": 3250,
            "numCiting": 154,
            "paperAbstract": {
                "fragments": [],
                "text": "What assumptions and methods allow us to turn observations into causal knowledge, and how can even incomplete causal knowledge be used in planning and prediction to influence and control our environment? In this book Peter Spirtes, Clark Glymour, and Richard Scheines address these questions using the formalism of Bayes networks, with results that have been applied in diverse areas of research in the social, behavioral, and physical sciences. The authors show that although experimental and observational study designs may not always permit the same inferences, they are subject to uniform principles. They axiomatize the connection between causal structure and probabilistic independence, explore several varieties of causal indistinguishability, formulate a theory of manipulation, and develop asymptotically reliable procedures for searching over equivalence classes of causal models, including models of categorical data and structural equation models with and without latent variables. The authors show that the relationship between causality and probability can also help to clarify such diverse topics in statistics as the comparative power of experimentation versus observation, Simpson's paradox, errors in regression models, retrospective versus prospective sampling, and variable selection. The second edition contains a new introduction and an extensive survey of advances and applications that have appeared since the first edition was published in 1993."
            },
            "slug": "Causation,-prediction,-and-search-Spirtes-Glymour",
            "title": {
                "fragments": [],
                "text": "Causation, prediction, and search"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The authors axiomatize the connection between causal structure and probabilistic independence, explore several varieties of causal indistinguishability, formulate a theory of manipulation, and develop asymptotically reliable procedures for searching over equivalence classes of causal models."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8993101"
                        ],
                        "name": "D. Fisher",
                        "slug": "D.-Fisher",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Fisher",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fisher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 54
                            }
                        ],
                        "text": "For example, small changes to the indices in COBWEB\u2019s (Fisher 1987) probabilistic information metric yields a metric suitable for judging splits in multitask decision trees."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59640753,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "358b49ab2eb6d981130a6d7facf9e40c71afe493",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Conceptual-Clustering,-Learning-from-Examples,-and-Fisher",
            "title": {
                "fragments": [],
                "text": "Conceptual Clustering, Learning from Examples, and Inference"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758106"
                        ],
                        "name": "Dayne Freitag",
                        "slug": "Dayne-Freitag",
                        "structuredName": {
                            "firstName": "Dayne",
                            "lastName": "Freitag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dayne Freitag"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14828509"
                        ],
                        "name": "J. McDermott",
                        "slug": "J.-McDermott",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "McDermott",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McDermott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39787642"
                        ],
                        "name": "David Zabowski",
                        "slug": "David-Zabowski",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zabowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Zabowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 105
                            }
                        ],
                        "text": "A more recent example of multiple tasks arising naturally is Mitchell\u2019s Calendar Apprentice System (CAP) (Dent et al. 1992; Mitchell et al. 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8528878,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9565f70db3a6b5e4ff1df272238b8d7a84a1337",
            "isKey": false,
            "numCitedBy": 512,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Personal software assistants that help users with tasks like finding information, scheduling calendars, or managing work-flow will require significant customization to each individual user. For example, an assistant that helps schedule a particular user\u2019s calendar will have to know that user\u2019s scheduling preferences. This paper explores the potential of machine learning methods to automatically create and maintain such customized knowledge for personal software assistants. We describe the design of one particular learning assistant: a calendar manager, called CAP (Calendar APprentice), that learns user scheduling preferences from experience. Results are summarized from approximately five user-years of experience, during which CAP has learned an evolving set of several thousand rules that characterize the scheduling preferences of its users. Based on this experience, we suggest that machine learning methods may play an important role in future personal software assistants."
            },
            "slug": "Experience-with-a-learning-personal-assistant-Mitchell-Caruana",
            "title": {
                "fragments": [],
                "text": "Experience with a learning personal assistant"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The design of one particular learning assistant is described: a calendar manager, called CAP (Calendar APprentice), that learns user scheduling preferences from experience and suggests that machine learning methods may play an important role in future personal software assistants."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726406"
                        ],
                        "name": "G. Cooper",
                        "slug": "G.-Cooper",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Cooper",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49397975"
                        ],
                        "name": "E. Herskovits",
                        "slug": "E.-Herskovits",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Herskovits",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Herskovits"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6047868,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99588b3ac889ac70845fbbad4a1d67c1f8b37306",
            "isKey": false,
            "numCitedBy": 2587,
            "numCiting": 106,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a Bayesian method for constructing probabilistic networks from databases. In particular, we focus on constructing Bayesian belief networks. Potential applications include computer-assisted hypothesis testing, automated scientific discovery, and automated construction of probabilistic expert systems. We extend the basic method to handle missing data and hidden (latent) variables. We show how to perform probabilistic inference by averaging over the inferences of multiple belief networks. Results are presented of a preliminary evaluation of an algorithm for constructing a belief network from a database of cases. Finally, we relate the methods in this paper to previous work, and we discuss open problems."
            },
            "slug": "A-Bayesian-method-for-the-induction-of-networks-Cooper-Herskovits",
            "title": {
                "fragments": [],
                "text": "A Bayesian method for the induction of probabilistic networks from data"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This paper presents a Bayesian method for constructing probabilistic networks from databases, focusing on constructing Bayesian belief networks, and extends the basic method to handle missing data and hidden variables."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700754"
                        ],
                        "name": "Volker Tresp",
                        "slug": "Volker-Tresp",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Tresp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Volker Tresp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109973472"
                        ],
                        "name": "Subutai Ahmad",
                        "slug": "Subutai-Ahmad",
                        "structuredName": {
                            "firstName": "Subutai",
                            "lastName": "Ahmad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Subutai Ahmad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759183"
                        ],
                        "name": "R. Neuneier",
                        "slug": "R.-Neuneier",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Neuneier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Neuneier"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 572604,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c840266125754b51a995684ad012155676b34f24",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze how data with uncertain or missing input features can be incorporated into the training of a neural network. The general solution requires a weighted integration over the unknown or uncertain input although computationally cheaper closed-form solutions can be found for certain Gaussian Basis Function (GBF) networks. We also discuss cases in which heuristical solutions such as substituting the mean of an unknown input can be harmful."
            },
            "slug": "Training-Neural-Networks-with-Deficient-Data-Tresp-Ahmad",
            "title": {
                "fragments": [],
                "text": "Training Neural Networks with Deficient Data"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The general solution requires a weighted integration over the unknown or uncertain input although computationally cheaper closed-form solutions can be found for certain Gaussian Basis Function networks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13921532,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "406033f22b6a671b94bcbdfaf63070b7ce6f3e48",
            "isKey": false,
            "numCitedBy": 762,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Unrestricted English text can be converted to speech by applying phonological rules and handling exceptions with a look-up table. However, this approach is highly labor intensive since each entry and rule must be hand-crafted. NETtalk is an alternative approach that is based on an automated learning procedure for a parallel network of deterministic processing units. ~ f t e r ' training on a corpus of informal continuous speech, it achieves good performance and generalizes to novel words. The distributed internal representations of the phonological regularities discovered by the network are damage resistant."
            },
            "slug": "NETtalk:-a-parallel-network-that-learns-to-read-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "NETtalk: a parallel network that learns to read aloud"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "NETtalk is an alternative approach that is based on an automated learning procedure for a parallel network of deterministic processing units that achieves good performance and generalizes to novel words."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069692323"
                        ],
                        "name": "C. Dent",
                        "slug": "C.-Dent",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Dent",
                            "middleNames": [
                                "Lisa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1962816"
                        ],
                        "name": "J. Boticario",
                        "slug": "J.-Boticario",
                        "structuredName": {
                            "firstName": "Jes\u00fas",
                            "lastName": "Boticario",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Boticario"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14828509"
                        ],
                        "name": "J. McDermott",
                        "slug": "J.-McDermott",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "McDermott",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McDermott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39787642"
                        ],
                        "name": "David Zabowski",
                        "slug": "David-Zabowski",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zabowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Zabowski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 105
                            }
                        ],
                        "text": "A more recent example of multiple tasks arising naturally is Mitchell\u2019s Calendar Apprentice System (CAP) (Dent et al. 1992; Mitchell et al. 1994)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 34702472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cf3250f13d98bd2bd1b23ff81a333119f82ccc3",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Personalized knowledge-based systems have not yet become widespread, despite their potential for valuable assistance in many daily tasks. This is due, in part, to the high cost of developing and maintaining customized knowledge bases. The construction of personal assistants as learning apprentices -- interactive assistants that learn continually from their users -- is one approach which could dramatically reduce the cost of knowledge-based advisors. We present one such personal learning apprentice, called CAP, which assists in managing a meeting calendar. CAP has been used since June 1991 by a secretary in our work place to manage a faculty member's meeting calendar and is the first instance of a fielded learning apprentice in routine use. This paper describes the organization of CAP, its performance in initial field tests, and more general lessons learned from this effort about learning apprentice systems."
            },
            "slug": "A-Personal-Learning-Apprentice-Dent-Boticario",
            "title": {
                "fragments": [],
                "text": "A Personal Learning Apprentice"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The organization of CAP, its performance in initial field tests, and more general lessons learned from this effort about learning apprentice systems are described."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808760"
                        ],
                        "name": "S. Omohundro",
                        "slug": "S.-Omohundro",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Omohundro",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omohundro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 124
                            }
                        ],
                        "text": "Omohundro presents algorithms for \u201cFamily Discovery\u201d where the goal is to learn a parameterized family of stochastic models (Omohundro 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17817815,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1cae71762b6dd2f415576de3915a59c7dd7da0f9",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "\"Family discovery\" is the task of learning the dimension and structure of a parameterized family of stochastic models. It is especially appropriate when the training examples are partitioned into \"episodes\" of samples drawn from a single parameter value. We present three family discovery algorithms based on surface learning and show that they significantly improve performance over two alternatives on a parameterized classification task."
            },
            "slug": "Family-Discovery-Omohundro",
            "title": {
                "fragments": [],
                "text": "Family Discovery"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Three family discovery algorithms based on surface learning are presented and it is shown that they significantly improve performance over two alternatives on a parameterized classification task."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2024710"
                        ],
                        "name": "A. Weigend",
                        "slug": "A.-Weigend",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Weigend",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Weigend"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794321"
                        ],
                        "name": "B. Huberman",
                        "slug": "B.-Huberman",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Huberman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Huberman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 217236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f707a81a278d1598cd0a4493ba73f22dcdf90639",
            "isKey": false,
            "numCitedBy": 656,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Inspired by the information theoretic idea of minimum description length, we add a term to the back propagation cost function that penalizes network complexity. We give the details of the procedure, called weight-elimination, describe its dynamics, and clarify the meaning of the parameters involved. From a Bayesian perspective, the complexity term can be usefully interpreted as an assumption about prior distribution of the weights. We use this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates."
            },
            "slug": "Generalization-by-Weight-Elimination-with-to-Weigend-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Generalization by Weight-Elimination with Application to Forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work adds a term to the back propagation cost function that penalizes network complexity, called weight-elimination, and uses this procedure to predict the sunspot time series and the notoriously noisy series of currency exchange rates."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2524190"
                        ],
                        "name": "J. Sill",
                        "slug": "J.-Sill",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Sill",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398965769"
                        ],
                        "name": "Y. Abu-Mostafa",
                        "slug": "Y.-Abu-Mostafa",
                        "structuredName": {
                            "firstName": "Yaser",
                            "lastName": "Abu-Mostafa",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Abu-Mostafa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16065046,
            "fieldsOfStudy": [
                "Economics",
                "Computer Science"
            ],
            "id": "db3d06ae3ee8a77d0fbd1d521d8ff29d29d05f94",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A hint is any piece of side information about the target function to be learned. We consider the monotonicity hint, which states that the function to be learned is monotonic in some or all of the input variables. The application of monotonicity hints is demonstrated on two real-world problems- a credit card application task, and a problem in medical diagnosis. A measure of the monotonicity error of a candidate function is defined and an objective function for the enforcement of monotonicity is derived from Bayesian principles. We report experimental results which show that using monotonicity hints leads to a statistically significant improvement in performance on both problems."
            },
            "slug": "Monotonicity-Hints-Sill-Abu-Mostafa",
            "title": {
                "fragments": [],
                "text": "Monotonicity Hints"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Experimental results show that using monotonicity hints leads to a statistically significant improvement in performance on both real-world problems and a problem in medical diagnosis."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3056361"
                        ],
                        "name": "J. Friedman",
                        "slug": "J.-Friedman",
                        "structuredName": {
                            "firstName": "Jerome",
                            "lastName": "Friedman",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Friedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121621272,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "c5f1b46a306a486bcf91be71f2a726b11f462514",
            "isKey": false,
            "numCitedBy": 475,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "We look at the problem of predicting several response variables from the same set of explanatory variables. The question is how to take advantage of correlations between the response variables to improve predictive accuracy compared with the usual procedure of doing individual regressions of each response variable on the common set of predictor variables. A new procedure is introduced called the curds and whey method. Its use can substantially reduce prediction errors when there are correlations between responses while maintaining accuracy even if the responses are uncorrelated. In extensive simulations, the new procedure is compared with several previously proposed methods for predicting multiple responses (including partial least squares) and exhibits superior accuracy. One version can be easily implemented in the context of standard statistical packages."
            },
            "slug": "Predicting-Multivariate-Responses-in-Multiple-Breiman-Friedman",
            "title": {
                "fragments": [],
                "text": "Predicting Multivariate Responses in Multiple Linear Regression"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145900862"
                        ],
                        "name": "Joel D. Martin",
                        "slug": "Joel-D.-Martin",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Martin",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joel D. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2140385"
                        ],
                        "name": "D. Billman",
                        "slug": "D.-Billman",
                        "structuredName": {
                            "firstName": "Dorrit",
                            "lastName": "Billman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Billman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12390673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56095422a81cf993fddfa55d9dfb15b0d206678a",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents OLOC, an incremental concept formation system that learns and uses overlapping concepts. OLOC learns probabilistic concepts that have overlapping extensions and does so to maximize expected predictive accuracy. When making predictions, OLOC can combine multiple overlapping concepts."
            },
            "slug": "Acquiring-and-Combining-Overlapping-Concepts-Martin-Billman",
            "title": {
                "fragments": [],
                "text": "Acquiring and Combining Overlapping Concepts"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "OLOC learns probabilistic concepts that have overlapping extensions and does so to maximize expected predictive accuracy and can combine multiple overlapping concepts when making predictions."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726406"
                        ],
                        "name": "G. Cooper",
                        "slug": "G.-Cooper",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Cooper",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Cooper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803212"
                        ],
                        "name": "C. Aliferis",
                        "slug": "C.-Aliferis",
                        "structuredName": {
                            "firstName": "Constantin",
                            "lastName": "Aliferis",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Aliferis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35679585"
                        ],
                        "name": "R. Ambrosino",
                        "slug": "R.-Ambrosino",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Ambrosino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ambrosino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3304904"
                        ],
                        "name": "J. Aronis",
                        "slug": "J.-Aronis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Aronis",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aronis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740513"
                        ],
                        "name": "B. Buchanan",
                        "slug": "B.-Buchanan",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Buchanan",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Buchanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144676704"
                        ],
                        "name": "M. Fine",
                        "slug": "M.-Fine",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fine",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3058012"
                        ],
                        "name": "C. Glymour",
                        "slug": "C.-Glymour",
                        "structuredName": {
                            "firstName": "Clark",
                            "lastName": "Glymour",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Glymour"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21889436"
                        ],
                        "name": "Geoffrey J. Gordon",
                        "slug": "Geoffrey-J.-Gordon",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Gordon",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey J. Gordon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3068231"
                        ],
                        "name": "B. Hanusa",
                        "slug": "B.-Hanusa",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hanusa",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hanusa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2635035"
                        ],
                        "name": "J. Janosky",
                        "slug": "J.-Janosky",
                        "structuredName": {
                            "firstName": "Janine",
                            "lastName": "Janosky",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Janosky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50004012"
                        ],
                        "name": "Christopher Meek",
                        "slug": "Christopher-Meek",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Meek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Meek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10726446"
                        ],
                        "name": "T. Richardson",
                        "slug": "T.-Richardson",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Richardson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143648560"
                        ],
                        "name": "P. Spirtes",
                        "slug": "P.-Spirtes",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Spirtes",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Spirtes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1175621,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "3a51a6d10f9739666c5e9176f2a86587eedb54e4",
            "isKey": false,
            "numCitedBy": 213,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-evaluation-of-machine-learning-methods-for-Cooper-Aliferis",
            "title": {
                "fragments": [],
                "text": "An evaluation of machine-learning methods for predicting pneumonia mortality"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell. Medicine"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402451824"
                        ],
                        "name": "R. Vald\u00e9s-P\u00e9rez",
                        "slug": "R.-Vald\u00e9s-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Ra\u00fal",
                            "lastName": "Vald\u00e9s-P\u00e9rez",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Vald\u00e9s-P\u00e9rez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110376725"
                        ],
                        "name": "Aurora P\u00e9rez",
                        "slug": "Aurora-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Aurora",
                            "lastName": "P\u00e9rez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aurora P\u00e9rez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 55
                            }
                        ],
                        "text": "This\nshuffle test is similar to the heuristic used in [Valdes-Perez & Simon 1994] to discover complex patterns in data."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16319930,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7169e879d26972b96834d9357d692152a945bfe8",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Powerful-Heuristic-for-the-Discovery-of-Complex-Vald\u00e9s-P\u00e9rez-P\u00e9rez",
            "title": {
                "fragments": [],
                "text": "A Powerful Heuristic for the Discovery of Complex Patterned Behaviour"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195972"
                        ],
                        "name": "P. Utgoff",
                        "slug": "P.-Utgoff",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Utgoff",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Utgoff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46796200"
                        ],
                        "name": "S. Saxena",
                        "slug": "S.-Saxena",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Saxena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Saxena"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60833111,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6445a1d765b78094c56cfa280c540e2dff193d4d",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-a-Preference-Predicate-Utgoff-Saxena",
            "title": {
                "fragments": [],
                "text": "Learning a Preference Predicate"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31084222"
                        ],
                        "name": "I. Davis",
                        "slug": "I.-Davis",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Davis",
                            "middleNames": [
                                "Lane"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1722938"
                        ],
                        "name": "A. Stentz",
                        "slug": "A.-Stentz",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Stentz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Stentz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 14
                            }
                        ],
                        "text": "Feature nets [Davis & Stentz 1995] is a competing approach that trains nets to predict the missing future measurements and uses the predictions, or the hidden layers learned for these predictions, as extra inputs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6144199,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7725f6bd3ec8e820a68411b904dc6e38209f244",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "For many navigation tasks, a single sensing modality is sufficiently rich to accomplish the desired motion control goals; for practical autonomous outdoor navigation, a single sensing modality is a crippling limitation on what tasks can be undertaken. Using a neural network paradigm particularly well suited to sensor fusion the authors have successfully performed simulated and real-world navigation tasks that required the use of multiple sensing modalities."
            },
            "slug": "Sensor-fusion-for-autonomous-outdoor-navigation-Davis-Stentz",
            "title": {
                "fragments": [],
                "text": "Sensor fusion for autonomous outdoor navigation using neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Using a neural network paradigm particularly well suited to sensor fusion the authors have successfully performed simulated and real-world navigation tasks that required the use of multiple sensing modalities."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1995 IEEE/RSJ International Conference on Intelligent Robots and Systems. Human Robot Interaction and Cooperative Robots"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144676704"
                        ],
                        "name": "M. Fine",
                        "slug": "M.-Fine",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Fine",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1931211"
                        ],
                        "name": "D. Singer",
                        "slug": "D.-Singer",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Singer",
                            "middleNames": [
                                "E."
                            ],
                            "suffix": "MD"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Singer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3068231"
                        ],
                        "name": "B. Hanusa",
                        "slug": "B.-Hanusa",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Hanusa",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hanusa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2459833"
                        ],
                        "name": "J. Lave",
                        "slug": "J.-Lave",
                        "structuredName": {
                            "firstName": "Judith",
                            "lastName": "Lave",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Lave"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796768"
                        ],
                        "name": "W. Kapoor",
                        "slug": "W.-Kapoor",
                        "structuredName": {
                            "firstName": "Wishwa",
                            "lastName": "Kapoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kapoor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30014220,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "29b113386947d64646a5a6c64fc5b4a520542d9e",
            "isKey": false,
            "numCitedBy": 183,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Validation-of-a-pneumonia-prognostic-index-using-Fine-Singer",
            "title": {
                "fragments": [],
                "text": "Validation of a pneumonia prognostic index using the MedisGroups Comparative Hospital Database."
            },
            "venue": {
                "fragments": [],
                "text": "The American journal of medicine"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2481028"
                        ],
                        "name": "Gregory I. Dzuba",
                        "slug": "Gregory-I.-Dzuba",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Dzuba",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gregory I. Dzuba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053217424"
                        ],
                        "name": "Alexander Filatov",
                        "slug": "Alexander-Filatov",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Filatov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Filatov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115633736"
                        ],
                        "name": "A. Volgunin",
                        "slug": "A.-Volgunin",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Volgunin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Volgunin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 34599359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01fba5ef76993b0cb36f6c1034dc239015a44ba3",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The encoding of delivery point code (DPC) for a handwritten address is one of the most complex problems of the US mail delivery automation. This paper describes a real-time system intended to recognize the 5-digit ZIP code part of DPC. To increase the system performance the results of ZIP code recognition are cross-validated with those of city and state name recognition. The main principles of the handwritten word recognizer which provide the core of the system are explained. The system throughput is 40,000 address blocks per hour. Experimental results on live mail pieces are presented. The ZIP code recognition rate is 73% with 1% error rate."
            },
            "slug": "Handwritten-ZIP-code-recognition-Dzuba-Filatov",
            "title": {
                "fragments": [],
                "text": "Handwritten ZIP code recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper describes a real-time system intended to recognize the 5-digit ZIP code part of DPC and the main principles of the handwritten word recognizer which provide the core of the system are explained."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth International Conference on Document Analysis and Recognition"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 45
                            }
                        ],
                        "text": "The MTL decision tree algorithm presented in [Caruana 1993] combines task gains by averaging them; the selected splits are the ones whose average utility across all tasks is highest."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 82
                            }
                        ],
                        "text": "Sometimes this same e ect is observed with simpler, fully connected MTL nets, too [Caruana 1993]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 29
                            }
                        ],
                        "text": "More detail can be found in [Caruana 1994, 1997]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 211
                            }
                        ],
                        "text": "Sharing what is learned by different tasks while tasks are trained in parallel is the central idea in multitask learning [Suddarth & Kergosien 1990; Dietterich, Hild & Bakiri 1990, 1995; Suddarth & Holden 1991; Caruana 1993a, 1993b, 1994, 1995; Baxter 1994, 1995, 1996; Caruana & de Sa 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multitask Connectionist Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1993 Connectionist Models Summer School"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118969901"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46831169"
                        ],
                        "name": "G. Hinton",
                        "slug": "G.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145993114"
                        ],
                        "name": "R. William",
                        "slug": "R.-William",
                        "structuredName": {
                            "firstName": "R",
                            "lastName": "William",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. William"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 151380665,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "749ce8ccd9453d1b34901143cddf5f9bee2977cf",
            "isKey": false,
            "numCitedBy": 1335,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-representations-by-back-propagation-nature-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning representations by back-propagation errors, nature"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145969624"
                        ],
                        "name": "C. Chatfield",
                        "slug": "C.-Chatfield",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Chatfield",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chatfield"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 227300792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "49bdcdcccfeccedeae6157cbbd6555ffaecaf359",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "19.-Statistical-Analysis-with-Missing-Data-Chatfield",
            "title": {
                "fragments": [],
                "text": "19. Statistical Analysis with Missing Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59736489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "577897e776dc3cfda04c3b66750ca7fef1b155c8",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Mixture-models-for-learning-from-incomplete-data-Ghahramani-Jordan",
            "title": {
                "fragments": [],
                "text": "Mixture models for learning from incomplete data"
            },
            "venue": {
                "fragments": [],
                "text": "COLT 1997"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 0
                            }
                        ],
                        "text": "(Hinton 1986) suggested that generalization in artificial neural nets would improve if nets learned to better represent underlying regularities of the domain."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53796860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ade4934db522fe6d634ff6f48887da46eedb4d1",
            "isKey": false,
            "numCitedBy": 903,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-distributed-representations-of-concepts.-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning distributed representations of concepts."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114788213"
                        ],
                        "name": "D. Signorini",
                        "slug": "D.-Signorini",
                        "structuredName": {
                            "firstName": "DavidF.",
                            "lastName": "Signorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Signorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4137433"
                        ],
                        "name": "J. Slattery",
                        "slug": "J.-Slattery",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Slattery",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Slattery"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058057862"
                        ],
                        "name": "S. Dodds",
                        "slug": "S.-Dodds",
                        "structuredName": {
                            "firstName": "Sally",
                            "lastName": "Dodds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dodds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51934565"
                        ],
                        "name": "V. Lane",
                        "slug": "V.-Lane",
                        "structuredName": {
                            "firstName": "V",
                            "lastName": "Lane",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Lane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095059657"
                        ],
                        "name": "P. Littlejohns",
                        "slug": "P.-Littlejohns",
                        "structuredName": {
                            "firstName": "P",
                            "lastName": "Littlejohns",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Littlejohns"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2878979,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "20b844e395355b40fa5940c61362ec40e56027aa",
            "isKey": false,
            "numCitedBy": 4712,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-Signorini-Slattery",
            "title": {
                "fragments": [],
                "text": "Neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "The Lancet"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799764"
                        ],
                        "name": "Lasse Holmstr\u00f6m",
                        "slug": "Lasse-Holmstr\u00f6m",
                        "structuredName": {
                            "firstName": "Lasse",
                            "lastName": "Holmstr\u00f6m",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lasse Holmstr\u00f6m"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38071249"
                        ],
                        "name": "P. Koistinen",
                        "slug": "P.-Koistinen",
                        "structuredName": {
                            "firstName": "Petri",
                            "lastName": "Koistinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Koistinen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 80
                            }
                        ],
                        "text": "For example, adding noise to backpropagation sometimes improves generalization [Holmstrom & Koistinen 1992]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 601336,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11823e4fd9aa67ff429c6a63d99495187547c093",
            "isKey": false,
            "numCitedBy": 351,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The possibility of improving the generalization capability of a neural network by introducing additive noise to the training samples is discussed. The network considered is a feedforward layered neural network trained with the back-propagation algorithm. Back-propagation training is viewed as nonlinear least-squares regression and the additive noise is interpreted as generating a kernel estimate of the probability density that describes the training vector distribution. Two specific application types are considered: pattern classifier networks and estimation of a nonstochastic mapping from data corrupted by measurement errors. It is not proved that the introduction of additive noise to the training vectors always improves network generalization. However, the analysis suggests mathematically justified rules for choosing the characteristics of noise if additive noise is used in training. Results of mathematical statistics are used to establish various asymptotic consistency results for the proposed method. Numerical simulations support the applicability of the training method."
            },
            "slug": "Using-additive-noise-in-back-propagation-training-Holmstr\u00f6m-Koistinen",
            "title": {
                "fragments": [],
                "text": "Using additive noise in back-propagation training"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is not proved that the introduction of additive noise to the training vectors always improves network generalization, but the analysis suggests mathematically justified rules for choosing the characteristics of noise if additive noise is used in training."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7402299"
                        ],
                        "name": "L. Hockey",
                        "slug": "L.-Hockey",
                        "structuredName": {
                            "firstName": "Lisbeth",
                            "lastName": "Hockey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Hockey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 25768078,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "46e40e996e01222039962ab0030bffd4b596c5c4",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-from-time.-Hockey",
            "title": {
                "fragments": [],
                "text": "Learning from time."
            },
            "venue": {
                "fragments": [],
                "text": "Nursing standard (Royal College of Nursing (Great Britain) : 1987)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144442133"
                        ],
                        "name": "L. Pratt",
                        "slug": "L.-Pratt",
                        "structuredName": {
                            "firstName": "Lorien",
                            "lastName": "Pratt",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Pratt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695106"
                        ],
                        "name": "Jack Mostow",
                        "slug": "Jack-Mostow",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Mostow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jack Mostow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769629"
                        ],
                        "name": "C. Kamm",
                        "slug": "C.-Kamm",
                        "structuredName": {
                            "firstName": "Candace",
                            "lastName": "Kamm",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kamm"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 79
                            }
                        ],
                        "text": "The early work on sequential transfer of learned structure between neural nets (Pratt et al. 1991; Pratt 1992; Sharkey & Sharkey 1992) clearly demonstrates that what is learned for one task can be used as a bias for other tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30657075,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "052f4d936ceaccbce8d7a3ad2449fb7d7676eb0c",
            "isKey": false,
            "numCitedBy": 180,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A touted advantage of symbolic representations is the ease of transferring learned information from one intelligent agent to another. This paper investigates an analogous problem: how to use information from one neural network to help a second network learn a related task. Rather than translate such information into symbolic form (in which it may not be readily expressible), we investigate the direct transfer of information encoded as weights. \n \nHere, we focus on how transfer can be used to address the important problem of improving neural network learning speed. First we present an exploratory study of the somewhat surprising effects of pre-setting network weights on subsequent learning. Guided by hypotheses from this study, we sped up back-propagation learning for two speech recognition tasks. By transferring weights from smaller networks trained on subtasks, we achieved speedups of up to an order of magnitude compared with training starting with random weights, even taking into account the time to train the smaller networks. We include results on how transfer scales to a large phoneme recognition problem."
            },
            "slug": "Direct-Transfer-of-Learned-Information-Among-Neural-Pratt-Mostow",
            "title": {
                "fragments": [],
                "text": "Direct Transfer of Learned Information Among Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "By transferring weights from smaller networks trained on subtasks, this paper achieved speedups of up to an order of magnitude compared with training starting with random weights, even taking into account the time to train the smaller networks."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145382036"
                        ],
                        "name": "R. Little",
                        "slug": "R.-Little",
                        "structuredName": {
                            "firstName": "Roderick",
                            "lastName": "Little",
                            "middleNames": [
                                "J.",
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Little"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235217"
                        ],
                        "name": "D. Rubin",
                        "slug": "D.-Rubin",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Rubin",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rubin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 125113713,
            "fieldsOfStudy": [
                "Mathematics",
                "Medicine",
                "Business",
                "Psychology"
            ],
            "id": "c5c95938c03caa0b73303bab1e5ceeacb1879177",
            "isKey": false,
            "numCitedBy": 3854,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface.PART I: OVERVIEW AND BASIC APPROACHES.Introduction.Missing Data in Experiments.Complete-Case and Available-Case Analysis, Including Weighting Methods.Single Imputation Methods.Estimation of Imputation Uncertainty.PART II: LIKELIHOOD-BASED APPROACHES TO THE ANALYSIS OF MISSING DATA.Theory of Inference Based on the Likelihood Function.Methods Based on Factoring the Likelihood, Ignoring the Missing-Data Mechanism.Maximum Likelihood for General Patterns of Missing Data: Introduction and Theory with Ignorable Nonresponse.Large-Sample Inference Based on Maximum Likelihood Estimates.Bayes and Multiple Imputation.PART III: LIKELIHOOD-BASED APPROACHES TO THE ANALYSIS OF MISSING DATA: APPLICATIONS TO SOME COMMON MODELS.Multivariate Normal Examples, Ignoring the Missing-Data Mechanism.Models for Robust Estimation.Models for Partially Classified Contingency Tables, Ignoring the Missing-Data Mechanism.Mixed Normal and Nonnormal Data with Missing Values, Ignoring the Missing-Data Mechanism.Nonignorable Missing-Data Models.References.Author Index.Subject Index."
            },
            "slug": "Statistical-Analysis-with-Missing-Data-Little-Rubin",
            "title": {
                "fragments": [],
                "text": "Statistical Analysis with Missing Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 64074620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f96cbb6c373c0e68c75ef23e5cdca7ed56af05a6",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": "In many real-world tasks, the ability to focus attention on the relevant portions of the input is crucial for good performance. This work has shown that, for temporally coherent inputs, a computed expectation of the next time step\u2019s inputs provides a basis upon which to focus attention. Expectations are useful in tasks which arise in visual and non-visual domains, ranging from scene analysis to anomaly detection. When temporally related inputs are available, an expectation of the next input\u2019s contents can be computed based upon the current inputs. A saliency map, which is based upon the computed expectation and the actual inputs, indicates which inputs will be important for performing the task in the next time step. For example, in many visual object tracking problems, the relevant features are predictable, while the distractions in the scene are either unpredictable or unrelated to the task. The task-specific selective attention methods can be used to create a saliency map which accentuates only the predictable inputs that are useful in solving the task. In a second use of expectation, anomaly detection, the unexpected features are important. Here, the role of expectation is reversed; it is used to emphasize the unpredicted features. The performance of these methods is demonstrated in artificial neural network based systems on two real-world vision tasks: lane-marker tracking for autonomous vehicle control and driver monitoring, and hand tracking in cluttered scenes. For the hand-tracking task, techniques for incorporating a priori available domain knowledge are presented. These methods are also demonstrated in a nonvision based task: anomaly detection in the plasma etch step of semiconductor wafer fabrication. In addition to explicitly creating a saliency map to indicate where a network should pay attention, techniques are developed to reveal a network\u2019s implicit saliency map. The implicit saliency map represents the portions of the input to which a network will pay attention in the absence of the explicit focusing mechanisms developed in this thesis. Methods to examine the features a network has encoded in its hidden layers are also presented. These techniques are applied to networks trained to perform face-detection in arbitrary visual scenes. The results clearly display the facial features the network determines to be the most important for face detection. These techniques address one of the largest criticisms of artificial neural networks \u2212 that it is difficult to understand what they encode."
            },
            "slug": "Expectation-based-selective-attention-Baluja",
            "title": {
                "fragments": [],
                "text": "Expectation-based selective attention"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Techniques are developed to reveal a network\u2019s implicit saliency map, which represents the portions of the input to which a network will pay attention in the absence of the explicit focusing mechanisms developed in this thesis."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736370"
                        ],
                        "name": "D. Koller",
                        "slug": "D.-Koller",
                        "structuredName": {
                            "firstName": "Daphne",
                            "lastName": "Koller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Koller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764547"
                        ],
                        "name": "M. Sahami",
                        "slug": "M.-Sahami",
                        "structuredName": {
                            "firstName": "Mehran",
                            "lastName": "Sahami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sahami"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1455429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ed4e1dbe10c0ac9fa00b30d1882cae1249a5a6a",
            "isKey": false,
            "numCitedBy": 1749,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we examine a method for feature subset selection based on Information Theory. Initially, a framework for defining the theoretically optimal, but computationally intractable, method for feature subset selection is presented. We show that our goal should be to eliminate a feature if it gives us little or no additional information beyond that subsumed by the remaining features. In particular, this will be the case for both irrelevant and redundant features. We then give an efficient algorithm for feature selection which computes an approximation to the optimal feature selection criterion. The conditions under which the approximate algorithm is successful are examined. Empirical results are given on a number of data sets, showing that the algorithm effectively handles datasets with a very large number of features."
            },
            "slug": "Toward-Optimal-Feature-Selection-Koller-Sahami",
            "title": {
                "fragments": [],
                "text": "Toward Optimal Feature Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "An efficient algorithm for feature selection which computes an approximation to the optimal feature selection criterion is given, showing that the algorithm effectively handles datasets with a very large number of features."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18086786,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5db7dc2239f820eae498b07a955f31b3d113179f",
            "isKey": false,
            "numCitedBy": 636,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Real-world learning tasks may involve high-dimensional data sets with arbitrary patterns of missing data. In this paper we present a framework based on maximum likelihood density estimation for learning from such data set.s. We use mixture models for the density estimates and make two distinct appeals to the Expectation-Maximization (EM) principle (Dempster et al., 1977) in deriving a learning algorithm--EM is used both for the estimation of mixture components and for coping with missing data. The resulting algorithm is applicable to a wide range of supervised as well as unsupervised learning problems. Results from a classification benchmark--the iris data set--are presented."
            },
            "slug": "Supervised-learning-from-incomplete-data-via-an-EM-Ghahramani-Jordan",
            "title": {
                "fragments": [],
                "text": "Supervised learning from incomplete data via an EM approach"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A framework based on maximum likelihood density estimation for learning from high-dimensional data sets with arbitrary patterns of missing data is presented and results from a classification benchmark--the iris data set--are presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "In these experiments the nets have sufficient capacity to find independent minima for the tasks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 4
                            }
                        ],
                        "text": "The Medis Pneumonia Database [Fine et al. 1995] contains 14,199 pneumonia cases collected from 78 hospitals in 1989."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 29
                            }
                        ],
                        "text": "The Medis Pneumonia Database (Fine et al. 1993) contains 14,199 pneumonia cases collected from 78 hospitals in 1989."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Validation of a Pneumonia Prognostic"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Specifying Selected Invariances in an Adaptive Neural Network,\" Advances in Neural Information Processing Systems 4, (Proceedings of NIPS-91"
            },
            "venue": {
                "fragments": [],
                "text": "Systems,\" International Journal of Man-Machine Studies,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Causation, Prediction, and Search, Springer-Verlag, New York"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 245
                            }
                        ],
                        "text": "Sharing what is learned by different tasks while tasks are trained in parallel is the central idea in multitask learning [Suddarth & Kergosien 1990; Dietterich, Hild & Bakiri 1990, 1995; Suddarth & Holden 1991; Caruana 1993a, 1993b, 1994, 1995; Baxter 1994, 1995, 1996; Caruana & de Sa 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Internal Representations The Flinders Univeristy of South Australia"
            },
            "venue": {
                "fragments": [],
                "text": "COLT-95)"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Predicting Pneumonia Mortality,"
            },
            "venue": {
                "fragments": [],
                "text": "Arti cial Intelligence in Medicine"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multi-Task Learning for Stock Selection,\" Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "(Proceedings of NIPS-96),"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Backpropa - gation Applied to Handwritten ZipCode Recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Representations by Back-propagating"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A Comparison of ID 3 and Backpropagationfor English Texttospeech Mapping"
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Classification with Unlabelled Data,"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 6, (Proceedings of NIPS-93),"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning Classification with Unlabelled Data"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 6Proceedings of NIPS-93)"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "More complex architectures than a fully connected hidden layer sometimes work better. See Section 7"
            },
            "venue": {
                "fragments": [],
                "text": "More complex architectures than a fully connected hidden layer sometimes work better. See Section 7"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Supervised Learning from Incomplete Data Using an EM Approach,\" Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A similar experiment using nets with 2 hidden layers containing 2, 4, 8, 16, or 32 hidden units per layer for STL and 32 hidden units per layer for MTL yielded similar results"
            },
            "venue": {
                "fragments": [],
                "text": "A similar experiment using nets with 2 hidden layers containing 2, 4, 8, 16, or 32 hidden units per layer for STL and 32 hidden units per layer for MTL yielded similar results"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training Neural Networks with Deficient Data Advances in Neural Information Processing Systems 6"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of NIPS-93) Proceedings of the 11th International Conference on Machine Learning"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Received Date Accepted Date Final Manuscript Date"
            },
            "venue": {
                "fragments": [],
                "text": "Received Date Accepted Date Final Manuscript Date"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 4
                            }
                        ],
                        "text": "See (Caruana 1997) for more detail about how the \u03bb parameters can be learned efficiently in MTL TDIDT."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 29
                            }
                        ],
                        "text": "More detail can be found in [Caruana 1994, 1997]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multitask Learning,"
            },
            "venue": {
                "fragments": [],
                "text": "Ph.D. Thesis,"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proceedings of AAAI-91"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of AAAI-91"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Programs for Machine"
            },
            "venue": {
                "fragments": [],
                "text": "Learning, Morgan Kaufman Publishers,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive Generalisation and the Transfer of Knowledge,"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Monotonicity Hints,\" to appear in Neural Information Processing Systems 9, (Proceedings of NIPS-96)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Neural Information Processing Systems 9, (Proceedings of NIPS-96)"
            },
            "venue": {
                "fragments": [],
                "text": "Better As Outputs,\""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 211
                            }
                        ],
                        "text": "Sharing what is learned by different tasks while tasks are trained in parallel is the central idea in multitask learning [Suddarth & Kergosien 1990; Dietterich, Hild & Bakiri 1990, 1995; Suddarth & Holden 1991; Caruana 1993a, 1993b, 1994, 1995; Baxter 1994, 1995, 1996; Caruana & de Sa 1996]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1994).\u201cMultitask Connectionist Learning,"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 1993 Connectionist Models Summer School,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Hints Neural Computation"
            },
            "venue": {
                "fragments": [],
                "text": "Hints Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Carnegie Mellon University: CS-94-184"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "More complex architectures than a fully connected hidden layer sometimes work better"
            },
            "venue": {
                "fragments": [],
                "text": "See Section"
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 26,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 99,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/Multitask-Learning-Caruana/161ffb54a3fdf0715b198bb57bd22f910242eb49?sort=total-citations"
}