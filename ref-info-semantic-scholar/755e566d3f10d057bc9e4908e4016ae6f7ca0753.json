{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50138636"
                        ],
                        "name": "Wu-Sheng Lu",
                        "slug": "Wu-Sheng-Lu",
                        "structuredName": {
                            "firstName": "Wu-Sheng",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wu-Sheng Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144782814"
                        ],
                        "name": "S. Pei",
                        "slug": "S.-Pei",
                        "structuredName": {
                            "firstName": "Soo-Chang",
                            "lastName": "Pei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108355081"
                        ],
                        "name": "Paul S. Wang",
                        "slug": "Paul-S.-Wang",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Wang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul S. Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 17
                            }
                        ],
                        "text": "Shpak (1990) and Lu et al. (1997) studied weighted-norm low-rank approximations for the design of two-dimensional digital filters where the weights arise from constraints of varying importance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 144
                            }
                        ],
                        "text": "\u2026more involved than the unweighted case, this is still significantly less than the prohibitive O(n3k3) required for each iteration suggested by Lu et al. (1997), or for Hessian methods on (U, V ) (Shpak, 1990), and is only a factor of k larger than the O(ndk) required just to compute the\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 156
                            }
                        ],
                        "text": "Although more involved than the unweighted case, this is still significantly less than the prohibitive O(n(3)k(3)) required for each iteration suggested by Lu et al. (1997), or for Hessian methods on (U, V ) (Shpak, 1990), and is only a factor of k larger than the O(ndk) required just to compute the prediction UV \u2032."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14224693,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0206e9729636f37b8fa8993f64897b22855fe6fe",
            "isKey": false,
            "numCitedBy": 47,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this brief we present a method for the weighted low-rank approximation of general complex matrices along with an algorithmic development for its computation. The method developed can be viewed as an extension of the conventional singular value decomposition to include a nontrivial weighting matrix in the approximation error measure. It is shown that the optimal rank-K weighted approximation can be achieved by computing K generalized Schmidt pairs and an iterative algorithm is presented to compute them. Application of the proposed algorithm to the design of FIR two-dimensional (2-D) digital filters is described to demonstrate the usefulness of the algorithm proposed."
            },
            "slug": "Weighted-Low-Rank-Approximation-of-General-Complex-Lu-Pei",
            "title": {
                "fragments": [],
                "text": "Weighted Low-Rank Approximation of General Complex Matrices and Its Application in the Design of 2-D Digital Filters"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Application of the proposed algorithm to the design of FIR two-dimensional (2-D) digital filters is described to demonstrate the usefulness of the algorithm proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 102
                            }
                        ],
                        "text": "In order to provide for a more robust method, we use the following variational bound on the logistic (Jaakkola & Jordan, 2000):\nlog g(yx) \u2265 log g(yx\u0303) + yx\u2212yx\u03032 \u2212 tanh(x\u0303/2) 4x\u0303\n( x2 \u2212 x\u03032 ) = \u2212 14 tanh(x\u0303/2) x\u0303 ( x\u2212 yx\u0303tanh(x\u0303/2) ) + Const,\nyielding the corresponding bound on the likelihood:\nlog\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In order to provide for a more robust method, we use the following variational bound on the logistic ( Jaakkola & Jordan, 2000 ): logg(yx) logg(y\u02dc"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13026917,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "ee606afe92d1998d516f1e2599ddf8a386880d2a",
            "isKey": false,
            "numCitedBy": 589,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a logistic regression model with a Gaussian prior distribution over the parameters. We show that an accurate variational transformation can be used to obtain a closed form approximation to the posterior distribution of the parameters thereby yielding an approximate posterior predictive model. This approach is readily extended to binary graphical model with complete observations. For graphical models with incomplete observations we utilize an additional variational transformation and again obtain a closed form approximation to the posterior. Finally, we show that the dual of the regression problem gives a latent variable density model, the variational formulation of which leads to exactly solvable EM updates."
            },
            "slug": "Bayesian-parameter-estimation-via-variational-Jaakkola-Jordan",
            "title": {
                "fragments": [],
                "text": "Bayesian parameter estimation via variational methods"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "It is shown that an accurate variational transformation can be used to obtain a closed form approximation to the posterior distribution of the parameters thereby yielding an approximate posterior predictive model."
            },
            "venue": {
                "fragments": [],
                "text": "Stat. Comput."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21889436"
                        ],
                        "name": "Geoffrey J. Gordon",
                        "slug": "Geoffrey-J.-Gordon",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Gordon",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey J. Gordon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6105738,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "4000e45dd466f23510dccdf396e1cea59d239560",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the Generalized2 Linear2 Model, a statistical estimator which combines features of nonlinear regression and factor analysis. A (GL)2M approximately decomposes a rectangular matrix X into a simpler representation f(g(A)h(B)). Here A and B are low-rank matrices, while f, g, and h arc link functions. (GL)2Ms include many useful models as special cases, including principal components analysis, exponential-family PCA, the infomax formulation of independent components analysis, linear regression, and generalized linear models. They also include new and interesting special cases, one of which we describe below. We also present an iterative procedure which optimizes the parameters of a (GL)2M. This procedure reduces to well-known algorithms for some of the special cases listed above; for other special cases, it is new."
            },
            "slug": "Generalized^2-Linear^2-Models-Gordon",
            "title": {
                "fragments": [],
                "text": "Generalized^2 Linear^2 Models"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "The Generalized2 Linear2 Model is introduced, a statistical estimator which combines features of nonlinear regression and factor analysis and an iterative procedure which optimizes the parameters of a (GL)2M is presented."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS 2002"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771670"
                        ],
                        "name": "D. Shpak",
                        "slug": "D.-Shpak",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Shpak",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Shpak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 42
                            }
                        ],
                        "text": "(1997), or for Hessian methods on (U, V ) (Shpak, 1990), and is only a factor of k larger than the O(ndk) required just to compute the prediction UV \u2032."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 0
                            }
                        ],
                        "text": "Shpak (1990) and Lu et al. (1997) studied weighted-norm low-rank approximations for the design of two-dimensional digital filters where the weights arise from constraints of varying importance."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 192
                            }
                        ],
                        "text": "\u2026involved than the unweighted case, this is still significantly less than the prohibitive O(n3k3) required for each iteration suggested by Lu et al. (1997), or for Hessian methods on (U, V ) (Shpak, 1990), and is only a factor of k larger than the O(ndk) required just to compute the prediction UV \u2032."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 109875195,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "f24b9c6a00237048917b181c2990b157e67fc280",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A matrix decomposition method that generates a decomposition which is similar to that produced by the singular-value decomposition (SVD) is proposed. When applied to the design of 2-D FIR or IIR digital filters, the method provides for different error weightings at each point in the frequency plane, thereby allowing for smaller errors or don't care conditions in specified regions. When compared to the SVD, the proposed method can be better suited to the design of two-dimensional filters when using a sum-of-products realization. Examples for the design of 2-D FIR lowpass filters are given.<<ETX>>"
            },
            "slug": "A-weighted-least-squares-matrix-decomposition-with-Shpak",
            "title": {
                "fragments": [],
                "text": "A weighted-least-squares matrix decomposition method with application to the design of two-dimensional digital filters"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 33rd Midwest Symposium on Circuits and Systems"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145981906"
                        ],
                        "name": "P. Anandan",
                        "slug": "P.-Anandan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Anandan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Anandan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144611617"
                        ],
                        "name": "M. Irani",
                        "slug": "M.-Irani",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Irani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Irani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "When W is of rank one, such concurrent diagonalization is possible, allowing for the same structure as in the unweighted case, and in particular an eigenvectorbased solution ( Irani & Anandan, 2000 )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 175
                            }
                        ],
                        "text": "When W is of rank one, such concurrent diagonalization is possible, allowing for the same structure as in the unweighted case, and in particular an eigenvectorbased solution (Irani & Anandan, 2000)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5516270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e98a2879e34deb499a18f61f08f92309359f5675",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Factorization using Singular Value Decomposition (SVD) is often used for recovering 3D shape and motion from feature correspondences across multiple views. SVD is powerful at finding the global solution to the associated least-square-error minimization problem. However, this is the correct error to minimize only when the x and y positional errors in the features are uncorrelated and identically distributed. But this is rarely the case in real data. Uncertainty in feature position depends on the underlying spatial intensity structure in the image, which has strong directionality to it. Hence, the proper measure to minimize is covariance-weighted squared-error (or the Mahalanobis distance). In this paper, we describe a new approach to covariance-weighted factorization, which can factor noisy feature correspondences with high degree of directional uncertainty into structure and motion. Our approach is based on transforming the raw-data into a covariance-weighted data space, where the components of noise in the different directions are uncorrelated and identically distributed. Applying SVD to the transformed data now minimizes a meaningful objective function in this new data space. This is followed by a linear but suboptimal second step to recover the shape and motion in the original data space. We empirically show that our algorithm gives very good results for varying degrees of directional uncertainty. In particular, we show that unlike other SVD-based factorization algorithms, our method does not degrade with increase in directionality of uncertainty, even in the extreme when only normal-flow data is available. It thus provides a unified approach for treating corner-like points together with points along linear structures in the image."
            },
            "slug": "Factorization-with-Uncertainty-Anandan-Irani",
            "title": {
                "fragments": [],
                "text": "Factorization with Uncertainty"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new approach to covariance-weighted factorization, which can factor noisy feature correspondences with high degree of directional uncertainty into structure and motion and provides a unified approach for treating corner-like points together with points along linear structures in the image."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143707112"
                        ],
                        "name": "M. Collins",
                        "slug": "M.-Collins",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Collins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880237"
                        ],
                        "name": "S. Dasgupta",
                        "slug": "S.-Dasgupta",
                        "structuredName": {
                            "firstName": "Sanjoy",
                            "lastName": "Dasgupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dasgupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Such low-rank logistic models were suggested by Collins et al. (2002) and by Gordon"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2897627,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "476acfd526048c2825d69977700c99b08e10f232",
            "isKey": false,
            "numCitedBy": 476,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Principal component analysis (PCA) is a commonly applied technique for dimensionality reduction. PCA implicitly minimizes a squared loss function, which may be inappropriate for data that is not real-valued, such as binary-valued data. This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances, to give a generalization of PCA to loss functions that we argue are better suited to other data types. We describe algorithms for minimizing the loss functions, and give examples on simulated data."
            },
            "slug": "A-Generalization-of-Principal-Components-Analysis-Collins-Dasgupta",
            "title": {
                "fragments": [],
                "text": "A Generalization of Principal Components Analysis to the Exponential Family"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper draws on ideas from the Exponential family, Generalized linear models, and Bregman distances to give a generalization of PCA to loss functions that it is argued are better suited to other data types."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144344283"
                        ],
                        "name": "Ken Goldberg",
                        "slug": "Ken-Goldberg",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "Goldberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken Goldberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47012025"
                        ],
                        "name": "T. Roeder",
                        "slug": "T.-Roeder",
                        "structuredName": {
                            "firstName": "Theresa",
                            "lastName": "Roeder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Roeder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143701259"
                        ],
                        "name": "Dhruv Gupta",
                        "slug": "Dhruv-Gupta",
                        "structuredName": {
                            "firstName": "Dhruv",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dhruv Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144846471"
                        ],
                        "name": "Chris Perkins",
                        "slug": "Chris-Perkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Perkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Perkins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 43
                            }
                        ],
                        "text": "We analyzed a subset of the Jester data(5) (Goldberg et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 42
                            }
                        ],
                        "text": "We analyzed a subset of the Jester data5 (Goldberg et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 0
                            }
                        ],
                        "text": "Goldberg\net al. (2001) use a low-rank approximation of a fullyobserved subset of columns of the matrix, thus avoiding the need to introduce weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2851591,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6ca4c2912db0708f6df1ad1525746915d392a45",
            "isKey": true,
            "numCitedBy": 1574,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Eigentaste is a collaborative filtering algorithm that uses universal queries to elicit real-valued user ratings on a common set of items and applies principal component analysis (PCA) to the resulting dense subset of the ratings matrix. PCA facilitates dimensionality reduction for offline clustering of users and rapid computation of recommendations. For a database of n users, standard nearest-neighbor techniques require O(n) processing time to compute recommendations, whereas Eigentaste requires O(1) (constant) time. We compare Eigentaste to alternative algorithms using data from Jester, an online joke recommending system.Jester has collected approximately 2,500,000 ratings from 57,000 users. We use the Normalized Mean Absolute Error (NMAE) measure to compare performance of different algorithms. In the Appendix we use Uniform and Normal distribution models to derive analytic estimates of NMAE when predictions are random. On the Jester dataset, Eigentaste computes recommendations two orders of magnitude faster with no loss of accuracy. Jester is online at: http://eigentaste.berkeley.edu"
            },
            "slug": "Eigentaste:-A-Constant-Time-Collaborative-Filtering-Goldberg-Roeder",
            "title": {
                "fragments": [],
                "text": "Eigentaste: A Constant Time Collaborative Filtering Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work compares Eigentaste to alternative algorithms using data from Jester, an online joke recommending system, and uses the Normalized Mean Absolute Error (NMAE) measure to compare performance of different algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Information Retrieval"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34568108"
                        ],
                        "name": "A. Schein",
                        "slug": "A.-Schein",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Schein",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1796044"
                        ],
                        "name": "L. Saul",
                        "slug": "L.-Saul",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Saul",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Saul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1412391493"
                        ],
                        "name": "L. Ungar",
                        "slug": "L.-Ungar",
                        "structuredName": {
                            "firstName": "Lyle",
                            "lastName": "Ungar",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ungar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 115
                            }
                        ],
                        "text": "Such low-rank logistic models were suggested by Collins et al. (2002) and by Gordon\n(2003) and recently studied by Schein et al. (2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 333,
                                "start": 312
                            }
                        ],
                        "text": "Although the results indicate that the wlra method performs better than the logistic method, it is interesting to note that for small ranks, k = 2, 3, the training performance of the logistic model is better\u2014in these cases the logistic view allows us to better capture the rankings than a sumsquared-error view (Schein et al. (2003) investigates the training error of other data sets, and arrives at similar conclusions)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 188
                            }
                        ],
                        "text": "\u2026for small ranks, k = 2, 3, the training performance of the logistic model is better\u2014in these cases the logistic view allows us to better capture the rankings than a sumsquared-error view (Schein et al. (2003) investigates the training error of other data sets, and arrives at similar conclusions)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6807890,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "630c644c21429cc4450b4bee374f12dc21c13bd4",
            "isKey": true,
            "numCitedBy": 169,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate a generalized linear model for dimensionality reduction of binary data. The model is related to principal component analysis (PCA) in the same way that logistic regression is related to linear regression. Thus we refer to the model as logistic PCA. In this paper, we derive an alternating least squares method to estimate the basis vectors and generalized linear coefficients of the logistic PCA model. The resulting updates have a simple closed form and are guaranteed at each iteration to improve the model\u2019s likelihood. We evaluate the performance of logistic PCA\u2014as measured by reconstruction error rates\u2014on data sets drawn from four real world applications. In general, we find that logistic PCA is much better suited to modeling binary data than conventional PCA."
            },
            "slug": "A-Generalized-Linear-Model-for-Principal-Component-Schein-Saul",
            "title": {
                "fragments": [],
                "text": "A Generalized Linear Model for Principal Component Analysis of Binary Data"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "An alternating least squares method is derived to estimate the basis vectors and generalized linear coefficients of the logistic PCA model, a generalized linear model for dimensionality reduction of binary data that is related to principal component analysis (PCA) and is much better suited to modeling binary data than conventional PCA."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691741"
                        ],
                        "name": "Daniel Billsus",
                        "slug": "Daniel-Billsus",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Billsus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Billsus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694780"
                        ],
                        "name": "M. Pazzani",
                        "slug": "M.-Pazzani",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pazzani",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pazzani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Billsus and Pazzani (1998) use a singular value decomposition of a sparse binary observation matrix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Billsus and Pazzani (1998)  use a singular value decomposition of a sparse binary observation matrix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3036907,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efbeedfbf13db70878618553f0c4a0fec6f493fe",
            "isKey": false,
            "numCitedBy": 1231,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Predicting items a user would like on the basis of other users\u2019 ratings for these items has become a well-established strategy adopted by many recommendation services on the Internet. Although this can be seen as a classification problem, algorithms proposed thus far do not draw on results from the machine learning literature. We propose a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm. We identify the shortcomings of current collaborative filtering techniques and propose the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches. Our best-performing algorithm is based on the singular value decomposition of an initial matrix of user ratings, exploiting latent structure that essentially eliminates the need for users to rate common items in order to become predictors for one another's preferences. We evaluate the proposed algorithm on a large database of user ratings for motion pictures and find that our approach significantly outperforms current collaborative filtering algorithms."
            },
            "slug": "Learning-Collaborative-Information-Filters-Billsus-Pazzani",
            "title": {
                "fragments": [],
                "text": "Learning Collaborative Information Filters"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a representation for collaborative filtering tasks that allows the application of virtually any machine learning algorithm, and identifies the shortcomings of current collaborative filtering techniques and proposes the use of learning algorithms paired with feature extraction techniques that specifically address the limitations of previous approaches."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684176"
                        ],
                        "name": "Y. Azar",
                        "slug": "Y.-Azar",
                        "structuredName": {
                            "firstName": "Yossi",
                            "lastName": "Azar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Azar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742404"
                        ],
                        "name": "A. Fiat",
                        "slug": "A.-Fiat",
                        "structuredName": {
                            "firstName": "Amos",
                            "lastName": "Fiat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fiat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693265"
                        ],
                        "name": "Anna R. Karlin",
                        "slug": "Anna-R.-Karlin",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Karlin",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna R. Karlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3137076"
                        ],
                        "name": "Frank McSherry",
                        "slug": "Frank-McSherry",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "McSherry",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank McSherry"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748846"
                        ],
                        "name": "J. Saia",
                        "slug": "J.-Saia",
                        "structuredName": {
                            "firstName": "Jared",
                            "lastName": "Saia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Saia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 139
                            }
                        ],
                        "text": "The matrix was extended to the remaining jokes by projecting each joke column onto the column subspace of this matrix. rescaling Following Azar et al. (2001), the ratings for each joke were scaled inversely proportional to the frequency with which the joke was rated (between 0.197 and 0.77)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Azar et al. (2001) proved asymptotic consistency of a method in which unobserved entries are replaced by zeros, and observed entries are scaled inversely proportionally to the probability of them being observed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 20
                            }
                        ],
                        "text": "rescaling Following Azar et al. (2001), the ratings for each joke were scaled inversely proportional to the frequency with which the joke was rated (between 0."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 0
                            }
                        ],
                        "text": "Billsus and Pazzani (1998) use a singular value decomposition of a sparse binary observation matrix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9990004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "620cf63f101f20c03a2e530c287c2603839de15e",
            "isKey": true,
            "numCitedBy": 303,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Experimental evidence suggests that spectral techniques are valuable for a wide range of applications. A partial list of such applications include (i) semantic analysis of documents used to cluster documents into areas of interest, (ii) collaborative filtering --- the reconstruction of missing data items, and (iii) determining the relative importance of documents based on citation/link structure. Intuitive arguments can explain some of the phenomena that has been observed but little theoretical study has been done. In this paper we present a model for framing data mining tasks and a unified approach to solving the resulting data mining problems using spectral analysis. These results give strong justification to the use of spectral techniques for latent semantic indexing, collaborative filtering, and web site ranking."
            },
            "slug": "Spectral-analysis-of-data-Azar-Fiat",
            "title": {
                "fragments": [],
                "text": "Spectral analysis of data"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A model for framing data mining tasks and a unified approach to solving the resulting data mining problems using spectral analysis are presented, which give strong justification to the use of spectral techniques for latent semantic indexing, collaborative filtering, and web site ranking."
            },
            "venue": {
                "fragments": [],
                "text": "STOC '01"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763295"
                        ],
                        "name": "J. Tenenbaum",
                        "slug": "J.-Tenenbaum",
                        "structuredName": {
                            "firstName": "Joshua",
                            "lastName": "Tenenbaum",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tenenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768236"
                        ],
                        "name": "W. Freeman",
                        "slug": "W.-Freeman",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Freeman",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Freeman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 88
                            }
                        ],
                        "text": "The standard unweighted low-rank approximation (e.g., for separating style and content (Tenenbaum & Freeman, 2000)) would in this context assume that the number of samples is uniform across the entries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9492646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e85f7d59e37972ec52cbabfef0512588d87f125",
            "isKey": false,
            "numCitedBy": 872,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "Perceptual systems routinely separate content from style, classifying familiar words spoken in an unfamiliar accent, identifying a font or handwriting style across letters, or recognizing a familiar face or object seen under unfamiliar viewing conditions. Yet a general and tractable computational model of this ability to untangle the underlying factors of perceptual observations remains elusive (Hofstadter, 1985). Existing factor models (Mardia, Kent, & Bibby, 1979; Hinton & Zemel, 1994; Ghahramani, 1995; Bell & Sejnowski, 1995; Hinton, Dayan, Frey, & Neal, 1995; Dayan, Hinton, Neal, & Zemel, 1995; Hinton & Ghahramani, 1997) are either insufficiently rich to capture the complex interactions of perceptually meaningful factors such as phoneme and speaker accent or letter and font, or do not allow efficient learning algorithms. We present a general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization. We report promising results on three different tasks in three different perceptual domains: spoken vowel classification with a benchmark multi-speaker database, extrapolation of fonts to unseen letters, and translation of faces to novel illuminants."
            },
            "slug": "Separating-Style-and-Content-with-Bilinear-Models-Tenenbaum-Freeman",
            "title": {
                "fragments": [],
                "text": "Separating Style and Content with Bilinear Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A general framework for learning to solve two-factor tasks using bilinear models, which provide sufficiently expressive representations of factor interactions but can nonetheless be fit to data using efficient algorithms based on the singular value decomposition and expectation-maximization are presented."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "86970661"
                        ],
                        "name": "G. Young",
                        "slug": "G.-Young",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Young",
                            "middleNames": [
                                "Marion"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Young"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 128
                            }
                        ],
                        "text": "While the extension to the weighted-norm case is conceptually straightforward, and dates back to early work on factor analysis (Young, 1940), standard algorithms (such as SVD) for solving the unweighted case do not carry over to the weighted case."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 120941345,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4d73b11a0d141c616441ad5280e782578e8d3d9e",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Fisher's method of maximum likelihood is applied to the problem of estimation in factor analysis, as initiated by Lawley, and found to lead to a generalization of the Eckart matrix approximation problem. The solution of this in a special case is applied to show how test fallability enters into factor determination, it being noted that the method of communalities underestimates the number of factors."
            },
            "slug": "Maximum-likelihood-estimation-and-factor-analysis-Young",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood estimation and factor analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1941
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113861474"
                        ],
                        "name": "C. Bishop",
                        "slug": "C.-Bishop",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bishop"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 195943514,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "01fe8b76ba467c0c6c97eaaf84486ee4c6f0a52b",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Proceedings-of-the-Ninth-International-Workshop-on-Bishop",
            "title": {
                "fragments": [],
                "text": "Proceedings of the Ninth International Workshop on Artificial Intelligence and Statistics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786843"
                        ],
                        "name": "A. Globerson",
                        "slug": "A.-Globerson",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Globerson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Globerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777660"
                        ],
                        "name": "Naftali Tishby",
                        "slug": "Naftali-Tishby",
                        "structuredName": {
                            "firstName": "Naftali",
                            "lastName": "Tishby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Naftali Tishby"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36115309,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6d860b9896570146a88c7fe3c9e84def68a09ee",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sufficient-Dimensionality-Reduction-A-novel-Method-Globerson-Tishby",
            "title": {
                "fragments": [],
                "text": "Sufficient Dimensionality Reduction - A novel Analysis Method"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 177
                            }
                        ],
                        "text": "We are also studying further loss functions, such as those in which the noise model is viewed as a nuisance parameter, and those arising in Sufficient Dimensionality Reductions [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] Amir Globerson and Naftali Tishby."
                    },
                    "intents": []
                }
            ],
            "corpusId": 236510237,
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sufficient dimensionality reduction - a novel analysis principle"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 9
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 15,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Weighted-Low-Rank-Approximations-Srebro-Jaakkola/755e566d3f10d057bc9e4908e4016ae6f7ca0753?sort=total-citations"
}