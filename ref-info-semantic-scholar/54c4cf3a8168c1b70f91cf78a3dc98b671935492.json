{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055155244"
                        ],
                        "name": "David Chapman",
                        "slug": "David-Chapman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chapman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chapman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7213327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45d2bdf5e7072c1d5a91a38efa365715def2f45d",
            "isKey": false,
            "numCitedBy": 291,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Delayed reinforcement learning is an attractive framework for the unsupervised learning of action policies for autonomous agents. Some existing delayed reinforcement learning techniques have shown promise in simple domains. However, a number of hurdles must be passed before they are applicable to realistic problems. This paper describes one such difficulty, the input generalization problem (whereby the system must generalize to produce similar actions in similar situations) and an implemented solution, the G algorithm. This algorithm is based on recursive splitting of the state space based on statistical measures of differences in reinforcements received. Connectionist backpropagation has previously been used for input generalization in reinforcement learning. We compare the two techniques analytically and empirically. The G algorithm's sound statistical basis makes it easy to predict when it should and should not work, whereas the behavior of back-propagation is unpredictable. We found that a previous successful use of backpropagation can be explained by the linearity of the application domain. We found that in another domain, G reliably found the optimal policy, whereas none of a set of runs of backpropagation with many combinations of parameters did."
            },
            "slug": "Input-Generalization-in-Delayed-Reinforcement-An-Chapman-Kaelbling",
            "title": {
                "fragments": [],
                "text": "Input Generalization in Delayed Reinforcement Learning: An Algorithm and Performance Comparisons"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper describes the input generalization problem (whereby the system must generalize to produce similar actions in similar situations) and an implemented solution, the G algorithm, which is based on recursive splitting of the state space based on statistical measures of differences in reinforcements received."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709512"
                        ],
                        "name": "L. Kaelbling",
                        "slug": "L.-Kaelbling",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Kaelbling",
                            "middleNames": [
                                "Pack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Kaelbling"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41302553,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "168bd65ae047afecf3eea54786d8e6becba932f7",
            "isKey": false,
            "numCitedBy": 795,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation addresses the problem of designing algorithms for learning in embedded systems. This problem differs from the traditional supervised learning problem. An agent, finding itself in a particular input situation must generate an action. It then receives a reinforcement value from the environment, indicating how valuable the current state of the environment is for the agent. The agent cannot, however, deduce the reinforcement value that would have resulted from executing any of its other actions. A number of algorithms for learning action strategies from reinforcement values are presented and compared empirically with existing reinforcement-learning algorithms. \nThe interval-estimation algorithm uses the statistical notion of confidence intervals to guide its generation of actions in the world, trading off acting to gain information against acting to gain reinforcement. It performs well in simple domains but does not exhibit any generalization and is computationally complex. \nThe cascade algorithm is a structural credit-assignment method that allows an action strategy with many output bits to be learned by a collection of reinforcement-learning modules that learn Boolean functions. This method represents an improvement in computational complexity and often in learning rate. \nTwo algorithms for learning Boolean functions in k-DNF are described. Both are based on Valiant's algorithm for learning such functions from input-output instances. The first uses Sutton's techniques for linear association and reinforcement comparison, while the second uses techniques from the interval estimation algorithm. They both perform well and have tractable complexity. \nA generate-and-test reinforcement-learning algorithm is presented. It allows symbolic representations of Boolean functions to be constructed incrementally and tested in the environment. It is highly parametrized and can be tuned to learn a broad range of function classes. Low-complexity functions can be learned very efficiently even in the presence of large numbers of irrelevant input bits. This algorithm is extended to construct simple sequential networks using a set-reset operator, which allows the agent to learn action strategies with state. \nThese algorithms, in addition to being studied in simulation, were implemented and tested on a physical mobile robot."
            },
            "slug": "Learning-in-embedded-systems-Kaelbling",
            "title": {
                "fragments": [],
                "text": "Learning in embedded systems"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This dissertation addresses the problem of designing algorithms for learning in embedded systems using Sutton's techniques for linear association and reinforcement comparison, while the interval estimation algorithm uses the statistical notion of confidence intervals to guide its generation of actions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39263338"
                        ],
                        "name": "Longxin Lin",
                        "slug": "Longxin-Lin",
                        "structuredName": {
                            "firstName": "Longxin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longxin Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 32035341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86ef873884ecc6361b8616aa1ac5695d89167f5e",
            "isKey": false,
            "numCitedBy": 199,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Programming robots is a tedious task. So, there is growing interest in building robots which can learn by themselves. Self-improving, which involves trial and error, however, is often a slow process and could be hazardous in a hostile environment. By teaching robots how tasks can be achieved, learning time can be shortened and hazard can be minimized. This paper presents a general approach to making robots which can improve their performance from experiences as well as from being taught. Based on this proposed approach and other learning speedup techniques, a simulated learning robot was developed and could learn three moderately complex behaviors, which were then integrated in a subsumption style so that the robot could navigate and recharge itself. Interestingly, a real robot could actually use what was learned in the simulator to operate in the real world quite successfully."
            },
            "slug": "Programming-Robots-Using-Reinforcement-Learning-and-Lin",
            "title": {
                "fragments": [],
                "text": "Programming Robots Using Reinforcement Learning and Teaching"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents a general approach to making robots which can improve their performance from experiences as well as from being taught, and develops a simulated learning robot which could learn three moderately complex behaviors and use what was learned in the simulator to operate in the real world quite successfully."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39263338"
                        ],
                        "name": "Longxin Lin",
                        "slug": "Longxin-Lin",
                        "structuredName": {
                            "firstName": "Longxin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longxin Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18783919,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2eb733470921af04df5c611a6a3c76c281ce498",
            "isKey": false,
            "numCitedBy": 140,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning is a type of unsupervised learning for sequential decision making. Q-learning is probably the best-understood reinforcement learning algorithm. In Q-learning, the agent learns a mapping from states and actions to their utilities. An important assumption of Q-learning is the Markovian environment assumption, meaning that any information needed to determine the optimal actions is reflected in the agent''s state representation. Consider an agent whose state representation is based solely on its immediate perceptual sensations. When its sensors are not able to make essential distinctions among world states, the Markov assumption is violated, causing a problem called perceptual aliasing. For example, when facing a closed box, an agent based on its current visual sensation cannot act optimally if the optimal action depends on the contents of the box. There are two basic approaches to addressing this problem -- using more sensors or using history to figure out the current world state. This paper studies three connectionist approaches which learn to use history to handle perceptual aliasing: the window-Q, recurrent-Q, and recurrent-model architectures. Empirical study of these architectures is presented. Their relative strengths and weaknesses are also discussed."
            },
            "slug": "Memory-Approaches-to-Reinforcement-Learning-in-Lin-Mitchell",
            "title": {
                "fragments": [],
                "text": "Memory Approaches to Reinforcement Learning in Non-Markovian Domains"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper studies three connectionist approaches which learn to use history to handle perceptual aliasing: the window-Q, recurrent- Q, and recurrent-model architectures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2351863,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e42cf7b98c0ea5294a3bca10acaa95d0714d44f3",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "How can artificial neural nets generalize better from fewer examples? In order to generalize successfully, neural network learning methods typically require large training data sets. We introduce a neural network learning method that generalizes rationally from many fewer data points, relying instead on prior knowledge encoded in previously learned neural networks. For example, in robot control learning tasks reported here, previously learned networks that model the effects of robot actions are used to guide subsequent learning of robot control functions. For each observed training example of the target function (e.g. the robot control policy), the learner explains the observed example in terms of its prior knowledge, then analyzes this explanation to infer additional information about the shape, or slope, of the target function. This shape knowledge is used to bias generalization when learning the target function. Results are presented applying this approach to a simulated robot task based on reinforcement learning."
            },
            "slug": "Explanation-Based-Neural-Network-Learning-for-Robot-Mitchell-Thrun",
            "title": {
                "fragments": [],
                "text": "Explanation-Based Neural Network Learning for Robot Control"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A neural network learning method that generalizes rationally from many fewer data points, relying instead on prior knowledge encoded in previously learned neural networks that is used to bias generalization when learning the target function."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60851166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "874b3a63422eeaf24c14435ee6091ed48247bff3",
            "isKey": false,
            "numCitedBy": 427,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation is about the application of machine learning to robot control. A system which has no initial model of the robot/world dynamics should be able to construct such a model using data received through its sensors|an approach which is formalized here as the SAB (State-ActionBehaviour) control cycle. A method of learning is presented in which all the experiences in the lifetime of the robot are explicitly remembered. The experiences are stored in a manner which permits fast recall of the closest previous experience to any new situation, thus permitting very quick predictions of the e ects of proposed actions and, given a goal behaviour, permitting fast generation of a candidate action. The learning can take place in high-dimensional non-linear control spaces with real-valued ranges of variables. Furthermore, the method avoids a number of shortcomings of earlier learning methods in which the controller can become trapped in inadequate performance which does not improve. Also considered is how the system is made resistant to noisy inputs and how it adapts to environmental changes. A well founded mechanism for choosing actions is introduced which solves the experiment/perform dilemma for this domain with adequate computational e ciency, and with fast convergence to the goal behaviour. The dissertation explains in detail how the SAB control cycle can be integrated into both low and high complexity tasks. The methods and algorithms are evaluated with numerous experiments using both real and simulated robot domains. The nal experiment also illustrates how a compound learning task can be structured into a hierarchy of simple learning tasks."
            },
            "slug": "Efficient-memory-based-learning-for-robot-control-Moore",
            "title": {
                "fragments": [],
                "text": "Efficient memory-based learning for robot control"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A method of learning is presented in which all the experiences in the lifetime of the robot are explicitly remembered, thus permitting very quick predictions of the e ects of proposed actions and, given a goal behaviour, permitting fast generation of a candidate action."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39263338"
                        ],
                        "name": "Longxin Lin",
                        "slug": "Longxin-Lin",
                        "structuredName": {
                            "firstName": "Longxin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longxin Lin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 54152081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8bab102150805b0181830d4fc03f28b45ba18bb",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The purpose of this work is to investigate and evaluate different reinforcement learning frameworks using connectionist networks. I study four frameworks, which are adopted from the ideas developed by Rich Sutton and his colleagues. The four frameworks are based on two learning procedures: the Temporal Difference methods for solving the credit assignment problem, and the backpropagation algorithm for developing appropriate internal representations. Two of them also involve learning a world model and using it to speed learning. To evaluate their performance, I design a dynamic environment and implement different learning agents, using the different frameworks, to survive in it. The environment is nontrivial and nondeterministic. Surprisingly, all of the agents can learn to survive fairly well in a reasonable time frame. This paper describes the learning agents and their performance, and summarizes the learning algorithms and the lessons I learned from this study. This research was supported by NASA under Contract NAGW-1175. The views and conclusions contained in this document are those of the author and should not be interpreted as representing the official policies, either expressed or implied, of NASA."
            },
            "slug": "Self-improving-reactive-agents:-case-studies-of-Lin",
            "title": {
                "fragments": [],
                "text": "Self-improving reactive agents: case studies of reinforcement learning frameworks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This paper describes the learning agents and their performance, and summarizes the learning algorithms and the lessons I learned from this study."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5972929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5cc2b2829a1fcf0260a1405f5f931efb21ffd5a",
            "isKey": false,
            "numCitedBy": 136,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning (RL) algorithms have traditionally been thought of as trial and error learning methods that use actual control experience to incrementally improve a control policy. Sutton's DYNA architecture demonstrated that RL algorithms can work as well using simulated experience from an environment model, and that the resulting computation was similar to doing one-step lookahead planning. Inspired by the literature on hierarchical planning, I propose learning a hierarchy of models of the environment that abstract temporal detail as a means of improving the scalability of RL algorithms. I present H-DYNA (Hierarchical DYNA), an extension to Sutton's DYNA architecture that is able to learn such a hierarchy of abstract models. H-DYNA differs from hierarchical planners in two ways: first, the abstract models are learned using experience gained while learning to solve other tasks in the same environment, and second, the abstract models can be used to solve stochastic control tasks. Simulations on a set of compositionally-structured navigation tasks show that H-DYNA can learn to solve them faster than conventional RL algorithms. The abstract models also serve as mechanisms for achieving transfer of learning across multiple tasks."
            },
            "slug": "Reinforcement-Learning-with-a-Hierarchy-of-Abstract-Singh",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning with a Hierarchy of Abstract Models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Simulations on a set of compositionally-structured navigation tasks show that H-DYNA can learn to solve them faster than conventional RL algorithms, and the abstract models can be used to solve stochastic control tasks."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144745483"
                        ],
                        "name": "M. Tan",
                        "slug": "M.-Tan",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33830667,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "044d1758742d5250a871f4d78f3d9eb1128a6f51",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-a-Cost-Sensitive-Internal-Representation-Tan",
            "title": {
                "fragments": [],
                "text": "Learning a Cost-Sensitive Internal Representation for Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1850503"
                        ],
                        "name": "S. Mahadevan",
                        "slug": "S.-Mahadevan",
                        "structuredName": {
                            "firstName": "Sridhar",
                            "lastName": "Mahadevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mahadevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706614"
                        ],
                        "name": "J. Connell",
                        "slug": "J.-Connell",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Connell",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Connell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37804840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "48c154af7f4cec7bc8b106cc0b1d5b44d8e0afe0",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Scaling-Reinforcement-Learning-to-Robotics-by-the-Mahadevan-Connell",
            "title": {
                "fragments": [],
                "text": "Scaling Reinforcement Learning to Robotics by Exploiting the Subsumption Architecture"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699868"
                        ],
                        "name": "Satinder Singh",
                        "slug": "Satinder-Singh",
                        "structuredName": {
                            "firstName": "Satinder",
                            "lastName": "Singh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Satinder Singh"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14921614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66e3294feae7f0dba35c94a7214fc5cba41e9def",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Scaling-Reinforcement-Learning-Algorithms-by-Models-Singh",
            "title": {
                "fragments": [],
                "text": "Scaling Reinforcement Learning Algorithms by Learning Variable Temporal Resolution Models"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 895695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d33f8ba2a2e0456ef282286eed74025be0bb1af",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This work addresses three problems with reinforcement learning and adaptive neuro-control: 1. Non-Markovian interfaces between learner and environment. 2. On-line learning based on system realization. 3. Vector-valued adaptive critics. An algorithm is described which is based on system realization and on two interacting fully recurrent continually running networks which may learn in parallel. Problems with parallel learning are attacked by 'adaptive randomness'. It is also described how interacting model/controller systems can be combined with vector-valued 'adaptive critics' (previous critics have been scalar)."
            },
            "slug": "Reinforcement-Learning-in-Markovian-and-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning in Markovian and Non-Markovian Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work addresses three problems with reinforcement learning and adaptive neuro-control: non-Markovian interfaces between learner and environment, problems with parallel learning and how interacting model/controller systems can be combined with vector-valued 'adaptive critics'."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 110954972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c67f0f00eaab360db1b9bd377e783c27c922dc86",
            "isKey": false,
            "numCitedBy": 597,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nVision based mobile robot guidance has proven difficult for classical machine vision methods because of the diversity and real time constraints inherent in the task. This book describes a connectionist system called ALVINN (Autonomous Land Vehicle In a Neural Network) that overcomes these difficulties. ALVINN learns to guide mobile robots using the back-propagation training algorithm. Because of its ability to learn from example, ALVINN can adapt to new situations and therefore cope with the diversity of the autonomous navigation task. But real world problems like vision based mobile robot guidance present a different set of challenges for the connectionist paradigm. Among them are: how to develop a general representation from a limited amount of real training data, how to understand the internal representations developed by artificial neural networks, how to estimate the reliability of individual networks, how to combine multiple networks trained for different situations into a single system, and how to combine connectionist perception with symbolic reasoning. Neural Network Perception for Mobile Robot Guidance presents novel solutions to each of these problems. Using these techniques, the ALVINN system can learn to control an autonomous van in under 5 minutes by watching a person drive. Once trained, individual ALVINN networks can drive in a variety of circumstances, including single-lane paved and unpaved roads, and multi-lane lined and unlined roads, at speeds of up to 55 miles per hour. The techniques also are shown to generalize to the task of controlling the precise foot placement of a walking robot."
            },
            "slug": "Neural-Network-Perception-for-Mobile-Robot-Guidance-Pomerleau",
            "title": {
                "fragments": [],
                "text": "Neural Network Perception for Mobile Robot Guidance"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This book describes a connectionist system called ALVINN (Autonomous Land Vehicle In a Neural Network) that overcomes difficulties and can learn to control an autonomous van in under 5 minutes by watching a person drive."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143753639"
                        ],
                        "name": "A. McCallum",
                        "slug": "A.-McCallum",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "McCallum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. McCallum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46054468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf2f6c4bf36958a1e3fd3d7b814ee2af6701603c",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Using-Transitional-Proximity-for-Faster-Learning-McCallum",
            "title": {
                "fragments": [],
                "text": "Using Transitional Proximity for Faster Reinforcement Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 106845402,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dcbef60688ca014a63e72e844fb997e738f6fde7",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Exploration plays a fundamental role in any active learning system. This study evaluates the role of exploration in active learning and describes several local techniques for exploration in finite, discrete domains, embedded in a reinforcement learning framework (delayed reinforcement). This paper distinguishes between two families of exploration schemes: undirected and directed exploration. While the former family is closely related to random walk exploration, directed exploration techniques memorize exploration-specific knowledge which is used for guiding the exploration search. In many finite deterministic domains, any learning technique based on undirected exploration is inefficient in terms of learning time, i.e., learning time is expected to scale exponentially with the size of the state space. We prove that for all these domains, reinforcement learning using a directed technique can always be performed in polynomial time, demonstrating the important role of exploration in reinforcement learning. (The proof is given for one specific directed exploration technique named counter-based exploration.) Subsequently, several exploration techniques found in recent reinforcement learning and connectionist adaptive control literature are described. In order to trade off efficiently between exploration and exploitation --- a trade-off which characterizes many real-world active learning tasks --- combination methods are described which explore and avoid costs simultaneously. This includes a selective attention mechanism, which allows smooth switching between exploration and exploitation. All techniques are evaluated and compared on a discrete reinforcement learning task (robot navigation). The empirical evaluation is followed by an extensive discussion of benefits and limitations of this work."
            },
            "slug": "Efficient-Exploration-In-Reinforcement-Learning-Thrun",
            "title": {
                "fragments": [],
                "text": "Efficient Exploration In Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is proved that for all finite deterministic domains, reinforcement learning using a directed technique can always be performed in polynomial time, demonstrating the important role of exploration in reinforcement learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7962049,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5f8a0858fb82ce0e50b55446577a70e40137aaf",
            "isKey": false,
            "numCitedBy": 1556,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Integrated-Architectures-for-Learning,-Planning,-on-Sutton",
            "title": {
                "fragments": [],
                "text": "Integrated Architectures for Learning, Planning, and Reacting Based on Approximating Dynamic Programming"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153570946"
                        ],
                        "name": "M. Dorigo",
                        "slug": "M.-Dorigo",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Dorigo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dorigo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075413"
                        ],
                        "name": "U. Schnepf",
                        "slug": "U.-Schnepf",
                        "structuredName": {
                            "firstName": "Uwe",
                            "lastName": "Schnepf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Schnepf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6687260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bac1bfe4408bde30641ead75db6e914db072b27",
            "isKey": false,
            "numCitedBy": 238,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Intelligent robots should be able to use sensor information to learn how to behave in a changing environment. As environmental complexity grows, the learning task becomes more and more difficult. This problem is faced using an architecture based on learning classifier systems and on the structural properties of animal behavioral organization, as proposed by ethologists. After a description of the learning technique used and of the organizational structure proposed, experiments that show how behavior acquisition can be achieved are presented. The simulated robot learns to follow a light and to avoid hot dangerous objects. While these two simple behavioral patterns are independently learned, coordination is attained by means of a learning coordination mechanism. >"
            },
            "slug": "Genetics-based-machine-learning-and-behavior-based-Dorigo-Schnepf",
            "title": {
                "fragments": [],
                "text": "Genetics-based machine learning and behavior-based robotics: a new synthesis"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experiments show how behavior acquisition can be achieved by means of a learning coordination mechanism using an architecture based on learning classifier systems and on the structural properties of animal behavioral organization as proposed by ethologists."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3256676"
                        ],
                        "name": "A. Christiansen",
                        "slug": "A.-Christiansen",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Christiansen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Christiansen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57393057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33f2a576e2eb0ca9bc2f72557131f85b69b28783",
            "isKey": false,
            "numCitedBy": 35,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A task theory is a collection of models that a robot uses to predict the effects of its actions. This thesis addresses how a robot can build a task theory from observed action effects, thus bypassing the usual human-programming method of defining \"what actions do\", and making robots considerably easier to train. The question that this thesis addresses is: \"What are sufficient conditions whereby a robotic manipulation system can learn action models that allow automated planning and successful execution of manipulation strategies?\" This thesis, by presenting an implemented learning robot system, shows that acquiring action models can be automated, and further shows that the required software mechanisms are simple and few. \nThe important characteristics of the learning agent and its environment are: (1) the complexity of the task, (2) the sensory and effectory abilities of the agent, (3) the learning mechanism, (4) the planning mechanism, (5) the experimentation method, and (6) the prior knowledge of the task possessed by the agent. Sufficient conditions for success are demonstrated with empirical results in three manipulation tasks: parallel-jaw grasping, the peg-in-hole problem, and the tray-tilting problem. The successful learning robots for these tasks generate task theories consisting of a set of funnels. Each funnel is a kind of operator that maps a region of the task state-action space to a region of the state space. Funnels can be acquired for continuous tasks using very little prior knowledge of the task, but the learning mechanism must be robust in the face of noise and non-determinism. A simple search-based planner suffices for generating manipulation plans, although this planner must reason about the reliabilities associated with each funnel. \nAfter demonstrating sufficient conditions for successful acquisition of task theories, the thesis considers the necessity of the characteristics of the implemented learning robots. In addition to sensing, effecting, planning, and learning, the thesis develops requirements on the prior knowledge that must be possessed by the learning robot. Task-specific prior knowledge can be very weak, but for some complex tasks, prior knowledge is required to achieve acceptable convergence times for the learned task theory."
            },
            "slug": "Automatic-acquisition-of-task-theories-for-robotic-Christiansen",
            "title": {
                "fragments": [],
                "text": "Automatic acquisition of task theories for robotic manipulation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This thesis, by presenting an implemented learning robot system, shows that acquiring action models can be automated, and further shows that the required software mechanisms are simple and few."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2624386"
                        ],
                        "name": "L. Chrisman",
                        "slug": "L.-Chrisman",
                        "structuredName": {
                            "firstName": "Lonnie",
                            "lastName": "Chrisman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Chrisman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1963904,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a1b055577a86141df13f13a3203c76a32bffdc3a",
            "isKey": false,
            "numCitedBy": 395,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "It is known that Perceptual Aliasing may significantly diminish the effectiveness of reinforcement learning algorithms [Whitehead and Ballard, 1991]. Perceptual aliasing occurs when multiple situations that are indistinguishable from immediate perceptual input require different responses from the system. For example, if a robot can only see forward, yet the presence of a battery charger behind it determines whether or not it should backup, immediate perception alone is insufficient for determining the most appropriate action. It is problematic since reinforcement algorithms typically learn a control policy from immediate perceptual input to the optimal choice of action. \n \nThis paper introduces the predictive distinctions approach to compensate for perceptual aliasing caused from incomplete perception of the world. An additional component, a predictive model, is utilized to track aspects of the world that may not be visible at all times. In addition to the control policy, the model must also be learned, and to allow for stochastic actions and noisy perception, a probabilistic model is learned from experience. In the process, the system must discover, on its own, the important distinctions in the world. Experimental results are given for a simple simulated domain, and additional issues are discussed."
            },
            "slug": "Reinforcement-Learning-with-Perceptual-Aliasing:-Chrisman",
            "title": {
                "fragments": [],
                "text": "Reinforcement Learning with Perceptual Aliasing: The Perceptual Distinctions Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The predictive distinctions approach to compensate for perceptual aliasing caused from incomplete perception of the world is introduced and Experimental results are given for a simple simulated domain, and additional issues are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2099637863"
                        ],
                        "name": "Mozer",
                        "slug": "Mozer",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Mozer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mozer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60335203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d9d72ee4dec47552bc609a33f958ec38e2cc9ba",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Expert systems seem to be quite the rage in artificial intelligence, but getting expert knowledge into these systems is a difficult problem. One solution would be to endow the systems with powerful learning procedures which could discover appropriate behaviors by observing an expert in action. A promising source of such learning procedures can be found in recent work on connectionist networks, that is, massively parallel networks of simple processing elements. This paper discusses a connectionist expert system that learns to play a simple video game by observing a human player. The game, Robots, is played on a two-dimensional board containing the player and a number of computer-controlled robots. The object of the game is for the player to move around the board in a manner that will force all of the robots to collide with one another before any robot is able to catch the player. The connectionist system learns to associate observed situations on the board with observed moves. It is capable not only of the human player, but of learning generalizations that apply to novel situations. Keywords: Parallel distributed processing."
            },
            "slug": "RAMBOT-(Restructuring-Associative-Memory-Based-on-a-Mozer",
            "title": {
                "fragments": [],
                "text": "RAMBOT (Restructuring Associative Memory Based on Training): a connectionist expert system that learns by example. Technical report, October 1985-April 1986"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A connectionist expert system that learns to play a simple video game by observing a human player and is capable not only of the human player, but of learning generalizations that apply to novel situations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720087"
                        ],
                        "name": "S. Whitehead",
                        "slug": "S.-Whitehead",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Whitehead",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Whitehead"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 44974425,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58435c05d20a0ba3ea1d342b03e9d9dd2852835d",
            "isKey": false,
            "numCitedBy": 90,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Complexity-and-Cooperation-in-Q-Learning-Whitehead",
            "title": {
                "fragments": [],
                "text": "Complexity and Cooperation in Q-Learning"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701876"
                        ],
                        "name": "P. Maes",
                        "slug": "P.-Maes",
                        "structuredName": {
                            "firstName": "Pattie",
                            "lastName": "Maes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Maes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72419159"
                        ],
                        "name": "R. Brooks",
                        "slug": "R.-Brooks",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Brooks",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Brooks"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1566702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "269047d9b8ea3594c665399e4d029b1990307ed4",
            "isKey": false,
            "numCitedBy": 480,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe an algorithm which allows a behavior-based robot to learn on the basis of positive and negative feedback when to activate its behaviors. In accordance with the philosophy of behavior-based robots, the algorithm is completely distributed: each of the behaviors independently tries to sensors find out (i) whether it is relevant (i.e. whether it is at all correlated to positive feedback) and (ii) what the conditions are under which it becomes reliable (i.e. the conditions under which it maximises the probability of receiving positive feedback and minimises the probability of receiving negative feedback). The algorithm has been tested successfully on an autonomous 6-legged robot which had to learn how to coordinate its legs so as to walk forward."
            },
            "slug": "Learning-to-Coordinate-Behaviors-Maes-Brooks",
            "title": {
                "fragments": [],
                "text": "Learning to Coordinate Behaviors"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "An algorithm which allows a behavior-based robot to learn on the basis of positive and negative feedback when to activate its behaviors has been described and tested successfully on an autonomous 6-legged robot."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53765548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "547b014cb7ed593e661b9c2847dc305bde77833b",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of locally weighted regression in memory-based robot learning is explored. A local model is formed to answer each query, using a weighted regression in which close points (similar experiences) are weighted more than distant points (less relevant experiences). This approach implements a philosophy of modeling a complex function with many simple local models. The author explains how an appropriate distance metric or measure of similarity can be found, and how the distance metric is used. How irrelevant input variables and terms in the local model are detected is also explained. An example from the control of a robot arm is used to compare this approach with other robot control and learning techniques.<<ETX>>"
            },
            "slug": "Using-locally-weighted-regression-for-robot-Atkeson",
            "title": {
                "fragments": [],
                "text": "Using locally weighted regression for robot learning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The author explains how an appropriate distance metric or measure of similarity can be found, and how the distance metric is used in the use of locally weighted regression in memory-based robot learning."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1991 IEEE International Conference on Robotics and Automation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145290695"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Watkins",
                            "middleNames": [
                                "J.",
                                "C.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 57372463,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "954c5ffcdd876e6646e78c89be4ae5e4113a8728",
            "isKey": false,
            "numCitedBy": 270,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "IN THIS REPORT WE SHOW HOW THE CLASS OF ADAPTIVE PREDICTION METHODS THAT SUTTON CALLED \"TEMPORAL DIFFERENCE\", OR TD, METHODS ARE RELATED TO THE THE- ORY OF SEQUENTIAL DECISION MAKING. TD METHODS HAVE BEEN USED AS \"ADAPTIVE CRITICS\" IN CONNECTIONIST LEARNING SYSTEMS,AND HAVE BEEN PROPOSED AS MODELS OF ANIMAL LEARNING IN CLASSICAL CONDITIONING EXPERIMENTS. HERE WE RELATE TD METHODS TO DECISION TASKS FORMULATED IN TERMS OF A STOCHASTIC DYNAMICAL SYSTEM WHOSE BEHAVIOR UNFOLDS OVER TIME UNDER THE INFLUENCE OF A DECISION MAKER''S ACTIONS. STRATEGIES ARE SOUGHT FOR SELECTING ACTIONS SO AS TO MAXI- MIZE A MEASURE OF LONG-TERM PAYOFF GAIN. MATHEMATICALLY, TASKS SUCH AS THIS CAN BE FORMULATED AS MARKOVIAN DECISION PROBLEMS, AND NUMEROUS METHODS HAVE BEEN PROPOSED FOR LEARNING HOW TO SOLVE SUCH PROBLEMS. WE SHOW HOW A TD METHOD CAN BE UNDERSTOOD AS A NOVEL SYNTHESIS OF CONCEPTS FROM THE THEORY OF STOCHASTIC DYNAMIC PROGRAMMING, WHICH COMPRISES THE STANDARD METHOD FOR SOLVING SUCH TASKS WHEN A MODEL OF THE DYNAMICAL SYSTEM IS AVAILABLE, AND THE THEORY OF PARAMETER ESTIMATION, WHICH PROVIDES THE APPROPRIATE CONTEXT FOR STUDYING LEARNING RULES IN THE FORM OF EQUATIONS FOR UPDATING ASSOCIA- TIVE STRENGTHS IN BEHAVIORAL MODELS, OR CONNECTION WEIGHTS IN CONNECTIONIST NETWORKS. BECAUSE THIS REPORT IS ORIENTED PRIMARILY TOWARD THE NON-ENGINEER INTERESTED IN ANIMAL LEARNING, IT PRESENTS TUTORIALS ON STOCHASTIC SEQUEN- TIAL DECISION TASKS, STOCHASTIC DYNAMIC PROGRAMMING, AND PARAMETER ESTIMATI"
            },
            "slug": "Learning-and-Sequential-Decision-Making-Barto-Sutton",
            "title": {
                "fragments": [],
                "text": "Learning and Sequential Decision Making"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown how a TD METHOD can beunderstood as a NOVEL SYNTHESIS of CONCEPTS from the theORY of STOCHASTIC DYNAMIC PROGRAMMING, which is the standard method for solving decision-making problems in binary systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 284549,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "722a3c365a134a9f9b9ae1511f018d9b1ecff3de",
            "isKey": false,
            "numCitedBy": 1020,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Most connectionist or \"neural network\" learning systems use some form of the back-propagation algorithm. However, back-propagation learning is too slow for many applications, and it scales up poorly as tasks become larger and more complex. The factors governing learning speed are poorly understood. I have begun a systematic, empirical study of learning speed in backprop-like algorithms, measured against a variety of benchmark problems. The goal is twofold: to develop faster learning algorithms and to contribute to the development of a methodology that will be of value in future studies of this kind. This paper is a progress report describing the results obtained during the first six months of this study. To date I have looked only at a limited set of benchmark problems, but the results on these are encouraging: I have developed a new learning algorithm that is faster than standard backprop by an order of magnitude or more and that appears to scale up very well as the problem size increases. This research was sponsored in part by the National Science Foundation under Contract Number EET-8716324 and by the Defense Advanced Research Projects Agency (DOD), ARPA Order No. 4976 under Contract F33615-87C-1499 and monitored by the Avionics Laboratory, Air Force Wright Aeronautical Laboratories, Aeronautical Systems Division (AFSC), Wright-Patterson AFB, OH 45433-6543. The views and conclusions contained in this document are those of the authors and should not be interpreted as representing the official policies, either expressed or implied, of these agencies or of the U.S. Government."
            },
            "slug": "An-empirical-study-of-learning-speed-in-networks-Fahlman",
            "title": {
                "fragments": [],
                "text": "An empirical study of learning speed in back-propagation networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new learning algorithm is developed that is faster than standard backprop by an order of magnitude or more and that appears to scale up very well as the problem size increases."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8483722"
                        ],
                        "name": "C. Atkeson",
                        "slug": "C.-Atkeson",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Atkeson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Atkeson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18299051,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fa1cfb6df9ff2b60da107b444a3b1cc7577908e3",
            "isKey": false,
            "numCitedBy": 336,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "In this review I have discussed how the form of representation used in internal models of the motor apparatus affects how and what a system can learn. Tabular models and structured models have benefits and drawbacks. Structured models incorporate knowledge of the structure of the controlled motor apparatus. If that knowledge is correct, or close to the actual system structure, the structured models will support global generalization and rapid, efficient learning. Tabular models can play an important role in learning to control systems when either the system structure is not known or only known approximately. Tabular models are general and flexible. Techniques for combining these different representations to attain the benefits of both are currently under investigation. In the control of multijoint systems such as the human arm, internal models of the motor apparatus are necessary to interpret performance errors. In the study of movements restricted to one joint, the problem of interpreting performance errors is greatly simplified and often overlooked, as performance errors can usually be related to command corrections by a single gain. When multijoint movements of the same motor systems are examined, however, the complex nature of the control and coordination problems faced by the nervous system become evident, as well as the sophistication of the brain's solutions to these problems. Recent progress in the understanding of adaptive control of eye movements provides a good example of this (Berthoz & Melvill-Jones 1985). Experimental studies of the psychophysics of motor learning can play an important role in bridging the gap between computational theories of how abstract motor systems might learn and physiological exploration of how actual nervous systems implement learning. Quantitative analyses of the patterns of motor learning of biological systems may help distinguish alternative hypotheses about the representations used for motor control and learning. What a system can and cannot learn, the amount of generalization, and the rate of learning give clues as to the underlying performance architecture. It is also important to know the actual performance level of the motor system (Loeb 1983). Different proposed control strategies will be able to attain different performance levels, and the use of simplifying control strategies may be evident in the control and learning performance of motor systems."
            },
            "slug": "Learning-arm-kinematics-and-dynamics.-Atkeson",
            "title": {
                "fragments": [],
                "text": "Learning arm kinematics and dynamics."
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "In this review I have discussed how the form of representation used in internal models of the motor apparatus affects how and what a system can learn."
            },
            "venue": {
                "fragments": [],
                "text": "Annual review of neuroscience"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 205118721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "266e07d0dd9a75b61e3632e9469993dbaf063f1c",
            "isKey": false,
            "numCitedBy": 882,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-of-backpropagation-with-application-Werbos",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation with application to a recurrent gas market model"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144867807"
                        ],
                        "name": "S. Thrun",
                        "slug": "S.-Thrun",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Thrun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thrun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061144973"
                        ],
                        "name": "K. M\u00f6ller",
                        "slug": "K.-M\u00f6ller",
                        "structuredName": {
                            "firstName": "Knut",
                            "lastName": "M\u00f6ller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00f6ller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144874769"
                        ],
                        "name": "A. Linden",
                        "slug": "A.-Linden",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Linden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Linden"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11282615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32bcc1d5137ff5f37a9d0313b87d4d0e1ce1777b",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new connectionist planning method [TML90]. By interaction with an unknown environment, a world model is progressively constructed using gradient descent. For deriving optimal actions with respect to future reinforcement, planning is applied in two steps: an experience network proposes a plan which is subsequently optimized by gradient descent with a chain of world models, so that an optimal reinforcement may be obtained when it is actually run. The appropriateness of this method is demonstrated by a robotics application and a pole balancing task."
            },
            "slug": "Planning-with-an-Adaptive-World-Model-Thrun-M\u00f6ller",
            "title": {
                "fragments": [],
                "text": "Planning with an Adaptive World Model"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new connectionist planning method by interaction with an unknown environment, a world model is progressively constructed using gradient descent to derive optimal actions with respect to future reinforcement."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14916630,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "593bfe3eb5996a8c8069cfbc51337bac89373b3f",
            "isKey": false,
            "numCitedBy": 164,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a robot control architecture which combines a stimulus-response subsystem for rapid reaction, with a search-based planner for handling unanticipated situations. The robot agent continually chooses which action it is to perform, using the stimulus-response subsystem when possible, and falling back on the planning subsystem when necessary. Whenever it is forced to plan, it applies an explanation-based learning mechanism to formulate a new stimulus-response rule to cover this new situation and others similar to it. With experience, the agent becomes increasingly reactive as its learning component acquires new stimulus-response rules that eliminate the need for planning in similar subsequent situations. This Theo-Agent architecture is described, and results are presented demonstrating its ability to reduce routine reaction time for a simple mobile robot from minutes to under a second."
            },
            "slug": "Becoming-Increasingly-Reactive-Mitchell",
            "title": {
                "fragments": [],
                "text": "Becoming Increasingly Reactive"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A robot control architecture which combines a stimulus-response subsystem for rapid reaction, with a search-based planner for handling unanticipated situations, and results are presented demonstrating its ability to reduce routine reaction time for a simple mobile robot from minutes to under a second."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60855956,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a8aea51f5a911e0964d51ac764dc04d5900b7b7",
            "isKey": false,
            "numCitedBy": 231,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Strategy-Learning-with-Multilayer-Connectionist-Anderson",
            "title": {
                "fragments": [],
                "text": "Strategy Learning with Multilayer Connectionist Representations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895771"
                        ],
                        "name": "D. Zipser",
                        "slug": "D.-Zipser",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Zipser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Zipser"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 14711886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce9a21b93ba29d4145a8ef6bf401e77f261848de",
            "isKey": false,
            "numCitedBy": 3857,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks. These algorithms have (1) the advantage that they do not require a precisely defined training interval, operating while the network runs; and (2) the disadvantage that they require nonlocal communication in the network being trained and are computationally expensive. These algorithms allow networks having recurrent connections to learn complex tasks that require the retention of information over time periods having either fixed or indefinite length."
            },
            "slug": "A-Learning-Algorithm-for-Continually-Running-Fully-Williams-Zipser",
            "title": {
                "fragments": [],
                "text": "A Learning Algorithm for Continually Running Fully Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The exact form of a gradient-following learning algorithm for completely recurrent networks running in continually sampled time is derived and used as the basis for practical algorithms for temporal supervised learning tasks."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144441192"
                        ],
                        "name": "M. A. Lewis",
                        "slug": "M.-A.-Lewis",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Lewis",
                            "middleNames": [
                                "Anthony"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712136"
                        ],
                        "name": "A. Fagg",
                        "slug": "A.-Fagg",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fagg",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fagg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1927968"
                        ],
                        "name": "Alan Solidum",
                        "slug": "Alan-Solidum",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Solidum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan Solidum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12833418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "908f836b4d2a019dcb72acbd07004b03a9d27210",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe the staged evolution of a complex motor pattern generator (MPG) for the control of a walking robot. The experiments were carried out on a six-legged, Brooks-style insect robot. The MPG was composed of a network of neurons with weights determined by genetic algorithm optimization. Staged evolution was used to improve the convergence rate of the algorithm. First, an oscillator for the individual leg movements was evolved. Then, a network of these oscillators was evolved to coordinate the movements of the different legs. By introducing a staged set of manageable challenges, the algorithm's performance was improved.<<ETX>>"
            },
            "slug": "Genetic-programming-approach-to-the-construction-of-Lewis-Fagg",
            "title": {
                "fragments": [],
                "text": "Genetic programming approach to the construction of a neural network for control of a walking robot"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The authors describe the staged evolution of a complex motor pattern generator (MPG) for the control of a walking robot that was composed of a network of neurons with weights determined by genetic algorithm optimization."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1992 IEEE International Conference on Robotics and Automation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145526918"
                        ],
                        "name": "Y. Gil",
                        "slug": "Y.-Gil",
                        "structuredName": {
                            "firstName": "Yolanda",
                            "lastName": "Gil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Gil"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 109306705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e1e995d813ebfb5781de054f3979a85720c26a7",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "In order for autonomous systems to interact with their environment in an intelligent way, they must be given the ability to adapt and learn incrementally and deliberately. It is virtually impossible to devise and hand code all potentially relevant domain knowledge for complex dynamic tasks. This thesis describes a framework to acquire domain knowledge for planning by failure-driven experimentation with the environment. The initial domain knowledge in the system is an approximate model for planning in the environment, defining the system's expectations. The framework exploits the characteristics of planning domains in order to search the space of plausible hypotheses without the need for additional background knowledge to build causal explanations for expectation failures. Plans are executed while the external environment is monitored, and differences between the internal state and external observations are detected by various methods each correlated with a typical cause for the expectation failure. The methods also construct a set of concrete hypotheses to repair the knowledge deficit. After being heuristically filtered, each hypothesis is tested in turn with an experiment. After the experiment is designed, a plan is constructed to achieve the situation required to carry out the experiment. The experiment plan must meet constraints such as minimizing plan length and negative interference with the main goals. The thesis describes a set of domain-independent constraints for experiments and their incorporation in the planning search space. After the execution of the plan and the experiment, observations are collected to conclude if the experiment was successful or not. Upon success, the hypothesis is confirmed and the domain knowledge is adjusted. Upon failure, the experimentation process is iterated on the remaining hypotheses until success or until no more hypotheses are left to be considered. This framework has shown to be an effective way to address incomplete planning knowledge and is demonstrated in a system called EXPO, implemented on the scPRODIGY planning architecture. The effectiveness and efficiency of EXPO's methods is empirically demonstrated in several domains, including a large-scale process planning task, where the planner can recover from situations missing up to 50% of domain knowledge through repeated experimentation."
            },
            "slug": "Acquiring-domain-knowledge-for-planning-by-Gil",
            "title": {
                "fragments": [],
                "text": "Acquiring domain knowledge for planning by experimentation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This thesis describes a framework to acquire domain knowledge for planning by failure-driven experimentation with the environment, which exploits the characteristics of planning domains in order to search the space of plausible hypotheses without the need for additional background knowledge to build causal explanations for expectation failures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 236321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbe8c61628896081998d1cd7d10343a45b7061bd",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Several strategies are described that overcome limitations of basic network models as steps towards the design of large connectionist speech recognition systems. The two major areas of concern are the problem of time and the problem of scaling. Speech signals continuously vary over time and encode and transmit enormous amounts of human knowledge. To decode these signals, neural networks must be able to use appropriate representations of time and it must be possible to extend these nets to almost arbitrary sizes and complexity within finite resources. The problem of time is addressed by the development of a Time-Delay Neural Network; the problem of scaling by Modularity and Incremental Design of large nets based on smaller subcomponent nets. It is shown that small networks trained to perform limited tasks develop time invariant, hidden abstractions that can subsequently be exploited to train larger, more complex nets efficiently. Using these techniques, phoneme recognition networks of increasing complexity can be constructed that all achieve superior recognition performance."
            },
            "slug": "Modular-Construction-of-Time-Delay-Neural-Networks-Waibel",
            "title": {
                "fragments": [],
                "text": "Modular Construction of Time-Delay Neural Networks for Speech Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that small networks trained to perform limited tasks develop time invariant, hidden abstractions that can be exploited to train larger, more complex nets efficiently, and phoneme recognition networks of increasing complexity can be constructed that all achieve superior recognition performance."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110509772"
                        ],
                        "name": "Ajay N. Jain",
                        "slug": "Ajay-N.-Jain",
                        "structuredName": {
                            "firstName": "Ajay",
                            "lastName": "Jain",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ajay N. Jain"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60491557,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b1e6efee19336d43ee9137ec97fcf8e58d2a48d7",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A great deal of research has been done developing parsers for natural language, but adequate solutions for some of the particular problems involved in spoken language are still in their infancy. Among the unsolved problems are: difficulty in constructing task-specific grammars, lack of tolerance to noisy input, and inability to effectively utilize complimentary non-symbolic information. \nThis thesis describes PARSEC--a system for generating connectionist parsing networks from example parses. PARSEC networks exhibit three strengths: (1) They automatically learn to parse, and they generalize well compared to hand-coded grammars. (2) They tolerate several types of noise without any explicit noise-modeling. (3) They can learn to use multi-modal input, e.g. a combination of intonation, syntax and semantics. \nThe PARSEC network architecture relies on a variation of supervised back-propagation learning. The architecture differs from other connectionist approaches in that it is highly structured, both at the macroscopic level of modules, and at the microscopic level of connections. Structure is exploited to enhance system performance. \nConference registration dialogs formed the primary development testbed for PARSEC. A separate simultaneous effort in speech recognition and translation for conference registration provided a useful data source for performance comparisons. \nPresented in this thesis are the PARSEC architecture, its training algorithms, and detailed performance analyses along several dimensions that concretely demonstrate PARSEC's advantages."
            },
            "slug": "Parsec:-a-connectionist-learning-architecture-for-Jain",
            "title": {
                "fragments": [],
                "text": "Parsec: a connectionist learning architecture for parsing spoken language"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The PARSEC architecture, its training algorithms, and detailed performance analyses along several dimensions that concretely demonstrate PARSEC's advantages are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749342"
                        ],
                        "name": "C. Lebiere",
                        "slug": "C.-Lebiere",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Lebiere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lebiere"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30443043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "995a3b11cc8a4751d8e167abc4aa937abc934df0",
            "isKey": false,
            "numCitedBy": 2940,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "slug": "The-Cascade-Correlation-Learning-Architecture-Fahlman-Lebiere",
            "title": {
                "fragments": [],
                "text": "The Cascade-Correlation Learning Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39263338"
                        ],
                        "name": "Longxin Lin",
                        "slug": "Longxin-Lin",
                        "structuredName": {
                            "firstName": "Longxin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longxin Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719955"
                        ],
                        "name": "R. Simmons",
                        "slug": "R.-Simmons",
                        "structuredName": {
                            "firstName": "Reid",
                            "lastName": "Simmons",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Simmons"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2490714"
                        ],
                        "name": "C. Fedor",
                        "slug": "C.-Fedor",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Fedor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fedor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11643443,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "128fbd5dd7ff1f6594c00092781510d7f42a41f2",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "T h i s paper presents a general-purpose architecture for oonyolling mobde robots, and describes a working mobile manipulator which uses the architecture to operate in a dynamic and uncertain environment. The target of this work is to develop a distributed robot architecture for planning, execution, monitoring, exception handling, and multiple task coordination. We report our progress to date on the architecture development and the performance of the working robot. In particular, we discuss temporal reasoning, execution monitoring. and context-dependent exception handling."
            },
            "slug": "Experience-with-a-Task-Control-Architecture-for-Lin-Simmons",
            "title": {
                "fragments": [],
                "text": "Experience with a Task Control Architecture for Mobile Robots"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A general-purpose architecture for oonyolling mobde robots is presented, and a working mobile manipulator which uses the architecture to operate in a dynamic and uncertain environment is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48855558"
                        ],
                        "name": "D. Pomerleau",
                        "slug": "D.-Pomerleau",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Pomerleau",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pomerleau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18420840,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "7786bc6c25ba38ff0135f1bdad192f6b3c4ad0b3",
            "isKey": false,
            "numCitedBy": 1479,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following. Currently ALVINN takes images from a camera and a laser range finder as input and produces as output the direction the vehicle should travel in order to follow the road. Training has been conducted using simulated road images. Successful tests on the Carnegie Mellon autonomous navigation test vehicle indicate that the network can effectively follow real roads under certain field conditions. The representation developed to perform the task differs dramatically when the network is trained under various conditions, suggesting the possibility of a novel adaptive autonomous navigation system capable of tailoring its processing to the conditions at hand."
            },
            "slug": "ALVINN:-An-Autonomous-Land-Vehicle-in-a-Neural-Pomerleau",
            "title": {
                "fragments": [],
                "text": "ALVINN: An Autonomous Land Vehicle in a Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "ALVINN (Autonomous Land Vehicle In a Neural Network) is a 3-layer back-propagation network designed for the task of road following that can effectively follow real roads under certain field conditions."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7587835,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3abac8d1bf1a6c69805e8aa6f0335b66f39ca999",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Successes with expert systems and other specialized systems have revived hopes for factory automation and productivity growth. A full realization of these potentials will require conscious effort to overcome obsolete rigidities, to develop unified and adaptive methods for integrating complex systems, and to increase our understanding of these systems (understanding which is vital to human productivity in developing software). How adaptive systems may be built and understood by extending control theory and statistics is discussed. Adaptive systems, like human infants, are less agile than young monkeys but have something important to contribute as they mature. It argues that the old dream of understanding intelligence in generalized terms, permitting a unified understanding of adaptive systems and of the human mind, was not incorrect; rather, the early attempts in that direction failed because they did not make full use of research possibilities in statistics, control theory, and numerical analysis (many of which are still unexploited) and were limited by hardware costs which are now coming down. A basic adaptive system derived from this approach fits the fundamental, qualitative empirical facts of human brain physiology in some detail (unlike the usual \"general neuron models,\" which rarely even discriminate between basic components of the brain), and offers opportunities for further research; it can even translate certain fundamental ideas of Freud into something more mathematical and scientific. The mathematics of the basic system, and the fit to the brain, are described in detail."
            },
            "slug": "Building-and-Understanding-Adaptive-Systems:-A-to-Werbos",
            "title": {
                "fragments": [],
                "text": "Building and Understanding Adaptive Systems: A Statistical/Numerical Approach to Factory Automation and Brain Research"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that the old dream of understanding intelligence in generalized terms, permitting a unified understanding of adaptive systems and of the human mind, was not incorrect; rather, the early attempts failed because they did not make full use of research possibilities in statistics, control theory, and numerical analysis and were limited by hardware costs which are now coming down."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Systems, Man, and Cybernetics"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2763403,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "668087f0ae7ce1de6e0bd0965dbb480c08103260",
            "isKey": false,
            "numCitedBy": 9891,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": "Time underlies many interesting human behaviors. Thus, the question of how to represent time in connectionist models is very important. One approach is to represent time implicitly by its effects on processing rather than explicitly (as in a spatial representation). The current report develops a proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory. In this approach, hidden unit patterns are fed back to themselves; the internal representations which develop thus reflect task demands in the context of prior internal states. A set of simulations is reported which range from relatively simple problems (temporal version of XOR) to discovering syntactic/semantic features for words. The networks are able to learn interesting internal representations which incorporate task demands with memory demands; indeed, in this approach the notion of memory is inextricably bound up with task processing. These representations reveal a rich structure, which allows them to be highly context-dependent while also expressing generalizations across classes of items. These representations suggest a method for representing lexical categories and the type/token distinction."
            },
            "slug": "Finding-Structure-in-Time-Elman",
            "title": {
                "fragments": [],
                "text": "Finding Structure in Time"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A proposal along these lines first described by Jordan (1986) which involves the use of recurrent links in order to provide networks with a dynamic memory and suggests a method for representing lexical categories and the type/token distinction is developed."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113911099"
                        ],
                        "name": "R. Rivest",
                        "slug": "R.-Rivest",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Rivest",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rivest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716301"
                        ],
                        "name": "R. Schapire",
                        "slug": "R.-Schapire",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Schapire",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Schapire"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4608586,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ae1dd016502b853908038a6dfd97d967e2dd185",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new procedure for inferring the structure of a finitestate automaton (FSA) from its input/output behavior, using access to the automaton to perform experiments. Our procedure uses a new representation for FSA's, based on the notion of equivalence between testa. We call the number of such equivalence classes the diversity of the automaton; the diversity may be as small as the logarithm of the number of states of the automaton. The size of our representation of the FSA, and the running time of our procedure (in some case provably, in others conjecturally) is polynomial in the diversity and ln(1/\u03b5), where \u03b5 is a given upper bound on the probability that our procedure returns an incorrect result. (Since our procedure uses randomization to perform experiments, there is a certain controllable chance that it will return an erroneous result.) We also present some evidence for the practical efficiency of our approach. For example, our procedure is able to infer the structure of an automaton based on Rubik's Cube (which has approximately 1019 states) in about 2 minutes on a DEC Micro Vax. This automaton is many orders of magnitude larger than possible with previous techniques, which would require time proportional at least to the number of global states. (Note that in this example, only a small fraction (10-14) of the global states were even visited.)"
            },
            "slug": "Diversity-based-inference-of-finite-automata-Rivest-Schapire",
            "title": {
                "fragments": [],
                "text": "Diversity-based inference of finite automata"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A new procedure for inferring the structure of a finitestate automaton (FSA) from its input/output behavior, using access to the automaton to perform experiments, based on the notion of equivalence between testa."
            },
            "venue": {
                "fragments": [],
                "text": "28th Annual Symposium on Foundations of Computer Science (sfcs 1987)"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15720720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e8cf03655d224b0994d0f9d4f5aa80bca07021a",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Cascade-Correlation (RCC) is a recurrent version of the Cascade-Correlation learning architecture of Fahlman and Lebiere [Fahlman, 1990]. RCC can learn from examples to map a sequence of inputs into a desired sequence of outputs. New hidden units with recurrent connections are added to the network as needed during training. In effect, the network builds up a finite-state machine tailored specifically for the current problem. RCC retains the advantages of Cascade-Correlation: fast learning, good generalization, automatic construction of a near-minimal multi-layered network, and incremental training."
            },
            "slug": "The-Recurrent-Cascade-Correlation-Architecture-Fahlman",
            "title": {
                "fragments": [],
                "text": "The Recurrent Cascade-Correlation Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Recurrent Cascade-Correlation is a recurrent version of the Cascade- Correlation learning architecture of Fahlman and Lebiere that can learn from examples to map a sequence of inputs into a desired sequence of outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363971"
                        ],
                        "name": "J. Hertz",
                        "slug": "J.-Hertz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hertz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hertz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46486898"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50760571"
                        ],
                        "name": "R. Palmer",
                        "slug": "R.-Palmer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Palmer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Palmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 38623065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c0cbbd275bb43e09f0527a31ddd61824eca295b",
            "isKey": false,
            "numCitedBy": 6638,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book is a comprehensive introduction to the neural network models currently under intensive study for computational applications. It is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning. It also provides coverage of neural network applications in a variety of problems of both theoretical and practical interest."
            },
            "slug": "Introduction-to-the-theory-of-neural-computation-Hertz-Krogh",
            "title": {
                "fragments": [],
                "text": "Introduction to the theory of neural computation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "The advanced book program"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 50027191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3106e66537a0c8f53278e553bcb38f0b0992ec0e",
            "isKey": false,
            "numCitedBy": 1247,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a \u0302 network of simple computing elements and some entities to be represented, the most straightforward scheme is to use one computing element for each entity. This is called a local representation. It is easy to understand and easy to implement because the structure of the physical network mirrors the structure of the knowledge it contains. This report describes a different type of representation that is less familiar and harder to think about than local representations. Each entity is represented by a pattern of activity distributed over many computing elements, and each computing element is involved in representing many different entities. The strength of this more complicated kind of representation does not lie in its notational convenience or its ease of implementation in a conventional computer, but rather in the efficiency with which it makes use of the processing abilities of networks of simple, neuron-like computing elements. Every representational scheme has its good and bad points. Distributed representations are no exception. Some desirable properties like content-addressable memory and automatic generalization arise very naturally from the use of patterns of activity as representations. Other properties, like the ability to temporarily store a large set of arbitrary associations, are much harder to achieve. The best psychological evidence for distributed representations is the degree to which their strengths and weaknesses match those of the human mind. ^This research was supported by a grant from the System Development Foundation. I thank Jim Anderson, Dave Ackley Dana Ballard, Francis Crick, Scott Fahlman, Jerry Feldman, Christopher Longuet-Higgins, Don Norman, Terry Sejnowski, and Tim Shallice for helpful discussions. Jay McClelland and Dave Rumelhart helped me refine and rewrite many of the ideas presented here A substantially revised version of this report will appear as a chapter by Hinton, McClelland and Rumelhart in Parallel Distributed Processing: Explorations in the micro-structure of cognition, edited by McClelland and Rumelhart)"
            },
            "slug": "Distributed-Representations-Hinton-McClelland",
            "title": {
                "fragments": [],
                "text": "Distributed Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This report describes a different type of representation that is less familiar and harder to think about than local representations, which makes use of the processing abilities of networks of simple, neuron-like computing elements."
            },
            "venue": {
                "fragments": [],
                "text": "The Philosophy of Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694621"
                        ],
                        "name": "Michael I. Jordan",
                        "slug": "Michael-I.-Jordan",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Jordan",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael I. Jordan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17147798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b2842e8db11fa2ec716fc4c4c7de0fae17b0eac",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The forward modeling approach is a methodology for learning control when data is available in distal coordinate systems. We extend previous work by considering how this methodology can be applied to the optimization of quantities that are distal not only in space but also in time."
            },
            "slug": "Learning-to-Control-an-Unstable-System-with-Forward-Jordan-Jacobs",
            "title": {
                "fragments": [],
                "text": "Learning to Control an Unstable System with Forward Modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work extends previous work by considering how this methodology can be applied to the optimization of quantities that are distal not only in space but also in time."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40975594"
                        ],
                        "name": "Tom Michael Mitchell",
                        "slug": "Tom-Michael-Mitchell",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Michael Mitchell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17162574,
            "fieldsOfStudy": [
                "Sociology"
            ],
            "id": "0df1aac45ff562089a3bdbcb34e2481a71478651",
            "isKey": false,
            "numCitedBy": 1780,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-as-Search-Mitchell",
            "title": {
                "fragments": [],
                "text": "Generalization as Search"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066894"
                        ],
                        "name": "Hans P. Moravec",
                        "slug": "Hans-P.-Moravec",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Moravec",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hans P. Moravec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144756941"
                        ],
                        "name": "A. Elfes",
                        "slug": "A.-Elfes",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Elfes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elfes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 41852334,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "d1ec836351c0e89f5834957953d9a040dab56985",
            "isKey": false,
            "numCitedBy": 1908,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the use of multiple wide-angle sonar range measurements to map the surroundings of an autonomous mobile robot. A sonar range reading provides information concerning empty and occupied volumes in a cone (subtending 30 degrees in our case) in front of the sensor. The reading is modelled as probability profiles projected onto a rasterized map, where somewhere occupied and everywhere empty areas are represented. Range measurements from multiple points of view (taken from multiple sensors on the robot, and from the same sensors after robot moves) are systematically integrated in the map. Overlapping empty volumes re-inforce each other, and serve to condense the range of occupied volumes. The map definition improves as more readings are added. The final map shows regions probably occupied, probably unoccupied, and unknown areas. The method deals effectively with clutter, and can be used for motion planning and for extended landmark recognition. This system has been tested on the Neptune mobile robot at CMU."
            },
            "slug": "High-resolution-maps-from-wide-angle-sonar-Moravec-Elfes",
            "title": {
                "fragments": [],
                "text": "High resolution maps from wide angle sonar"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "The use of multiple wide-angle sonar range measurements to map the surroundings of an autonomous mobile robot deals effectively with clutter, and can be used for motion planning and for extended landmark recognition."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 1985 IEEE International Conference on Robotics and Automation"
            },
            "year": 1985
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1848627"
                        ],
                        "name": "U. Bodenhausen",
                        "slug": "U.-Bodenhausen",
                        "structuredName": {
                            "firstName": "Ulrich",
                            "lastName": "Bodenhausen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Bodenhausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15180051,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "420f2fd930251db10344c45adf75c3f38cb9d713",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we describe a new method that adjusts time-delays and the widths of time-windows in artificial neural networks automatically. The input of the units are weighted by a gaussian input-window over time which allows the learning rules for the delays and widths to be derived in the same way as it is used for the weights. Our results on a phoneme classification task compare well with results obtained with the TDNN by Waibel et al., which was manually optimized for the same task."
            },
            "slug": "The-Tempo-2-Algorithm:-Adjusting-Time-Delays-By-Bodenhausen-Waibel",
            "title": {
                "fragments": [],
                "text": "The Tempo 2 Algorithm: Adjusting Time-Delays By Supervised Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new method that adjusts time-delays and the widths of time-windows in artificial neural networks automatically, which compares well with results obtained with the TDNN by Waibel et al., which was manually optimized for the same task."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055155244"
                        ],
                        "name": "David Chapman",
                        "slug": "David-Chapman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chapman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Chapman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5130222,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "7ab2f22dee5cea8ee3d50503b00b70f521333b79",
            "isKey": false,
            "numCitedBy": 356,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis describes Sonja, a system which uses instructions in the course of visually-guided activity. The thesis explores an integration of research in vision, activity, and natural language pragmatics. Sonja''s visual system demonstrates the use of several intermediate visual processes, particularly visual search and routines, previously proposed on psychophysical grounds. The computations Sonja performs are compatible with the constraints imposed by neuroscientifically plausible hardware. Although Sonja can operate autonomously, it can also make flexible use of instructions provided by a human advisor. The system grounds its understanding of these instructions in perception and action."
            },
            "slug": "Vision,-instruction,-and-action-Chapman",
            "title": {
                "fragments": [],
                "text": "Vision, instruction, and action"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This thesis describes Sonja, a system which uses instructions in the course of visually-guided activity which can operate autonomously and make flexible use of instructions provided by a human advisor."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7991309"
                        ],
                        "name": "A. Samuel",
                        "slug": "A.-Samuel",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Samuel",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Samuel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2126705,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9e6bb5f2a04ae30d8ecc9287f8b702eedd7b772",
            "isKey": false,
            "numCitedBy": 3147,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method. Full use is made of the so-called \u201calpha-beta\u201d pruning and several forms of forward pruning to restrict the spread of the move tree and to permit the program to look ahead to a much greater depth than it otherwise could do. While still unable to outplay checker masters, the program's playing ability has been greatly improved.tplay checker masters, the"
            },
            "slug": "Some-Studies-in-Machine-Learning-Using-the-Game-of-Samuel",
            "title": {
                "fragments": [],
                "text": "Some Studies in Machine Learning Using the Game of Checkers"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "A new signature-table technique is described together with an improved book-learning procedure which is thought to be much superior to the linear polynomial method and to permit the program to look ahead to a much greater depth than it otherwise could do."
            },
            "venue": {
                "fragments": [],
                "text": "IBM J. Res. Dev."
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682627"
                        ],
                        "name": "R. Korf",
                        "slug": "R.-Korf",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Korf",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Korf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 36824478,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "47ebbd156f6835f3c0965601b76df3cf7d9da681",
            "isKey": false,
            "numCitedBy": 475,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Planning-as-Search:-A-Quantitative-Approach-Korf",
            "title": {
                "fragments": [],
                "text": "Planning as Search: A Quantitative Approach"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745117"
                        ],
                        "name": "Craig A. Knoblock",
                        "slug": "Craig-A.-Knoblock",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Knoblock",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Craig A. Knoblock"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12372104,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f700af43e57fe772dcb2af5feaa51a0d8c4168c6",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 115,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : A major source of inefficiency in automated problem solvers is their inability to decompose problems and work on the more difficult parts first. This issue can be addressed by employing a hierarchy of abstract problem spaces to focus the search. Instead of solving a problem in the original problem space, a problem is first solved in an abstract space, and the abstract solution is then refined at successive levels in the hierarchy. While this use of abstraction can significantly reduce search, it is often difficult to find good abstractions, and the abstractions must be manually engineered by the designer of a problem domain."
            },
            "slug": "Automatically-generating-abstractions-for-problem-Knoblock",
            "title": {
                "fragments": [],
                "text": "Automatically generating abstractions for problem solving"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "A major source of inefficiency in automated problem solvers is their inability to decompose problems and work on the more difficult parts first, which can be addressed by employing a hierarchy of abstract problem spaces to focus the search."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19408,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145708111"
                        ],
                        "name": "C. Watkins",
                        "slug": "C.-Watkins",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Watkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Watkins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59809750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c8bb027eb65b6d250a22e9b6db22853a552ac81",
            "isKey": false,
            "numCitedBy": 2924,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-from-delayed-rewards-Watkins",
            "title": {
                "fragments": [],
                "text": "Learning from delayed rewards"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60564875,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "22069cd4504656d3bb85748a4d43be7a4d7d5545",
            "isKey": false,
            "numCitedBy": 870,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Temporal-credit-assignment-in-reinforcement-Sutton",
            "title": {
                "fragments": [],
                "text": "Temporal credit assignment in reinforcement learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1984
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913418"
                        ],
                        "name": "B. Widrow",
                        "slug": "B.-Widrow",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Widrow",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Widrow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1976925"
                        ],
                        "name": "M. Hoff",
                        "slug": "M.-Hoff",
                        "structuredName": {
                            "firstName": "Marcian",
                            "lastName": "Hoff",
                            "middleNames": [
                                "E."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hoff"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60830585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e14b2ff9dc2234df94fc24d89fc25e797d0e9e7",
            "isKey": false,
            "numCitedBy": 2595,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-switching-circuits-Widrow-Hoff",
            "title": {
                "fragments": [],
                "text": "Adaptive switching circuits"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144590683"
                        ],
                        "name": "W. Stornetta",
                        "slug": "W.-Stornetta",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Stornetta",
                            "middleNames": [
                                "Scott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Stornetta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794321"
                        ],
                        "name": "B. Huberman",
                        "slug": "B.-Huberman",
                        "structuredName": {
                            "firstName": "Bernardo",
                            "lastName": "Huberman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Huberman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59918773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e7742958d272c241e5b2fa4ad530518a169c684",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "AN-IMPROVED-THREE-LAYER,-BACK-PROPAGATION-ALGORITHM-Stornetta-Huberman",
            "title": {
                "fragments": [],
                "text": "AN IMPROVED THREE LAYER, BACK PROPAGATION ALGORITHM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118278417"
                        ],
                        "name": "M. Tan",
                        "slug": "M.-Tan",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Tan",
                            "middleNames": [
                                "Kui"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Tan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59660421,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff8c3651034b56234ad0683e16f925acb462c16e",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Cost-sensitive-robot-learning-Tan",
            "title": {
                "fragments": [],
                "text": "Cost-sensitive robot learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94653581"
                        ],
                        "name": "J. H. Holland",
                        "slug": "J.-H.-Holland",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holland",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Holland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56489392,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e3cbd911b175d510a8cb3c65c79a5595915e42f",
            "isKey": false,
            "numCitedBy": 1132,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Escaping-brittleness:-the-possibilities-of-learning-Holland",
            "title": {
                "fragments": [],
                "text": "Escaping brittleness: the possibilities of general-purpose learning algorithms applied to parallel rule-based systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720087"
                        ],
                        "name": "S. Whitehead",
                        "slug": "S.-Whitehead",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Whitehead",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Whitehead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691804"
                        ],
                        "name": "D. Ballard",
                        "slug": "D.-Ballard",
                        "structuredName": {
                            "firstName": "Dana",
                            "lastName": "Ballard",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ballard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37916261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3577d7097c46e4f136b30c3c02ee1377e59d98a6",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Role-for-Anticipation-in-Reactive-Systems-that-Whitehead-Ballard",
            "title": {
                "fragments": [],
                "text": "A Role for Anticipation in Reactive Systems that Learn"
            },
            "venue": {
                "fragments": [],
                "text": "ML"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Appendix B Parameter Settings Used in Chapter"
            },
            "venue": {
                "fragments": [],
                "text": "Appendix B Parameter Settings Used in Chapter"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Variable resolution dynamic programming: Efficiently learning action maps in multivariate real-valued state-spaces"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Eight International Workfhop on Machine Learning"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An approach to learning sequences of actions: Combining delayed reinforcement and input generalization"
            },
            "venue": {
                "fragments": [],
                "text": "An approach to learning sequences of actions: Combining delayed reinforcement and input generalization"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A case study in autonomous robot behavior"
            },
            "venue": {
                "fragments": [],
                "text": "A case study in autonomous robot behavior"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The complexity of real-time search"
            },
            "venue": {
                "fragments": [],
                "text": "The complexity of real-time search"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J.S. Albus. Brains, Behaviour and Robotics. BYTE Books"
            },
            "venue": {
                "fragments": [],
                "text": "J.S. Albus. Brains, Behaviour and Robotics. BYTE Books"
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning from natural selection in an artificial environment"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist Modeling and Control of Finite State Environments"
            },
            "venue": {
                "fragments": [],
                "text": "Connectionist Modeling and Control of Finite State Environments"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "1 + 0.02. n) 2. r -a random number between 0 and 1 3. kn--r,'og~l + r'Cew-1))/w"
            },
            "venue": {
                "fragments": [],
                "text": "1 + 0.02. n) 2. r -a random number between 0 and 1 3. kn--r,'og~l + r'Cew-1))/w"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gref'nstette. Credit assignment in rule discovery systems based on genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Mar@BULLETine Learning"
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Real-time learning and control using asynchronous dynamic programming"
            },
            "venue": {
                "fragments": [],
                "text": "Real-time learning and control using asynchronous dynamic programming"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 71,
        "totalPages": 8
    },
    "page_url": "https://www.semanticscholar.org/paper/Reinforcement-learning-for-robots-using-neural-Lin/54c4cf3a8168c1b70f91cf78a3dc98b671935492?sort=total-citations"
}