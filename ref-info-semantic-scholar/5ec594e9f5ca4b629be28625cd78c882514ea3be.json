{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50579876"
                        ],
                        "name": "Yu-hsin Chen",
                        "slug": "Yu-hsin-Chen",
                        "structuredName": {
                            "firstName": "Yu-hsin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-hsin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145984583"
                        ],
                        "name": "T. Krishna",
                        "slug": "T.-Krishna",
                        "structuredName": {
                            "firstName": "Tushar",
                            "lastName": "Krishna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Krishna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775477"
                        ],
                        "name": "J. Emer",
                        "slug": "J.-Emer",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Emer",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Emer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691305"
                        ],
                        "name": "V. Sze",
                        "slug": "V.-Sze",
                        "structuredName": {
                            "firstName": "Vivienne",
                            "lastName": "Sze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 52819773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4806e654b58efda85928c96e1c79c27a9a47ed81",
            "isKey": false,
            "numCitedBy": 321,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep learning using convolutional neural networks (CNN) gives state-of-the-art accuracy on many computer vision tasks (e.g. object detection, recognition, segmentation). Convolutions account for over 90% of the processing in CNNs for both inference/testing and training, and fully convolutional networks are increasingly being used. To achieve state-of-the-art accuracy requires CNNs with not only a larger number of layers, but also millions of filters weights, and varying shapes (i.e. filter sizes, number of filters, number of channels) as shown in Fig. 14.5.1. For instance, AlexNet [1] uses 2.3 million weights (4.6MB of storage) and requires 666 million MACs per 227\u00d7227 image (13kMACs/pixel). VGG16 [2] uses 14.7 million weights (29.4MB of storage) and requires 15.3 billion MACs per 224\u00d7224 image (306kMACs/pixel). The large number of filter weights and channels results in substantial data movement, which consumes significant energy."
            },
            "slug": "14.5-Eyeriss:-An-energy-efficient-reconfigurable-Chen-Krishna",
            "title": {
                "fragments": [],
                "text": "14.5 Eyeriss: An energy-efficient reconfigurable accelerator for deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "To achieve state-of-the-art accuracy, CNNs with not only a larger number of layers, but also millions of filters weights, and varying shapes are needed, which results in substantial data movement, which consumes significant energy."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE International Solid-State Circuits Conference (ISSCC)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678776"
                        ],
                        "name": "Zidong Du",
                        "slug": "Zidong-Du",
                        "structuredName": {
                            "firstName": "Zidong",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zidong Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785040"
                        ],
                        "name": "Robert Fasthuber",
                        "slug": "Robert-Fasthuber",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Fasthuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Fasthuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144049725"
                        ],
                        "name": "Tianshi Chen",
                        "slug": "Tianshi-Chen",
                        "structuredName": {
                            "firstName": "Tianshi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianshi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748620"
                        ],
                        "name": "P. Ienne",
                        "slug": "P.-Ienne",
                        "structuredName": {
                            "firstName": "Paolo",
                            "lastName": "Ienne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ienne"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3353457"
                        ],
                        "name": "Ling Li",
                        "slug": "Ling-Li",
                        "structuredName": {
                            "firstName": "Ling",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ling Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068286576"
                        ],
                        "name": "Tao Luo",
                        "slug": "Tao-Luo",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32215073"
                        ],
                        "name": "Xiaobing Feng",
                        "slug": "Xiaobing-Feng",
                        "structuredName": {
                            "firstName": "Xiaobing",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaobing Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7377735"
                        ],
                        "name": "Yunji Chen",
                        "slug": "Yunji-Chen",
                        "structuredName": {
                            "firstName": "Yunji",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunji Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731764"
                        ],
                        "name": "O. Temam",
                        "slug": "O.-Temam",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Temam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Temam"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "Examples: A variant of MOC-MOP dataflow appears in [20], and variants of SOC-MOP and MOC-SOP dataflows appear in [23] and [18]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11504619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd6507b5c9deaf87bda81e59ce15b2309df0bf37",
            "isKey": false,
            "numCitedBy": 743,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, neural network accelerators have been shown to achieve both high energy efficiency and high performance for a broad application scope within the important category of recognition and mining applications. Still, both the energy efficiency and peiformance of such accelerators remain limited by memory accesses. In this paper, we focus on image applications, arguably the most important category among recognition and mining applications. The neural networks which are state-of-the-art for these applications are Convolutional Neural Networks (CNN), and they have an important property: weights are shared among many neurons, considerably reducing the neural network memory footprint. This property allows to entirely map a CNN within an SRAM, eliminating all DRAM accesses for weights. By further hoisting this accelerator next to the image sensor, it is possible to eliminate all remaining DRAM accesses, i.e., for inputs and outputs. In this paper, we propose such a CNN accelerator, placed next to a CMOS or CCD sensor. The absence of DRAM accesses combined with a careful exploitation of the specific data access patterns within CNNs allows us to design an accelerator which is 60x more energy efficient than the previous state-of-the-art neural network accelerator. We present a fult design down to the layout at 65 nm, with a modest footprint of 4.86 mm2 and consuming only 320 mW, but still about 30x faster than high-end GPUs."
            },
            "slug": "ShiDianNao:-Shifting-vision-processing-closer-to-Du-Fasthuber",
            "title": {
                "fragments": [],
                "text": "ShiDianNao: Shifting vision processing closer to the sensor"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes an accelerator which is 60x more energy efficient than the previous state-of-the-art neural network accelerator, designed down to the layout at 65 nm, with a modest footprint and consuming only 320 mW, but still about 30x faster than high-end GPUs."
            },
            "venue": {
                "fragments": [],
                "text": "2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2020514"
                        ],
                        "name": "Maurice Peemen",
                        "slug": "Maurice-Peemen",
                        "structuredName": {
                            "firstName": "Maurice",
                            "lastName": "Peemen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Maurice Peemen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145101451"
                        ],
                        "name": "A. Setio",
                        "slug": "A.-Setio",
                        "structuredName": {
                            "firstName": "Arnaud",
                            "lastName": "Setio",
                            "middleNames": [
                                "Arindra",
                                "Adiyoso"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Setio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145248324"
                        ],
                        "name": "B. Mesman",
                        "slug": "B.-Mesman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Mesman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Mesman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684335"
                        ],
                        "name": "H. Corporaal",
                        "slug": "H.-Corporaal",
                        "structuredName": {
                            "firstName": "Henk",
                            "lastName": "Corporaal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Corporaal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "Examples: A variant of MOC-MOP dataflow appears in [20], and variants of SOC-MOP and MOC-SOP dataflows appear in [23] and [18]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2398978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b71de48a1c7a5d7a7850bbe33a2bb055ea594ef",
            "isKey": false,
            "numCitedBy": 255,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In the near future, cameras will be used everywhere as flexible sensors for numerous applications. For mobility and privacy reasons, the required image processing should be local on embedded computer platforms with performance requirements and energy constraints. Dedicated acceleration of Convolutional Neural Networks (CNN) can achieve these targets with enough flexibility to perform multiple vision tasks. A challenging problem for the design of efficient accelerators is the limited amount of external memory bandwidth. We show that the effects of the memory bottleneck can be reduced by a flexible memory hierarchy that supports the complex data access patterns in CNN workload. The efficiency of the on-chip memories is maximized by our scheduler that uses tiling to optimize for data locality. Our design flow ensures that on-chip memory size is minimized, which reduces area and energy usage. The design flow is evaluated by a High Level Synthesis implementation on a Virtex 6 FPGA board. Compared to accelerators with standard scratchpad memories the FPGA resources can be reduced up to 13\u00d7 while maintaining the same performance. Alternatively, when the same amount of FPGA resources is used our accelerators are up to 11\u00d7 faster."
            },
            "slug": "Memory-centric-accelerator-design-for-Convolutional-Peemen-Setio",
            "title": {
                "fragments": [],
                "text": "Memory-centric accelerator design for Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the effects of the memory bottleneck can be reduced by a flexible memory hierarchy that supports the complex data access patterns in CNN workload and ensures that on-chip memory size is minimized, which reduces area and energy usage."
            },
            "venue": {
                "fragments": [],
                "text": "2013 IEEE 31st International Conference on Computer Design (ICCD)"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144049725"
                        ],
                        "name": "Tianshi Chen",
                        "slug": "Tianshi-Chen",
                        "structuredName": {
                            "firstName": "Tianshi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianshi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678776"
                        ],
                        "name": "Zidong Du",
                        "slug": "Zidong-Du",
                        "structuredName": {
                            "firstName": "Zidong",
                            "lastName": "Du",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zidong Du"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145550877"
                        ],
                        "name": "Ninghui Sun",
                        "slug": "Ninghui-Sun",
                        "structuredName": {
                            "firstName": "Ninghui",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ninghui Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110368816"
                        ],
                        "name": "Jia Wang",
                        "slug": "Jia-Wang",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7514065"
                        ],
                        "name": "Chengyong Wu",
                        "slug": "Chengyong-Wu",
                        "structuredName": {
                            "firstName": "Chengyong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengyong Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7377735"
                        ],
                        "name": "Yunji Chen",
                        "slug": "Yunji-Chen",
                        "structuredName": {
                            "firstName": "Yunji",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunji Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731764"
                        ],
                        "name": "O. Temam",
                        "slug": "O.-Temam",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Temam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Temam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "In [22], special registers are implemented at the end of each PE array column to hold the psums, which reduces the number of global buffer R/W for psums."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 49
                            }
                        ],
                        "text": "Examples: Variants of the NLR dataflow appear in [21, 22, 24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207209696,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22e477a9fdde86ab1f8f4dafdb4d88ea37e31fbd",
            "isKey": false,
            "numCitedBy": 1235,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine-Learning tasks are becoming pervasive in a broad range of domains, and in a broad range of systems (from embedded systems to data centers). At the same time, a small set of machine-learning algorithms (especially Convolutional and Deep Neural Networks, i.e., CNNs and DNNs) are proving to be state-of-the-art across many applications. As architectures evolve towards heterogeneous multi-cores composed of a mix of cores and accelerators, a machine-learning accelerator can achieve the rare combination of efficiency (due to the small number of target algorithms) and broad application scope. Until now, most machine-learning accelerator designs have focused on efficiently implementing the computational part of the algorithms. However, recent state-of-the-art CNNs and DNNs are characterized by their large size. In this study, we design an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy. We show that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s (key NN operations such as synaptic weight multiplications and neurons outputs additions) in a small footprint of 3.02 mm2 and 485 mW; compared to a 128-bit 2GHz SIMD processor, the accelerator is 117.87x faster, and it can reduce the total energy by 21.08x. The accelerator characteristics are obtained after layout at 65 nm. Such a high throughput in a small footprint can open up the usage of state-of-the-art machine-learning algorithms in a broad set of systems and for a broad set of applications."
            },
            "slug": "DianNao:-a-small-footprint-high-throughput-for-Chen-Du",
            "title": {
                "fragments": [],
                "text": "DianNao: a small-footprint high-throughput accelerator for ubiquitous machine-learning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This study designs an accelerator for large-scale CNNs and DNNs, with a special emphasis on the impact of memory on accelerator design, performance and energy, and shows that it is possible to design an accelerator with a high throughput, capable of performing 452 GOP/s in a small footprint."
            },
            "venue": {
                "fragments": [],
                "text": "ASPLOS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752242"
                        ],
                        "name": "S. Chakradhar",
                        "slug": "S.-Chakradhar",
                        "structuredName": {
                            "firstName": "Srimat",
                            "lastName": "Chakradhar",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chakradhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329060"
                        ],
                        "name": "M. Sankaradass",
                        "slug": "M.-Sankaradass",
                        "structuredName": {
                            "firstName": "Murugan",
                            "lastName": "Sankaradass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sankaradass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101580"
                        ],
                        "name": "V. Jakkula",
                        "slug": "V.-Jakkula",
                        "structuredName": {
                            "firstName": "Venkata",
                            "lastName": "Jakkula",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Jakkula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2468150"
                        ],
                        "name": "S. Cadambi",
                        "slug": "S.-Cadambi",
                        "structuredName": {
                            "firstName": "Srihari",
                            "lastName": "Cadambi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cadambi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "These state-of-the-art CNNs [2\u2013 5] are orders of magnitude larger than those used in the 1990s [9], requiring up to hundreds of megabytes for filter weight storage and 30k-600k operations per input pixel."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3350152,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1c4e2fa071046569a05e9cfdf13496d094025dd",
            "isKey": false,
            "numCitedBy": 350,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional neural networks (CNN) applications range from recognition and reasoning (such as handwriting recognition, facial expression recognition and video surveillance) to intelligent text applications such as semantic text analysis and natural language processing applications. Two key observations drive the design of a new architecture for CNN. First, CNN workloads exhibit a widely varying mix of three types of parallelism: parallelism within a convolution operation, intra-output parallelism where multiple input sources (features) are combined to create a single output, and inter-output parallelism where multiple, independent outputs (features) are computed simultaneously. Workloads differ significantly across different CNN applications, and across different layers of a CNN. Second, the number of processing elements in an architecture continues to scale (as per Moore's law) much faster than off-chip memory bandwidth (or pin-count) of chips. Based on these two observations, we show that for a given number of processing elements and off-chip memory bandwidth, a new CNN hardware architecture that dynamically configures the hardware on-the-fly to match the specific mix of parallelism in a given workload gives the best throughput performance. Our CNN compiler automatically translates high abstraction network specification into a parallel microprogram (a sequence of low-level VLIW instructions) that is mapped, scheduled and executed by the coprocessor. Compared to a 2.3 GHz quad-core, dual socket Intel Xeon, 1.35 GHz C870 GPU, and a 200 MHz FPGA implementation, our 120 MHz dynamically configurable architecture is 4x to 8x faster. This is the first CNN architecture to achieve real-time video stream processing (25 to 30 frames per second) on a wide range of object detection and recognition tasks."
            },
            "slug": "A-dynamically-configurable-coprocessor-for-neural-Chakradhar-Sankaradass",
            "title": {
                "fragments": [],
                "text": "A dynamically configurable coprocessor for convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This is the first CNN architecture to achieve real-time video stream processing (25 to 30 frames per second) on a wide range of object detection and recognition tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ISCA"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7377735"
                        ],
                        "name": "Yunji Chen",
                        "slug": "Yunji-Chen",
                        "structuredName": {
                            "firstName": "Yunji",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunji Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068286576"
                        ],
                        "name": "Tao Luo",
                        "slug": "Tao-Luo",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39419985"
                        ],
                        "name": "Shaoli Liu",
                        "slug": "Shaoli-Liu",
                        "structuredName": {
                            "firstName": "Shaoli",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoli Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145407329"
                        ],
                        "name": "Shijin Zhang",
                        "slug": "Shijin-Zhang",
                        "structuredName": {
                            "firstName": "Shijin",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijin Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37167270"
                        ],
                        "name": "Liqiang He",
                        "slug": "Liqiang-He",
                        "structuredName": {
                            "firstName": "Liqiang",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liqiang He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110368816"
                        ],
                        "name": "Jia Wang",
                        "slug": "Jia-Wang",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3353457"
                        ],
                        "name": "Ling Li",
                        "slug": "Ling-Li",
                        "structuredName": {
                            "firstName": "Ling",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ling Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144049725"
                        ],
                        "name": "Tianshi Chen",
                        "slug": "Tianshi-Chen",
                        "structuredName": {
                            "firstName": "Tianshi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianshi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719934"
                        ],
                        "name": "Zhiwei Xu",
                        "slug": "Zhiwei-Xu",
                        "structuredName": {
                            "firstName": "Zhiwei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiwei Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145550877"
                        ],
                        "name": "Ninghui Sun",
                        "slug": "Ninghui-Sun",
                        "structuredName": {
                            "firstName": "Ninghui",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ninghui Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731764"
                        ],
                        "name": "O. Temam",
                        "slug": "O.-Temam",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Temam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Temam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 49
                            }
                        ],
                        "text": "Examples: Variants of the NLR dataflow appear in [21, 22, 24]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6838992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4157ed3db4c656854e69931cb6089b64b08784b9",
            "isKey": false,
            "numCitedBy": 1065,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects."
            },
            "slug": "DaDianNao:-A-Machine-Learning-Supercomputer-Chen-Luo",
            "title": {
                "fragments": [],
                "text": "DaDianNao: A Machine-Learning Supercomputer"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article introduces a custom multi-chip machine-learning architecture, showing that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system."
            },
            "venue": {
                "fragments": [],
                "text": "2014 47th Annual IEEE/ACM International Symposium on Microarchitecture"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6647160"
                        ],
                        "name": "Vinayak Gokhale",
                        "slug": "Vinayak-Gokhale",
                        "structuredName": {
                            "firstName": "Vinayak",
                            "lastName": "Gokhale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vinayak Gokhale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2592429"
                        ],
                        "name": "Jonghoon Jin",
                        "slug": "Jonghoon-Jin",
                        "structuredName": {
                            "firstName": "Jonghoon",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonghoon Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2130620"
                        ],
                        "name": "A. Dundar",
                        "slug": "A.-Dundar",
                        "structuredName": {
                            "firstName": "Aysegul",
                            "lastName": "Dundar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dundar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940285"
                        ],
                        "name": "B. Martini",
                        "slug": "B.-Martini",
                        "structuredName": {
                            "firstName": "Berin",
                            "lastName": "Martini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Martini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889774"
                        ],
                        "name": "E. Culurciello",
                        "slug": "E.-Culurciello",
                        "structuredName": {
                            "firstName": "Eugenio",
                            "lastName": "Culurciello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Culurciello"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "These state-of-the-art CNNs [2\u2013 5] are orders of magnitude larger than those used in the 1990s [9], requiring up to hundreds of megabytes for filter weight storage and 30k-600k operations per input pixel."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7311716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "233b1774f28c9972df2dfcf20dfbb0df45792bd0",
            "isKey": false,
            "numCitedBy": 253,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep networks are state-of-the-art models used for understanding the content of images, videos, audio and raw input data. Current computing systems are not able to run deep network models in real-time with low power consumption. In this paper we present nn-X: a scalable, low-power coprocessor for enabling real-time execution of deep neural networks. nn-X is implemented on programmable logic devices and comprises an array of configurable processing elements called collections. These collections perform the most common operations in deep networks: convolution, subsampling and non-linear functions. The nn-X system includes 4 high-speed direct memory access interfaces to DDR3 memory and two ARM Cortex-A9 processors. Each port is capable of a sustained throughput of 950 MB/s in full duplex. nn-X is able to achieve a peak performance of 227 G-ops/s, a measured performance in deep learning applications of up to 200 G-ops/s while consuming less than 4 watts of power. This translates to a performance per power improvement of 10 to 100 times that of conventional mobile and desktop processors."
            },
            "slug": "A-240-G-ops/s-Mobile-Coprocessor-for-Deep-Neural-Gokhale-Jin",
            "title": {
                "fragments": [],
                "text": "A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The nn-X system is presented, a scalable, low-power coprocessor for enabling real-time execution of deep neural networks, able to achieve a peak performance of 227 G-ops/s, which translates to a performance per power improvement of 10 to 100 times that of conventional mobile and desktop processors."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111573067"
                        ],
                        "name": "Chen Zhang",
                        "slug": "Chen-Zhang",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50492686"
                        ],
                        "name": "Peng Li",
                        "slug": "Peng-Li",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695860"
                        ],
                        "name": "Guangyu Sun",
                        "slug": "Guangyu-Sun",
                        "structuredName": {
                            "firstName": "Guangyu",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guangyu Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2238828"
                        ],
                        "name": "Yijin Guan",
                        "slug": "Yijin-Guan",
                        "structuredName": {
                            "firstName": "Yijin",
                            "lastName": "Guan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yijin Guan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37525788"
                        ],
                        "name": "Bingjun Xiao",
                        "slug": "Bingjun-Xiao",
                        "structuredName": {
                            "firstName": "Bingjun",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bingjun Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259796"
                        ],
                        "name": "J. Cong",
                        "slug": "J.-Cong",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Cong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 49
                            }
                        ],
                        "text": "Examples: Variants of the NLR dataflow appear in [21, 22, 24]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207220904,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c91eb0f9bbae8e2d3d007db73b8422b61ed1d68",
            "isKey": false,
            "numCitedBy": 1482,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional neural network (CNN) has been widely employed for image recognition because it can achieve high accuracy by emulating behavior of optic nerves in living creatures. Recently, rapid growth of modern applications based on deep learning algorithms has further improved research and implementations. Especially, various accelerators for deep CNN have been proposed based on FPGA platform because it has advantages of high performance, reconfigurability, and fast development round, etc. Although current FPGA accelerators have demonstrated better performance over generic processors, the accelerator design space has not been well exploited. One critical problem is that the computation throughput may not well match the memory bandwidth provided an FPGA platform. Consequently, existing approaches cannot achieve best performance due to under-utilization of either logic resource or memory bandwidth. At the same time, the increasing complexity and scalability of deep learning applications aggravate this problem. In order to overcome this problem, we propose an analytical design scheme using the roofline model. For any solution of a CNN design, we quantitatively analyze its computing throughput and required memory bandwidth using various optimization techniques, such as loop tiling and transformation. Then, with the help of rooine model, we can identify the solution with best performance and lowest FPGA resource requirement. As a case study, we implement a CNN accelerator on a VC707 FPGA board and compare it to previous approaches. Our implementation achieves a peak performance of 61.62 GFLOPS under 100MHz working frequency, which outperform previous approaches significantly."
            },
            "slug": "Optimizing-FPGA-based-Accelerator-Design-for-Deep-Zhang-Li",
            "title": {
                "fragments": [],
                "text": "Optimizing FPGA-based Accelerator Design for Deep Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work implements a CNN accelerator on a VC707 FPGA board and compares it to previous approaches, achieving a peak performance of 61.62 GFLOPS under 100MHz working frequency, which outperform previous approaches significantly."
            },
            "venue": {
                "fragments": [],
                "text": "FPGA"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701257"
                        ],
                        "name": "L. Cavigelli",
                        "slug": "L.-Cavigelli",
                        "structuredName": {
                            "firstName": "Lukas",
                            "lastName": "Cavigelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Cavigelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803355"
                        ],
                        "name": "David Gschwend",
                        "slug": "David-Gschwend",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Gschwend",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Gschwend"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055035715"
                        ],
                        "name": "Christoph Mayer",
                        "slug": "Christoph-Mayer",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Mayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christoph Mayer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34660747"
                        ],
                        "name": "Samuel Willi",
                        "slug": "Samuel-Willi",
                        "structuredName": {
                            "firstName": "Samuel",
                            "lastName": "Willi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samuel Willi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776389"
                        ],
                        "name": "Beat Muheim",
                        "slug": "Beat-Muheim",
                        "structuredName": {
                            "firstName": "Beat",
                            "lastName": "Muheim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beat Muheim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710649"
                        ],
                        "name": "L. Benini",
                        "slug": "L.-Benini",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Benini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Benini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "These state-of-the-art CNNs [2\u2013 5] are orders of magnitude larger than those used in the 1990s [9], requiring up to hundreds of megabytes for filter weight storage and 30k-600k operations per input pixel."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7289957,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd9caf7e34acd8ad47fd35fd9d81bd22db202a81",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Today advanced computer vision (CV) systems of ever increasing complexity are being deployed in a growing number of application scenarios with strong real-time and power constraints. Current trends in CV clearly show a rise of neural network-based algorithms, which have recently broken many object detection and localization records. These approaches are very flexible and can be used to tackle many different challenges by only changing their parameters. In this paper, we present the first convolutional network accelerator which is scalable to network sizes that are currently only handled by workstation GPUs, but remains within the power envelope of embedded systems. The architecture has been implemented on 3.09 mm2 core area in UMC 65 nm technology, capable of a throughput of 274 GOp/s at 369 GOp/s/W with an external memory bandwidth of just 525 MB/s full-duplex \" a decrease of more than 90% from previous work."
            },
            "slug": "Origami:-A-Convolutional-Network-Accelerator-Cavigelli-Gschwend",
            "title": {
                "fragments": [],
                "text": "Origami: A Convolutional Network Accelerator"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper presents the first convolutional network accelerator which is scalable to network sizes that are currently only handled by workstation GPUs, but remains within the power envelope of embedded systems."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Great Lakes Symposium on VLSI"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368774"
                        ],
                        "name": "Seongwook Park",
                        "slug": "Seongwook-Park",
                        "structuredName": {
                            "firstName": "Seongwook",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seongwook Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1928148"
                        ],
                        "name": "Kyeongryeol Bong",
                        "slug": "Kyeongryeol-Bong",
                        "structuredName": {
                            "firstName": "Kyeongryeol",
                            "lastName": "Bong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyeongryeol Bong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2966754"
                        ],
                        "name": "Dongjoo Shin",
                        "slug": "Dongjoo-Shin",
                        "structuredName": {
                            "firstName": "Dongjoo",
                            "lastName": "Shin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dongjoo Shin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2232915"
                        ],
                        "name": "Jinmook Lee",
                        "slug": "Jinmook-Lee",
                        "structuredName": {
                            "firstName": "Jinmook",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinmook Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2267220"
                        ],
                        "name": "Sungpill Choi",
                        "slug": "Sungpill-Choi",
                        "structuredName": {
                            "firstName": "Sungpill",
                            "lastName": "Choi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sungpill Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678901"
                        ],
                        "name": "H. Yoo",
                        "slug": "H.-Yoo",
                        "structuredName": {
                            "firstName": "Hoi-Jun",
                            "lastName": "Yoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Yoo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 3252800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbaebcde717a6669b40ebbc598ddb434d8ca93e4",
            "isKey": false,
            "numCitedBy": 87,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, deep learning (DL) has become a popular approach for big-data analysis in image retrieval with high accuracy [1]. As Fig. 4.6.1 shows, various applications, such as text, 2D image and motion recognition use DL due to its best-in-class recognition accuracy. There are 2 types of DL: supervised DL with labeled data and unsupervised DL with unlabeled data. With unsupervised DL, most of learning time is spent in massively iterative weight updates for a restricted Boltzmann machine [2]. For a -100MB training dataset, >100 TOP computational capability and ~40GB/s IO and SRAM data bandwidth is required. So, a 3.4GHz CPU needs >10 hours learning time with a -100K input-vector dataset and takes ~1 second for recognition, which is far from real-time processing. Thus, DL is typically done using cloud servers or high-performance GPU environments with learning-on-server capability. However, the wide use of smart portable devices, such as smartphones and tablets, results in many applications which need big-data processing with machine learning, such as tagging private photos in personal devices. A high-performance and energy-efficient DL/DI (deep inference) processor is required to realize user-centric pattern recognition in portable devices."
            },
            "slug": "4.6-A1.93TOPS/W-scalable-deep-learning/inference-Park-Bong",
            "title": {
                "fragments": [],
                "text": "4.6 A1.93TOPS/W scalable deep learning/inference processor with tetra-parallel MIMD architecture for big-data applications"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "A high-performance and energy-efficient DL/DI (deep inference) processor is required to realize user-centric pattern recognition in portable devices."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Solid-State Circuits Conference - (ISSCC) Digest of Technical Papers"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157222093"
                        ],
                        "name": "Wei Liu",
                        "slug": "Wei-Liu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39863668"
                        ],
                        "name": "Andrew Rabinovich",
                        "slug": "Andrew-Rabinovich",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Rabinovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Rabinovich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "This is realized by exploiting local data reuse of filter weights and feature map pixels, i.e., activations, in the high-dimensional convolutions, and minimizing data movement of partial sum accumulations."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206592484,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e15cf50aa89fee8535703b9f9512fca5bfc43327",
            "isKey": false,
            "numCitedBy": 29921,
            "numCiting": 278,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a deep convolutional neural network architecture codenamed Inception that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14). The main hallmark of this architecture is the improved utilization of the computing resources inside the network. By a carefully crafted design, we increased the depth and width of the network while keeping the computational budget constant. To optimize quality, the architectural decisions were based on the Hebbian principle and the intuition of multi-scale processing. One particular incarnation used in our submission for ILSVRC14 is called GoogLeNet, a 22 layers deep network, the quality of which is assessed in the context of classification and detection."
            },
            "slug": "Going-deeper-with-convolutions-Szegedy-Liu",
            "title": {
                "fragments": [],
                "text": "Going deeper with convolutions"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A deep convolutional neural network architecture codenamed Inception is proposed that achieves the new state of the art for classification and detection in the ImageNet Large-Scale Visual Recognition Challenge 2014 (ILSVRC14)."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "However, they are not directly applicable for CNN processing for two reasons: \u2022 The filter weights in CNNs are obtained through training\ninstead of fixed in the processing system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "These state-of-the-art CNNs [2\u2013 5] are orders of magnitude larger than those used in the 1990s [9], requiring up to hundreds of megabytes for filter weight storage and 30k-600k operations per input pixel."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "In most of the widely used CNNs, such as AlexNet [2] and VGG16 [3], CONV layers account for over 90% of the overall operations and generate a large amount of data movement."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Before CNNs became mainstream, there was already research on high-efficiency convolution due to its wide applicability in image signal processing (ISP) [40]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "Therefore, they have a significant impact on the throughput and energy efficiency of CNNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "Overall, however, CONV layers still consume approximately 80% of total energy in AlexNet, and the percentage is expected to go even higher in modern CNNs that have more CONV layers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 135
                            }
                        ],
                        "text": "Computation of ACT layers is trivial, and we believe support for the NORM layer can be omitted due to its reduced usage in recent CNNs [3, 5]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "I. INTRODUCTION\nThe recent popularity of deep learning [1], specifically deep convolutional neural networks (CNNs), can be attributed to its ability to achieve unprecedented accuracy for tasks ranging from object recognition [2\u20135] and detection [6, 7] to scene understanding [8]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Modern CNNs are able to achieve superior performance by employing a very deep hierarchy of layers."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14124313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42cf88027de515750f230b23b1a057dc782108",
            "isKey": true,
            "numCitedBy": 63201,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            "slug": "Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049736"
                        ],
                        "name": "Sergey Karayev",
                        "slug": "Sergey-Karayev",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Karayev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Karayev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117314646"
                        ],
                        "name": "Jonathan Long",
                        "slug": "Jonathan-Long",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Long",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Long"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687120"
                        ],
                        "name": "S. Guadarrama",
                        "slug": "S.-Guadarrama",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Guadarrama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Guadarrama"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "Table II CONV/FC LAYER SHAPE CONFIGURATIONS IN ALEXNET [39]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1799558,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bdb186ec4726e00a8051119636d4df3b94043b5",
            "isKey": false,
            "numCitedBy": 13814,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models. The framework is a BSD-licensed C++ library with Python and MATLAB bindings for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures. Caffe fits industry and internet-scale media needs by CUDA GPU computation, processing over 40 million images a day on a single K40 or Titan GPU (approx 2 ms per image). By separating model representation from actual implementation, Caffe allows experimentation and seamless switching among platforms for ease of development and deployment from prototyping machines to cloud environments. Caffe is maintained and developed by the Berkeley Vision and Learning Center (BVLC) with the help of an active community of contributors on GitHub. It powers ongoing research projects, large-scale industrial applications, and startup prototypes in vision, speech, and multimedia."
            },
            "slug": "Caffe:-Convolutional-Architecture-for-Fast-Feature-Jia-Shelhamer",
            "title": {
                "fragments": [],
                "text": "Caffe: Convolutional Architecture for Fast Feature Embedding"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "Caffe provides multimedia scientists and practitioners with a clean and modifiable framework for state-of-the-art deep learning algorithms and a collection of reference models for training and deploying general-purpose convolutional neural networks and other deep models efficiently on commodity architectures."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 52
                            }
                        ],
                        "text": "4), and whose functionality has been verified using AlexNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 10
                            }
                        ],
                        "text": "From five [2] to even several hundred [5] CONV layers are commonly used in recent CNN models."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 92
                            }
                        ],
                        "text": "10 shows the energy breakdown across the storage hierarchy in the 5 CONV and 3 FC layers of AlexNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 74
                            }
                        ],
                        "text": "We run the same experiments as in Section VII-B but with the FC layers of AlexNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 49
                            }
                        ],
                        "text": "In most of the widely used CNNs, such as AlexNet [2] and VGG16 [3], CONV layers account for over 90% of the overall operations and generate a large amount of data movement."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 135
                            }
                        ],
                        "text": "We sweep the number of PEs from 32 to 288 and adjust the size of RF and global buffer to find the lowest energy cost in CONV layers of AlexNet for each setup."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 153
                            }
                        ],
                        "text": "The analytical model uses energy/area numbers from a commercial 65nm process and all R/W numbers are exact based on real CNN shape configurations, i.e., AlexNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 102
                            }
                        ],
                        "text": "The simulations assume 16 bits per word, and the result aggregates data from all CONV or FC layers in AlexNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 155
                            }
                        ],
                        "text": "We compare the RS dataflow with existing dataflows in (1) DRAM accesses, (2) energy consumption and (3) energydelay product (EDP) using the CONV layers of AlexNet."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 81
                            }
                        ],
                        "text": "Overall, however, CONV layers still consume approximately 80% of total energy in AlexNet, and the percentage is expected to go even higher in modern CNNs that have more CONV layers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 43
                            }
                        ],
                        "text": "Table II shows the shape configurations of AlexNet as an example."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 44
                            }
                        ],
                        "text": "Experiments using the CNN configurations of AlexNet show that the proposed RS dataflow is more energy efficient than existing dataflows in both convolutional (1.4\u00d7 to 2.5\u00d7) and fully-connected layers (at least 1.3\u00d7 for batch size larger than 16)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "AlexNet [2] is used as the CNN model for benchmarking due to its high popularity."
                    },
                    "intents": []
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": true,
            "numCitedBy": 82053,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143840275"
                        ],
                        "name": "Song Han",
                        "slug": "Song-Han",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3123774"
                        ],
                        "name": "Huizi Mao",
                        "slug": "Huizi-Mao",
                        "structuredName": {
                            "firstName": "Huizi",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huizi Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80724002"
                        ],
                        "name": "W. Dally",
                        "slug": "W.-Dally",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dally",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dally"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The DRAM accesses, however, can be reduced by techniques such as pruning and quantization of the values [38]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Even though FC layers use most of the filter weights, a recent study has demonstrated that these weights are largely compressible to 1\u20135% of their original size [38], which greatly reduces the Shape Parameter Description N batch size of 3D fmaps M # of 3D filters / # of ofmap channels C # of ifmap/filter channels H ifmap plane width/height R filter plane width/height (= H in FC) E ofmap plane width/height (= 1 in FC)"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2134321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "642d0f49b7826adcf986616f4af77e736229990f",
            "isKey": false,
            "numCitedBy": 5732,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency."
            },
            "slug": "Deep-Compression:-Compressing-Deep-Neural-Network-Han-Mao",
            "title": {
                "fragments": [],
                "text": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work introduces \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2329060"
                        ],
                        "name": "M. Sankaradass",
                        "slug": "M.-Sankaradass",
                        "structuredName": {
                            "firstName": "Murugan",
                            "lastName": "Sankaradass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sankaradass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101580"
                        ],
                        "name": "V. Jakkula",
                        "slug": "V.-Jakkula",
                        "structuredName": {
                            "firstName": "Venkata",
                            "lastName": "Jakkula",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Jakkula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2468150"
                        ],
                        "name": "S. Cadambi",
                        "slug": "S.-Cadambi",
                        "structuredName": {
                            "firstName": "Srihari",
                            "lastName": "Cadambi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Cadambi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752242"
                        ],
                        "name": "S. Chakradhar",
                        "slug": "S.-Chakradhar",
                        "structuredName": {
                            "firstName": "Srimat",
                            "lastName": "Chakradhar",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chakradhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1844230"
                        ],
                        "name": "Igor Durdanovic",
                        "slug": "Igor-Durdanovic",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Durdanovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Igor Durdanovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3165487"
                        ],
                        "name": "E. Cosatto",
                        "slug": "E.-Cosatto",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Cosatto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Cosatto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775043"
                        ],
                        "name": "H. Graf",
                        "slug": "H.-Graf",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Graf",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Graf"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 276
                            }
                        ],
                        "text": "I. INTRODUCTION\nThe recent popularity of deep learning [1], specifically deep convolutional neural networks (CNNs), can be attributed to its ability to achieve unprecedented accuracy for tasks ranging from object recognition [2\u20135] and detection [6, 7] to scene understanding [8]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9736053,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4f40ea0248653d4ffb6ef4857cd23f0f713d8c69",
            "isKey": false,
            "numCitedBy": 206,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a massively parallel coprocessor for accelerating Convolutional Neural Networks (CNNs), a class of important machine learning algorithms. The coprocessor functional units, consisting of parallel 2D convolution primitives and programmable units performing sub-sampling and non-linear functions specific to CNNs, implement a \u201cmeta-operator\u201d to which a CNN may be compiled to. The coprocessor is serviced by distributed off-chip memory banks with large data bandwidth. As a key feature, we use low precision data and further increase the effective memory bandwidth by packing multiple words in every memory operation, and leverage the algorithm\u2019s simple data access patterns to use off-chip memory as a scratchpad for intermediate data, critical for CNNs. A CNN is mapped to the coprocessor hardware primitives with instructions to transfer data between the memory and coprocessor. We have implemented a prototype of the CNN coprocessor on an off-the-shelf PCI FPGA card with a single Xilinx Virtex5 LX330T FPGA and 4 DDR2 memory banks totaling 1GB. The coprocessor prototype can process at the rate of 3.4 billion multiply accumulates per second (GMACs) for CNN forward propagation, a speed that is 31x faster than a software implementation on a 2.2 GHz AMD Opteron processor. For a complete face recognition application with the CNN on the coprocessor and the rest of the image processing tasks on the host, the prototype is 6-10x faster, depending on the host-coprocessor bandwidth."
            },
            "slug": "A-Massively-Parallel-Coprocessor-for-Convolutional-Sankaradass-Jakkula",
            "title": {
                "fragments": [],
                "text": "A Massively Parallel Coprocessor for Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A massively parallel coprocessor for accelerating Convolutional Neural Networks (CNNs), a class of important machine learning algorithms, is presented, which uses low precision data and further increase the effective memory bandwidth by packing multiple words in every memory operation."
            },
            "venue": {
                "fragments": [],
                "text": "2009 20th IEEE International Conference on Application-specific Systems, Architectures and Processors"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259796"
                        ],
                        "name": "J. Cong",
                        "slug": "J.-Cong",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Cong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082437233"
                        ],
                        "name": "Bing-Yu Xiao",
                        "slug": "Bing-Yu-Xiao",
                        "structuredName": {
                            "firstName": "Bing-Yu",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing-Yu Xiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "Convolutions account for over 90% of the CNN operations and dominates runtime [10]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16946782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5f1beada9e269b2a7faed8dfe936919ac0c2397",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional Neural Networks (CNNs) have been successfully used for many computer vision applications. It would be beneficial to these applications if the computational workload of CNNs could be reduced. In this work we analyze the linear algebraic properties of CNNs and propose an algorithmic modification to reduce their computational workload. An up to a 47% reduction can be achieved without any change in the image recognition results or the addition of any hardware accelerators."
            },
            "slug": "Minimizing-Computation-in-Convolutional-Neural-Cong-Xiao",
            "title": {
                "fragments": [],
                "text": "Minimizing Computation in Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work analyzes the linear algebraic properties of CNNs and proposes an algorithmic modification to reduce their computational workload, achieving up to a 47% reduction."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145291669"
                        ],
                        "name": "Bolei Zhou",
                        "slug": "Bolei-Zhou",
                        "structuredName": {
                            "firstName": "Bolei",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bolei Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2677488"
                        ],
                        "name": "\u00c0. Lapedriza",
                        "slug": "\u00c0.-Lapedriza",
                        "structuredName": {
                            "firstName": "\u00c0gata",
                            "lastName": "Lapedriza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c0. Lapedriza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599257"
                        ],
                        "name": "Jianxiong Xiao",
                        "slug": "Jianxiong-Xiao",
                        "structuredName": {
                            "firstName": "Jianxiong",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianxiong Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143805211"
                        ],
                        "name": "A. Torralba",
                        "slug": "A.-Torralba",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Torralba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Torralba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143868587"
                        ],
                        "name": "A. Oliva",
                        "slug": "A.-Oliva",
                        "structuredName": {
                            "firstName": "Aude",
                            "lastName": "Oliva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Oliva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 259
                            }
                        ],
                        "text": "The recent popularity of deep learning [1], specifically deep convolutional neural networks (CNNs), can be attributed to its ability to achieve unprecedented accuracy for tasks ranging from object recognition [2\u20135] and detection [6, 7] to scene understanding [8]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1849990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9667f8264745b626c6173b1310e2ff0298b09cfc",
            "isKey": false,
            "numCitedBy": 2621,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Scene recognition is one of the hallmark tasks of computer vision, allowing definition of a context for object recognition. Whereas the tremendous recent progress in object recognition tasks is due to the availability of large datasets like ImageNet and the rise of Convolutional Neural Networks (CNNs) for learning high-level features, performance at scene recognition has not attained the same level of success. This may be because current deep features trained from ImageNet are not competitive enough for such tasks. Here, we introduce a new scene-centric database called Places with over 7 million labeled pictures of scenes. We propose new methods to compare the density and diversity of image datasets and show that Places is as dense as other scene datasets and has more diversity. Using CNN, we learn deep features for scene recognition tasks, and establish new state-of-the-art results on several scene-centric datasets. A visualization of the CNN layers' responses allows us to show differences in the internal representations of object-centric and scene-centric networks."
            },
            "slug": "Learning-Deep-Features-for-Scene-Recognition-using-Zhou-Lapedriza",
            "title": {
                "fragments": [],
                "text": "Learning Deep Features for Scene Recognition using Places Database"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A new scene-centric database called Places with over 7 million labeled pictures of scenes is introduced with new methods to compare the density and diversity of image datasets and it is shown that Places is as dense as other scene datasets and has more diversity."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2983898"
                        ],
                        "name": "Ross B. Girshick",
                        "slug": "Ross-B.-Girshick",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Girshick",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ross B. Girshick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7408951"
                        ],
                        "name": "Jeff Donahue",
                        "slug": "Jeff-Donahue",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Donahue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Donahue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153652147"
                        ],
                        "name": "J. Malik",
                        "slug": "J.-Malik",
                        "structuredName": {
                            "firstName": "Jitendra",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 229
                            }
                        ],
                        "text": "The recent popularity of deep learning [1], specifically deep convolutional neural networks (CNNs), can be attributed to its ability to achieve unprecedented accuracy for tasks ranging from object recognition [2\u20135] and detection [6, 7] to scene understanding [8]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 215827080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f4df08d9072fc2ac181b7fced6a245315ce05c8",
            "isKey": false,
            "numCitedBy": 17382,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Object detection performance, as measured on the canonical PASCAL VOC dataset, has plateaued in the last few years. The best-performing methods are complex ensemble systems that typically combine multiple low-level image features with high-level context. In this paper, we propose a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%. Our approach combines two key insights: (1) one can apply high-capacity convolutional neural networks (CNNs) to bottom-up region proposals in order to localize and segment objects and (2) when labeled training data is scarce, supervised pre-training for an auxiliary task, followed by domain-specific fine-tuning, yields a significant performance boost. Since we combine region proposals with CNNs, we call our method R-CNN: Regions with CNN features. We also present experiments that provide insight into what the network learns, revealing a rich hierarchy of image features. Source code for the complete system is available at http://www.cs.berkeley.edu/~rbg/rcnn."
            },
            "slug": "Rich-Feature-Hierarchies-for-Accurate-Object-and-Girshick-Donahue",
            "title": {
                "fragments": [],
                "text": "Rich Feature Hierarchies for Accurate Object Detection and Semantic Segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper proposes a simple and scalable detection algorithm that improves mean average precision (mAP) by more than 30% relative to the previous best result on VOC 2012 -- achieving a mAP of 53.3%."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2872576"
                        ],
                        "name": "W. Qadeer",
                        "slug": "W.-Qadeer",
                        "structuredName": {
                            "firstName": "Wajahat",
                            "lastName": "Qadeer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Qadeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37116969"
                        ],
                        "name": "R. Hameed",
                        "slug": "R.-Hameed",
                        "structuredName": {
                            "firstName": "Rehan",
                            "lastName": "Hameed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hameed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788630"
                        ],
                        "name": "O. Shacham",
                        "slug": "O.-Shacham",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Shacham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Shacham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72076105"
                        ],
                        "name": "P. Venkatesan",
                        "slug": "P.-Venkatesan",
                        "structuredName": {
                            "firstName": "Preethi",
                            "lastName": "Venkatesan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Venkatesan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700331"
                        ],
                        "name": "C. Kozyrakis",
                        "slug": "C.-Kozyrakis",
                        "structuredName": {
                            "firstName": "Christoforos",
                            "lastName": "Kozyrakis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kozyrakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144764327"
                        ],
                        "name": "M. Horowitz",
                        "slug": "M.-Horowitz",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Horowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Horowitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Before CNNs became mainstream, there was already research on high-efficiency convolution due to its wide applicability in image signal processing (ISP) [40]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3117823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d50d7cd78126cd4df5dda62f1a75c89d095bc8a5",
            "isKey": false,
            "numCitedBy": 165,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper focuses on the trade-off between flexibility and efficiency in specialized computing. We observe that specialized units achieve most of their efficiency gains by tuning data storage and compute structures and their connectivity to the data-flow and data-locality patterns in the kernels. Hence, by identifying key data-flow patterns used in a domain, we can create efficient engines that can be programmed and reused across a wide range of applications. We present an example, the Convolution Engine (CE), specialized for the convolution-like data-flow that is common in computational photography, image processing, and video processing applications. CE achieves energy efficiency by capturing data reuse patterns, eliminating data transfer overheads, and enabling a large number of operations per memory access. We quantify the tradeoffs in efficiency and flexibility and demonstrate that CE is within a factor of 2-3x of the energy and area efficiency of custom units optimized for a single kernel. CE improves energy and area efficiency by 8-15x over a SIMD engine for most applications."
            },
            "slug": "Convolution-engine:-balancing-efficiency-&-in-Qadeer-Hameed",
            "title": {
                "fragments": [],
                "text": "Convolution engine: balancing efficiency & flexibility in specialized computing"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Convolution Engine, specialized for the convolution-like data-flow that is common in computational photography, image processing, and video processing applications, is presented and it is demonstrated that CE is within a factor of 2-3x of the energy and area efficiency of custom units optimized for a single kernel."
            },
            "venue": {
                "fragments": [],
                "text": "ISCA"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 100
                            }
                        ],
                        "text": "However, they are not directly applicable for CNN processing for two reasons: \u2022 The filter weights in CNNs are obtained through training\ninstead of fixed in the processing system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "These state-of-the-art CNNs [2\u2013 5] are orders of magnitude larger than those used in the 1990s [9], requiring up to hundreds of megabytes for filter weight storage and 30k-600k operations per input pixel."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 38
                            }
                        ],
                        "text": "From five [2] to even several hundred [5] CONV layers are commonly used in recent CNN models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "In most of the widely used CNNs, such as AlexNet [2] and VGG16 [3], CONV layers account for over 90% of the overall operations and generate a large amount of data movement."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Before CNNs became mainstream, there was already research on high-efficiency convolution due to its wide applicability in image signal processing (ISP) [40]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "Therefore, they have a significant impact on the throughput and energy efficiency of CNNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 149
                            }
                        ],
                        "text": "Overall, however, CONV layers still consume approximately 80% of total energy in AlexNet, and the percentage is expected to go even higher in modern CNNs that have more CONV layers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 135
                            }
                        ],
                        "text": "Computation of ACT layers is trivial, and we believe support for the NORM layer can be omitted due to its reduced usage in recent CNNs [3, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "I. INTRODUCTION\nThe recent popularity of deep learning [1], specifically deep convolutional neural networks (CNNs), can be attributed to its ability to achieve unprecedented accuracy for tasks ranging from object recognition [2\u20135] and detection [6, 7] to scene understanding [8]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Modern CNNs are able to achieve superior performance by employing a very deep hierarchy of layers."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": true,
            "numCitedBy": 97664,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46725379"
                        ],
                        "name": "Jesmin Jahan Tithi",
                        "slug": "Jesmin-Jahan-Tithi",
                        "structuredName": {
                            "firstName": "Jesmin",
                            "lastName": "Tithi",
                            "middleNames": [
                                "Jahan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesmin Jahan Tithi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764152"
                        ],
                        "name": "N. Crago",
                        "slug": "N.-Crago",
                        "structuredName": {
                            "firstName": "Neal",
                            "lastName": "Crago",
                            "middleNames": [
                                "Clayton"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Crago"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775477"
                        ],
                        "name": "J. Emer",
                        "slug": "J.-Emer",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Emer",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Emer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "The implementation of the RS dataflow in Eyeriss is inspired by the idea of applying a strip mining technique in a spatial architecture [42]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8572615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b82108089107d726ad6b4167ed59b3de78654f5",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we demonstrate the ability of spatial architectures to significantly improve both runtime performance and energy efficiency on edit distance, a broadly used dynamic programming algorithm. Spatial architectures are an emerging class of application accelerators that consist of a network of many small and efficient processing elements that can be exploited by a large domain of applications. In this paper, we utilize the dataflow characteristics and inherent pipeline parallelism within the edit distance algorithm to develop efficient and scalable implementations on a previously proposed spatial accelerator. We evaluate our edit distance implementations using a cycle-accurate performance and physical design model of a previously proposed triggered instruction-based spatial architecture in order to compare against real performance and power measurements on an x86 processor. We show that when chip area is normalized between the two platforms, it is possible to get more than a 50\u00d7 runtime performance improvement and over 100\u00d7 reduction in energy consumption compared to an optimized and vectorized x86 implementation. This dramatic improvement comes from leveraging the massive parallelism available in spatial architectures and from the dramatic reduction of expensive memory accesses through conversion to relatively inexpensive local communication."
            },
            "slug": "Exploiting-spatial-architectures-for-edit-distance-Tithi-Crago",
            "title": {
                "fragments": [],
                "text": "Exploiting spatial architectures for edit distance algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that when chip area is normalized between the two platforms, it is possible to get more than a 50\u00d7 runtime performance improvement and over 100\u00d7 reduction in energy consumption compared to an optimized and vectorized x86 implementation."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE International Symposium on Performance Analysis of Systems and Software (ISPASS)"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142556"
                        ],
                        "name": "Pierre Sermanet",
                        "slug": "Pierre-Sermanet",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Sermanet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pierre Sermanet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060028"
                        ],
                        "name": "D. Eigen",
                        "slug": "D.-Eigen",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eigen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eigen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46447747"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276554"
                        ],
                        "name": "R. Fergus",
                        "slug": "R.-Fergus",
                        "structuredName": {
                            "firstName": "Rob",
                            "lastName": "Fergus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Fergus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 229
                            }
                        ],
                        "text": "The recent popularity of deep learning [1], specifically deep convolutional neural networks (CNNs), can be attributed to its ability to achieve unprecedented accuracy for tasks ranging from object recognition [2\u20135] and detection [6, 7] to scene understanding [8]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4071727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1109b663453e78a59e4f66446d71720ac58cec25",
            "isKey": false,
            "numCitedBy": 4388,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an integrated framework for using Convolutional Networks for classification, localization and detection. We show how a multiscale and sliding window approach can be efficiently implemented within a ConvNet. We also introduce a novel deep learning approach to localization by learning to predict object boundaries. Bounding boxes are then accumulated rather than suppressed in order to increase detection confidence. We show that different tasks can be learned simultaneously using a single shared network. This integrated framework is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 (ILSVRC2013) and obtained very competitive results for the detection and classifications tasks. In post-competition work, we establish a new state of the art for the detection task. Finally, we release a feature extractor from our best model called OverFeat."
            },
            "slug": "OverFeat:-Integrated-Recognition,-Localization-and-Sermanet-Eigen",
            "title": {
                "fragments": [],
                "text": "OverFeat: Integrated Recognition, Localization and Detection using Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This integrated framework for using Convolutional Networks for classification, localization and detection is the winner of the localization task of the ImageNet Large Scale Visual Recognition Challenge 2013 and obtained very competitive results for the detection and classifications tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116011472"
                        ],
                        "name": "Suyog Gupta",
                        "slug": "Suyog-Gupta",
                        "structuredName": {
                            "firstName": "Suyog",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suyog Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31651864"
                        ],
                        "name": "A. Agrawal",
                        "slug": "A.-Agrawal",
                        "structuredName": {
                            "firstName": "Ankur",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33678523"
                        ],
                        "name": "K. Gopalakrishnan",
                        "slug": "K.-Gopalakrishnan",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Gopalakrishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Gopalakrishnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32967358"
                        ],
                        "name": "P. Narayanan",
                        "slug": "P.-Narayanan",
                        "structuredName": {
                            "firstName": "Pritish",
                            "lastName": "Narayanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Narayanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 175
                            }
                        ],
                        "text": "We also model the MOCMOP OS dataflow to capture convolutional reuse in the PE array, which exploits more reuse compared with the plain matrix multiplication implementation in [20]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Examples: A variant of MOC-MOP dataflow appears in [20], and variants of SOC-MOP and MOC-SOP dataflows appear in [23] and [18]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 33
                            }
                        ],
                        "text": "Note that the MOC-MOP variant in [20] does not exploit convolutional data reuse since it simply treats the convolutions as a matrix multiplication."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2547043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7cf49e30355633af2db19f35189410c8515e91f",
            "isKey": false,
            "numCitedBy": 1510,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding."
            },
            "slug": "Deep-Learning-with-Limited-Numerical-Precision-Gupta-Agrawal",
            "title": {
                "fragments": [],
                "text": "Deep Learning with Limited Numerical Precision"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256269"
                        ],
                        "name": "C. Farabet",
                        "slug": "C.-Farabet",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Farabet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Farabet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 120
                            }
                        ],
                        "text": "A convolutional neural network (CNN) is constructed by stacking multiple computation layers as a directed acyclic graph [36]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7625356,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c43025c429b1fbf6f1379f61801a1b40834d62e7",
            "isKey": false,
            "numCitedBy": 1590,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Intelligent tasks, such as visual perception, auditory perception, and language understanding require the construction of good internal representations of the world (or \"features\")? which must be invariant to irrelevant variations of the input while, preserving relevant information. A major question for Machine Learning is how to learn such good features automatically. Convolutional Networks (ConvNets) are a biologically-inspired trainable architecture that can learn invariant features. Each stage in a ConvNets is composed of a filter bank, some nonlinearities, and feature pooling layers. With multiple stages, a ConvNet can learn multi-level hierarchies of features. While ConvNets have been successfully deployed in many commercial applications from OCR to video surveillance, they require large amounts of labeled training samples. We describe new unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples. Applications to visual object recognition and vision navigation for off-road mobile robots are described."
            },
            "slug": "Convolutional-networks-and-applications-in-vision-LeCun-Kavukcuoglu",
            "title": {
                "fragments": [],
                "text": "Convolutional networks and applications in vision"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "New unsupervised learning algorithms, and new non-linear stages that allow ConvNets to be trained with very few labeled samples are described, including one for visual object recognition and vision navigation for off-road mobile robots."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2010 IEEE International Symposium on Circuits and Systems"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3003738"
                        ],
                        "name": "Sharan Chetlur",
                        "slug": "Sharan-Chetlur",
                        "structuredName": {
                            "firstName": "Sharan",
                            "lastName": "Chetlur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sharan Chetlur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2266717"
                        ],
                        "name": "Cliff Woolley",
                        "slug": "Cliff-Woolley",
                        "structuredName": {
                            "firstName": "Cliff",
                            "lastName": "Woolley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cliff Woolley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101730"
                        ],
                        "name": "Philippe Vandermersch",
                        "slug": "Philippe-Vandermersch",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Vandermersch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philippe Vandermersch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145611200"
                        ],
                        "name": "Jonathan M. Cohen",
                        "slug": "Jonathan-M.-Cohen",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Cohen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan M. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066786849"
                        ],
                        "name": "J. Tran",
                        "slug": "J.-Tran",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2301680"
                        ],
                        "name": "Bryan Catanzaro",
                        "slug": "Bryan-Catanzaro",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Catanzaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan Catanzaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "The inputs can be off-loaded from the CPU or GPU to DRAM and processed by the accelerator."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 97
                            }
                        ],
                        "text": "Many previous papers have proposed specialized CNN dataflows on various platforms, including GPU [14], FPGA [15\u201321], and ASIC [22\u201326]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12330432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31c36d445367ba204244bb74893c5654e31c3869",
            "isKey": false,
            "numCitedBy": 1419,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a library that provides optimized implementations for deep learning primitives. Deep learning workloads are computationally intensive, and optimizing the kernels of deep learning workloads is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized for new processors, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS) [2]. However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, and similarly to the BLAS library, could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36% on a standard model while also reducing memory consumption."
            },
            "slug": "cuDNN:-Efficient-Primitives-for-Deep-Learning-Chetlur-Woolley",
            "title": {
                "fragments": [],
                "text": "cuDNN: Efficient Primitives for Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A library similar in intent to BLAS, with optimized routines for deep learning workloads, that contains routines for GPUs, and similarly to the BLAS library, could be implemented for other platforms."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145933680"
                        ],
                        "name": "Vinay Sriram",
                        "slug": "Vinay-Sriram",
                        "structuredName": {
                            "firstName": "Vinay",
                            "lastName": "Sriram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vinay Sriram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2042941"
                        ],
                        "name": "D. Cox",
                        "slug": "D.-Cox",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780226"
                        ],
                        "name": "K. H. Tsoi",
                        "slug": "K.-H.-Tsoi",
                        "structuredName": {
                            "firstName": "Kuen",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Hung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. H. Tsoi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708627"
                        ],
                        "name": "W. Luk",
                        "slug": "W.-Luk",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Luk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Luk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 1
                            }
                        ],
                        "text": "These state-of-the-art CNNs [2\u2013 5] are orders of magnitude larger than those used in the 1990s [9], requiring up to hundreds of megabytes for filter weight storage and 30k-600k operations per input pixel."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6692330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15ab4f60005ea42d5e57f365a478b28b92ea2238",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Biologically-inspired machine vision algorithms - those that attempt to capture aspects of the computational architecture of the brain - have proven to be a promising class of algorithms for performing a variety of object and face recognition tasks. However these algorithms typically require a large number of arithmetic operations per image frame evaluated. Meanwhile, the increasing ubiquity of inexpensive cameras in a wide array of embedded devices presents an enormous opportunity for the deployment of embedded machine vision systems. As a first step towards an embedded implementation, we consider the main requirements for the design of an embedded processor for biologically-inspired object recognition and demonstrate an FPGA prototype of the V1-like algorithm, a simple biologically-inspired system from the literature [1], [2], [3]. We present a multiple instruction, single data (MISD) pipeline implementation of V1-like, and show that such designs are feasible in an FPGA context, particularly for small frame sizes (e.g. 100\u00d7100). In addition, we show that such an implementation offers good performance per unit silicon area and power dissipation in comparison to traditional CPU and GPU implementations. Finally, we discuss the constraints under which such an embedded strategy would be feasible for a more general biologically inspired face recognition system, and consider paths forward towards a wider range of possible embedded targets."
            },
            "slug": "Towards-an-embedded-biologically-inspired-machine-Sriram-Cox",
            "title": {
                "fragments": [],
                "text": "Towards an embedded biologically-inspired machine vision processor"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A multiple instruction, single data (MISD) pipeline implementation of V1-like, and it is shown that such designs are feasible in an FPGA context, particularly for small frame sizes (e.g. 100\u00d7100)."
            },
            "venue": {
                "fragments": [],
                "text": "2010 International Conference on Field-Programmable Technology"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37116969"
                        ],
                        "name": "R. Hameed",
                        "slug": "R.-Hameed",
                        "structuredName": {
                            "firstName": "Rehan",
                            "lastName": "Hameed",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hameed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2872576"
                        ],
                        "name": "W. Qadeer",
                        "slug": "W.-Qadeer",
                        "structuredName": {
                            "firstName": "Wajahat",
                            "lastName": "Qadeer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Qadeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40402375"
                        ],
                        "name": "M. Wachs",
                        "slug": "M.-Wachs",
                        "structuredName": {
                            "firstName": "Megan",
                            "lastName": "Wachs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wachs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096950929"
                        ],
                        "name": "Omid Azizi",
                        "slug": "Omid-Azizi",
                        "structuredName": {
                            "firstName": "Omid",
                            "lastName": "Azizi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Omid Azizi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3344899"
                        ],
                        "name": "A. Solomatnikov",
                        "slug": "A.-Solomatnikov",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Solomatnikov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Solomatnikov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152840677"
                        ],
                        "name": "Benjamin C. Lee",
                        "slug": "Benjamin-C.-Lee",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Lee",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145326337"
                        ],
                        "name": "S. Richardson",
                        "slug": "S.-Richardson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Richardson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700331"
                        ],
                        "name": "C. Kozyrakis",
                        "slug": "C.-Kozyrakis",
                        "structuredName": {
                            "firstName": "Christoforos",
                            "lastName": "Kozyrakis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kozyrakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144764327"
                        ],
                        "name": "M. Horowitz",
                        "slug": "M.-Horowitz",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Horowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Horowitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "Experiments using the CNN configurations of AlexNet show that the proposed RS dataflow is more energy efficient than existing dataflows in both convolutional (1.4\u00d7 to 2.5\u00d7) and fully-connected layers (at least 1.3\u00d7 for batch size larger than 16)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 3165696,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f59cd7ebe88df0a4da66657faa4d4a50c7c36004",
            "isKey": true,
            "numCitedBy": 464,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": "Due to their high volume, general-purpose processors, and now chip multiprocessors (CMPs), are much more cost effective than ASICs, but lag significantly in terms of performance and energy efficiency. This paper explores the sources of these performance and energy overheads in general-purpose processing systems by quantifying the overheads of a 720p HD H.264 encoder running on a general-purpose CMP system. It then explores methods to eliminate these overheads by transforming the CPU into a specialized system for H.264 encoding. We evaluate the gains from customizations useful to broad classes of algorithms, such as SIMD units, as well as those specific to particular computation, such as customized storage and functional units. The ASIC is 500x more energy efficient than our original four-processor CMP. Broadly applicable optimizations improve performance by 10x and energy by 7x. However, the very low energy costs of actual core ops (100s fJ in 90nm) mean that over 90% of the energy used in these solutions is still \"overhead\". Achieving ASIC-like performance and efficiency requires algorithm-specific optimizations. For each sub-algorithm of H.264, we create a large, specialized functional unit that is capable of executing 100s of operations per instruction. This improves performance and energy by an additional 25x and the final customized CMP matches an ASIC solution's performance within 3x of its energy and within comparable area."
            },
            "slug": "Understanding-sources-of-inefficiency-in-chips-Hameed-Qadeer",
            "title": {
                "fragments": [],
                "text": "Understanding sources of inefficiency in general-purpose chips"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The sources of these performance and energy overheads in general-purpose processing systems are explored by quantifying the overheads of a 720p HD H.264 encoder running on a general- Purpose CMP system and exploring methods to eliminate these overheads by transforming the CPU into a specialized system for H. 264 encoding."
            },
            "venue": {
                "fragments": [],
                "text": "ISCA"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48930265"
                        ],
                        "name": "Venkatraman Govindaraju",
                        "slug": "Venkatraman-Govindaraju",
                        "structuredName": {
                            "firstName": "Venkatraman",
                            "lastName": "Govindaraju",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Venkatraman Govindaraju"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802116"
                        ],
                        "name": "C. Ho",
                        "slug": "C.-Ho",
                        "structuredName": {
                            "firstName": "Chen-Han",
                            "lastName": "Ho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720300"
                        ],
                        "name": "K. Sankaralingam",
                        "slug": "K.-Sankaralingam",
                        "structuredName": {
                            "firstName": "Karthikeyan",
                            "lastName": "Sankaralingam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sankaralingam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 3
                            }
                        ],
                        "text": "Coarse-grained SAs are currently a very popular implementation choice for specialized CNN accelerators for two reasons."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14408065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "319993de8b5ba6f42a416f6cb7f856268c348baa",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Due to limits in technology scaling, energy efficiency of logic devices is decreasing in successive generations. To provide continued performance improvements without increasing power, regardless of the sequential or parallel nature of the application, microarchitectural energy efficiency must improve. We propose Dynamically Specialized Datapaths to improve the energy efficiency of general purpose programmable processors. The key insights of this work are the following. First, applications execute in phases and these phases can be determined by creating a path-tree of basic-blocks rooted at the inner-most loop. Second, specialized datapaths corresponding to these path-trees, which we refer to as DySER blocks, can be constructed by interconnecting a set of heterogeneous computation units with a circuit-switched network. These blocks can be easily integrated with a processor pipeline. A synthesized RTL implementation using an industry 55nm technology library shows a 64-functional-unit DySER block occupies approximately the same area as a 64 KB single-ported SRAM and can execute at 2 GHz. We extend the GCC compiler to identify path-trees and code-mapping to DySER and evaluate the PAR-SEC, SPEC and Parboil benchmarks suites. Our results show that in most cases two DySER blocks can achieve the same performance (within 5%) as having a specialized hardware module for each path-tree. A 64-FU DySER block can cover 12% to 100% of the dynamically executed instruction stream. When integrated with a dual-issue out-of-order processor, two DySER blocks provide geometric mean speedup of 2.1X (1.15X to 10X), and geometric mean energy reduction of 40% (up to 70%), and 60% energy reduction if no performance improvement is required."
            },
            "slug": "Dynamically-Specialized-Datapaths-for-energy-Govindaraju-Ho",
            "title": {
                "fragments": [],
                "text": "Dynamically Specialized Datapaths for energy efficient computing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "D Dynamically Specialized Datapaths are proposed to improve the energy efficiency of general purpose programmable processors and show that in most cases two DySER blocks can achieve the same performance as having a specialized hardware module for each path-tree."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE 17th International Symposium on High Performance Computer Architecture"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145893236"
                        ],
                        "name": "E. Mirsky",
                        "slug": "E.-Mirsky",
                        "structuredName": {
                            "firstName": "Ethan",
                            "lastName": "Mirsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Mirsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680859"
                        ],
                        "name": "A. DeHon",
                        "slug": "A.-DeHon",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "DeHon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. DeHon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "SAs come in two flavors: coarse-grained SAs that consist of tiled arrays of ALU-style PEs connected together via on-chip networks [27\u201329], and fine-grained SAs that are usually in the form of an FPGA."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8954064,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4bfbc9252fd331907aa4dc3759f66b40ad57ee1",
            "isKey": false,
            "numCitedBy": 409,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "MATRIX is a novel, coarse-grain, reconfigurable computing architecture which supports configurable instruction distribution. Device resources are allocated to controlling and describing the computation on a per task basis. Application-specific regularity allows us to compress the resources allocated to instruction control and distribution, in many situations yielding more resources for datapaths and computations. The adaptability is made possible by a multi-level configuration scheme, a unified configurable network supporting both datapaths and instruction distribution, and a coarse-grained building block which can serve as an instruction store, a memory element, or a computational element. In a 0.5 /spl mu/ CMOS process, the 8-bit functional unit at the heart of the MATRIX architecture has a footprint of roughly 1.5 mm/spl times/1.2 mm, making single dies with over a hundred function units practical today. At this process point, 100 MHz operation is easily achievable, allowing MATRIX components to deliver on the order of 10 Gop/s (8-bit ops)."
            },
            "slug": "MATRIX:-a-reconfigurable-computing-architecture-and-Mirsky-DeHon",
            "title": {
                "fragments": [],
                "text": "MATRIX: a reconfigurable computing architecture with configurable instruction distribution and deployable resources"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "MATRIX is a novel, coarse-grain, reconfigurable computing architecture which supports configurable instruction distribution that can serve as an instruction store, a memory element, or a computational element, and the adaptability is made possible by a multi-level configuration scheme."
            },
            "venue": {
                "fragments": [],
                "text": "1996 Proceedings IEEE Symposium on FPGAs for Custom Computing Machines"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2807420"
                        ],
                        "name": "Krishna T. Malladi",
                        "slug": "Krishna-T.-Malladi",
                        "structuredName": {
                            "firstName": "Krishna",
                            "lastName": "Malladi",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Krishna T. Malladi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3383599"
                        ],
                        "name": "Frank A. Nothaft",
                        "slug": "Frank-A.-Nothaft",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Nothaft",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank A. Nothaft"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3379225"
                        ],
                        "name": "Karthika Periyathambi",
                        "slug": "Karthika-Periyathambi",
                        "structuredName": {
                            "firstName": "Karthika",
                            "lastName": "Periyathambi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karthika Periyathambi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152840677"
                        ],
                        "name": "Benjamin C. Lee",
                        "slug": "Benjamin-C.-Lee",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Lee",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700331"
                        ],
                        "name": "C. Kozyrakis",
                        "slug": "C.-Kozyrakis",
                        "structuredName": {
                            "firstName": "Christoforos",
                            "lastName": "Kozyrakis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Kozyrakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144764327"
                        ],
                        "name": "M. Horowitz",
                        "slug": "M.-Horowitz",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Horowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Horowitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 73
                            }
                        ],
                        "text": "Similar to the energy consumption quantification in previous experiments [11, 12, 43], Table IV shows the energy cost of accessing each level relative to a MAC operation under the listed conditions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14680002,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cece950bc68c594e94b8220f1ccdce0eb7b35102",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "To increase datacenter energy efficiency, we need memory systems that keep pace with processor efficiency gains. Currently, servers use DDR3 memory, which is designed for high bandwidth but not for energy proportionality. A system using 20% of the peak DDR3 bandwidth consumes 2.3\u00d7 the energy per bit compared to the energy consumed by a system with fully utilized memory bandwidth. Nevertheless, many datacenter applications stress memory capacity and latency but not memory bandwidth. In response, we architect server memory systems using mobile DRAM devices, trading peak bandwidth for lower energy consumption per bit and more efficient idle modes. We demonstrate 3-5\u00d7 lower memory power, better proportionality, and negligible performance penalties for data-center workloads."
            },
            "slug": "Towards-energy-proportional-datacenter-memory-with-Malladi-Nothaft",
            "title": {
                "fragments": [],
                "text": "Towards energy-proportional datacenter memory with mobile DRAM"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work architects server memory systems using mobile DRAM devices, trading peak bandwidth for lower energy consumption per bit and more efficient idle modes, and demonstrates 3-5\u00d7 lower memory power, better proportionality, and negligible performance penalties for data-center workloads."
            },
            "venue": {
                "fragments": [],
                "text": "2012 39th Annual International Symposium on Computer Architecture (ISCA)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39902371"
                        ],
                        "name": "B. Mei",
                        "slug": "B.-Mei",
                        "structuredName": {
                            "firstName": "Bingfeng",
                            "lastName": "Mei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Mei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2480555"
                        ],
                        "name": "S. Vernalde",
                        "slug": "S.-Vernalde",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Vernalde",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Vernalde"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766789"
                        ],
                        "name": "D. Verkest",
                        "slug": "D.-Verkest",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Verkest",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Verkest"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144865374"
                        ],
                        "name": "H. Man",
                        "slug": "H.-Man",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Man",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Man"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707947"
                        ],
                        "name": "R. Lauwereins",
                        "slug": "R.-Lauwereins",
                        "structuredName": {
                            "firstName": "Rudy",
                            "lastName": "Lauwereins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lauwereins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "SAs come in two flavors: coarse-grained SAs that consist of tiled arrays of ALU-style PEs connected together via on-chip networks [27\u201329], and fine-grained SAs that are usually in the form of an FPGA."
                    },
                    "intents": []
                }
            ],
            "corpusId": 39182312,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a11bfb1db5bd877f5e32a7f23f64b0a68f8b2c9b",
            "isKey": false,
            "numCitedBy": 550,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The coarse-grained reconfigurable architectures have advantages over the traditional FPGAs in terms of delay, area and configuration time. To execute entire applications, most of them combine an instruction set processor(ISP) and a reconfigurable matrix. However, not much attention is paid to the integration of these two parts, which results in high communication overhead and programming difficulty. To address this problem, we propose a novel architecture with tightly coupled very long instruction word (VLIW) processor and coarse-grained reconfigurable matrix. The advantages include simplified programming model, shared resource costs, and reduced communication overhead. To exploit this architecture, our previously developed compiler framework is adapted to the new architecture. The results show that the new architecture has good performance and is very compiler-friendly."
            },
            "slug": "ADRES:-An-Architecture-with-Tightly-Coupled-VLIW-Mei-Vernalde",
            "title": {
                "fragments": [],
                "text": "ADRES: An Architecture with Tightly Coupled VLIW Processor and Coarse-Grained Reconfigurable Matrix"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel architecture with tightly coupled very long instruction word (VLIW) processor and coarse-grained reconfigurable matrix is proposed, which has good performance and is very compiler-friendly."
            },
            "venue": {
                "fragments": [],
                "text": "FPL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143846212"
                        ],
                        "name": "S. Swanson",
                        "slug": "S.-Swanson",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Swanson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Swanson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944675"
                        ],
                        "name": "Andrew Schwerin",
                        "slug": "Andrew-Schwerin",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Schwerin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Schwerin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079536"
                        ],
                        "name": "M. Kim",
                        "slug": "M.-Kim",
                        "structuredName": {
                            "firstName": "Martha",
                            "lastName": "Kim",
                            "middleNames": [
                                "Mercaldi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064339267"
                        ],
                        "name": "Andrew Petersen",
                        "slug": "Andrew-Petersen",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Petersen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Petersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3651513"
                        ],
                        "name": "A. Putnam",
                        "slug": "A.-Putnam",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Putnam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Putnam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3231400"
                        ],
                        "name": "Ken Michelson",
                        "slug": "Ken-Michelson",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "Michelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken Michelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723213"
                        ],
                        "name": "M. Oskin",
                        "slug": "M.-Oskin",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Oskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oskin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2010354"
                        ],
                        "name": "S. Eggers",
                        "slug": "S.-Eggers",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Eggers",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Eggers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "Coarse-grained SAs are currently a very popular implementation choice for specialized CNN accelerators for two reasons."
                    },
                    "intents": []
                }
            ],
            "corpusId": 580383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c77a7f135c2f703a2d27f8ffb05fbedab797882f",
            "isKey": false,
            "numCitedBy": 133,
            "numCiting": 97,
            "paperAbstract": {
                "fragments": [],
                "text": "Silicon technology will continue to provide an exponential increase in the availability of raw transistors. Effectively translating this resource into application performance, however, is an open challenge that conventional superscalar designs will not be able to meet. We present WaveScalar as a scalable alternative to conventional designs. WaveScalar is a dataflow instruction set and execution model designed for scalable, low-complexity/high-performance processors. Unlike previous dataflow machines, WaveScalar can efficiently provide the sequential memory semantics that imperative languages require. To allow programmers to easily express parallelism, WaveScalar supports pthread-style, coarse-grain multithreading and dataflow-style, fine-grain threading. In addition, it permits blending the two styles within an application, or even a single function.\n To execute WaveScalar programs, we have designed a scalable, tile-based processor architecture called the WaveCache. As a program executes, the WaveCache maps the program's instructions onto its array of processing elements (PEs). The instructions remain at their processing elements for many invocations, and as the working set of instructions changes, the WaveCache removes unused instructions and maps new ones in their place. The instructions communicate directly with one another over a scalable, hierarchical on-chip interconnect, obviating the need for long wires and broadcast communication.\n This article presents the WaveScalar instruction set and evaluates a simulated implementation based on current technology. For single-threaded applications, the WaveCache achieves performance on par with conventional processors, but in less area. For coarse-grain threaded applications the WaveCache achieves nearly linear speedup with up to 64 threads and can sustain 7--14 multiply-accumulates per cycle on fine-grain threaded versions of well-known kernels. Finally, we apply both styles of threading to equake from Spec2000 and speed it up by 9x compared to the serial version."
            },
            "slug": "The-WaveScalar-architecture-Swanson-Schwerin",
            "title": {
                "fragments": [],
                "text": "The WaveScalar architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The WaveScalar instruction set is presented and a simulated implementation based on current technology is evaluated, finding that for single-threaded applications, the WaveCache achieves performance on par with conventional processors, but in less area."
            },
            "venue": {
                "fragments": [],
                "text": "TOCS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751579"
                        ],
                        "name": "Tony Nowatzki",
                        "slug": "Tony-Nowatzki",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Nowatzki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tony Nowatzki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979955"
                        ],
                        "name": "Vinay Gangadhar",
                        "slug": "Vinay-Gangadhar",
                        "structuredName": {
                            "firstName": "Vinay",
                            "lastName": "Gangadhar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vinay Gangadhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720300"
                        ],
                        "name": "K. Sankaralingam",
                        "slug": "K.-Sankaralingam",
                        "structuredName": {
                            "firstName": "Karthikeyan",
                            "lastName": "Sankaralingam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sankaralingam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "Coarse-grained SAs are currently a very popular implementation choice for specialized CNN accelerators for two reasons."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6730278,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1adfd146fdacddfe543a7a45d0c3b9c1f1937e8f",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "General purpose processors (GPPs), from small inorder designs to many-issue out-of-order, incur large power overheads which must be addressed for future technology generations. Major sources of overhead include structures which dynamically extract the data-dependence graph or maintain precise state. Considering irregular workloads, current specialization approaches either heavily curtail performance, or provide simply too little benefit. Interestingly, well known explicit-dataflow architectures eliminate these overheads by directly executing the data-dependence graph and eschewing instruction-precise recoverability. However, even after decades of research, dataflow architectures have yet to come into prominence as a solution. We attribute this to a lack of effective control speculation and the latency overhead of explicit communication, which is crippling for certain codes. This paper makes the observation that if both out-of-order and explicit-dataflow were available in one processor, many types of GPP cores can benefit from dynamically switching during certain phases of an application's lifetime. Analysis reveals that an ideal explicit-dataflow engine could be profitable for more than half of instructions, providing significant performance and energy improvements. The challenge is to achieve these benefits without introducing excess hardware complexity. To this end, we propose the Specialization Engine for Explicit-Dataflow (SEED). Integrated with an inorder core, we see 1.67\u00d7 performance and 1.65\u00d7 energy benefits, with an Out-Of-Order (OOO) dual-issue core we see 1.33\u00d7 and 1.70\u00d7, and with a quad-issue OOO, 1.14\u00d7 and 1.54\u00d7."
            },
            "slug": "Exploring-the-potential-of-heterogeneous-Von-models-Nowatzki-Gangadhar",
            "title": {
                "fragments": [],
                "text": "Exploring the potential of heterogeneous Von Neumann/dataflow execution models"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is made the observation that if both out-of-order and explicit-dataflow were available in one processor, many types of GPP cores can benefit from dynamically switching during certain phases of an application's lifetime."
            },
            "venue": {
                "fragments": [],
                "text": "2015 ACM/IEEE 42nd Annual International Symposium on Computer Architecture (ISCA)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816924"
                        ],
                        "name": "B. Dally",
                        "slug": "B.-Dally",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Dally",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dally"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 73
                            }
                        ],
                        "text": "Similar to the energy consumption quantification in previous experiments [11, 12, 43], Table IV shows the energy cost of accessing each level relative to a MAC operation under the listed conditions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 112
                            }
                        ],
                        "text": "For instance, fetching data from off-chip DRAMs costs orders of magnitude more energy than from on-chip storage [11, 12]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6548081,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "52206ebb2b53a7cfebfd6025fff6c3621b0b0809",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Reaching an ExaScale computer by the end of the decade, and enabling the continued performance scaling of smaller systems requires signifcant research breakthroughs in three key areas: power effciency, programmability, and execution granularity. To build an ExaScale machine in a power budget of 20 MW requires a 200-fold improvement in energy per instruction: from 2 nJ to 10 pJ. Only 4x is expected from improved technology. The remaining 50x must come from improvements in architecture and circuits. To program a machine of this scale requires more productive parallel programming environments \u2014 that make parallel programming as easy as sequential programming is today. Finally, problem size and memory size constraints prevent the continued use of weak scaling, requiring these machines to extract parallelism at very fne granularity \u2014 down to the level of a few instructions. This talk discusses these challenges and current approaches to address them."
            },
            "slug": "Power,-Programmability,-and-Granularity:-The-of-Dally",
            "title": {
                "fragments": [],
                "text": "Power, Programmability, and Granularity: The Challenges of ExaScale Computing"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This talk discusses challenges and current approaches to address them: power effciency, programmability, and execution granularity, and how to extract parallelism at very fne granularity down to the level of a few instructions."
            },
            "venue": {
                "fragments": [],
                "text": "2011 IEEE International Parallel & Distributed Processing Symposium"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790421"
                        ],
                        "name": "A. Parashar",
                        "slug": "A.-Parashar",
                        "structuredName": {
                            "firstName": "Angshuman",
                            "lastName": "Parashar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Parashar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790200"
                        ],
                        "name": "Michael Pellauer",
                        "slug": "Michael-Pellauer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Pellauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Pellauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038590409"
                        ],
                        "name": "Michael Adler",
                        "slug": "Michael-Adler",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Adler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Adler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754836"
                        ],
                        "name": "Bushra Ahsan",
                        "slug": "Bushra-Ahsan",
                        "structuredName": {
                            "firstName": "Bushra",
                            "lastName": "Ahsan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bushra Ahsan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764152"
                        ],
                        "name": "N. Crago",
                        "slug": "N.-Crago",
                        "structuredName": {
                            "firstName": "Neal",
                            "lastName": "Crago",
                            "middleNames": [
                                "Clayton"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Crago"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3583740"
                        ],
                        "name": "Daniel Lustig",
                        "slug": "Daniel-Lustig",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lustig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Lustig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058529153"
                        ],
                        "name": "Vladimir Pavlov",
                        "slug": "Vladimir-Pavlov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Pavlov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vladimir Pavlov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144904164"
                        ],
                        "name": "Antonia Zhai",
                        "slug": "Antonia-Zhai",
                        "structuredName": {
                            "firstName": "Antonia",
                            "lastName": "Zhai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonia Zhai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34926773"
                        ],
                        "name": "M. Gambhir",
                        "slug": "M.-Gambhir",
                        "structuredName": {
                            "firstName": "Mohit",
                            "lastName": "Gambhir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gambhir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684691"
                        ],
                        "name": "A. Jaleel",
                        "slug": "A.-Jaleel",
                        "structuredName": {
                            "firstName": "Aamer",
                            "lastName": "Jaleel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Jaleel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765285"
                        ],
                        "name": "R. Allmon",
                        "slug": "R.-Allmon",
                        "structuredName": {
                            "firstName": "Randy",
                            "lastName": "Allmon",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Allmon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756864"
                        ],
                        "name": "Rachid Rayess",
                        "slug": "Rachid-Rayess",
                        "structuredName": {
                            "firstName": "Rachid",
                            "lastName": "Rayess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rachid Rayess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752976"
                        ],
                        "name": "S. Maresh",
                        "slug": "S.-Maresh",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Maresh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Maresh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775477"
                        ],
                        "name": "J. Emer",
                        "slug": "J.-Emer",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Emer",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Emer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 212
                            }
                        ],
                        "text": "The expected performance advantage and large design space of coarse-grained SAs has inspired much research on the evaluation of its architectures, control schemes, operation scheduling and dataflow models [30\u201335]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17000973,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91ac70c712b1fce4836ffccb25b3ff6c7de0adfd",
            "isKey": false,
            "numCitedBy": 98,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present triggered instructions, a novel control paradigm for arrays of processing elements (PEs) aimed at exploiting spatial parallelism. Triggered instructions completely eliminate the program counter and allow programs to transition concisely between states without explicit branch instructions. They also allow efficient reactivity to inter-PE communication traffic. The approach provides a unified mechanism to avoid over-serialized execution, essentially achieving the effect of techniques such as dynamic instruction reordering and multithreading, which each require distinct hardware mechanisms in a traditional sequential architecture. Our analysis shows that a triggered-instruction based spatial accelerator can achieve 8X greater area-normalized performance than a traditional general-purpose processor. Further analysis shows that triggered control reduces the number of static and dynamic instructions in the critical paths by 62% and 64% respectively over a program-counter style spatial baseline, resulting in a speedup of 2.0X."
            },
            "slug": "Triggered-instructions:-a-control-paradigm-for-Parashar-Pellauer",
            "title": {
                "fragments": [],
                "text": "Triggered instructions: a control paradigm for spatially-programmed architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The approach provides a unified mechanism to avoid over-serialized execution, essentially achieving the effect of techniques such as dynamic instruction reordering and multithreading, which each require distinct hardware mechanisms in a traditional sequential architecture."
            },
            "venue": {
                "fragments": [],
                "text": "ISCA"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073603971"
                        ],
                        "name": "Vinod Nair",
                        "slug": "Vinod-Nair",
                        "structuredName": {
                            "firstName": "Vinod",
                            "lastName": "Nair",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vinod Nair"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 122
                            }
                        ],
                        "text": "Each of the CONV and FC layers is also immediately followed by an activation (ACT) layer, such as a rectified linear unit [37]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15539264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a538b05ebb01a40323997629e171c91aa28b8e2f",
            "isKey": false,
            "numCitedBy": 12986,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Restricted Boltzmann machines were developed using binary stochastic hidden units. These can be generalized by replacing each binary unit by an infinite number of copies that all have the same weights but have progressively more negative biases. The learning and inference rules for these \"Stepped Sigmoid Units\" are unchanged. They can be approximated efficiently by noisy, rectified linear units. Compared with binary units, these units learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset. Unlike binary units, rectified linear units preserve information about relative intensities as information travels through multiple layers of feature detectors."
            },
            "slug": "Rectified-Linear-Units-Improve-Restricted-Boltzmann-Nair-Hinton",
            "title": {
                "fragments": [],
                "text": "Rectified Linear Units Improve Restricted Boltzmann Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Restricted Boltzmann machines were developed using binary stochastic hidden units that learn features that are better for object recognition on the NORB dataset and face verification on the Labeled Faces in the Wild dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144859824"
                        ],
                        "name": "D. Burger",
                        "slug": "D.-Burger",
                        "structuredName": {
                            "firstName": "Doug",
                            "lastName": "Burger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Burger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715863"
                        ],
                        "name": "S. Keckler",
                        "slug": "S.-Keckler",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Keckler",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Keckler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766093"
                        ],
                        "name": "K. McKinley",
                        "slug": "K.-McKinley",
                        "structuredName": {
                            "firstName": "Kathryn",
                            "lastName": "McKinley",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. McKinley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072487776"
                        ],
                        "name": "M. Dahlin",
                        "slug": "M.-Dahlin",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Dahlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dahlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703238"
                        ],
                        "name": "L. John",
                        "slug": "L.-John",
                        "structuredName": {
                            "firstName": "Lizy",
                            "lastName": "John",
                            "middleNames": [
                                "Kurian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. John"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145236769"
                        ],
                        "name": "Calvin Lin",
                        "slug": "Calvin-Lin",
                        "structuredName": {
                            "firstName": "Calvin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Calvin Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144046725"
                        ],
                        "name": "C. R. Moore",
                        "slug": "C.-R.-Moore",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Moore",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. R. Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49092254"
                        ],
                        "name": "J. Burrill",
                        "slug": "J.-Burrill",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Burrill",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Burrill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8990807"
                        ],
                        "name": "Robert G. McDonald",
                        "slug": "Robert-G.-McDonald",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "McDonald",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert G. McDonald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2655705"
                        ],
                        "name": "William Yode",
                        "slug": "William-Yode",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Yode",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "William Yode"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Compared with SIMD/SIMT architectures, SAs are particularly suitable for applications whose dataflow exhibits producer-consumer relationships or can leverage efficient data sharing among a region of PEs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 76
                            }
                        ],
                        "text": "The expected performance advantage and large design space of coarse-grained SAs has inspired much research on the evaluation of its architectures, control schemes, operation scheduling and dataflow models [30\u201335]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "Coarse-grained SAs are currently a very popular implementation choice for specialized CNN accelerators for two reasons."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 23
                            }
                        ],
                        "text": "Spatial architectures (SAs) are a class of accelerators that can exploit high compute parallelism using direct communication between an array of relatively simple processing engines (PEs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "SAs come in two flavors: coarse-grained SAs that consist of tiled arrays of ALU-style PEs connected together via on-chip networks [27\u201329], and fine-grained SAs that are usually in the form of an FPGA."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16309496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "246174d13e20abd8626e4a25933605cb9974cebe",
            "isKey": true,
            "numCitedBy": 367,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Microprocessor designs are on the verge of a post-RISC era in which companies must introduce new ISAs to address the challenges that modern CMOS technologies pose while also exploiting the massive levels of integration now possible. To meet these challenges, we have developed a new class of ISAs, called explicit data graph execution (EDGE), that will match the characteristics of semiconductor technology over the next decade. The TRIPS architecture is the first instantiation of an EDGE instruction set, a new, post-RISC class of instruction set architectures intended to match semiconductor technology evolution over the next decade, scaling to new levels of power efficiency and high performance."
            },
            "slug": "Scaling-to-the-end-of-silicon-with-EDGE-Burger-Keckler",
            "title": {
                "fragments": [],
                "text": "Scaling to the end of silicon with EDGE architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The TRIPS architecture is the first instantiation of an EDGE instruction set, a new, post-RISC class of instruction set architectures intended to match semiconductor technology evolution over the next decade, scaling to new levels of power efficiency and high performance."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144764327"
                        ],
                        "name": "M. Horowitz",
                        "slug": "M.-Horowitz",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Horowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Horowitz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 22028726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "947620a1854655ed91a86b90d12695e05be85983",
            "isKey": false,
            "numCitedBy": 716,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Our challenge is clear: The drive for performance and the end of voltage scaling have made power, and not the number of transistors, the principal factor limiting further improvements in computing performance. Continuing to scale compute performance will require the creation and effective use of new specialized compute engines, and will require the participation of application experts to be successful. If we play our cards right, and develop the tools that allow our customers to become part of the design process, we will create a new wave of innovative and efficient computing devices."
            },
            "slug": "1.1-Computing's-energy-problem-(and-what-we-can-do-Horowitz",
            "title": {
                "fragments": [],
                "text": "1.1 Computing's energy problem (and what we can do about it)"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "If the drive for performance and the end of voltage scaling have made power, and not the number of transistors, the principal factor limiting further improvements in computing performance, a new wave of innovative and efficient computing devices will be created."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE International Solid-State Circuits Conference Digest of Technical Papers (ISSCC)"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775043"
                        ],
                        "name": "H. Graf",
                        "slug": "H.-Graf",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Graf",
                            "middleNames": [
                                "Peter"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Graf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 95
                            }
                        ],
                        "text": "These state-of-the-art CNNs [2\u2013 5] are orders of magnitude larger than those used in the 1990s [9], requiring up to hundreds of megabytes for filter weight storage and 30k-600k operations per input pixel."
                    },
                    "intents": []
                }
            ],
            "corpusId": 41491078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3aa4c691289f56f9af6cf543633cfb3917274281",
            "isKey": false,
            "numCitedBy": 375,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Two novel methods for achieving handwritten digit recognition are described. The first method is based on a neural network chip that performs line thinning and feature extraction using local template matching. The second method is implemented on a digital signal processor and makes extensive use of constrained automatic learning. Experimental results obtained using isolated handwritten digits taken from postal zip codes, a rather difficult data set, are reported and discussed.<<ETX>>"
            },
            "slug": "Handwritten-digit-recognition:-applications-of-and-LeCun-Jackel",
            "title": {
                "fragments": [],
                "text": "Handwritten digit recognition: applications of neural network chips and automatic learning"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "Two novel methods for achieving handwritten digit recognition are described, based on a neural network chip that performs line thinning and feature extraction using local template matching and on a digital signal processor that makes extensive use of constrained automatic learning."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Communications Magazine"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750304"
                        ],
                        "name": "H. Schmit",
                        "slug": "H.-Schmit",
                        "structuredName": {
                            "firstName": "Herman",
                            "lastName": "Schmit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schmit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2958903"
                        ],
                        "name": "D. Whelihan",
                        "slug": "D.-Whelihan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Whelihan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Whelihan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054672832"
                        ],
                        "name": "A. Tsai",
                        "slug": "A.-Tsai",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Tsai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144244099"
                        ],
                        "name": "M. Moe",
                        "slug": "M.-Moe",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Moe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2009212"
                        ],
                        "name": "B. Levine",
                        "slug": "B.-Levine",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Levine",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Levine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1409059772"
                        ],
                        "name": "R. Reed Taylor",
                        "slug": "R.-Reed-Taylor",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Reed Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Reed Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Coarse-grained SAs are currently a very popular implementation choice for specialized CNN accelerators for two reasons."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15462245,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8356d496cded8b8eb8e4f90cfd33f4d8411df6c",
            "isKey": false,
            "numCitedBy": 167,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "PipeRench is a programmable datapath that can be used to accelerate numerically intensive applications. The unique aspect of PipeRench is its ability to virtualize hardware through self-managed dynamic reconfiguration. This capability provides application portability and scalability without redesign or recompilation. This paper describes the implementation of PipeRench in a 0.18 micron process. The implementation has 3.65 million transistors and runs at 120 MHz. Performance is competitive with high-end commercial DSP architectures and more than five times faster than a commercial microprocessor. Executing at 33 MHz, an FIR filter without virtualization consumes 519 mW. When virtualization is required, the implementation consumes approximately 675 mW."
            },
            "slug": "PipeRench:-A-virtualized-programmable-datapath-in-Schmit-Whelihan",
            "title": {
                "fragments": [],
                "text": "PipeRench: A virtualized programmable datapath in 0.18 micron technology"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The implementation of PipeRench in a 0.18 micron process has 3.65 million transistors and runs at 120 MHz, performance is competitive with high-end commercial DSP architectures and more than five times faster than a commercial microprocessor."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE 2002 Custom Integrated Circuits Conference (Cat. No.02CH37285)"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061629065"
                        ],
                        "name": "J. Hauser",
                        "slug": "J.-Hauser",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hauser",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762709"
                        ],
                        "name": "J. Wawrzynek",
                        "slug": "J.-Wawrzynek",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Wawrzynek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Wawrzynek"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "SAs come in two flavors: coarse-grained SAs that consist of tiled arrays of ALU-style PEs connected together via on-chip networks [27\u201329], and fine-grained SAs that are usually in the form of an FPGA."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15210864,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd2be2e142e6d9db27b4d65c321f9af1490601c0",
            "isKey": false,
            "numCitedBy": 951,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Typical reconfigurable machines exhibit shortcomings that make them less than ideal for general-purpose computing. The Garp Architecture combines reconfigurable hardware with a standard MIPS processor on the same die to retain the better features of both. Novel aspects of the architecture are presented, as well as a prototype software environment and preliminary performance results. Compared to an UltraSPARC, a Garp of similar technology could achieve speedups ranging from a factor of 2 to as high as a factor of 24 for some useful applications."
            },
            "slug": "Garp:-a-MIPS-processor-with-a-reconfigurable-Hauser-Wawrzynek",
            "title": {
                "fragments": [],
                "text": "Garp: a MIPS processor with a reconfigurable coprocessor"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Novel aspects of the Garp Architecture are presented, as well as a prototype software environment and preliminary performance results, which suggest that a Garp of similar technology could achieve speedups ranging from a factor of 2 to as high as a factors of 24 for some useful applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. The 5th Annual IEEE Symposium on Field-Programmable Custom Computing Machines Cat. No.97TB100186)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50579876"
                        ],
                        "name": "Yu-hsin Chen",
                        "slug": "Yu-hsin-Chen",
                        "structuredName": {
                            "firstName": "Yu-hsin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-hsin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145984583"
                        ],
                        "name": "T. Krishna",
                        "slug": "T.-Krishna",
                        "structuredName": {
                            "firstName": "Tushar",
                            "lastName": "Krishna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Krishna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775477"
                        ],
                        "name": "J. Emer",
                        "slug": "J.-Emer",
                        "structuredName": {
                            "firstName": "Joel",
                            "lastName": "Emer",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Emer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691305"
                        ],
                        "name": "V. Sze",
                        "slug": "V.-Sze",
                        "structuredName": {
                            "firstName": "Vivienne",
                            "lastName": "Sze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "The RS dataflow is a key feature of the Eyeriss architecture, which has been implemented in a fabricated chip [41] (Fig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Details on these techniques are described in [41]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "Die photo and spec of the Eyeriss chip [41]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 207882941,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3aada7d05f0478baaa3e92f51b66112171e4e53e",
            "isKey": false,
            "numCitedBy": 1227,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Eyeriss is an accelerator for state-of-the-art deep convolutional neural networks (CNNs). It optimizes for the energy efficiency of the entire system, including the accelerator chip and off-chip DRAM, for various CNN shapes by reconfiguring the architecture. CNNs are widely used in modern AI systems but also bring challenges on throughput and energy efficiency to the underlying hardware. This is because its computation requires a large amount of data, creating significant data movement from on-chip and off-chip that is more energy-consuming than computation. Minimizing data movement energy cost for any CNN shape, therefore, is the key to high throughput and energy efficiency. Eyeriss achieves these goals by using a proposed processing dataflow, called row stationary (RS), on a spatial architecture with 168 processing elements. RS dataflow reconfigures the computation mapping of a given shape, which optimizes energy efficiency by maximally reusing data locally to reduce expensive data movement, such as DRAM accesses. Compression and data gating are also applied to further improve energy efficiency. Eyeriss processes the convolutional layers at 35 frames/s and 0.0029 DRAM access/multiply and accumulation (MAC) for AlexNet at 278 mW (batch size  $N = 4$ ), and 0.7 frames/s and 0.0035 DRAM access/MAC for VGG-16 at 236 mW ( $N = 3$ )."
            },
            "slug": "Eyeriss:-An-Energy-Efficient-Reconfigurable-for-Chen-Krishna",
            "title": {
                "fragments": [],
                "text": "Eyeriss: An Energy-Efficient Reconfigurable Accelerator for Deep Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "Eyeriss is an accelerator for state-of-the-art deep convolutional neural networks (CNNs) that optimizes for the energy efficiency of the entire system, including the accelerator chip and off-chip DRAM, for various CNN shapes by reconfiguring the architecture."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Journal of Solid-State Circuits"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113979381"
                        ],
                        "name": "Xing Hao",
                        "slug": "Xing-Hao",
                        "structuredName": {
                            "firstName": "Xing",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xing Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8273966"
                        ],
                        "name": "Guigang Zhang",
                        "slug": "Guigang-Zhang",
                        "structuredName": {
                            "firstName": "Guigang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guigang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118869556"
                        ],
                        "name": "Shang Ma",
                        "slug": "Shang-Ma",
                        "structuredName": {
                            "firstName": "Shang",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shang Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "The recent popularity of deep learning [1], specifically deep convolutional neural networks (CNNs), can be attributed to its ability to achieve unprecedented accuracy for tasks ranging from object recognition [2\u20135] and detection [6, 7] to scene understanding [8]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1779661,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
            "isKey": false,
            "numCitedBy": 31389,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deep-Learning-Hao-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Semantic Comput."
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "These state-of-the-art CNNs [2\u2013 5] are orders of magnitude larger than those used in the 1990s [9], requiring up to hundreds of megabytes for filter weight storage and 30k-600k operations per input pixel."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A 1.93TOPS/W scalable deep learning/inference processor with tetra-parallel MIMD architecture for big-data applications"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ISSCC, 2015."
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 73
                            }
                        ],
                        "text": "Similar to the energy consumption quantification in previous experiments [11, 12, 43], Table IV shows the energy cost of accessing each level relative to a MAC operation under the listed conditions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 112
                            }
                        ],
                        "text": "For instance, fetching data from off-chip DRAMs costs orders of magnitude more energy than from on-chip storage [11, 12]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computing\u2019s energy problem (and what we can do about it)"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE ISSCC, 2014."
            },
            "year": 2014
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 46,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Eyeriss:-A-Spatial-Architecture-for-Dataflow-for-Chen-Emer/5ec594e9f5ca4b629be28625cd78c882514ea3be?sort=total-citations"
}