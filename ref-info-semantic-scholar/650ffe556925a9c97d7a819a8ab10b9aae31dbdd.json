{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136341779"
                        ],
                        "name": "Yizhi Liu",
                        "slug": "Yizhi-Liu",
                        "structuredName": {
                            "firstName": "Yizhi",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yizhi Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119049210"
                        ],
                        "name": "Yao Wang",
                        "slug": "Yao-Wang",
                        "structuredName": {
                            "firstName": "Yao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "81228496"
                        ],
                        "name": "Ruofei Yu",
                        "slug": "Ruofei-Yu",
                        "structuredName": {
                            "firstName": "Ruofei",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruofei Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112144126"
                        ],
                        "name": "Mu Li",
                        "slug": "Mu-Li",
                        "structuredName": {
                            "firstName": "Mu",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mu Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153403129"
                        ],
                        "name": "Vin Sharma",
                        "slug": "Vin-Sharma",
                        "structuredName": {
                            "firstName": "Vin",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vin Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Yida Wang",
                        "slug": "Yida-Wang",
                        "structuredName": {
                            "firstName": "Yida",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yida Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "CPUs [26]), but to the best of our knowledge, our work is the first time to search execution scheme of deep learning model inference on integrated GPUs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "level optimizations, including operator fusion, pre-computing, simplifying inference for batch-norm and dropout [5], we also applied the graph tuner technique proposed in [26] to fine-tune the data layout to achieve better end-to-end performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For the computationally-intensive operators like convolution, we resort to the machine learning-based approach [6, 26] to automatically search good optimization schemes for different convolution workloads on different GPUs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The first place to execute the models is mobile CPUs [39], which is heavily studied and optimized [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2), and then used AutoTVM [6] as well as graph tuner [26] to search the best schedules for different workloads (sec-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "for satisfying schedules to execute the model inference, including the tensor-level [6] and graph-level [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "As in [26], we used spatial packing, namely loop tiling, to maximally reuse memory during the convolution computation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52183221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d60ea2461172d30487743fa6ef8788db4b53759",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "The popularity of Convolutional Neural Network (CNN) models and the ubiquity of CPUs imply that better performance of CNN model inference on CPUs can deliver significant gain to a large number of users. To improve the performance of CNN inference on CPUs, current approaches like MXNet and Intel OpenVINO usually treat the model as a graph and use the high-performance libraries such as Intel MKL-DNN to implement the operations of the graph. While achieving reasonable performance on individual operations from the off-the-shelf libraries, this solution makes it inflexible to conduct optimizations at the graph level, as the local operation-level optimizations are predefined. Therefore, it is restrictive and misses the opportunity to optimize the end-to-end inference pipeline as a whole. This paper presents \\emph{NeoCPU}, a comprehensive approach of CNN model inference on CPUs that employs a full-stack and systematic scheme of optimizations. \\emph{NeoCPU} optimizes the operations as templates without relying on third-parties libraries, which enables further improvement of the performance via operation- and graph-level joint optimization. Experiments show that \\emph{NeoCPU} achieves up to 3.45$\\times$ lower latency for CNN model inference than the current state-of-the-art implementations on various kinds of popular CPUs."
            },
            "slug": "Optimizing-CNN-Model-Inference-on-CPUs-Liu-Wang",
            "title": {
                "fragments": [],
                "text": "Optimizing CNN Model Inference on CPUs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A comprehensive approach of CNN model inference on CPUs that employs a full-stack and systematic scheme of optimizations, which optimizes the operations as templates without relying on third-parties libraries, which enables further improvement of the performance via operation- and graph-level joint optimization."
            },
            "venue": {
                "fragments": [],
                "text": "USENIX Annual Technical Conference"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2926507"
                        ],
                        "name": "Huynh Nguyen Loc",
                        "slug": "Huynh-Nguyen-Loc",
                        "structuredName": {
                            "firstName": "Huynh",
                            "lastName": "Loc",
                            "middleNames": [
                                "Nguyen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huynh Nguyen Loc"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2311745"
                        ],
                        "name": "Youngki Lee",
                        "slug": "Youngki-Lee",
                        "structuredName": {
                            "firstName": "Youngki",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youngki Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9140912"
                        ],
                        "name": "R. Balan",
                        "slug": "R.-Balan",
                        "structuredName": {
                            "firstName": "Rajesh",
                            "lastName": "Balan",
                            "middleNames": [
                                "Krishna"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Balan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 146
                            }
                        ],
                        "text": "FLOPs and consumes more power, it is more favorable to leverage other compute units on the SoCs to handle the deep learning model inference tasks [19, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9948141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d289ce63055c10937e5715e940a4bb9d0af7a8c5",
            "isKey": false,
            "numCitedBy": 259,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "The rapid emergence of head-mounted devices such as the Microsoft Holo-lens enables a wide variety of continuous vision applications. Such applications often adopt deep-learning algorithms such as CNN and RNN to extract rich contextual information from the first-person-view video streams. Despite the high accuracy, use of deep learning algorithms in mobile devices raises critical challenges, i.e., high processing latency and power consumption. In this paper, we propose DeepMon, a mobile deep learning inference system to run a variety of deep learning inferences purely on a mobile device in a fast and energy-efficient manner. For this, we designed a suite of optimization techniques to efficiently offload convolutional layers to mobile GPUs and accelerate the processing; note that the convolutional layers are the common performance bottleneck of many deep learning models. Our experimental results show that DeepMon can classify an image over the VGG-VeryDeep-16 deep learning model in 644ms on Samsung Galaxy S7, taking an important step towards continuous vision without imposing any privacy concerns nor networking cost."
            },
            "slug": "DeepMon:-Mobile-GPU-based-Deep-Learning-Framework-Loc-Lee",
            "title": {
                "fragments": [],
                "text": "DeepMon: Mobile GPU-based Deep Learning Framework for Continuous Vision Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes DeepMon, a mobile deep learning inference system to run a variety of deep learning inferences purely on a mobile device in a fast and energy-efficient manner and designs a suite of optimization techniques to efficiently offload convolutional layers to mobile GPUs and accelerate the processing."
            },
            "venue": {
                "fragments": [],
                "text": "MobiSys"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144948031"
                        ],
                        "name": "N. Lane",
                        "slug": "N.-Lane",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Lane",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Lane"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144623952"
                        ],
                        "name": "S. Bhattacharya",
                        "slug": "S.-Bhattacharya",
                        "structuredName": {
                            "firstName": "Sourav",
                            "lastName": "Bhattacharya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bhattacharya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737522"
                        ],
                        "name": "Petko Georgiev",
                        "slug": "Petko-Georgiev",
                        "structuredName": {
                            "firstName": "Petko",
                            "lastName": "Georgiev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Petko Georgiev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779159"
                        ],
                        "name": "Claudio Forlivesi",
                        "slug": "Claudio-Forlivesi",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "Forlivesi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Claudio Forlivesi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072513626"
                        ],
                        "name": "Lei Jiao",
                        "slug": "Lei-Jiao",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Jiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Jiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1994899"
                        ],
                        "name": "Lorena Qendro",
                        "slug": "Lorena-Qendro",
                        "structuredName": {
                            "firstName": "Lorena",
                            "lastName": "Qendro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lorena Qendro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776175"
                        ],
                        "name": "F. Kawsar",
                        "slug": "F.-Kawsar",
                        "structuredName": {
                            "firstName": "Fahim",
                            "lastName": "Kawsar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Kawsar"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "FLOPs and consumes more power, it is more favorable to leverage other compute units on the SoCs to handle the deep learning model inference tasks [19, 23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15599732,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d594839932b3ec17b2f0e30f00d1095937eabf74",
            "isKey": false,
            "numCitedBy": 379,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Breakthroughs from the field of deep learning are radically changing how sensor data are interpreted to extract the high-level information needed by mobile apps. It is critical that the gains in inference accuracy that deep models afford become embedded in future generations of mobile apps. In this work, we present the design and implementation of DeepX, a software accelerator for deep learning execution. DeepX signif- icantly lowers the device resources (viz. memory, computation, energy) required by deep learning that currently act as a severe bottleneck to mobile adoption. The foundation of DeepX is a pair of resource control algorithms, designed for the inference stage of deep learning, that: (1) decompose monolithic deep model network architectures into unit- blocks of various types, that are then more efficiently executed by heterogeneous local device processors (e.g., GPUs, CPUs); and (2), perform principled resource scaling that adjusts the architecture of deep models to shape the overhead each unit-blocks introduces. Experiments show, DeepX can allow even large-scale deep learning models to execute efficently on modern mobile processors and significantly outperform existing solutions, such as cloud-based offloading."
            },
            "slug": "DeepX:-A-Software-Accelerator-for-Low-Power-Deep-on-Lane-Bhattacharya",
            "title": {
                "fragments": [],
                "text": "DeepX: A Software Accelerator for Low-Power Deep Learning Inference on Mobile Devices"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experiments show, DeepX can allow even large-scale deep learning models to execute efficently on modern mobile processors and significantly outperform existing solutions, such as cloud-based offloading."
            },
            "venue": {
                "fragments": [],
                "text": "2016 15th ACM/IEEE International Conference on Information Processing in Sensor Networks (IPSN)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913774"
                        ],
                        "name": "Tianqi Chen",
                        "slug": "Tianqi-Chen",
                        "structuredName": {
                            "firstName": "Tianqi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianqi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47108160"
                        ],
                        "name": "T. Moreau",
                        "slug": "T.-Moreau",
                        "structuredName": {
                            "firstName": "Thierry",
                            "lastName": "Moreau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Moreau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732910"
                        ],
                        "name": "Ziheng Jiang",
                        "slug": "Ziheng-Jiang",
                        "structuredName": {
                            "firstName": "Ziheng",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziheng Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149970173"
                        ],
                        "name": "Lianmin Zheng",
                        "slug": "Lianmin-Zheng",
                        "structuredName": {
                            "firstName": "Lianmin",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lianmin Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2621619"
                        ],
                        "name": "Eddie Q. Yan",
                        "slug": "Eddie-Q.-Yan",
                        "structuredName": {
                            "firstName": "Eddie",
                            "lastName": "Yan",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eddie Q. Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3050154"
                        ],
                        "name": "Haichen Shen",
                        "slug": "Haichen-Shen",
                        "structuredName": {
                            "firstName": "Haichen",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haichen Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37270394"
                        ],
                        "name": "M. Cowan",
                        "slug": "M.-Cowan",
                        "structuredName": {
                            "firstName": "Meghan",
                            "lastName": "Cowan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Cowan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2185540"
                        ],
                        "name": "Leyuan Wang",
                        "slug": "Leyuan-Wang",
                        "structuredName": {
                            "firstName": "Leyuan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leyuan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49994783"
                        ],
                        "name": "Yuwei Hu",
                        "slug": "Yuwei-Hu",
                        "structuredName": {
                            "firstName": "Yuwei",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuwei Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717411"
                        ],
                        "name": "L. Ceze",
                        "slug": "L.-Ceze",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ceze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ceze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695691"
                        ],
                        "name": "A. Krishnamurthy",
                        "slug": "A.-Krishnamurthy",
                        "structuredName": {
                            "firstName": "Arvind",
                            "lastName": "Krishnamurthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krishnamurthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "level optimizations, including operator fusion, pre-computing, simplifying inference for batch-norm and dropout [5], we also applied the graph tuner technique proposed in [26] to fine-tune the data layout to achieve better end-to-end performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Our solution is built on top of the open-source deep learning compiler stack TVM [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52939079,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df013a17ab84d5403361da4538a04d574f58be83",
            "isKey": false,
            "numCitedBy": 713,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "There is an increasing need to bring machine learning to a wide diversity of hardware devices. Current frameworks rely on vendor-specific operator libraries and optimize for a narrow range of server-class GPUs. Deploying workloads to new platforms -- such as mobile phones, embedded devices, and accelerators (e.g., FPGAs, ASICs) -- requires significant manual effort. We propose TVM, a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends. TVM solves optimization challenges specific to deep learning, such as high-level operator fusion, mapping to arbitrary hardware primitives, and memory latency hiding. It also automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations. Experimental results show that TVM delivers performance across hardware back-ends that are competitive with state-of-the-art, hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPUs. We also demonstrate TVM's ability to target new accelerator back-ends, such as the FPGA-based generic deep learning accelerator. The system is open sourced and in production use inside several major companies."
            },
            "slug": "TVM:-An-Automated-End-to-End-Optimizing-Compiler-Chen-Moreau",
            "title": {
                "fragments": [],
                "text": "TVM: An Automated End-to-End Optimizing Compiler for Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "TVM is a compiler that exposes graph-level and operator-level optimizations to provide performance portability to deep learning workloads across diverse hardware back-ends and automates optimization of low-level programs to hardware characteristics by employing a novel, learning-based cost modeling method for rapid exploration of code optimizations."
            },
            "venue": {
                "fragments": [],
                "text": "OSDI"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913774"
                        ],
                        "name": "Tianqi Chen",
                        "slug": "Tianqi-Chen",
                        "structuredName": {
                            "firstName": "Tianqi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianqi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149970173"
                        ],
                        "name": "Lianmin Zheng",
                        "slug": "Lianmin-Zheng",
                        "structuredName": {
                            "firstName": "Lianmin",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lianmin Zheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2621619"
                        ],
                        "name": "Eddie Q. Yan",
                        "slug": "Eddie-Q.-Yan",
                        "structuredName": {
                            "firstName": "Eddie",
                            "lastName": "Yan",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eddie Q. Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732910"
                        ],
                        "name": "Ziheng Jiang",
                        "slug": "Ziheng-Jiang",
                        "structuredName": {
                            "firstName": "Ziheng",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziheng Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47108160"
                        ],
                        "name": "T. Moreau",
                        "slug": "T.-Moreau",
                        "structuredName": {
                            "firstName": "Thierry",
                            "lastName": "Moreau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Moreau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717411"
                        ],
                        "name": "L. Ceze",
                        "slug": "L.-Ceze",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Ceze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Ceze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695691"
                        ],
                        "name": "A. Krishnamurthy",
                        "slug": "A.-Krishnamurthy",
                        "structuredName": {
                            "firstName": "Arvind",
                            "lastName": "Krishnamurthy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krishnamurthy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Although AutoTVM has been used on ARM Mali GPUs [6], we conduct a more comprehensive study on applying it to a number of mainstream integrated GPUs for a wide range of CNN models."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For the computationally-intensive operators like convolution, we resort to the machine learning-based approach [6, 26] to automatically search good optimization schemes for different convolution workloads on different GPUs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "2), and then used AutoTVM [6] as well as graph tuner [26] to search the best schedules for different workloads (sec-"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "for satisfying schedules to execute the model inference, including the tensor-level [6] and graph-level [26]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "AutoTVM [6] provides a venue that automatically optimizes the common tensor operators for given hardware and builds up an optimization space composing possible transformed versions of tensors."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "3, our solution is based on TVM [6] with a number of improvements."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 29160233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb91c2f8d3cac0b655a39be318b603334eb18987",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a learning-based framework to optimize tensor programs for deep learning workloads. Efficient implementations of tensor operators, such as matrix multiplication and high dimensional convolution, are key enablers of effective deep learning systems. However, existing systems rely on manually optimized libraries such as cuDNN where only a narrow range of server class GPUs are well-supported. The reliance on hardware-specific operator libraries limits the applicability of high-level graph optimizations and incurs significant engineering costs when deploying to new hardware targets. We use learning to remove this engineering burden. We learn domain-specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants. We further accelerate the search by effective model transfer across workloads. Experimental results show that our framework delivers performance competitive with state-of-the-art hand-tuned libraries for low-power CPU, mobile GPU, and server-class GPU."
            },
            "slug": "Learning-to-Optimize-Tensor-Programs-Chen-Zheng",
            "title": {
                "fragments": [],
                "text": "Learning to Optimize Tensor Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A learning-based framework to optimize tensor programs for deep learning workloads that learns domain-specific statistical cost models to guide the search of tensor operator implementations over billions of possible program variants and accelerates the search by effective model transfer across workloads."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2125768"
                        ],
                        "name": "Thomas B. Preu\u00dfer",
                        "slug": "Thomas-B.-Preu\u00dfer",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Preu\u00dfer",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas B. Preu\u00dfer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779727"
                        ],
                        "name": "G. Gambardella",
                        "slug": "G.-Gambardella",
                        "structuredName": {
                            "firstName": "Giulio",
                            "lastName": "Gambardella",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Gambardella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809409"
                        ],
                        "name": "Nicholas J. Fraser",
                        "slug": "Nicholas-J.-Fraser",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Fraser",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicholas J. Fraser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1791399"
                        ],
                        "name": "Michaela Blott",
                        "slug": "Michaela-Blott",
                        "structuredName": {
                            "firstName": "Michaela",
                            "lastName": "Blott",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michaela Blott"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[31] proposed to use a hardware accelerator for the inference of quantized neural networks on a heterogeneous embedded system, which mainly focused on the network produced by a specific framework and the platforms with restricted resources."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5057754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b377e30e58347719a44cdb4488dde617918fbce",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks have established as a generic and powerful means to approach challenging problems such as image classification, object detection or decision making. Their successful employment foots on an enormous demand of compute. The quantization of network parameters and the processed data has proven a valuable measure to reduce the challenges of network inference so effectively that the feasible scope of applications is expanded even into the embedded domain. This paper describes the making of a real-time object detection in a live video stream processed on an embedded all-programmable device. The presented case illustrates how the required processing is tamed and parallelized across both the CPU cores and the programmable logic and how the most suitable resources and powerful extensions, such as NEON vectorization, are leveraged for the individual processing steps. The crafted result is an extended Darknet framework implementing a fully integrated, end-to-end solution from video capture over object annotation to video output applying neural network inference at different quantization levels running at 16 frames per second on an embedded Zynq UltraScale+ (XCZU3EG) platform."
            },
            "slug": "Inference-of-quantized-neural-networks-on-devices-Preu\u00dfer-Gambardella",
            "title": {
                "fragments": [],
                "text": "Inference of quantized neural networks on heterogeneous all-programmable devices"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The presented case illustrates how the required processing is tamed and parallelized across both the CPU cores and the programmable logic and how the most suitable resources and powerful extensions, such as NEON vectorization, are leveraged for the individual processing steps."
            },
            "venue": {
                "fragments": [],
                "text": "2018 Design, Automation & Test in Europe Conference & Exhibition (DATE)"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3003738"
                        ],
                        "name": "Sharan Chetlur",
                        "slug": "Sharan-Chetlur",
                        "structuredName": {
                            "firstName": "Sharan",
                            "lastName": "Chetlur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sharan Chetlur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2266717"
                        ],
                        "name": "Cliff Woolley",
                        "slug": "Cliff-Woolley",
                        "structuredName": {
                            "firstName": "Cliff",
                            "lastName": "Woolley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cliff Woolley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2101730"
                        ],
                        "name": "Philippe Vandermersch",
                        "slug": "Philippe-Vandermersch",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Vandermersch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philippe Vandermersch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145611200"
                        ],
                        "name": "Jonathan M. Cohen",
                        "slug": "Jonathan-M.-Cohen",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Cohen",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan M. Cohen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066786849"
                        ],
                        "name": "J. Tran",
                        "slug": "J.-Tran",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2301680"
                        ],
                        "name": "Bryan Catanzaro",
                        "slug": "Bryan-Catanzaro",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Catanzaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan Catanzaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1782282"
                        ],
                        "name": "Evan Shelhamer",
                        "slug": "Evan-Shelhamer",
                        "structuredName": {
                            "firstName": "Evan",
                            "lastName": "Shelhamer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Evan Shelhamer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 64
                            }
                        ],
                        "text": "Intel clDNN [8], ARMCompute Library (ACL) [2], and Nvidia cuDNN [7], along with some deep learning frameworks or vendor-provided inference-only pipelines (Intel OpenVINO [9] and e."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 101
                            }
                        ],
                        "text": "While integrated GPUs from ARM and Nvidia have been gathered a number of research interests recently [1, 7, 11], it is challenging to extend the successful optimization tricks to Intel Graphics."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12330432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "31c36d445367ba204244bb74893c5654e31c3869",
            "isKey": false,
            "numCitedBy": 1419,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a library that provides optimized implementations for deep learning primitives. Deep learning workloads are computationally intensive, and optimizing the kernels of deep learning workloads is difficult and time-consuming. As parallel architectures evolve, kernels must be reoptimized for new processors, which makes maintaining codebases difficult over time. Similar issues have long been addressed in the HPC community by libraries such as the Basic Linear Algebra Subroutines (BLAS) [2]. However, there is no analogous library for deep learning. Without such a library, researchers implementing deep learning workloads on parallel processors must create and optimize their own implementations of the main computational kernels, and this work must be repeated as new parallel processors emerge. To address this problem, we have created a library similar in intent to BLAS, with optimized routines for deep learning workloads. Our implementation contains routines for GPUs, and similarly to the BLAS library, could be implemented for other platforms. The library is easy to integrate into existing frameworks, and provides optimized performance and memory usage. For example, integrating cuDNN into Caffe, a popular framework for convolutional networks, improves performance by 36% on a standard model while also reducing memory consumption."
            },
            "slug": "cuDNN:-Efficient-Primitives-for-Deep-Learning-Chetlur-Woolley",
            "title": {
                "fragments": [],
                "text": "cuDNN: Efficient Primitives for Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A library similar in intent to BLAS, with optimized routines for deep learning workloads, that contains routines for GPUs, and similarly to the BLAS library, could be implemented for other platforms."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2163220"
                        ],
                        "name": "Onur Kayiran",
                        "slug": "Onur-Kayiran",
                        "structuredName": {
                            "firstName": "Onur",
                            "lastName": "Kayiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Onur Kayiran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2456813"
                        ],
                        "name": "N. Nachiappan",
                        "slug": "N.-Nachiappan",
                        "structuredName": {
                            "firstName": "Nachiappan",
                            "lastName": "Nachiappan",
                            "middleNames": [
                                "Chidambaram"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Nachiappan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111543"
                        ],
                        "name": "Adwait Jog",
                        "slug": "Adwait-Jog",
                        "structuredName": {
                            "firstName": "Adwait",
                            "lastName": "Jog",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adwait Jog"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1999972"
                        ],
                        "name": "Rachata Ausavarungnirun",
                        "slug": "Rachata-Ausavarungnirun",
                        "structuredName": {
                            "firstName": "Rachata",
                            "lastName": "Ausavarungnirun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rachata Ausavarungnirun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145879376"
                        ],
                        "name": "M. Kandemir",
                        "slug": "M.-Kandemir",
                        "structuredName": {
                            "firstName": "Mahmut",
                            "lastName": "Kandemir",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kandemir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308405"
                        ],
                        "name": "Gabriel H. Loh",
                        "slug": "Gabriel-H.-Loh",
                        "structuredName": {
                            "firstName": "Gabriel",
                            "lastName": "Loh",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gabriel H. Loh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145929920"
                        ],
                        "name": "O. Mutlu",
                        "slug": "O.-Mutlu",
                        "structuredName": {
                            "firstName": "Onur",
                            "lastName": "Mutlu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mutlu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8948708"
                        ],
                        "name": "C. Das",
                        "slug": "C.-Das",
                        "structuredName": {
                            "firstName": "Chita",
                            "lastName": "Das",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Das"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "high variance of the model inference duration [22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2784175,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bd5d02b18da6fe877fdee359c0813a5b02aface",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Heterogeneous architectures consisting of general-purpose CPUs and throughput-optimized GPUs are projected to be the dominant computing platforms for many classes of applications. The design of such systems is more complex than that of homogeneous architectures because maximizing resource utilization while minimizing shared resource interference between CPU and GPU applications is difficult. We show that GPU applications tend to monopolize the shared hardware resources, such as memory and network, because of their high thread-level parallelism (TLP), and discuss the limitations of existing GPU-based concurrency management techniques when employed in heterogeneous systems. To solve this problem, we propose an integrated concurrency management strategy that modulates the TLP in GPUs to control the performance of both CPU and GPU applications. This mechanism considers both GPU core state and system-wide memory and network congestion information to dynamically decide on the level of GPU concurrency to maximize system performance. We propose and evaluate two schemes: one (CM-CPU) for boosting CPU performance in the presence of GPU interference, the other (CM-BAL) for improving both CPU and GPU performance in a balanced manner and thus overall system performance. Our evaluations show that the first scheme improves average CPU performance by 24%, while reducing average GPU performance by 11%. The second scheme provides 7% average performance improvement for both CPU and GPU applications. We also show that our solution allows the user to control performance trade-offs between CPUs and GPUs."
            },
            "slug": "Managing-GPU-Concurrency-in-Heterogeneous-Kayiran-Nachiappan",
            "title": {
                "fragments": [],
                "text": "Managing GPU Concurrency in Heterogeneous Architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An integrated concurrency management strategy that modulates the TLP in GPUs to control the performance of both CPU and GPU applications is proposed and evaluated, which improves average CPU performance by 24%, while reducing average GPU performance by 11%."
            },
            "venue": {
                "fragments": [],
                "text": "2014 47th Annual IEEE/ACM International Symposium on Microarchitecture"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1491252080"
                        ],
                        "name": "Jiawen Liu",
                        "slug": "Jiawen-Liu",
                        "structuredName": {
                            "firstName": "Jiawen",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiawen Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46431089"
                        ],
                        "name": "Hengyu Zhao",
                        "slug": "Hengyu-Zhao",
                        "structuredName": {
                            "firstName": "Hengyu",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hengyu Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26942060"
                        ],
                        "name": "Matheus A. Ogleari",
                        "slug": "Matheus-A.-Ogleari",
                        "structuredName": {
                            "firstName": "Matheus",
                            "lastName": "Ogleari",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matheus A. Ogleari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48937589"
                        ],
                        "name": "Dong Li",
                        "slug": "Dong-Li",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109858015"
                        ],
                        "name": "Jishen Zhao",
                        "slug": "Jishen-Zhao",
                        "structuredName": {
                            "firstName": "Jishen",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jishen Zhao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A few previous works proposed software/hardware co-design of a heterogeneous memory system for acceleration of neural networks [3, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52841200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3bc7d98be0dfedeedfd449ed5d95bc7a2600d244",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks (NNs) have been adopted in a wide range of application domains, such as image classification, speech recognition, object detection, and computer vision. However, training NNs \u2013 especially deep neural networks (DNNs) \u2013 can be energy and time consuming, because of frequent data movement between processor and memory. Furthermore, training involves massive fine-grained operations with various computation and memory access characteristics. Exploiting high parallelism with such diverse operations is challenging. To address these challenges, we propose a software/hardware co-design of heterogeneous processing-in-memory (PIM) system. Our hardware design incorporates hundreds of fix-function arithmetic units and ARM-based programmable cores on the logic layer of a 3D die-stacked memory to form a heterogeneous PIM architecture attached to CPU. Our software design offers a programming model and a runtime system that program, offload, and schedule various NN training operations across compute resources provided by CPU and heterogeneous PIM. By extending the OpenCL programming model and employing a hardware heterogeneity-aware runtime system, we enable high program portability and easy program maintenance across various heterogeneous hardware, optimize system energy efficiency, and improve hardware utilization."
            },
            "slug": "Processing-in-Memory-for-Energy-Efficient-Neural-A-Liu-Zhao",
            "title": {
                "fragments": [],
                "text": "Processing-in-Memory for Energy-Efficient Neural Network Training: A Heterogeneous Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work proposes a software/hardware co-design of heterogeneous processing-in-memory (PIM) system that enables high program portability and easy program maintenance across various heterogeneous hardware, optimize system energy efficiency, and improve hardware utilization."
            },
            "venue": {
                "fragments": [],
                "text": "2018 51st Annual IEEE/ACM International Symposium on Microarchitecture (MICRO)"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2177547"
                        ],
                        "name": "E. Azarkhish",
                        "slug": "E.-Azarkhish",
                        "structuredName": {
                            "firstName": "Erfan",
                            "lastName": "Azarkhish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Azarkhish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48307511"
                        ],
                        "name": "D. Rossi",
                        "slug": "D.-Rossi",
                        "structuredName": {
                            "firstName": "Davide",
                            "lastName": "Rossi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rossi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718585"
                        ],
                        "name": "Igor Loi",
                        "slug": "Igor-Loi",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Loi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Igor Loi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710649"
                        ],
                        "name": "L. Benini",
                        "slug": "L.-Benini",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Benini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Benini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A few previous works proposed software/hardware co-design of a heterogeneous memory system for acceleration of neural networks [3, 24]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11822264,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4857110cb00f9877077400dddfeb5b0150cf9df4",
            "isKey": false,
            "numCitedBy": 76,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "High-performance computing systems are moving towards 2.5D and 3D memory hierarchies, based on High Bandwidth Memory (HBM) and Hybrid Memory Cube (HMC) to mitigate the main memory bottlenecks. This trend is also creating new opportunities to revisit near-memory computation. In this paper, we propose a flexible processor-in-memory (PIM) solution for scalable and energy-efficient execution of deep convolutional networks (ConvNets), one of the fastest-growing workloads for servers and high-end embedded systems. Our co-design approach consists of a network of Smart Memory Cubes (modular extensions to the standard HMC) each augmented with a many-core PIM platform called NeuroCluster. NeuroClusters have a modular design based on NeuroStream coprocessors (for Convolution-intensive computations) and general-purpose RISC-V cores. In addition, a DRAM-friendly tiling mechanism and a scalable computation paradigm are presented to efficiently harness this computational capability with a very low programming effort. NeuroCluster occupies only 8 percent of the total logic-base (LoB) die area in a standard HMC and achieves an average performance of 240\u00a0GFLOPS for complete execution of full-featured state-of-the-art (SoA) ConvNets within a power budget of 2.5\u00a0W. Overall 11\u00a0W is consumed in a single SMC device, with 22.5\u00a0GFLOPS/W energy-efficiency which is 3.5X better than the best GPU implementations in similar technologies. The minor increase in system-level power and the negligible area increase make our PIM system a cost-effective and energy efficient solution, easily scalable to 955\u00a0GFLOPS with a small network of just four SMCs."
            },
            "slug": "Neurostream:-Scalable-and-Energy-Efficient-Deep-Azarkhish-Rossi",
            "title": {
                "fragments": [],
                "text": "Neurostream: Scalable and Energy Efficient Deep Learning with Smart Memory Cubes"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This paper proposes a flexible processor-in-memory (PIM) solution for scalable and energy-efficient execution of deep convolutional networks (ConvNets), one of the fastest-growing workloads for servers and high-end embedded systems."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Parallel and Distributed Systems"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120805419"
                        ],
                        "name": "Mingxing Tan",
                        "slug": "Mingxing-Tan",
                        "structuredName": {
                            "firstName": "Mingxing",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingxing Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Bo Chen",
                        "slug": "Bo-Chen",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34320634"
                        ],
                        "name": "Ruoming Pang",
                        "slug": "Ruoming-Pang",
                        "structuredName": {
                            "firstName": "Ruoming",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruoming Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053781980"
                        ],
                        "name": "Vijay Vasudevan",
                        "slug": "Vijay-Vasudevan",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Vasudevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vijay Vasudevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 184
                            }
                        ],
                        "text": "There is another way to enable and expedite the deep learning model inference at the edge which either quantizes the model to reduce size [12] or customizes the model for mobile usage [17, 27, 36, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 51891697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "693c97ecedb0a84539b7162c95e89fa3cd84ca73",
            "isKey": false,
            "numCitedBy": 1613,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8\u00d7 faster than MobileNetV2 with 0.5% higher accuracy and 2.3\u00d7 faster than NASNet with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet."
            },
            "slug": "MnasNet:-Platform-Aware-Neural-Architecture-Search-Tan-Chen",
            "title": {
                "fragments": [],
                "text": "MnasNet: Platform-Aware Neural Architecture Search for Mobile"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3295224"
                        ],
                        "name": "Ivan Grasso",
                        "slug": "Ivan-Grasso",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Grasso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivan Grasso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3135771"
                        ],
                        "name": "Petar Radojkovic",
                        "slug": "Petar-Radojkovic",
                        "structuredName": {
                            "firstName": "Petar",
                            "lastName": "Radojkovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Petar Radojkovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2462317"
                        ],
                        "name": "Nikola Rajovic",
                        "slug": "Nikola-Rajovic",
                        "structuredName": {
                            "firstName": "Nikola",
                            "lastName": "Rajovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikola Rajovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2750421"
                        ],
                        "name": "Isaac Gelado",
                        "slug": "Isaac-Gelado",
                        "structuredName": {
                            "firstName": "Isaac",
                            "lastName": "Gelado",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Isaac Gelado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145759993"
                        ],
                        "name": "A. Ram\u00edrez",
                        "slug": "A.-Ram\u00edrez",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Ram\u00edrez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ram\u00edrez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ators. 3.2 Computationally-intensive Operators 3.2.1 Optimization Consideration on Intel Graphics. While integrated GPUs from ARM and Nvidia have been gathered a number of research interests recently [1, 7, 11], it is challenging to extend the successful optimization tricks to Intel Graphics. To the best of our knowledge, there is virtually no published work that provides detailed insights regarding the met"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2146571,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "887c52174de63d53c7c11adb06101e907597d26e",
            "isKey": true,
            "numCitedBy": 57,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "A lot of effort from academia and industry has been invested in exploring the suitability of low-power embedded technologies for HPC. Although state-of-the-art embedded systems-on-chip (SoCs) inherently contain GPUs that could be used for HPC, their performance and energy capabilities have never been evaluated. Two reasons contribute to the above. Primarily, embedded GPUs until now, have not supported 64-bit floating point arithmetic - a requirement for HPC. Secondly, embedded GPUs did not provide support for parallel programming languages such as OpenCL and CUDA. However, the situation is changing, and the latest GPUs integrated in embedded SoCs do support 64-bit floating point precision and parallel programming models. In this paper, we analyze performance and energy advantages of embedded GPUs for HPC. In particular, we analyze ARM Mali-T604 GPU - the first embedded GPUs with OpenCL Full Profile support. We identify, implement and evaluate software optimization techniques for efficient utilization of the ARM Mali GPU Compute Architecture. Our results show that, HPC benchmarks running on the ARM Mali-T604 GPU integrated into Exynos 5250 SoC, on average, achieve speed-up of 8.7X over a single Cortex-A15 core, while consuming only 32% of the energy. Overall results show that embedded GPUs have performance and energy qualities that make them candidates for future HPC systems."
            },
            "slug": "Energy-Efficient-HPC-on-Embedded-SoCs:-Optimization-Grasso-Radojkovic",
            "title": {
                "fragments": [],
                "text": "Energy Efficient HPC on Embedded SoCs: Optimization Techniques for Mali GPU"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Overall results show that embedded GPUs have performance and energy qualities that make them candidates for future HPC systems."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE 28th International Parallel and Distributed Processing Symposium"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143840275"
                        ],
                        "name": "Song Han",
                        "slug": "Song-Han",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3123774"
                        ],
                        "name": "Huizi Mao",
                        "slug": "Huizi-Mao",
                        "structuredName": {
                            "firstName": "Huizi",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huizi Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80724002"
                        ],
                        "name": "W. Dally",
                        "slug": "W.-Dally",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dally",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dally"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 138
                            }
                        ],
                        "text": "There is another way to enable and expedite the deep learning model inference at the edge which either quantizes the model to reduce size [12] or customizes the model for mobile usage [17, 27, 36, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2134321,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "642d0f49b7826adcf986616f4af77e736229990f",
            "isKey": false,
            "numCitedBy": 5732,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks are both computationally intensive and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources. To address this limitation, we introduce \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy. Our method first prunes the network by learning only the important connections. Next, we quantize the weights to enforce weight sharing, finally, we apply Huffman coding. After the first two steps we retrain the network to fine tune the remaining connections and the quantized centroids. Pruning, reduces the number of connections by 9x to 13x; Quantization then reduces the number of bits that represent each connection from 32 to 5. On the ImageNet dataset, our method reduced the storage required by AlexNet by 35x, from 240MB to 6.9MB, without loss of accuracy. Our method reduced the size of VGG-16 by 49x from 552MB to 11.3MB, again with no loss of accuracy. This allows fitting the model into on-chip SRAM cache rather than off-chip DRAM memory. Our compression method also facilitates the use of complex neural networks in mobile applications where application size and download bandwidth are constrained. Benchmarked on CPU, GPU and mobile GPU, compressed network has 3x to 4x layerwise speedup and 3x to 7x better energy efficiency."
            },
            "slug": "Deep-Compression:-Compressing-Deep-Neural-Network-Han-Mao",
            "title": {
                "fragments": [],
                "text": "Deep Compression: Compressing Deep Neural Network with Pruning, Trained Quantization and Huffman Coding"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work introduces \"deep compression\", a three stage pipeline: pruning, trained quantization and Huffman coding, that work together to reduce the storage requirement of neural networks by 35x to 49x without affecting their accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50875121"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiangyu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148927556"
                        ],
                        "name": "Xinyu Zhou",
                        "slug": "Xinyu-Zhou",
                        "structuredName": {
                            "firstName": "Xinyu",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyu Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3287035"
                        ],
                        "name": "Mengxiao Lin",
                        "slug": "Mengxiao-Lin",
                        "structuredName": {
                            "firstName": "Mengxiao",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mengxiao Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "There is another way to enable and expedite the deep learning model inference at the edge which either quantizes the model to reduce size [12] or customizes the model for mobile usage [17, 27, 36, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 24982157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9da734397acd7ff7c557960c62fb1b400b27bd89",
            "isKey": false,
            "numCitedBy": 3251,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13\u00c3\u2014 actual speedup over AlexNet while maintaining comparable accuracy."
            },
            "slug": "ShuffleNet:-An-Extremely-Efficient-Convolutional-Zhang-Zhou",
            "title": {
                "fragments": [],
                "text": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An extremely computation-efficient CNN architecture named ShuffleNet is introduced, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs), to greatly reduce computation cost while maintaining accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1560397753"
                        ],
                        "name": "Jing Pu",
                        "slug": "Jing-Pu",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Pu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Pu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32534031"
                        ],
                        "name": "Steven Bell",
                        "slug": "Steven-Bell",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Bell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Bell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128659231"
                        ],
                        "name": "Xuan S. Yang",
                        "slug": "Xuan-S.-Yang",
                        "structuredName": {
                            "firstName": "Xuan",
                            "lastName": "Yang",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuan S. Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5291030"
                        ],
                        "name": "Jeff Setter",
                        "slug": "Jeff-Setter",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Setter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Setter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145326337"
                        ],
                        "name": "S. Richardson",
                        "slug": "S.-Richardson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Richardson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Richardson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401885963"
                        ],
                        "name": "Jonathan Ragan-Kelley",
                        "slug": "Jonathan-Ragan-Kelley",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Ragan-Kelley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Ragan-Kelley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144764327"
                        ],
                        "name": "M. Horowitz",
                        "slug": "M.-Horowitz",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Horowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Horowitz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[32] extended the image processing language, Halide [33], to allow users to specify which part of their applications they want to"
                    },
                    "intents": []
                }
            ],
            "corpusId": 16086906,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9bf383dd76f2df2ed84ea07949b63852557f174",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Specialized image processing accelerators are necessary to deliver the performance and energy efficiency required by important applications in computer vision, computational photography, and augmented reality. But creating, \u201cprogramming,\u201d and integrating this hardware into a hardware/software system is difficult. We address this problem by extending the image processing language Halide so users can specify which portions of their applications should become hardware accelerators, and then we provide a compiler that uses this code to automatically create the accelerator along with the \u201cglue\u201d code needed for the user\u2019s application to access this hardware. Starting with Halide not only provides a very high-level functional description of the hardware but also allows our compiler to generate a complete software application, which accesses the hardware for acceleration when appropriate. Our system also provides high-level semantics to explore different mappings of applications to a heterogeneous system, including the flexibility of being able to change the throughput rate of the generated hardware. We demonstrate our approach by mapping applications to a commercial Xilinx Zynq system. Using its FPGA with two low-power ARM cores, our design achieves up to 6\u00d7 higher performance and 38\u00d7 lower energy compared to the quad-core ARM CPU on an NVIDIA Tegra K1, and 3.5\u00d7 higher performance with 12\u00d7 lower energy compared to the K1\u2019s 192-core GPU."
            },
            "slug": "Programming-Heterogeneous-Systems-from-an-Image-DSL-Pu-Bell",
            "title": {
                "fragments": [],
                "text": "Programming Heterogeneous Systems from an Image Processing DSL"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The image processing language Halide is extended so users can specify which portions of their applications should become hardware accelerators, and a compiler is provided that uses this code to automatically create the accelerator along with the \u201cglue\u201d code needed for the user\u2019s application to access this hardware."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Archit. Code Optim."
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46641573"
                        ],
                        "name": "W. Liu",
                        "slug": "W.-Liu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838674"
                        ],
                        "name": "Dragomir Anguelov",
                        "slug": "Dragomir-Anguelov",
                        "structuredName": {
                            "firstName": "Dragomir",
                            "lastName": "Anguelov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dragomir Anguelov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761978"
                        ],
                        "name": "D. Erhan",
                        "slug": "D.-Erhan",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Erhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Erhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144828948"
                        ],
                        "name": "Scott E. Reed",
                        "slug": "Scott-E.-Reed",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Reed",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott E. Reed"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2084646762"
                        ],
                        "name": "Cheng-Yang Fu",
                        "slug": "Cheng-Yang-Fu",
                        "structuredName": {
                            "firstName": "Cheng-Yang",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Yang Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39668247"
                        ],
                        "name": "A. Berg",
                        "slug": "A.-Berg",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Berg",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Berg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "Sorting, essentially, argsort, is a common operator in CNN models, such as SSD [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "For example, SSD yields a large number of predictions to achieve more coverage of location, scale, and aspect ratios."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "We tested out the performance across a wide range of CNN models from image classification (ResNet [14], MobileNet [17], SqueezeNet [20]) to object detection (SSD [25], Yolo [34])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "For example, the NMS operators frequently used in SSDmodels contain sorting operations to sort the small data blocks where each of them may vary in the input size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "For example, running SSD model inference (backed with ResNet) of one sample entirely on the integrated GPU of AWSDeepLens took 1010.23 ms, while falling back the nms operators which involves sorting to CPU took 1015.14 ms, leading to an overhead less than 0.5%."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "However, a number of commonly-used CNN models, e.g. SSD [25] and Yolo [34], are not fully supported due to the missing of some vision-specific operators."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "SSD [25] and Yolo [34], are not fully supported due to the missing of some vision-specific"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "As a result, it is practically difficult to run object detection models, such as SSD and Yolo, entirely on integrated GPUs."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2141740,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d7a9197433acbfb24ef0e9d0f33ed1699e4a5b0",
            "isKey": true,
            "numCitedBy": 15747,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for detecting objects in images using a single deep neural network. Our approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location. At prediction time, the network generates scores for the presence of each object category in each default box and produces adjustments to the box to better match the object shape. Additionally, the network combines predictions from multiple feature maps with different resolutions to naturally handle objects of various sizes. Our SSD model is simple relative to methods that require object proposals because it completely eliminates proposal generation and subsequent pixel or feature resampling stage and encapsulates all computation in a single network. This makes SSD easy to train and straightforward to integrate into systems that require a detection component. Experimental results on the PASCAL VOC, MS COCO, and ILSVRC datasets confirm that SSD has comparable accuracy to methods that utilize an additional object proposal step and is much faster, while providing a unified framework for both training and inference. Compared to other single stage methods, SSD has much better accuracy, even with a smaller input image size. For $300\\times 300$ input, SSD achieves 72.1% mAP on VOC2007 test at 58 FPS on a Nvidia Titan X and for $500\\times 500$ input, SSD achieves 75.1% mAP, outperforming a comparable state of the art Faster R-CNN model. Code is available at this https URL ."
            },
            "slug": "SSD:-Single-Shot-MultiBox-Detector-Liu-Anguelov",
            "title": {
                "fragments": [],
                "text": "SSD: Single Shot MultiBox Detector"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "The approach, named SSD, discretizes the output space of bounding boxes into a set of default boxes over different aspect ratios and scales per feature map location, which makes SSD easy to train and straightforward to integrate into systems that require a detection component."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144727050"
                        ],
                        "name": "Andrew G. Howard",
                        "slug": "Andrew-G.-Howard",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Howard",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew G. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2717876"
                        ],
                        "name": "Menglong Zhu",
                        "slug": "Menglong-Zhu",
                        "structuredName": {
                            "firstName": "Menglong",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Menglong Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Bo Chen",
                        "slug": "Bo-Chen",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2741985"
                        ],
                        "name": "Dmitry Kalenichenko",
                        "slug": "Dmitry-Kalenichenko",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Kalenichenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dmitry Kalenichenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108301072"
                        ],
                        "name": "Weijun Wang",
                        "slug": "Weijun-Wang",
                        "structuredName": {
                            "firstName": "Weijun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weijun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47447630"
                        ],
                        "name": "Tobias Weyand",
                        "slug": "Tobias-Weyand",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "Weyand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tobias Weyand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612392"
                        ],
                        "name": "M. Andreetto",
                        "slug": "M.-Andreetto",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Andreetto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Andreetto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595180"
                        ],
                        "name": "Hartwig Adam",
                        "slug": "Hartwig-Adam",
                        "structuredName": {
                            "firstName": "Hartwig",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hartwig Adam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 114
                            }
                        ],
                        "text": "We tested out the performance across a wide range of CNN models from image classification (ResNet [14], MobileNet [17], SqueezeNet [20]) to object detection (SSD [25], Yolo [34])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 184
                            }
                        ],
                        "text": "There is another way to enable and expedite the deep learning model inference at the edge which either quantizes the model to reduce size [12] or customizes the model for mobile usage [17, 27, 36, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 18
                            }
                        ],
                        "text": "\u2022 Our solution for MobileNet on Intel Graphics (Deeplens) is not as good as on Mali GPUs (aiSage) or Nvidia GPUs (Nano)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12670695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3647d6d0f151dc05626449ee09cc7bce55be497e",
            "isKey": false,
            "numCitedBy": 10323,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization."
            },
            "slug": "MobileNets:-Efficient-Convolutional-Neural-Networks-Howard-Zhu",
            "title": {
                "fragments": [],
                "text": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work introduces two simple global hyper-parameters that efficiently trade off between latency and accuracy and demonstrates the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "er edge devices featured with similar integrated GPUs because our programming environment is unified. We tested out the performance across a wide range of CNN models from image classification (ResNet [14], MobileNet [17], SqueezeNet [20]) to object detection (SSD [25], Yolo [34]). The models are retrieved from GluonCV model zoo 4, pre-trained using MXNet. The chosen models represent the majority of th"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": false,
            "numCitedBy": 97653,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3346186"
                        ],
                        "name": "Forrest N. Iandola",
                        "slug": "Forrest-N.-Iandola",
                        "structuredName": {
                            "firstName": "Forrest",
                            "lastName": "Iandola",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Forrest N. Iandola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2318023"
                        ],
                        "name": "M. Moskewicz",
                        "slug": "M.-Moskewicz",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Moskewicz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moskewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059241"
                        ],
                        "name": "Khalid Ashraf",
                        "slug": "Khalid-Ashraf",
                        "structuredName": {
                            "firstName": "Khalid",
                            "lastName": "Ashraf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Khalid Ashraf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143840275"
                        ],
                        "name": "Song Han",
                        "slug": "Song-Han",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80724002"
                        ],
                        "name": "W. Dally",
                        "slug": "W.-Dally",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dally",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dally"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732330"
                        ],
                        "name": "K. Keutzer",
                        "slug": "K.-Keutzer",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Keutzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Keutzer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 69
                            }
                        ],
                        "text": "For example, our solution achieves up to 39.3\u00d7 and 12.78\u00d7 speedups for SqueezeNet on Nvidia Jetson Nano and Acer aiSage, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "We tested out the performance across a wide range of CNN models from image classification (ResNet [14], MobileNet [17], SqueezeNet [20]) to object detection (SSD [25], Yolo [34])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 14
                            }
                        ],
                        "text": "The reason of SqueezeNet being improved the most is that the network is fairly new so there is no manually written implementation of it in good performance."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14136028,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "969fbdcd0717bec06228053788c2ff78bbb4daac",
            "isKey": true,
            "numCitedBy": 4092,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent research on deep neural networks has focused primarily on improving accuracy. For a given accuracy level, it is typically possible to identify multiple DNN architectures that achieve that accuracy level. With equivalent accuracy, smaller DNN architectures offer at least three advantages: (1) Smaller DNNs require less communication across servers during distributed training. (2) Smaller DNNs require less bandwidth to export a new model from the cloud to an autonomous car. (3) Smaller DNNs are more feasible to deploy on FPGAs and other hardware with limited memory. To provide all of these advantages, we propose a small DNN architecture called SqueezeNet. SqueezeNet achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters. Additionally, with model compression techniques we are able to compress SqueezeNet to less than 0.5MB (510x smaller than AlexNet). \nThe SqueezeNet architecture is available for download here: this https URL"
            },
            "slug": "SqueezeNet:-AlexNet-level-accuracy-with-50x-fewer-Iandola-Moskewicz",
            "title": {
                "fragments": [],
                "text": "SqueezeNet: AlexNet-level accuracy with 50x fewer parameters and <1MB model size"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes a small DNN architecture called SqueezeNet, which achieves AlexNet-level accuracy on ImageNet with 50x fewer parameters and is able to compress to less than 0.5MB (510x smaller than AlexNet)."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1971458"
                        ],
                        "name": "K. Hou",
                        "slug": "K.-Hou",
                        "structuredName": {
                            "firstName": "Kaixi",
                            "lastName": "Hou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49663558"
                        ],
                        "name": "Weifeng Liu",
                        "slug": "Weifeng-Liu",
                        "structuredName": {
                            "firstName": "Weifeng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weifeng Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144236891"
                        ],
                        "name": "Hao Wang",
                        "slug": "Hao-Wang",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145476815"
                        ],
                        "name": "Wu-chun Feng",
                        "slug": "Wu-chun-Feng",
                        "structuredName": {
                            "firstName": "Wu-chun",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wu-chun Feng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 92
                            }
                        ],
                        "text": "Note that although our implementation uses similar ideas as in previous segmented sort work [16] and merge sort work [38], it is not limited to Nvidia GPUs but uses the unified program to run efficiently on different integrated GPUs with different architectures."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 8074252,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abc8aafc50b5e81b29ee4d543261d04146f2e335",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Segmented sort, as a generalization of classical sort, orders a batch of independent segments in a whole array. Along with the wider adoption of manycore processors for HPC and big data applications, segmented sort plays an increasingly important role than sort. In this paper, we present an adaptive segmented sort mechanism on GPUs. Our mechanisms include two core techniques: (1) a differentiated method for different segment lengths to eliminate the irregularity caused by various workloads and thread divergence; and (2) a register-based sort method to support N-to-M data-thread binding and in-register data communication. We also implement a shared memory-based merge method to support non-uniform length chunk merge via multiple warps. Our segmented sort mechanism shows great improvements over the methods from CUB, CUSP and ModernGPU on NVIDIA K80-Kepler and TitanX-Pascal GPUs. Furthermore, we apply our mechanism on two applications, i.e., suffix array construction and sparse matrix-matrix multiplication, and obtain obvious gains over state-of-the-art implementations."
            },
            "slug": "Fast-segmented-sort-on-GPUs-Hou-Liu",
            "title": {
                "fragments": [],
                "text": "Fast segmented sort on GPUs"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper presents an adaptive segmented sort mechanism on GPUs that shows great improvements over the methods from CUB, CUSP and ModernGPU on NVIDIA K80-Kepler and TitanX-Pascal GPUs and applies it on two applications, i.e., suffix array construction and sparse matrix-matrix multiplication, and obtains obvious gains over state-of-the-art implementations."
            },
            "venue": {
                "fragments": [],
                "text": "ICS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797270"
                        ],
                        "name": "Carole-Jean Wu",
                        "slug": "Carole-Jean-Wu",
                        "structuredName": {
                            "firstName": "Carole-Jean",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carole-Jean Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1896817"
                        ],
                        "name": "D. Brooks",
                        "slug": "D.-Brooks",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Brooks",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Brooks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152955923"
                        ],
                        "name": "Kevin Chen",
                        "slug": "Kevin-Chen",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158194412"
                        ],
                        "name": "Douglas Chen",
                        "slug": "Douglas-Chen",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89754631"
                        ],
                        "name": "Sy Choudhury",
                        "slug": "Sy-Choudhury",
                        "structuredName": {
                            "firstName": "Sy",
                            "lastName": "Choudhury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sy Choudhury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3193064"
                        ],
                        "name": "Marat Dukhan",
                        "slug": "Marat-Dukhan",
                        "structuredName": {
                            "firstName": "Marat",
                            "lastName": "Dukhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marat Dukhan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775500"
                        ],
                        "name": "Kim M. Hazelwood",
                        "slug": "Kim-M.-Hazelwood",
                        "structuredName": {
                            "firstName": "Kim",
                            "lastName": "Hazelwood",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kim M. Hazelwood"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89274305"
                        ],
                        "name": "Eldad Isaac",
                        "slug": "Eldad-Isaac",
                        "structuredName": {
                            "firstName": "Eldad",
                            "lastName": "Isaac",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eldad Isaac"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33920592"
                        ],
                        "name": "Bill Jia",
                        "slug": "Bill-Jia",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bill Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3316156"
                        ],
                        "name": "Tommer Leyvand",
                        "slug": "Tommer-Leyvand",
                        "structuredName": {
                            "firstName": "Tommer",
                            "lastName": "Leyvand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tommer Leyvand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115608531"
                        ],
                        "name": "Hao Lu",
                        "slug": "Hao-Lu",
                        "structuredName": {
                            "firstName": "Hao",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hao Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146557923"
                        ],
                        "name": "Yang Lu",
                        "slug": "Yang-Lu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065508882"
                        ],
                        "name": "Lin Qiao",
                        "slug": "Lin-Qiao",
                        "structuredName": {
                            "firstName": "Lin",
                            "lastName": "Qiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lin Qiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2732658"
                        ],
                        "name": "Brandon Reagen",
                        "slug": "Brandon-Reagen",
                        "structuredName": {
                            "firstName": "Brandon",
                            "lastName": "Reagen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brandon Reagen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90591458"
                        ],
                        "name": "Joe Spisak",
                        "slug": "Joe-Spisak",
                        "structuredName": {
                            "firstName": "Joe",
                            "lastName": "Spisak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joe Spisak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075373634"
                        ],
                        "name": "Fei Sun",
                        "slug": "Fei-Sun",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3609856"
                        ],
                        "name": "Andrew Tulloch",
                        "slug": "Andrew-Tulloch",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Tulloch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Tulloch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48682997"
                        ],
                        "name": "P\u00e9ter Vajda",
                        "slug": "P\u00e9ter-Vajda",
                        "structuredName": {
                            "firstName": "P\u00e9ter",
                            "lastName": "Vajda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P\u00e9ter Vajda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108429236"
                        ],
                        "name": "Xiaodong Wang",
                        "slug": "Xiaodong-Wang",
                        "structuredName": {
                            "firstName": "Xiaodong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaodong Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146020227"
                        ],
                        "name": "Yanghan Wang",
                        "slug": "Yanghan-Wang",
                        "structuredName": {
                            "firstName": "Yanghan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanghan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46240090"
                        ],
                        "name": "Bram Wasti",
                        "slug": "Bram-Wasti",
                        "structuredName": {
                            "firstName": "Bram",
                            "lastName": "Wasti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bram Wasti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115877352"
                        ],
                        "name": "Yiming Wu",
                        "slug": "Yiming-Wu",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066571092"
                        ],
                        "name": "Ran Xian",
                        "slug": "Ran-Xian",
                        "structuredName": {
                            "firstName": "Ran",
                            "lastName": "Xian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ran Xian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808405"
                        ],
                        "name": "S. Yoo",
                        "slug": "S.-Yoo",
                        "structuredName": {
                            "firstName": "Sungjoo",
                            "lastName": "Yoo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yoo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2918780"
                        ],
                        "name": "Peizhao Zhang",
                        "slug": "Peizhao-Zhang",
                        "structuredName": {
                            "firstName": "Peizhao",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peizhao Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "oyed at the edge devices, there is active research on optimizing the deep learning model inference, especially CNN models, locally on mobile SoCs. The first place to execute the models is mobile CPUs [39], which is heavily studied and optimized [26]. However, as mobile CPUs typically have less FLOPs and consumes more power, it is more favorable to leverage other compute units on the SoCs to handle the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "y much less powerful than server side discrete GPUs, are able to deliver higher FLOPs than the accompanying CPUs. However, in practice, the majority of model inference at the edge is executed on CPUs [39] because of easier programmability and more flexible portability across different SoCs. It is actually less favorable to execute deep learning model inferencesonCPUsattheedgeduetotworeasons.First,CPUi"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 89617717,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67df7bf02fe2d618c7c18448c2668a526dc4d423",
            "isKey": false,
            "numCitedBy": 242,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "At Facebook, machine learning provides a wide range of capabilities that drive many aspects of user experience including ranking posts, content understanding, object detection and tracking for augmented and virtual reality, speech and text translations. While machine learning models are currently trained on customized datacenter infrastructure, Facebook is working to bring machine learning inference to the edge. By doing so, user experience is improved with reduced latency (inference time) and becomes less dependent on network connectivity. Furthermore, this also enables many more applications of deep learning with important features only made available at the edge. This paper takes a datadriven approach to present the opportunities and design challenges faced by Facebook in order to enable machine learning inference locally on smartphones and other edge platforms."
            },
            "slug": "Machine-Learning-at-Facebook:-Understanding-at-the-Wu-Brooks",
            "title": {
                "fragments": [],
                "text": "Machine Learning at Facebook: Understanding Inference at the Edge"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper takes a datadriven approach to present the opportunities and design challenges faced by Facebook in order to enable machine learning inference locally on smartphones and other edge platforms."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076486589"
                        ],
                        "name": "M. Anglada",
                        "slug": "M.-Anglada",
                        "structuredName": {
                            "firstName": "Mart\u00ed",
                            "lastName": "Anglada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Anglada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055561549"
                        ],
                        "name": "Enrique de Lucas",
                        "slug": "Enrique-de-Lucas",
                        "structuredName": {
                            "firstName": "Enrique",
                            "lastName": "Lucas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enrique de Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144767339"
                        ],
                        "name": "Joan-Manuel Parcerisa",
                        "slug": "Joan-Manuel-Parcerisa",
                        "structuredName": {
                            "firstName": "Joan-Manuel",
                            "lastName": "Parcerisa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joan-Manuel Parcerisa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145092078"
                        ],
                        "name": "Juan L. Arag\u00f3n",
                        "slug": "Juan-L.-Arag\u00f3n",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Arag\u00f3n",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Juan L. Arag\u00f3n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144359098"
                        ],
                        "name": "Antonio Gonz\u00e1lez",
                        "slug": "Antonio-Gonz\u00e1lez",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Gonz\u00e1lez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antonio Gonz\u00e1lez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "ators. 3.2 Computationally-intensive Operators 3.2.1 Optimization Consideration on Intel Graphics. While integrated GPUs from ARM and Nvidia have been gathered a number of research interests recently [1, 7, 11], it is challenging to extend the successful optimization tricks to Intel Graphics. To the best of our knowledge, there is virtually no published work that provides detailed insights regarding the met"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 89617518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebd31202aee7bf00ae6b136633e73692d7c5c868",
            "isKey": true,
            "numCitedBy": 6,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "GPUs' main workload is real-time image rendering. These applications take a description of a (animated) scene and produce the corresponding image(s). An image is rendered by computing the colors of all its pixels. It is normal that multiple objects overlap at each pixel. Consequently, a significant amount of processing is devoted to objects that will not be visible in the final image, in spite of the widespread use of the Early Depth Test in modern GPUs, which attempts to discard computations related to occluded objects. Since animations are created by a sequence of similar images, visibility usually does not change much across consecutive frames. Based on this observation, we present Early Visibility Resolution (EVR), a mechanism that leverages the visibility information obtained in a frame to predict the visibility in the following one. Our proposal speculatively determines visibility much earlier in the pipeline than the Early Depth Test. We leverage this early visibility estimation to remove ineffectual computations at two different granularities: pixel-level and tile-level. Results show that such optimizations lead to 39% performance improvement and 43% energy savings for a set of commercial Android graphics applications running on stateof-the-art mobile GPUs."
            },
            "slug": "Early-Visibility-Resolution-for-Removing-in-the-Anglada-Lucas",
            "title": {
                "fragments": [],
                "text": "Early Visibility Resolution for Removing Ineffectual Computations in the Graphics Pipeline"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes Early Visibility Resolution (EVR), a mechanism that leverages the visibility information obtained in a frame to predict the visibility in the following one, and leverages this early visibility estimation to remove ineffectual computations at two different granularities: pixel-level and tile-level."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE International Symposium on High Performance Computer Architecture (HPCA)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8143586"
                        ],
                        "name": "K. Karimi",
                        "slug": "K.-Karimi",
                        "structuredName": {
                            "firstName": "Kamran",
                            "lastName": "Karimi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Karimi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2838365"
                        ],
                        "name": "N. Dickson",
                        "slug": "N.-Dickson",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Dickson",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dickson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1811875"
                        ],
                        "name": "F. Hamze",
                        "slug": "F.-Hamze",
                        "structuredName": {
                            "firstName": "Firas",
                            "lastName": "Hamze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hamze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "On the other hand, Nvidia products run CUDA as its proprietary driver which outperforms OpenCL [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16717155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ec8e9fe1b6150aaa0995134c001234a71304730",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "CUDA and OpenCL are two different frameworks for GPU programming. OpenCL is an open standard that can be used to program CPUs, GPUs, and other devices from different vendors, while CUDA is specific to NVIDIA GPUs. Although OpenCL promises a portable language for GPU programming, its generality may entail a performance penalty. In this paper, we use complex, near-identical kernels from a Quantum Monte Carlo application to compare the performance of CUDA and OpenCL. We show that when using NVIDIA compiler tools, converting a CUDA kernel to an OpenCL kernel involves minimal modifications. Making such a kernel compile with ATI's build tools involves more modifications. Our performance tests measure and compare data transfer times to and from the GPU, kernel execution times, and end-to-end application execution times for both CUDA and OpenCL."
            },
            "slug": "A-Performance-Comparison-of-CUDA-and-OpenCL-Karimi-Dickson",
            "title": {
                "fragments": [],
                "text": "A Performance Comparison of CUDA and OpenCL"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper uses complex, near-identical kernels from a Quantum Monte Carlo application to compare the performance of CUDA and OpenCL and shows that when using NVIDIA compiler tools, converting a CUDA kernel to an OpenCL kernel involves minimal modifications."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841852"
                        ],
                        "name": "Bradley McDanel",
                        "slug": "Bradley-McDanel",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "McDanel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bradley McDanel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3242151"
                        ],
                        "name": "Surat Teerapittayanon",
                        "slug": "Surat-Teerapittayanon",
                        "structuredName": {
                            "firstName": "Surat",
                            "lastName": "Teerapittayanon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Surat Teerapittayanon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144153718"
                        ],
                        "name": "H. T. Kung",
                        "slug": "H.-T.-Kung",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Kung",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. T. Kung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 184
                            }
                        ],
                        "text": "There is another way to enable and expedite the deep learning model inference at the edge which either quantizes the model to reduce size [12] or customizes the model for mobile usage [17, 27, 36, 40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6804572,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e3f5aeec99ad94886c74fdd283bd369e3272936",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We study embedded Binarized Neural Networks (eBNNs) with the aim of allowing current binarized neural networks (BNNs) in the literature to perform feedforward inference efficiently on small embedded devices. We focus on minimizing the required memory footprint, given that these devices often have memory as small as tens of kilobytes (KB). Beyond minimizing the memory required to store weights, as in a BNN, we show that it is essential to minimize the memory used for temporaries which hold intermediate results between layers in feedforward inference. To accomplish this, eBNN reorders the computation of inference while preserving the original BNN structure, and uses just a single floating-point temporary for the entire neural network. All intermediate results from a layer are stored as binary values, as opposed to floating-points used in current BNN implementations, leading to a 32x reduction in required temporary space. We provide empirical evidence that our proposed eBNN approach allows efficient inference (10s of ms) on devices with severely limited memory (10s of KB). For example, eBNN achieves 95\\% accuracy on the MNIST dataset running on an Intel Curie with only 15 KB of usable memory with an inference runtime of under 50 ms per sample. To ease the development of applications in embedded contexts, we make our source code available that allows users to train and discover eBNN models for a learning task at hand, which fit within the memory constraint of the target device."
            },
            "slug": "Embedded-Binarized-Neural-Networks-McDanel-Teerapittayanon",
            "title": {
                "fragments": [],
                "text": "Embedded Binarized Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "eBNN reorders the computation of inference while preserving the original BNN structure, and uses just a single floating-point temporary for the entire neural network, leading to a 32x reduction in required temporary space."
            },
            "venue": {
                "fragments": [],
                "text": "EWSN"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401885963"
                        ],
                        "name": "Jonathan Ragan-Kelley",
                        "slug": "Jonathan-Ragan-Kelley",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Ragan-Kelley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Ragan-Kelley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2496412"
                        ],
                        "name": "Connelly Barnes",
                        "slug": "Connelly-Barnes",
                        "structuredName": {
                            "firstName": "Connelly",
                            "lastName": "Barnes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Connelly Barnes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144187067"
                        ],
                        "name": "Andrew Adams",
                        "slug": "Andrew-Adams",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Adams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Adams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145799132"
                        ],
                        "name": "Sylvain Paris",
                        "slug": "Sylvain-Paris",
                        "structuredName": {
                            "firstName": "Sylvain",
                            "lastName": "Paris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sylvain Paris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145403226"
                        ],
                        "name": "F. Durand",
                        "slug": "F.-Durand",
                        "structuredName": {
                            "firstName": "Fr\u00e9do",
                            "lastName": "Durand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Durand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709150"
                        ],
                        "name": "Saman P. Amarasinghe",
                        "slug": "Saman-P.-Amarasinghe",
                        "structuredName": {
                            "firstName": "Saman",
                            "lastName": "Amarasinghe",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saman P. Amarasinghe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "inherited from Halide [33], TVM utilizes a unified IR to lower the optimization schemes of different integrated GPUs to CUDA or OpenCL for code generation on those devices."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[32] extended the image processing language, Halide [33], to allow users to specify which part of their applications they want to"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This idea was inherited from Halide [33]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5885207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d032f74b16457584f8a60ae07cfef9b617033638",
            "isKey": false,
            "numCitedBy": 878,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Image processing pipelines combine the challenges of stencil computations and stream programs. They are composed of large graphs of different stencil stages, as well as complex reductions, and stages with global or data-dependent access patterns. Because of their complex structure, the performance difference between a naive implementation of a pipeline and an optimized one is often an order of magnitude. Efficient implementations require optimization of both parallelism and locality, but due to the nature of stencils, there is a fundamental tension between parallelism, locality, and introducing redundant recomputation of shared values. We present a systematic model of the tradeoff space fundamental to stencil pipelines, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule. Combining this compiler with stochastic search over the space of schedules enables terse, composable programs to achieve state-of-the-art performance on a wide range of real image processing pipelines, and across different hardware architectures, including multicores with SIMD, and heterogeneous CPU+GPU execution. From simple Halide programs written in a few hours, we demonstrate performance up to 5x faster than hand-tuned C, intrinsics, and CUDA implementations optimized by experts over weeks or months, for image processing applications beyond the reach of past automatic compilers."
            },
            "slug": "Halide:-a-language-and-compiler-for-optimizing-and-Ragan-Kelley-Barnes",
            "title": {
                "fragments": [],
                "text": "Halide: a language and compiler for optimizing parallelism, locality, and recomputation in image processing pipelines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A systematic model of the tradeoff space fundamental to stencil pipelines is presented, a schedule representation which describes concrete points in this space for each stage in an image processing pipeline, and an optimizing compiler for the Halide image processing language that synthesizes high performance implementations from a Halide algorithm and a schedule are presented."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI 2013"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2185540"
                        ],
                        "name": "Leyuan Wang",
                        "slug": "Leyuan-Wang",
                        "structuredName": {
                            "firstName": "Leyuan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leyuan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37145355"
                        ],
                        "name": "S. Baxter",
                        "slug": "S.-Baxter",
                        "structuredName": {
                            "firstName": "Sean",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baxter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758404"
                        ],
                        "name": "John Douglas Owens",
                        "slug": "John-Douglas-Owens",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Owens",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Douglas Owens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "Note that although our implementation uses similar ideas as in previous segmented sort work [16] and merge sort work [38], it is not limited to Nvidia GPUs but uses the unified program to run efficiently on different integrated GPUs with different architectures."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 9021788,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64c59e9339293984f13fa01c21a98dbebeced8d4",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Suffix arrays are fundamental full\u2010text index data structures of importance to a broad spectrum of applications in such fields as bioinformatics, Burrows\u2013Wheeler transform\u2010based lossless data compression, and information retrieval. In this work, we propose and implement two massively parallel approaches on the graphics processing unit (GPU) based on two classes of suffix array construction algorithms. The first, parallel skew, makes algorithmic improvements to the previous work of Deo and Keely to achieve a speedup of 1.45x over their work. The second, a hybrid skew and prefix\u2010doubling implementation, is the first of its kind on the GPU and achieves a speedup of 2.3\u20134.4x over Osipov's prefix\u2010doubling and 2.4\u20137.9x over our skew implementation on large datasets. Our implementations rely on two efficient parallel primitives, a merge and a segmented sort. We theoretically analyze the two formulations of suffix array construction algorithms and show performance comparisons on a large variety of practical inputs. We conclude that, with the novel use of our efficient segmented sort, prefix\u2010doubling is more competitive than skew on the GPU. We also demonstrate the effectiveness of our methods in our implementations of the Burrows\u2010Wheeler transform and in a parallel full\u2010text, minute\u2010space\u2010index for pattern searching. Copyright \u00a9 2016 John Wiley & Sons, Ltd."
            },
            "slug": "Fast-parallel-skew-and-prefix\u2010doubling-suffix-array-Wang-Baxter",
            "title": {
                "fragments": [],
                "text": "Fast parallel skew and prefix\u2010doubling suffix array construction on the GPU"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work proposes and implements two massively parallel approaches on the graphics processing unit (GPU) based on two classes of suffix array construction algorithms, and concludes that, with the novel use of the efficient segmented sort, prefix\u2010doubling is more competitive than skew on the GPU."
            },
            "venue": {
                "fragments": [],
                "text": "Concurr. Comput. Pract. Exp."
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1953172"
                        ],
                        "name": "S. Sengupta",
                        "slug": "S.-Sengupta",
                        "structuredName": {
                            "firstName": "Shubhabrata",
                            "lastName": "Sengupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sengupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937132"
                        ],
                        "name": "Mark J. Harris",
                        "slug": "Mark-J.-Harris",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Harris",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark J. Harris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118389666"
                        ],
                        "name": "Yao Zhang",
                        "slug": "Yao-Zhang",
                        "structuredName": {
                            "firstName": "Yao",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yao Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758404"
                        ],
                        "name": "John Douglas Owens",
                        "slug": "John-Douglas-Owens",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Owens",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Douglas Owens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "x sum (Scan)is a trivial sequential algorithm on the CPU. But GPU is not optimized for doing sequential computations especially when work items have data dependence. Many algorithms in the literature [13, 30, 35] have optimized scan on Nvidia GPUs (CUDA specific) but none of them targeted all integrated GPUs. Based on the classic parallel scan by Hillis and Steele [15], we proposed an efficient prefix sum for"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 93702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ec88941835c88a2956fc84c9e7cfd6dedf1b36f6",
            "isKey": false,
            "numCitedBy": 616,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The scan primitives are powerful, general-purpose data-parallel primitives that are building blocks for a broad range of applications. We describe GPU implementations of these primitives, specifically an efficient formulation and implementation of segmented scan, on NVIDIA GPUs using the CUDA API. Using the scan primitives, we show novel GPU implementations of quicksort and sparse matrix-vector multiply, and analyze the performance of the scan primitives, several sort algorithms that use the scan primitives, and a graphical shallow-water fluid simulation using the scan framework for a tridiagonal matrix solver."
            },
            "slug": "Scan-primitives-for-GPU-computing-Sengupta-Harris",
            "title": {
                "fragments": [],
                "text": "Scan primitives for GPU computing"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Using the scan primitives, this work shows novel GPU implementations of quicksort and sparse matrix-vector multiply, and analyzes the performance of the scanPrimitives, several sort algorithms that use the scan Primitives, and a graphical shallow-water fluid simulation using the scan framework for a tridiagonal matrix solver."
            },
            "venue": {
                "fragments": [],
                "text": "GH '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40497777"
                        ],
                        "name": "Joseph Redmon",
                        "slug": "Joseph-Redmon",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Redmon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Redmon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787583"
                        ],
                        "name": "Ali Farhadi",
                        "slug": "Ali-Farhadi",
                        "structuredName": {
                            "firstName": "Ali",
                            "lastName": "Farhadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ali Farhadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "SSD [25] and Yolo [34], are not fully supported due to the missing of some vision-specific"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 173
                            }
                        ],
                        "text": "We tested out the performance across a wide range of CNN models from image classification (ResNet [14], MobileNet [17], SqueezeNet [20]) to object detection (SSD [25], Yolo [34])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "As a result, it is practically difficult to run object detection models, such as SSD and Yolo, entirely on integrated GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "However, a number of commonly-used CNN models, e.g. SSD [25] and Yolo [34], are not fully supported due to the missing of some vision-specific operators."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4714433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4845fb1e624965d4f036d7fd32e8dcdd2408148",
            "isKey": true,
            "numCitedBy": 8863,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We present some updates to YOLO! We made a bunch of little design changes to make it better. We also trained this new network that's pretty swell. It's a little bigger than last time but more accurate. It's still fast though, don't worry. At 320x320 YOLOv3 runs in 22 ms at 28.2 mAP, as accurate as SSD but three times faster. When we look at the old .5 IOU mAP detection metric YOLOv3 is quite good. It achieves 57.9 mAP@50 in 51 ms on a Titan X, compared to 57.5 mAP@50 in 198 ms by RetinaNet, similar performance but 3.8x faster. As always, all the code is online at this https URL"
            },
            "slug": "YOLOv3:-An-Incremental-Improvement-Redmon-Farhadi",
            "title": {
                "fragments": [],
                "text": "YOLOv3: An Incremental Improvement"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40384319"
                        ],
                        "name": "Zhangxiaowen Gong",
                        "slug": "Zhangxiaowen-Gong",
                        "structuredName": {
                            "firstName": "Zhangxiaowen",
                            "lastName": "Gong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhangxiaowen Gong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117099952"
                        ],
                        "name": "Zhi Chen",
                        "slug": "Zhi-Chen",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30436131"
                        ],
                        "name": "J. Szaday",
                        "slug": "J.-Szaday",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Szaday",
                            "middleNames": [
                                "Josef"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Szaday"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49744349"
                        ],
                        "name": "D. Wong",
                        "slug": "D.-Wong",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wong",
                            "middleNames": [
                                "Chi-Leung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2937166"
                        ],
                        "name": "Zehra Sura",
                        "slug": "Zehra-Sura",
                        "structuredName": {
                            "firstName": "Zehra",
                            "lastName": "Sura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zehra Sura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35222148"
                        ],
                        "name": "N. Watkinson",
                        "slug": "N.-Watkinson",
                        "structuredName": {
                            "firstName": "Neftali",
                            "lastName": "Watkinson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Watkinson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144834015"
                        ],
                        "name": "Saeed Maleki",
                        "slug": "Saeed-Maleki",
                        "structuredName": {
                            "firstName": "Saeed",
                            "lastName": "Maleki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saeed Maleki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729097"
                        ],
                        "name": "D. Padua",
                        "slug": "D.-Padua",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Padua",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Padua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764886"
                        ],
                        "name": "A. Veidenbaum",
                        "slug": "A.-Veidenbaum",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Veidenbaum",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Veidenbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145330843"
                        ],
                        "name": "A. Nicolau",
                        "slug": "A.-Nicolau",
                        "structuredName": {
                            "firstName": "Alexandru",
                            "lastName": "Nicolau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nicolau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695950"
                        ],
                        "name": "J. Torrellas",
                        "slug": "J.-Torrellas",
                        "structuredName": {
                            "firstName": "Josep",
                            "lastName": "Torrellas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Torrellas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 283,
                                "start": 279
                            }
                        ],
                        "text": "Loop unrolling normally attains benefits including reduced control overhead (by removing testing exit conditions), increased instruction level parallelism, better opportunity of scalar optimizations such as constant folding, and/or more friendly code for (partial) vectorization [10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52839502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d931023bc739cd7594d25e1a3c8699b27ad3110",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern compiler optimization is a complex process that offers no guarantees to deliver the fastest, most efficient target code. For this reason, compilers struggle to produce a stable performance from versions of code that carry out the same computation and only differ in the order of operations. This instability makes compilers much less effective program optimization tools and often forces programmers to carry out a brute force search when tuning for performance. In this paper, we analyze the stability of the compilation process and the performance headroom of three widely used general purpose compilers: GCC, ICC, and Clang. For the study, we extracted over 1,000 for loop nests from well-known benchmarks, libraries, and real applications; then, we applied sequences of source-level loop transformations to these loop nests to create numerous semantically equivalent mutations; finally, we analyzed the impact of transformations on code quality in terms of locality, dynamic instruction count, and vectorization. Our results show that, by applying source-to-source transformations and searching for the best vectorization setting, the percentage of loops sped up by at least 1.15x is 46.7% for GCC, 35.7% for ICC, and 46.5% for Clang, and on average the potential for performance improvement is estimated to be at least 23.7% for GCC, 18.1% for ICC, and 26.4% for Clang. Our stability analysis shows that, under our experimental setup, the average coefficient of variation of the execution time across all mutations is 18.2% for GCC, 19.5% for ICC, and 16.9% for Clang, and the highest coefficient of variation for a single loop nest reaches 118.9% for GCC, 124.3% for ICC, and 110.5% for Clang. We conclude that the evaluated compilers need further improvements to claim they have stable behavior."
            },
            "slug": "An-empirical-study-of-the-effect-of-source-level-on-Gong-Chen",
            "title": {
                "fragments": [],
                "text": "An empirical study of the effect of source-level loop transformations on compiler stability"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper analyzes the stability of the compilation process and the performance headroom of three widely used general purpose compilers: GCC, ICC, and Clang and concludes that the evaluated compilers need further improvements to claim they have stable behavior."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. ACM Program. Lang."
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143937132"
                        ],
                        "name": "Mark J. Harris",
                        "slug": "Mark-J.-Harris",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Harris",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark J. Harris"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "x sum (Scan)is a trivial sequential algorithm on the CPU. But GPU is not optimized for doing sequential computations especially when work items have data dependence. Many algorithms in the literature [13, 30, 35] have optimized scan on Nvidia GPUs (CUDA specific) but none of them targeted all integrated GPUs. Based on the classic parallel scan by Hillis and Steele [15], we proposed an efficient prefix sum for"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61762466,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "d0b4a903649fa45d5eb9e40edd632c607052aec8",
            "isKey": false,
            "numCitedBy": 731,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ".............................................................................................................1 Table of"
            },
            "slug": "Parallel-Prefix-Sum-(Scan)-with-CUDA-Harris",
            "title": {
                "fragments": [],
                "text": "Parallel Prefix Sum (Scan) with CUDA"
            },
            "tldr": {
                "abstractSimilarityScore": 8,
                "text": "The water needs of this region have changed in recent years from being primarily for agricultural purposes to domestic and industrial uses now, and the needs of these industries have changed as well."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143731352"
                        ],
                        "name": "W. Hillis",
                        "slug": "W.-Hillis",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Hillis",
                            "middleNames": [
                                "Daniel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hillis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1381202602"
                        ],
                        "name": "G. Steele",
                        "slug": "G.-Steele",
                        "structuredName": {
                            "firstName": "Guy L.",
                            "lastName": "Steele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Steele"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 83
                            }
                        ],
                        "text": "The reduction results from all cores are then processed with a parallel scan using Hillis and Steele\u2019s method without global synchronization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 99
                            }
                        ],
                        "text": "Given n input elements, logn passes are needed to complete the scan in the cooperative scan of the Hillis and Steele\u2019s method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Based on the classic parallel scan by Hillis and Steele [15], we proposed an efficient prefix sum for both CUDAand OpenCL-oriented integrated GPUs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2315965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4b7a72d4e34d767b0c20953c3a124e4b28f2544",
            "isKey": false,
            "numCitedBy": 1006,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Parallel computers with tens of thousands of processors are typically programmed in a data parallel style, as opposed to the control parallel style used in multiprocessing. The success of data parallel algorithms\u2014even on problems that at first glance seem inherently serial\u2014suggests that this style of programming has much wider applicability than was previously thought."
            },
            "slug": "Data-parallel-algorithms-Hillis-Steele",
            "title": {
                "fragments": [],
                "text": "Data parallel algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The success of data parallel algorithms\u2014even on problems that at first glance seem inherently serial\u2014suggests that this style of programming has much wider applicability than was previously thought."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 141
                            }
                        ],
                        "text": "In order to efficiently execute the entire model inference workloads, Nvidia and Intel wrap their high-performance libraries as\nTensorRT and OpenVINO, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 55
                            }
                        ],
                        "text": "For the device with ARM Mali GPU, we could not find an OpenVINO counterpart for ARM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 178
                            }
                        ],
                        "text": "Tables 1, 2, and 3 detail the latency numbers of our solution (noted by \u201cOurs\u201d) running on AWS DeepLens, Acer aiSage and Nvidia Jetson Nano, compared to the ones consumed by Intel OpenVINO, ACL, and cuDNN, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 170
                            }
                        ],
                        "text": "Intel clDNN [8], ARMCompute Library (ACL) [2], and Nvidia cuDNN [7], along with some deep learning frameworks or vendor-provided inference-only pipelines (Intel OpenVINO [9] and e."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 50
                            }
                        ],
                        "text": "For the device with Intel Graphics, we used Intel OpenVINO toolkit as the baseline, which does optimized model inference on Intel Graphics using Intel clDNN along with some graph-level optimizations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 0
                            }
                        ],
                        "text": "OpenVINO only restricts the support of the image classification models but not the object detection ones."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 283
                            }
                        ],
                        "text": "In order to provide GPU support, practitioners usually leverage the chip vendor provided high-performance libraries, e.g. Intel clDNN [8], ARMCompute Library (ACL) [2], and Nvidia cuDNN [7], along with some deep learning frameworks or vendor-provided inference-only pipelines (Intel OpenVINO [9] and e.g. Nvidia TensorRT [37]), to build up their applications."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 30
                            }
                        ],
                        "text": "However, neither TensorRT nor OpenVINO is open-sourced, making it inflexible for developers customize them for new or slightly changed models."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OpenVINO Toolkit Release Notes"
            },
            "venue": {
                "fragments": [],
                "text": "https: //software.intel.com/en-us/articles/OpenVINO-RelNotes. [Online; accessed"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "For example, our approach only requires around 100 lines of TVM IR code (vs 325\nlines of CUDA code in the original implementation) to generate efficient code for both CUDA and OpenCL supported platforms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "These threads are organized into blocks (OpenCL: workgroups) and the hardware schedules blocks of threads onto hardware cores (CUDA: streaming multiprocessors, OpenCL: compute units)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "Specifically, inherited from Halide [33], TVM utilizes a unified IR to lower the optimization schemes of different integrated GPUs to CUDA or OpenCL for code generation on those devices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Despite significant difference in details, OpenCL and CUDA actually share similar abstractions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "In addition, different GPUs may require different software drivers and programming languages, such as CUDA on Nvidia products and OpenCL on Intel and ARM products."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "CUDA programs (\u201ckernels\u201d) specify the number of blocks and threads per block under a SIMT (singleinstruction, multiple-thread) programming model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "Integrated GPUs target two similar programming models for general-purpose programmability, OpenCL [18] (managed byKhronos) and CUDA [29] (developed by Nvidia)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "Many algorithms in the literature [13, 30, 35] have optimized scan on Nvidia GPUs (CUDA specific) but none of them targeted all integrated GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Nvidia GPUs have on the order of 16 cores, each of which contains 32-wide SIMD processors (CUDA: CUDA cores, OpenCL: SIMD units) that run 32 threads in lockstep."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "In integrated GPUs, a work item corresponds to a SIMD entry, processed by a CUDA core (CUDA term) or a virtual thread (OpenCL term in Intel Graphics)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "This paper uses a unified IR to lower the computation to CUDA or OpenCL codes for different devices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "This programming model of CUDA and OpenCL is suitable for the compute pattern of neural networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "These libraries take advantage of properties of CUDA/OpenCL described above to achieve good performance on GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Therefore, a warp (CUDA term) or hardware thread (OpenCL term in Intel Graphics) processes multiple work items at the same time, inherently implementing the SIMD vectorization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Based on the classic parallel scan by Hillis and Steele [15], we proposed an efficient prefix sum for both CUDAand OpenCL-oriented integrated GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "There are fast and efficient argsort implementations for discrete GPUs in CUDA [4, 28]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "On the other hand, Nvidia products run CUDA as its proprietary driver which outperforms OpenCL [21]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "NVIDIA CUDA C programming guide"
            },
            "venue": {
                "fragments": [],
                "text": "PG-02829-001_v6.5, August"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "For example, our approach only requires around 100 lines of TVM IR code (vs 325\nlines of CUDA code in the original implementation) to generate efficient code for both CUDA and OpenCL supported platforms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "These threads are organized into blocks (OpenCL: workgroups) and the hardware schedules blocks of threads onto hardware cores (CUDA: streaming multiprocessors, OpenCL: compute units)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "Specifically, inherited from Halide [33], TVM utilizes a unified IR to lower the optimization schemes of different integrated GPUs to CUDA or OpenCL for code generation on those devices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Despite significant difference in details, OpenCL and CUDA actually share similar abstractions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "In addition, different GPUs may require different software drivers and programming languages, such as CUDA on Nvidia products and OpenCL on Intel and ARM products."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "CUDA programs (\u201ckernels\u201d) specify the number of blocks and threads per block under a SIMT (singleinstruction, multiple-thread) programming model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "Integrated GPUs target two similar programming models for general-purpose programmability, OpenCL [18] (managed byKhronos) and CUDA [29] (developed by Nvidia)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "Many algorithms in the literature [13, 30, 35] have optimized scan on Nvidia GPUs (CUDA specific) but none of them targeted all integrated GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Nvidia GPUs have on the order of 16 cores, each of which contains 32-wide SIMD processors (CUDA: CUDA cores, OpenCL: SIMD units) that run 32 threads in lockstep."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "In integrated GPUs, a work item corresponds to a SIMD entry, processed by a CUDA core (CUDA term) or a virtual thread (OpenCL term in Intel Graphics)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "This paper uses a unified IR to lower the computation to CUDA or OpenCL codes for different devices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "This programming model of CUDA and OpenCL is suitable for the compute pattern of neural networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "These libraries take advantage of properties of CUDA/OpenCL described above to achieve good performance on GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Therefore, a warp (CUDA term) or hardware thread (OpenCL term in Intel Graphics) processes multiple work items at the same time, inherently implementing the SIMD vectorization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Based on the classic parallel scan by Hillis and Steele [15], we proposed an efficient prefix sum for both CUDAand OpenCL-oriented integrated GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 79
                            }
                        ],
                        "text": "There are fast and efficient argsort implementations for discrete GPUs in CUDA [4, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "On the other hand, Nvidia products run CUDA as its proprietary driver which outperforms OpenCL [21]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Cub: Flexible library of cooperative threadblock primitives and other utilities for CUDA kernel programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "For example, our approach only requires around 100 lines of TVM IR code (vs 325\nlines of CUDA code in the original implementation) to generate efficient code for both CUDA and OpenCL supported platforms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "These threads are organized into blocks (OpenCL: workgroups) and the hardware schedules blocks of threads onto hardware cores (CUDA: streaming multiprocessors, OpenCL: compute units)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "Specifically, inherited from Halide [33], TVM utilizes a unified IR to lower the optimization schemes of different integrated GPUs to CUDA or OpenCL for code generation on those devices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "Despite significant difference in details, OpenCL and CUDA actually share similar abstractions."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "In addition, different GPUs may require different software drivers and programming languages, such as CUDA on Nvidia products and OpenCL on Intel and ARM products."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "CUDA programs (\u201ckernels\u201d) specify the number of blocks and threads per block under a SIMT (singleinstruction, multiple-thread) programming model."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "Integrated GPUs target two similar programming models for general-purpose programmability, OpenCL [18] (managed byKhronos) and CUDA [29] (developed by Nvidia)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "Many algorithms in the literature [13, 30, 35] have optimized scan on Nvidia GPUs (CUDA specific) but none of them targeted all integrated GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "Nvidia GPUs have on the order of 16 cores, each of which contains 32-wide SIMD processors (CUDA: CUDA cores, OpenCL: SIMD units) that run 32 threads in lockstep."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "In integrated GPUs, a work item corresponds to a SIMD entry, processed by a CUDA core (CUDA term) or a virtual thread (OpenCL term in Intel Graphics)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "This paper uses a unified IR to lower the computation to CUDA or OpenCL codes for different devices."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "This programming model of CUDA and OpenCL is suitable for the compute pattern of neural networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "These libraries take advantage of properties of CUDA/OpenCL described above to achieve good performance on GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Therefore, a warp (CUDA term) or hardware thread (OpenCL term in Intel Graphics) processes multiple work items at the same time, inherently implementing the SIMD vectorization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "Based on the classic parallel scan by Hillis and Steele [15], we proposed an efficient prefix sum for both CUDAand OpenCL-oriented integrated GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 79
                            }
                        ],
                        "text": "There are fast and efficient argsort implementations for discrete GPUs in CUDA [4, 28]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "On the other hand, Nvidia products run CUDA as its proprietary driver which outperforms OpenCL [21]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Moderngpu: Patterns and behaviors for GPU computing"
            },
            "venue": {
                "fragments": [],
                "text": "http: //moderngpu.github.io/moderngpu,"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "NVIDIA Corporation. NVIDIA CUDA C programming guide"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The OpenCL Specification (Version 2.0, Document Revision 26)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "OpenVINO Toolkit Release Notes. https: //software.intel.com/en-us/articles/OpenVINO-RelNotes"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Integrated GPUs target two similar programming models for general-purpose programmability, OpenCL [18] (managed byKhronos) and CUDA [29] (developed by Nvidia)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The OpenCL Specification (Version 2.0"
            },
            "venue": {
                "fragments": [],
                "text": "Document Revision"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Compute Library for Deep Neural Networks (clDNN)"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 18,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 40,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Unified-Optimization-Approach-for-CNN-Model-on-Wang-Chen/650ffe556925a9c97d7a819a8ab10b9aae31dbdd?sort=total-citations"
}