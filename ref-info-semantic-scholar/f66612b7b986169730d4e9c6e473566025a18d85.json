{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38940652"
                        ],
                        "name": "H. Soltau",
                        "slug": "H.-Soltau",
                        "structuredName": {
                            "firstName": "Hagen",
                            "lastName": "Soltau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Soltau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718611"
                        ],
                        "name": "L. Mangu",
                        "slug": "L.-Mangu",
                        "structuredName": {
                            "firstName": "Lidia",
                            "lastName": "Mangu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mangu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698208"
                        ],
                        "name": "G. Saon",
                        "slug": "G.-Saon",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Saon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Saon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 48
                            }
                        ],
                        "text": "For the details of our VTLN implementation, see [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 162
                            }
                        ],
                        "text": "The ATR is the basis of all our evaluation systems, including Englis h, Arabic and Mandarin broadcast, English and Spanish parliamentary [20], and conversational [7] tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3065967,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e35a50c251edc9e1ebcd919c09413661420c7ccf",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the technical advances in IBM's conversational telephony submission to the DARPA-sponsored 2004 rich transcription evaluation (RT-04). These advances include a system architecture based on cross-adaptation; a new form of feature-based MPE training; the use of a full-scale discriminatively trained full covariance Gaussian system; the use of septaphone cross-word acoustic context in static decoding graphs; and the incorporation of 2100 hours of training data in every system component. These advances reduced the error rate by approximately 21% relative, on the 2003 test set, over the best-performing system in last year's evaluation, and produced the best results on the RT-04 current and progress CTS data."
            },
            "slug": "The-IBM-2004-conversational-telephony-system-for-Soltau-Kingsbury",
            "title": {
                "fragments": [],
                "text": "The IBM 2004 conversational telephony system for rich transcription"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Technical advances in IBM's conversational telephony submission to the DARPA-sponsored 2004 rich transcription evaluation (RT-04) reduced the error rate by approximately 21% relative, on the 2003 test set, over the best-performing system in last year's evaluation, and produced the best results on the RT-04 current and progress CTS data."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. (ICASSP '05). IEEE International Conference on Acoustics, Speech, and Signal Processing, 2005."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144301186"
                        ],
                        "name": "S. Wegmann",
                        "slug": "S.-Wegmann",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Wegmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wegmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3114073"
                        ],
                        "name": "Don McAllaster",
                        "slug": "Don-McAllaster",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "McAllaster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Don McAllaster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33433401"
                        ],
                        "name": "J. Orloff",
                        "slug": "J.-Orloff",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Orloff",
                            "middleNames": [
                                "N"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Orloff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780994"
                        ],
                        "name": "B. Peskin",
                        "slug": "B.-Peskin",
                        "structuredName": {
                            "firstName": "Barbara",
                            "lastName": "Peskin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Peskin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "Vocal tract length normalization (VTLN [6])."
                    },
                    "intents": []
                }
            ],
            "corpusId": 38438540,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1b5d3400cd132d110d37de3eec45c19797cb60f",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports on a simplified system for determining vocal tract normalization. Such normalization has led to significant gains in recognition accuracy by reducing variability among speakers and allowing the pooling of training data and the construction of sharper models. But standard methods for determining the warp scale have been extremely cumbersome, generally requiring multiple recognition passes. We present a new system for warp scale selection which uses a simple generic voiced speech model to rapidly select appropriate frequency scales. The selection is sufficiently streamlined that it can moved completely into the front-end processing. Using this system on a standard test of the Switchboard Corpus, we have achieved relative reductions in word error rates of 12% over unnormalized gender-independent models and 6% over our best unnormalized gender-dependent models."
            },
            "slug": "Speaker-normalization-on-conversational-telephone-Wegmann-McAllaster",
            "title": {
                "fragments": [],
                "text": "Speaker normalization on conversational telephone speech"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new system for warp scale selection which uses a simple generic voiced speech model to rapidly select appropriate frequency scales and is sufficiently streamlined that it can moved completely into the front-end processing."
            },
            "venue": {
                "fragments": [],
                "text": "1996 IEEE International Conference on Acoustics, Speech, and Signal Processing Conference Proceedings"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698208"
                        ],
                        "name": "G. Saon",
                        "slug": "G.-Saon",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Saon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Saon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 30
                            }
                        ],
                        "text": "The FSM decoder, described in [14, 17], can produce a variety of outputs: 1-best hypothe ses, word-level and HMM state-level lattices, and confusio n networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 161
                            }
                        ],
                        "text": "We use a weak language model (unigram), and the language model states during decoding are given by a hashing of the entire word sequence up to the current frame [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13266287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4993485330a0a847e6a500dfef47060c5cebe2ac",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We report in detail the decoding strategy that we used for the past two Darpa Rich Transcription evaluations (RT\u201903 and RT\u201904) which is based on finite state automata (FSA). We discuss the format of the static decoding graphs, the particulars of our Viterbi implementation, the lattice generation and the likelihood evaluation. This paper is intended to familiarize the reader with some of the design issues encountered when building an FSA decoder. Experimental results are given on the EARS database (English conversational telephone speech) with emphasis on our faster than real-time system."
            },
            "slug": "Anatomy-of-an-extremely-fast-LVCSR-decoder-Saon-Povey",
            "title": {
                "fragments": [],
                "text": "Anatomy of an extremely fast LVCSR decoder"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "The decoding strategy that is used for the past two Darpa Rich Transcription evaluations (RT\u201903 and RT\u201904) which is based on finite state automata (FSA) is reported in detail and experimental results are given on the EARS database."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731014"
                        ],
                        "name": "S. Chu",
                        "slug": "S.-Chu",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Chu",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145395455"
                        ],
                        "name": "H. Kuo",
                        "slug": "H.-Kuo",
                        "structuredName": {
                            "firstName": "Hong-Kwang",
                            "lastName": "Kuo",
                            "middleNames": [
                                "Jeff"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kuo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718611"
                        ],
                        "name": "L. Mangu",
                        "slug": "L.-Mangu",
                        "structuredName": {
                            "firstName": "Lidia",
                            "lastName": "Mangu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mangu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47179760"
                        ],
                        "name": "Shilei Zhang",
                        "slug": "Shilei-Zhang",
                        "structuredName": {
                            "firstName": "Shilei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shilei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087134695"
                        ],
                        "name": "Qin Shi",
                        "slug": "Qin-Shi",
                        "structuredName": {
                            "firstName": "Qin",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qin Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144104145"
                        ],
                        "name": "Yong Qin",
                        "slug": "Yong-Qin",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Qin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Qin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "Table 2 shows re sults for ASR systems that were trained on hundreds to thousands of hours using the toolkit and fielded in speech recognition evaluations [19, 21, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16254325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea13ec5a68cfc9ae6a3ca344fdad11d6149cdfde",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper gives an up-to-date description of the IBM Mandarin broadcast transcription system developed under the DARPA GALE program. Technical advances over our previous system include a novel acoustic modeling approach using subspace Gaussian mixture models, a speaking rate adaptation method using frame rate normalization, and an effective recipe for lattice combination. We present results on three consortium-defined test sets. It is shown that with these advances, the new system attains a 9% relative reduction in character error rate compared to our previous GALE evaluation system. The reported 9.1% error rate on the phase three evaluation set represents the state of the art in Mandarin broadcast speech transcription."
            },
            "slug": "The-2009-IBM-GALE-Mandarin-broadcast-transcription-Chu-Povey",
            "title": {
                "fragments": [],
                "text": "The 2009 IBM GALE Mandarin broadcast transcription system"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper gives an up-to-date description of the IBM Mandarin broadcast transcription system developed under the DARPA GALE program, and shows that the new system attains a 9% relative reduction in character error rate compared to the previous GALE evaluation system."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720857"
                        ],
                        "name": "B. Ramabhadran",
                        "slug": "B.-Ramabhadran",
                        "structuredName": {
                            "firstName": "Bhuvana",
                            "lastName": "Ramabhadran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ramabhadran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2523830"
                        ],
                        "name": "O. Siohan",
                        "slug": "O.-Siohan",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Siohan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Siohan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718611"
                        ],
                        "name": "L. Mangu",
                        "slug": "L.-Mangu",
                        "structuredName": {
                            "firstName": "Lidia",
                            "lastName": "Mangu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mangu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681543"
                        ],
                        "name": "G. Zweig",
                        "slug": "G.-Zweig",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Zweig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zweig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37120614"
                        ],
                        "name": "Hans-J\u00f6rg Schulz",
                        "slug": "Hans-J\u00f6rg-Schulz",
                        "structuredName": {
                            "firstName": "Hans-J\u00f6rg",
                            "lastName": "Schulz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hans-J\u00f6rg Schulz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1886563"
                        ],
                        "name": "A. Soneiro",
                        "slug": "A.-Soneiro",
                        "structuredName": {
                            "firstName": "Alvaro",
                            "lastName": "Soneiro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Soneiro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "The ATR is the basis of all our evaluation systems, including Englis h, Arabic and Mandarin broadcast, English and Spanish parliamentary [20], and conversational [7] tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33564798,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d02fe1e19901f4147ed5d37d66fd280716ecaa4",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "TC-STAR is an European Union funded speech to speech translation project to transcribe, translate and synthesize European Parliamentary Plenary Speeches (EPPS). This paper describes IBM\u2019s English and Spanish speech recognition systems submitted to the TC-STAR 2006 Evaluation. The technical advances in this submission include two different algorithms for automatic segmentation and speaker clustering of the input audio; a system architecture that is based on cross-adaptation across these two segmentation schemes and system combination through generation of an ensemble of systems using randomized decision tree state-tying; automatic punctuation of the speech recognition output; and the incorporation of an additional 35 hours of in-domain EPPS acoustic training data. These advances reduced the error rate by 30% relative over the best-performing system in the TC-STAR 2005 Evaluation on the 2006 English development test set, and produced one of the best performing systems on the 2006 evaluation in English with a word error rate of 8.3%."
            },
            "slug": "The-IBM-2006-Speech-Transcription-System-Ramabhadran-Siohan",
            "title": {
                "fragments": [],
                "text": "The IBM 2006 Speech Transcription System"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "IBM\u2019s English and Spanish speech recognition systems submitted to the TC-STAR 2006 Evaluation are described, which produced one of the best performing systems on the 2006 evaluation in English with a word error rate of 8.3%."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2121786"
                        ],
                        "name": "V. Digalakis",
                        "slug": "V.-Digalakis",
                        "structuredName": {
                            "firstName": "Vassilios",
                            "lastName": "Digalakis",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Digalakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40599180"
                        ],
                        "name": "P. Monaco",
                        "slug": "P.-Monaco",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Monaco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Monaco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781290"
                        ],
                        "name": "H. Murveit",
                        "slug": "H.-Murveit",
                        "structuredName": {
                            "firstName": "Hy",
                            "lastName": "Murveit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Murveit"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 73
                            }
                        ],
                        "text": "This allows us to build semi-continuous models, also calledGenone models [4]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18508499,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21eabd8d1f6fcf27bcfa9e08e40384600ca4507f",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm is proposed that achieves a good tradeoff between modeling resolution and robustness by using a new, general scheme for tying of mixture components in continuous mixture-density hidden Markov model (HMM)-based speech recognizers. The sets of HMM states that share the same mixture components are determined automatically using agglomerative clustering techniques. Experimental results on ARPA's Wall Street Journal corpus show that this scheme reduces errors by 25% over typical tied-mixture systems. New fast algorithms for computing Gaussian likelihoods-the-most time-consuming aspect of continuous-density HMM systems-are also presented. These new algorithms-significantly reduce the number of Gaussian densities that are evaluated with little or no impact on speech recognition accuracy."
            },
            "slug": "Genones:-generalized-mixture-tying-in-continuous-Digalakis-Monaco",
            "title": {
                "fragments": [],
                "text": "Genones: generalized mixture tying in continuous hidden Markov model-based speech recognizers"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "An algorithm is proposed that achieves a good tradeoff between modeling resolution and robustness by using a new, general scheme for tying of mixture components in continuous mixture-density hidden Markov model (HMM)-based speech recognizers."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38940652"
                        ],
                        "name": "H. Soltau",
                        "slug": "H.-Soltau",
                        "structuredName": {
                            "firstName": "Hagen",
                            "lastName": "Soltau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Soltau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698208"
                        ],
                        "name": "G. Saon",
                        "slug": "G.-Saon",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Saon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Saon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145395455"
                        ],
                        "name": "H. Kuo",
                        "slug": "H.-Kuo",
                        "structuredName": {
                            "firstName": "Hong-Kwang",
                            "lastName": "Kuo",
                            "middleNames": [
                                "Jeff"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kuo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718611"
                        ],
                        "name": "L. Mangu",
                        "slug": "L.-Mangu",
                        "structuredName": {
                            "firstName": "Lidia",
                            "lastName": "Mangu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mangu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35797272"
                        ],
                        "name": "Ahmad Emami",
                        "slug": "Ahmad-Emami",
                        "structuredName": {
                            "firstName": "Ahmad",
                            "lastName": "Emami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ahmad Emami"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15020971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac2351860c3cc77883b5ff7b7e50295b163b75a4",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the Arabic broadcast transcription system fielded by IBM in the GALE Phase 2.5 machine translation evaluation. Key advances include the use of additional training data from the Linguistic Data Consortium (LDC), use of a very large vocabulary comprising 737 K words and 2.5 M pronunciation variants, automatic vowelization using flat-start training, cross-adaptation between unvowelized and vowelized acoustic models, and rescoring with a neural-network language model. The resulting system achieves word error rates below 10% on Arabic broadcasts. Very large scale experiments with unsupervised training demonstrate that the utility of unsupervised data depends on the amount of supervised data available. While unsupervised training improves system performance when a limited amount (135 h) of supervised data is available, these gains disappear when a greater amount (848 h) of supervised data is used, even with a very large (7069 h) corpus of unsupervised data. We also describe a method for modeling Arabic dialects that avoids the problem of data sparseness entailed by dialect-specific acoustic models via the use of non-phonetic, dialect questions in the decision trees. We show how this method can be used with a statically compiled decoding graph by partitioning the decision trees into a static component and a dynamic component, with the dynamic component being replaced by a mapping that is evaluated at run-time."
            },
            "slug": "Advances-in-Arabic-Speech-Transcription-at-IBM-the-Soltau-Saon",
            "title": {
                "fragments": [],
                "text": "Advances in Arabic Speech Transcription at IBM Under the DARPA GALE Program"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method for modeling Arabic dialects that avoids the problem of data sparseness entailed by dialect-specific acoustic models via the use of non-phonetic, dialect questions in the decision trees is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Audio, Speech, and Language Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784851"
                        ],
                        "name": "T. Sainath",
                        "slug": "T.-Sainath",
                        "structuredName": {
                            "firstName": "Tara",
                            "lastName": "Sainath",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sainath"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720857"
                        ],
                        "name": "B. Ramabhadran",
                        "slug": "B.-Ramabhadran",
                        "structuredName": {
                            "firstName": "Bhuvana",
                            "lastName": "Ramabhadran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ramabhadran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774515"
                        ],
                        "name": "M. Picheny",
                        "slug": "M.-Picheny",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Picheny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Picheny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 82
                            }
                        ],
                        "text": "it also works very well on small tasks such as TIMIT, establishing a new baseline [22] for this task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2141386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "956b80fe8a56f90782fcb4fe1c536b24496094ba",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "While research in large vocabulary continuous speech recognition (LVCSR) has sparked the development of many state of the art research ideas, research in this domain suffers from two main drawbacks. First, because of the large number of parameters and poorly labeled transcriptions, gaining insight into further improvements based on error analysis is very difficult. Second, LVCSR systems often take a significantly longer time to train and test new research ideas compared to small vocabulary tasks. A small vocabulary task like TIMIT provides a phonetically rich and hand-labeled corpus and offers a good test bed to study algorithmic improvements. However, oftentimes research ideas explored for small vocabulary tasks do not always provide gains on LVCSR systems. In this paper, we address these issues by taking the standard \u021crecipe\u021d used in typical LVCSR systems and applying it to the TIMIT phonetic recognition corpus, which provides a standard benchmark to compare methods. We find that at the speaker-independent (SI) level, our results offer comparable performance to other SI HMM systems. By taking advantage of speaker adaptation and discriminative training techniques commonly used in LVCSR systems, we achieve an error rate of 20%, the best results reported on the TIMIT task to date, moving us closer to the human reported phonetic recognition error rate of 15%. We propose the use of this system as the baseline for future research and believe that it will serve as a good framework to explore ideas that will carry over to LVCSR systems."
            },
            "slug": "An-exploration-of-large-vocabulary-tools-for-small-Sainath-Ramabhadran",
            "title": {
                "fragments": [],
                "text": "An exploration of large vocabulary tools for small vocabulary phonetic recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper takes the standard \u021crecipe\u021d used in typical LVCSR systems and applies it to the TIMIT phonetic recognition corpus, which provides a standard benchmark to compare methods and finds that at the speaker-independent (SI) level, the results offer comparable performance to other SI HMM systems."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Workshop on Automatic Speech Recognition & Understanding"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716393"
                        ],
                        "name": "P. Woodland",
                        "slug": "P.-Woodland",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Woodland",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Woodland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 199
                            }
                        ],
                        "text": "The toolkit supports a variety of techniques for discrimina tive training, operating in both feature and model space and using several criteria: maximum mutual information (MMI), minimum phone error [11] and boosted MMI [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16095655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0509bf552a0d1fe895c019e4e8f1b1599c7112e4",
            "isKey": false,
            "numCitedBy": 797,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce the Minimum Phone Error (MPE) and Minimum Word Error (MWE) criteria for the discriminative training of HMM systems. The MPE/MWE criteria are smoothed approximations to the phone or word error rate respectively. We also discuss I-smoothing which is a novel technique for smoothing discriminative training criteria using statistics for maximum likelihood estimation (MLE). Experiments have been performed on the Switchboard/Call Home corpora of telephone conversations with up to 265 hours of training data. It is shown that for the maximum mutual information estimation (MMIE) criterion, I-smoothing reduces the word error rate (WER) by 0.4% absolute over the MMIE baseline. The combination of MPE and I-smoothing gives an improvement of 1 % over MMIE and a total reduction in WER of 4.8% absolute over the original MLE system."
            },
            "slug": "Minimum-Phone-Error-and-I-smoothing-for-improved-Povey-Woodland",
            "title": {
                "fragments": [],
                "text": "Minimum Phone Error and I-smoothing for improved discriminative training"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The Minimum Phone Error (MPE) and Minimum Word Error (MWE) criteria are smoothed approximations to the phone or word error rate respectively and I-smoothing which is a novel technique for smoothing discriminative training criteria using statistics for maximum likelihood estimation (MLE)."
            },
            "venue": {
                "fragments": [],
                "text": "2002 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110909951"
                        ],
                        "name": "Stanley F. Chen",
                        "slug": "Stanley-F.-Chen",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley F. Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 111
                            }
                        ],
                        "text": "The static decoding graph can be constructed either through incremental expansion of the context decision tree [15], or through an incremental composition of the component finite state machines (FSMs) [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2177405,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f218ed30fb1c48731f256ba69d8dcc8ab2afc69",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that the use of finite-state transducers (FST\u2019s) has many advantages in large vocabulary speech recognition. Most past work has focused on the use of triphone phonetic decision trees. However, numerous applications use decision trees that condition on wider contexts; for example, many systems at IBM use 11-phone phonetic decision trees. Alas, large-context phonetic decision trees cannot be compiled straightforwardly into FST\u2019s due to memory constraints. In this work, we discuss memory-efficient techniques for manipulating large-context phonetic decision trees in the FST framework. First, we describe a lazy expansion technique that is applicable when expanding small word graphs. For general applications, we discuss how to construct large-context transducers via a sequence of simple, efficient finite-state operations; we also introduce a memory-efficient implementation of determinization."
            },
            "slug": "Compiling-large-context-phonetic-decision-trees-Chen",
            "title": {
                "fragments": [],
                "text": "Compiling large-context phonetic decision trees into finite-state transducers"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work discusses memory-efficient techniques for manipulating large-context phonetic decision trees in the FST framework, and describes a lazy expansion technique that is applicable when expanding small word graphs."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740397"
                        ],
                        "name": "M. Gales",
                        "slug": "M.-Gales",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Gales",
                            "middleNames": [
                                "John",
                                "Francis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gales"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 90
                            }
                        ],
                        "text": "Feature-space maximum likelihood linear regression (FMLLR, also known as constrained MLLR [8])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "While the recipe was designed for large vocabulary tasks,\n\u2212 Alignments with MLLR adapted models \u2212 LDA on stacked features \u2212 GMM training with embedded STC \u2212 Decision Tree for Quinphones\nCD Models\n\u2212 static features \u2212 GMM training for CI models\nFlatstart\n\u2212 Random Initialization\n\u2212 Alignments with MLLR adapted models \u2212 Voicing Model \u2212 LDA in VTL space \u2212 GMM training with embedded STC in VTL space \u2212 Decision Tree for Quinphones in VTL space\nVTL Models\n\u2212 FMLLR for training speakers \u2212 Alignments with FMLLR and MLLR adapted models \u2212 Decision Tree in FMLLR space \u2212 GMM training in FMLLR space\nFMLLR\u2212SAT Models\nDiscriminative Training \u2212 Lattice generation \u2212 Transform estimation \u2212 Model estimation\nit also works very well on small tasks such as TIMIT, establishing a new baseline [22] for this task."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 181
                            }
                        ],
                        "text": "At the same time, the toolkit is flexible and easy to use, making it a good en-\nStep WER inital CD models 34.1% retrained CD models 31.6% retrained CD models 31.4% VTLN models 25.6% FMLLR-SAT models 23.1% fMMI models 18.9% fMMI+bMMI models 18.0%\nTable 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 529,
                                "start": 525
                            }
                        ],
                        "text": "Available front end modules include audio input from a file, audio input from a socket, audio input from a sound card, downsampling, power spectrum, Mel binning with support for vocal tract length normalization (VTLN), Gaussianization, PLP coefficients, Mel-frequency cepstral (MFCC) coefficients, mean and variance normalization, splicing of successive frames into supervectors, application of a linear transform or projection such as linear discriminant analysis (LDA), feature-space maximum likelihood linear regression (FMLLR), fusion of parallel feature streams, pitch estimation, a d feature-space discriminative transforms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "Model-space adaptation is performed with multiple MLLR transforms [10] using a regression tree to generate transform for different sets of mixture components."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9241826,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2109f8f91301abec8497286160cd6b0f2e65ed05",
            "isKey": true,
            "numCitedBy": 1756,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper examines the application of linear transformations for speaker and environmental adaptation in an HMM-based speech recognition system. In particular, transformations that are trained in a maximum likelihood sense on adaptation data are investigated. Only model-based linear transforms are considered, since, for linear transforms, they subsume the appropriate feature\u2013space transforms. The paper compares the two possible forms of model-based transforms: (i) unconstrained, where any combination of mean and variance transform may be used, and (ii) constrained, which requires the variance transform to have the same form as the mean transform. Re-estimation formulae for all appropriate cases of transform are given. This includes a new and efficient full variance transform and the extension of the constrained model\u2013space transform from the simple diagonal case to the full or block\u2013diagonal case. The constrained and unconstrained transforms are evaluated in terms of computational cost, recognition time efficiency, and use for speaker adaptive training. The recognition performance of the two model\u2013space transforms on a large vocabulary speech recognition task using incremental adaptation is investigated. In addition, initial experiments using the constrained model\u2013space transform for speaker adaptive training are detailed."
            },
            "slug": "Maximum-likelihood-linear-transformations-for-Gales",
            "title": {
                "fragments": [],
                "text": "Maximum likelihood linear transformations for HMM-based speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The paper compares the two possible forms of model-based transforms: unconstrained, where any combination of mean and variance transform may be used, and constrained, which requires the variance transform to have the same form as the mean transform."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Speech Lang."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38940652"
                        ],
                        "name": "H. Soltau",
                        "slug": "H.-Soltau",
                        "structuredName": {
                            "firstName": "Hagen",
                            "lastName": "Soltau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Soltau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698208"
                        ],
                        "name": "G. Saon",
                        "slug": "G.-Saon",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Saon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Saon"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 30
                            }
                        ],
                        "text": "The FSM decoder, described in [14, 17], can produce a variety of outputs: 1-best hypothe ses, word-level and HMM state-level lattices, and confusio n networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "A more detailed discussion can be found in [17]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "The dynamic network decoder [17] is a one-pass Viterbi decoder capable of handling large across-word contexts and large language model histories."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5523083,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e82fa0572896aea5c17a994e4ad461e6c2364ffb",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a dynamic network decoder capable of using large cross-word context models and large n-gram histories. Our method for constructing the search network is designed to process large cross-word context models very efficiently and we address the optimization of the search network to minimize any overhead during run-time for the dynamic network decoder. The search procedure uses the full LM history for lookahead, and path recombination is done as early as possible. In our systematic comparison to a static FSM based decoder, we find the dynamic decoder can run at comparable speed as the static decoder when large language models are used, while the static decoder performs best for small language models. We discuss the use of very large vocabularies of up to 2.5 million words for both decoding approaches and analyze the effect of weak acoustic models for pruning."
            },
            "slug": "Dynamic-network-decoding-revisited-Soltau-Saon",
            "title": {
                "fragments": [],
                "text": "Dynamic network decoding revisited"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "In this work, a dynamic network decoder capable of using large cross-word context models and large n-gram histories is presented and a systematic comparison to a static FSM based decoder finds the dynamic decoder can run at comparable speed when large language models are used, while the static decoder performs best for small language models."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Workshop on Automatic Speech Recognition & Understanding"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144906845"
                        ],
                        "name": "J. Ousterhout",
                        "slug": "J.-Ousterhout",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Ousterhout",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ousterhout"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 197,
                                "start": 194
                            }
                        ],
                        "text": "To accomplish these goals, the toolkit makes a clear distinc tion between core algorithms and glue code by combining the advantages of a high-level scripting language with the efficiency of C++ [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12452877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4209c39dcb14322bcecb6b778487ef08ea1916ab",
            "isKey": false,
            "numCitedBy": 812,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "A fundamental change is occurring in the way people write computer programs, away from system programming languages such as C or C++ to scripting languages such as Perl or Tcl. Although many people are participating in the change, few realize that the change is occurring and even fewer know why it is happening. This article explains why scripting languages will handle many of the programming tasks in the next century better than system programming languages. System programming languages were designed for building data structures and algorithms from scratch, starting from the most primitive computer elements. Scripting languages are designed for gluing. They assume the existence of a set of powerful components and are intended primarily for connecting components."
            },
            "slug": "Scripting:-Higher-Level-Programming-for-the-21st-Ousterhout",
            "title": {
                "fragments": [],
                "text": "Scripting: Higher-Level Programming for the 21st Century"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This article explains why scripting languages will handle many of the programming tasks in the next century better than system programming languages."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49025046"
                        ],
                        "name": "Jing Huang",
                        "slug": "Jing-Huang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2484153"
                        ],
                        "name": "K. Visweswariah",
                        "slug": "K.-Visweswariah",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Visweswariah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Visweswariah"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 189
                            }
                        ],
                        "text": "Wit h tree arrays, the acoustic model can specialize for phonemes , while the visual model specializes for visemes, reducing th e error rate by3% on an audio-visual speech recognition task [3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 21173104,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8f05b07c3b679d54ee16251bac87a1c53cf79e2",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "HMM-based audio-visual speech recognition (AVSR) systems have shown success in continuous speech recognition by combining visual and audio information, especially in noisy environments. In this paper we study how to improve decision trees used to create context classes in HMM-based AVSR systems. Traditionally, visual models have been trained with the same context classes as the audio only models. In this paper we investigate the use of separate decision trees to model the context classes for the audio and visual streams independently. Additionally we investigate the use of viseme classes in the decision tree building for the visual stream. On experiments with a 37-speaker 1.5 hours test set (about 12000 words) of continuous digits in noise, we obtain about a 3% absolute (20% relative) gain on AVSR performance by using separate decision trees for the audio and visual streams when using viseme classes in decision tree building for the visual stream."
            },
            "slug": "Improved-decision-trees-for-multi-stream-HMM-based-Huang-Visweswariah",
            "title": {
                "fragments": [],
                "text": "Improved decision trees for multi-stream HMM-based audio-visual continuous speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper studies how to improve decision trees used to create context classes in HMM-based AVSR systems and investigates the use of separate decision trees to model the context classes for the audio and visual streams independently."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Workshop on Automatic Speech Recognition & Understanding"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749064"
                        ],
                        "name": "D. Kanevsky",
                        "slug": "D.-Kanevsky",
                        "structuredName": {
                            "firstName": "Dimitri",
                            "lastName": "Kanevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kanevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720857"
                        ],
                        "name": "B. Ramabhadran",
                        "slug": "B.-Ramabhadran",
                        "structuredName": {
                            "firstName": "Bhuvana",
                            "lastName": "Ramabhadran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ramabhadran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698208"
                        ],
                        "name": "G. Saon",
                        "slug": "G.-Saon",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Saon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Saon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2484153"
                        ],
                        "name": "K. Visweswariah",
                        "slug": "K.-Visweswariah",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Visweswariah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Visweswariah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 96
                            }
                        ],
                        "text": "[12] D. Povey, D. Kanevsky, B. Kingsbury, B. Ramabhadran,\nG. Saon, and K. Visweswariah, \u201cBoosted MMI for model and feature space discriminative training,\u201d inProc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 93
                            }
                        ],
                        "text": "It is also possible to train networks using sequence-discriminative criteria such as boosted MMI or MPE [5]: a capability that was easily added because of the toolkit\u2019s modular structure and standardized interfaces."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 224
                            }
                        ],
                        "text": "The main benefit of this design is that it offers maximal flexibility without requiring the writing of interface code and without sacrificing effi-\nFront\u2212end Trainer TranscriptsDatabase\nPhones Tags Lexicon Tree Matrix PLP MFCC\nMMI EstimatorML EstimatorGMM AccumulatorGMM Set\nHMM Alignment Dynamic Decoder FSM Decoder\nC++ Library\nInterface Code (SWIG)\nPython Library\nAttila Design\nFig."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 172
                            }
                        ],
                        "text": "The toolkit supports a variety of techniques for discriminative training, operating in both feature and model space and using several criteria: maximum mutual information (MMI), minimum phone error [11] and boosted MMI [12]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "We also support mean and diagonal variance transform estimation under discriminative objective functions such as boosted MMI."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 220
                            }
                        ],
                        "text": "The toolkit supports a variety of techniques for discrimina tive training, operating in both feature and model space and using several criteria: maximum mutual information (MMI), minimum phone error [11] and boosted MMI [12]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 54
                            }
                        ],
                        "text": "While the ML estimator uses only one accumulator, the MMI estimators will update the models using two accumulators, one for the numerator statistics and one for the denominator statistics."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 204
                            }
                        ],
                        "text": "At the same time, the toolkit is flexible and easy to use, making it a good en-\nStep WER inital CD models 34.1% retrained CD models 31.6% retrained CD models 31.4% VTLN models 25.6% FMLLR-SAT models 23.1% fMMI models 18.9% fMMI+bMMI models 18.0%\nTable 1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 173
                            }
                        ],
                        "text": "An alignment can be populated using several different methods: Viterbi, Baum-Welch, modified forwardbackward routines over lattices for minimum phone error (MPE) or boosted MMI training, uniform segmentation for flat-start training, or conversion of manual labels as in the TIMIT corpus."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "between the numerator and denominator paths, as explained in [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 118,
                                "start": 115
                            }
                        ],
                        "text": "For example, the accumulator routines can be used for both maximum likelihood (ML) and maximum mutual information (MMI) estimation."
                    },
                    "intents": []
                }
            ],
            "corpusId": 14254768,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8770b4a5ca7734c88e5755f9558f79e93229c023",
            "isKey": false,
            "numCitedBy": 430,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a modified form of the maximum mutual information (MMI) objective function which gives improved results for discriminative training. The modification consists of boosting the likelihoods of paths in the denominator lattice that have a higher phone error relative to the correct transcript, by using the same phone accuracy function that is used in Minimum Phone Error (MPE) training. We combine this with another improvement to our implementation of the Extended Baum-Welch update equations for MMI, namely the canceling of any shared part of the numerator and denominator statistics on each frame (a procedure that is already done in MPE). This change affects the Gaussian-specific learning rate. We also investigate another modification whereby we replace I-smoothing to the ML estimate with I-smoothing to the previous iteration's value. Boosted MMI gives better results than MPE in both model and feature-space discriminative training, although not consistently."
            },
            "slug": "Boosted-MMI-for-model-and-feature-space-training-Povey-Kanevsky",
            "title": {
                "fragments": [],
                "text": "Boosted MMI for model and feature-space discriminative training"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "A modified form of the maximum mutual information (MMI) objective function which gives improved results for discriminative training by boosting the likelihoods of paths in the denominator lattice that have a higher phone error relative to the correct transcript."
            },
            "venue": {
                "fragments": [],
                "text": "2008 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51076456"
                        ],
                        "name": "T. J. Watson",
                        "slug": "T.-J.-Watson",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Watson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. J. Watson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 201
                            }
                        ],
                        "text": "The static decoding graph can be constructed either through incremental expansion of the context decision tree [15], or through an incremental composition of the component finite state machines (FSMs) [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9987567,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "724369eeb6ed026b633ae54553852a791bd7bdbf",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "A fast, scalable and memory-efficient method for static decoding graph construction is presented. As an alternative to the traditional transducer-based approach, it is based on incremental composition. Memory efficiency is achieved by combining composition, determinization and minimization into a single step, thus eliminating large intermediate graphs. We have previously reported the use of incremental composition limited to grammars and left cross-word context [1]. Here, this approach is extended to n-gram models with explicit arcs and right cross-word context."
            },
            "slug": "Incremental-composition-of-static-decoding-graphs-Watson",
            "title": {
                "fragments": [],
                "text": "Incremental composition of static decoding graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A fast, scalable and memory-efficient method for static decoding graph construction is presented, based on incremental composition, that combines composition, determinization and minimization into a single step, thus eliminating large intermediate graphs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144707379"
                        ],
                        "name": "Brian Kingsbury",
                        "slug": "Brian-Kingsbury",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Kingsbury",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Kingsbury"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 100
                            }
                        ],
                        "text": "It is also possible to train networks using sequence-discriminative criteria such as boosted MMI or MPE [5]: a capability that was easily added because of the toolkit\u2019s modular structure and standardized interfaces."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 74
                            }
                        ],
                        "text": "[13] D. Povey, B. Kingsbury, L. Mangu, G. Saon, H. Soltau, and G. Zweig, \u201cfMPE: Discriminatively trained features for speech recognition,\u201d inProc."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 157
                            }
                        ],
                        "text": "An alignment can be populated using several different methods: Viterbi, Baum-Welch, modified forwardbackward routines over lattices for minimum phone error (MPE) or boosted MMI training, uniform segmentation for flat-start training, or conversion of manual labels as in the TIMIT corpus."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 52
                            }
                        ],
                        "text": "For feature space transformation estimation (e.g., fMPE [13]), the posteriors in the alignment objects are used to compute the gradient of the objective function with respect to the transformed features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "It is also possible to train netwo rks using sequence-discriminative criteria such as boosted MM I or MPE [5]: a capability that was easily added because of the toolkit\u2019s modular structure and standardized interfaces."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14733612,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2443dc59cf3d6cc1deba6d3220d61664b1a7eada",
            "isKey": false,
            "numCitedBy": 294,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Acoustic models used in hidden Markov model/neural-network (HMM/NN) speech recognition systems are usually trained with a frame-based cross-entropy error criterion. In contrast, Gaussian mixture HMM systems are discriminatively trained using sequence-based criteria, such as minimum phone error or maximum mutual information, that are more directly related to speech recognition accuracy. This paper demonstrates that neural-network acoustic models can be trained with sequence classification criteria using exactly the same lattice-based methods that have been developed for Gaussian mixture HMMs, and that using a sequence classification criterion in training leads to considerably better performance. A neural network acoustic model with 153K weights trained on 50 hours of broadcast news has a word error rate of 34.0% on the rt04 English broadcast news test set. When this model is trained with the state-level minimum Bayes risk criterion, the rt04 word error rate is 27.7%."
            },
            "slug": "Lattice-based-optimization-of-sequence-criteria-for-Kingsbury",
            "title": {
                "fragments": [],
                "text": "Lattice-based optimization of sequence classification criteria for neural-network acoustic modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper demonstrates that neural-network acoustic models can be trained with sequence classification criteria using exactly the same lattice-based methods that have been developed for Gaussian mixture HMMs, and that using a sequence classification criterion in training leads to considerably better performance."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE International Conference on Acoustics, Speech and Signal Processing"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110909951"
                        ],
                        "name": "Stanley F. Chen",
                        "slug": "Stanley-F.-Chen",
                        "structuredName": {
                            "firstName": "Stanley",
                            "lastName": "Chen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanley F. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718611"
                        ],
                        "name": "L. Mangu",
                        "slug": "L.-Mangu",
                        "structuredName": {
                            "firstName": "Lidia",
                            "lastName": "Mangu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mangu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720857"
                        ],
                        "name": "B. Ramabhadran",
                        "slug": "B.-Ramabhadran",
                        "structuredName": {
                            "firstName": "Bhuvana",
                            "lastName": "Ramabhadran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Ramabhadran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705701"
                        ],
                        "name": "R. Sarikaya",
                        "slug": "R.-Sarikaya",
                        "structuredName": {
                            "firstName": "Ruhi",
                            "lastName": "Sarikaya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sarikaya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3027446"
                        ],
                        "name": "A. Sethy",
                        "slug": "A.-Sethy",
                        "structuredName": {
                            "firstName": "Abhinav",
                            "lastName": "Sethy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sethy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 155
                            }
                        ],
                        "text": "Table 2 shows re sults for ASR systems that were trained on hundreds to thousands of hours using the toolkit and fielded in speech recognition evaluations [19, 21, 18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "class based Maximum Entropy model described in [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 955162,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed23c461535afc492e80c63ee8d1ed55b8a176e1",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "In [1], we show that a novel class-based language model, Model M, and the method of regularized minimum discrimination information (rMDI) models outperform comparable methods on moderate amounts of Wall Street Journal data. Both of these methods are motivated by the observation that shrinking the sum of parameter magnitudes in an exponential language model tends to improve performance [2]. In this paper, we investigate whether these shrinkage-based techniques also perform well on larger training sets and on other domains. First, we explain why good performance on large data sets is uncertain, by showing that gains relative to a baseline n-gram model tend to decrease as training set size increases. Next, we evaluate several methods for data/model combination with Model M and rMDI models on limited-scale domains, to uncover which techniques should work best on large domains. Finally, we apply these methods on a variety of medium-to-large-scale domains covering several languages, and show that Model M consistently provides significant gains over existing language models for state-of-the-art systems in both speech recognition and machine translation."
            },
            "slug": "Scaling-shrinkage-based-language-models-Chen-Mangu",
            "title": {
                "fragments": [],
                "text": "Scaling shrinkage-based language models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that Model M consistently provides significant gains over existing language models for state-of-the-art systems in both speech recognition and machine translation."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Workshop on Automatic Speech Recognition & Understanding"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698208"
                        ],
                        "name": "G. Saon",
                        "slug": "G.-Saon",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Saon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Saon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756096"
                        ],
                        "name": "S. Dharanipragada",
                        "slug": "S.-Dharanipragada",
                        "structuredName": {
                            "firstName": "Satya",
                            "lastName": "Dharanipragada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dharanipragada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 139
                            }
                        ],
                        "text": "Gaussianization, a form of histogram normalization implemented using a table lookup of inverse Gaussian cumulative density function values [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15880071,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "817a22b58fc056aa023825ca7f783768ce86f85b",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a non-linear feature space transformation for speaker/environment adaptation which forces the individual dimensions of the acoustic data for every speaker to be Gaussian distributed. The transformation is given by the preimage under the Gaussian cumulative distribution function (CDF) of the empirical CDF on a per dimension basis. We show that, for a given dimension, this transformation achieves minimum divergence between the density function of the transformed adaptation data and the normal density with zero mean and unit variance. Experimental results on both small and large vocabulary tasks show consistent improvements over the application of linear adaptation transforms only."
            },
            "slug": "Feature-space-Gaussianization-Saon-Dharanipragada",
            "title": {
                "fragments": [],
                "text": "Feature space Gaussianization"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A non-linear feature space transformation which forces the individual dimensions of the acoustic data for every speaker to be Gaussian distributed and achieves minimum divergence between the density function of the transformed adaptation data and the normal density with zero mean and unit variance is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "2004 IEEE International Conference on Acoustics, Speech, and Signal Processing"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49025046"
                        ],
                        "name": "Jing Huang",
                        "slug": "Jing-Huang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792214"
                        ],
                        "name": "Daniel Povey",
                        "slug": "Daniel-Povey",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Povey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Povey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2917408,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5de33b812cc0280ef5de56c8b2954cedd65a0269",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "fMPE is a recently introduced discriminative training technique that uses the Minimum Phone Error (MPE) discriminative criterion to train a feature-level transformation. In this paper we investigate fMPE trained audio/visual features for multistream HMM-based audio-visual speech recognition. A flexible, layer-based implementation of fMPE allows us to combine the the visual information with the audio stream using the discriminative traning process, and dispense with the multiple stream approach. Experiments are reported on the IBM infrared headset audio-visual database. On average of 20-speaker 1 hour speaker independent test data, the fMPE trained acoustic features achieve 33% relative gain. Adding video layers on top of audio layers gives additional 10% gain over fMPE trained features from the audio stream alone. The fMPE trained visual features achieve 14% relative gain, while the decision fusion of audio/visual streams with fMPE trained features achieves 29% relative gain. However, fMPE trained models do not improve over the original models on the mismatched noisy test data."
            },
            "slug": "Discriminatively-trained-features-using-fMPE-for-Huang-Povey",
            "title": {
                "fragments": [],
                "text": "Discriminatively trained features using fMPE for multi-stream audio-visual speech recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A flexible, layer-based implementation of fMPE allows us to combine the the visual information with the audio stream using the discriminative traning process, and dispense with the multiple stream approach."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2340686"
                        ],
                        "name": "D. Beazley",
                        "slug": "D.-Beazley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Beazley",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Beazley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 21
                            }
                        ],
                        "text": "A key enabler isSWIG [2], a tool that automatically generates interface code from the header files of a C++ library."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20337738,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "29c88180b5b146e92d94fa73a581b3b8acaf58ee",
            "isKey": false,
            "numCitedBy": 395,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "I present SWIG (Simplified Wrapper and Interface Generator), a program development tool that automatically generates the bindings between C/C++ code and common scripting languages including Tcl, Python, Perl and Guile. SWIG supports most C/C++ datatypes including pointers, structures, and classes. Unlike many other approaches, SWIG uses ANSI C/C++ declarations and requires the user to make virtually no modifications to the underlying C code. In addition, SWIG automatically produces documentation in HTML, LaTeX, or ASCII format. SWIG has been primarily designed for scientists, engineers, and application developers who would like to use scripting languages with their C/C++ programs without worrying about the underlying implementation details of each language or using a complicated software development tool. This paper concentrates on SWIG's use with Tcl/Tk."
            },
            "slug": "SWIG:-An-Easy-to-Use-Tool-for-Integrating-Scripting-Beazley",
            "title": {
                "fragments": [],
                "text": "SWIG: An Easy to Use Tool for Integrating Scripting Languages with C and C++"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Simplified Wrapper and Interface Generator has been primarily designed for scientists, engineers, and application developers who would like to use scripting languages with their C/C++ programs without worrying about the underlying implementation details of each language or using a complicated software development tool."
            },
            "venue": {
                "fragments": [],
                "text": "Tcl/Tk Workshop"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1905330"
                        ],
                        "name": "C. Leggetter",
                        "slug": "C.-Leggetter",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Leggetter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Leggetter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716393"
                        ],
                        "name": "P. Woodland",
                        "slug": "P.-Woodland",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Woodland",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Woodland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 66
                            }
                        ],
                        "text": "Model-space adaptation is performed with multiple MLLR transforms [10] using a regression tree to generate transfo rm for different sets of mixture components."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42464701,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1adca110b5387b3db3735b3012f0165551fe384",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Speaker-adaptation-of-continuous-density-HMMs-using-Leggetter-Woodland",
            "title": {
                "fragments": [],
                "text": "Speaker adaptation of continuous density HMMs using multivariate linear regression"
            },
            "venue": {
                "fragments": [],
                "text": "ICSLP"
            },
            "year": 1994
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 5,
            "methodology": 16
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 22,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/The-IBM-Attila-speech-recognition-toolkit-Soltau-Saon/f66612b7b986169730d4e9c6e473566025a18d85?sort=total-citations"
}