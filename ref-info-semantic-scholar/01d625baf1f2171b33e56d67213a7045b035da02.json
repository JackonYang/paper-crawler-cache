{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061550"
                        ],
                        "name": "Heng-Tze Cheng",
                        "slug": "Heng-Tze-Cheng",
                        "structuredName": {
                            "firstName": "Heng-Tze",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng-Tze Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40338695"
                        ],
                        "name": "L. Koc",
                        "slug": "L.-Koc",
                        "structuredName": {
                            "firstName": "Levent",
                            "lastName": "Koc",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Koc"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066076307"
                        ],
                        "name": "Jeremiah Harmsen",
                        "slug": "Jeremiah-Harmsen",
                        "structuredName": {
                            "firstName": "Jeremiah",
                            "lastName": "Harmsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeremiah Harmsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3296031"
                        ],
                        "name": "Tal Shaked",
                        "slug": "Tal-Shaked",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Shaked",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tal Shaked"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073806959"
                        ],
                        "name": "Tushar Chandra",
                        "slug": "Tushar-Chandra",
                        "structuredName": {
                            "firstName": "Tushar",
                            "lastName": "Chandra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tushar Chandra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3312922"
                        ],
                        "name": "H. Aradhye",
                        "slug": "H.-Aradhye",
                        "structuredName": {
                            "firstName": "Hrishikesh",
                            "lastName": "Aradhye",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aradhye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064997996"
                        ],
                        "name": "Glen Anderson",
                        "slug": "Glen-Anderson",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Glen Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055400243"
                        ],
                        "name": "Wei Chai",
                        "slug": "Wei-Chai",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Chai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Chai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37413761"
                        ],
                        "name": "M. Ispir",
                        "slug": "M.-Ispir",
                        "structuredName": {
                            "firstName": "Mustafa",
                            "lastName": "Ispir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ispir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1508890387"
                        ],
                        "name": "Rohan Anil",
                        "slug": "Rohan-Anil",
                        "structuredName": {
                            "firstName": "Rohan",
                            "lastName": "Anil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rohan Anil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50730596"
                        ],
                        "name": "Zakaria Haque",
                        "slug": "Zakaria-Haque",
                        "structuredName": {
                            "firstName": "Zakaria",
                            "lastName": "Haque",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zakaria Haque"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217278"
                        ],
                        "name": "Lichan Hong",
                        "slug": "Lichan-Hong",
                        "structuredName": {
                            "firstName": "Lichan",
                            "lastName": "Hong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lichan Hong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20048351"
                        ],
                        "name": "Vihan Jain",
                        "slug": "Vihan-Jain",
                        "structuredName": {
                            "firstName": "Vihan",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vihan Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109059862"
                        ],
                        "name": "Xiaobing Liu",
                        "slug": "Xiaobing-Liu",
                        "structuredName": {
                            "firstName": "Xiaobing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaobing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068799083"
                        ],
                        "name": "Hemal Shah",
                        "slug": "Hemal-Shah",
                        "structuredName": {
                            "firstName": "Hemal",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hemal Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3352400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "657fbf29ea0b4904a3e98d1556f9acf38dddae5f",
            "isKey": false,
            "numCitedBy": 2152,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow."
            },
            "slug": "Wide-&-Deep-Learning-for-Recommender-Systems-Cheng-Koc",
            "title": {
                "fragments": [],
                "text": "Wide & Deep Learning for Recommender Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Wide & Deep learning is presented---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems and is open-sourced in TensorFlow."
            },
            "venue": {
                "fragments": [],
                "text": "DLRS@RecSys"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145089705"
                        ],
                        "name": "Y. Shan",
                        "slug": "Y.-Shan",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Shan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755821"
                        ],
                        "name": "T. R. Hoens",
                        "slug": "T.-R.-Hoens",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Hoens",
                            "middleNames": [
                                "Ryan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. R. Hoens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49097406"
                        ],
                        "name": "Jian Jiao",
                        "slug": "Jian-Jiao",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Jiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Jiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145341507"
                        ],
                        "name": "Haijing Wang",
                        "slug": "Haijing-Wang",
                        "structuredName": {
                            "firstName": "Haijing",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haijing Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143877014"
                        ],
                        "name": "J. C. Mao",
                        "slug": "J.-C.-Mao",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Mao",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Mao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 168
                            }
                        ],
                        "text": "We compare DCN with five models: the DCN model with no cross network (DNN), logistic regression (LR), Factorization Machines (FMs), Wide and Deep Model (W&D), and Deep Crossing (DC)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 5
                            }
                        ],
                        "text": "Deep Crossing [15] extends residual networks and achieves automatic feature learning by stacking all types of inputs.\ne remarkable success of deep learning has elicited theoretical analyses on its representative power. ere has been research [16, 17] showing that DNNs are able to approximate an arbitrary function under certain smoothness assumptions to an arbitrary accuracy, given sufficiently many hidden units or hidden layers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "Deep Crossing [15] extends residual networks and achieves automatic feature learning by stacking all types of inputs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9704646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a83c778e918539941cba9dcaa6ec881b3ae7a29a",
            "isKey": true,
            "numCitedBy": 288,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Manually crafted combinatorial features have been the \"secret sauce\" behind many successful models. For web-scale applications, however, the variety and volume of features make these manually crafted features expensive to create, maintain, and deploy. This paper proposes the Deep Crossing model which is a deep neural network that automatically combines features to produce superior models. The input of Deep Crossing is a set of individual features that can be either dense or sparse. The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units. Deep Crossing is implemented with a modeling tool called the Computational Network Tool Kit (CNTK), powered by a multi-GPU platform. It was able to build, from scratch, two web-scale models for a major paid search engine, and achieve superior results with only a sub-set of the features used in the production models. This demonstrates the potential of using Deep Crossing as a general modeling paradigm to improve existing products, as well as to speed up the development of new models with a fraction of the investment in feature engineering and acquisition of deep domain knowledge."
            },
            "slug": "Deep-Crossing:-Web-Scale-Modeling-without-Manually-Shan-Hoens",
            "title": {
                "fragments": [],
                "text": "Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Deep Crossing model is proposed which is a deep neural network that automatically combines features to produce superior models and was able to build, from scratch, two web-scale models for a major paid search engine, and achieve superior results with only a sub-set of the features used in the production models."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145019134"
                        ],
                        "name": "Yu-Chin Juan",
                        "slug": "Yu-Chin-Juan",
                        "structuredName": {
                            "firstName": "Yu-Chin",
                            "lastName": "Juan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-Chin Juan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056431469"
                        ],
                        "name": "Yong Zhuang",
                        "slug": "Yong-Zhuang",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Zhuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Zhuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40429042"
                        ],
                        "name": "Wei-Sheng Chin",
                        "slug": "Wei-Sheng-Chin",
                        "structuredName": {
                            "firstName": "Wei-Sheng",
                            "lastName": "Chin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Sheng Chin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 42
                            }
                        ],
                        "text": "Field-aware factorization machines (FFMs) [7, 8] further allow each feature to learn several vectors where each vector is associated with a \u0080eld."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1472236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a802eccf56ea69c65b14831335a484a69e5cd849",
            "isKey": false,
            "numCitedBy": 492,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Click-through rate (CTR) prediction plays an important role in computational advertising. Models based on degree-2 polynomial mappings and factorization machines (FMs) are widely used for this task. Recently, a variant of FMs, field-aware factorization machines (FFMs), outperforms existing models in some world-wide CTR-prediction competitions. Based on our experiences in winning two of them, in this paper we establish FFMs as an effective method for classifying large sparse data including those from CTR prediction. First, we propose efficient implementations for training FFMs. Then we comprehensively analyze FFMs and compare this approach with competing models. Experiments show that FFMs are very useful for certain classification problems. Finally, we have released a package of FFMs for public use."
            },
            "slug": "Field-aware-Factorization-Machines-for-CTR-Juan-Zhuang",
            "title": {
                "fragments": [],
                "text": "Field-aware Factorization Machines for CTR Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper establishes FFMs as an effective method for classifying large sparse data including those from CTR prediction, and proposes efficient implementations for training FFMs and comprehensively analyze FFMs."
            },
            "venue": {
                "fragments": [],
                "text": "RecSys"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799898"
                        ],
                        "name": "Andreas Veit",
                        "slug": "Andreas-Veit",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Veit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Veit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3035230"
                        ],
                        "name": "Michael J. Wilber",
                        "slug": "Michael-J.-Wilber",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Wilber",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael J. Wilber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50172592"
                        ],
                        "name": "Serge J. Belongie",
                        "slug": "Serge-J.-Belongie",
                        "structuredName": {
                            "firstName": "Serge",
                            "lastName": "Belongie",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serge J. Belongie"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "\u008cere has been research [16, 17] showing that DNNs are able to approximate an arbitrary function under certain smoothness assumptions to an arbitrary accuracy, given sufficiently many hidden units or hidden layers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 715122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a24b68ef180c0c8742bd494a55fb6f68864efed",
            "isKey": false,
            "numCitedBy": 727,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we propose a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length. Moreover, residual networks seem to enable very deep networks by leveraging only the short paths during training. To support this observation, we rewrite residual networks as an explicit collection of paths. Unlike traditional models, paths through residual networks vary in length. Further, a lesion study reveals that these paths show ensemble-like behavior in the sense that they do not strongly depend on each other. Finally, and most surprising, most paths are shorter than one might expect, and only the short paths are needed during training, as longer paths do not contribute any gradient. For example, most of the gradient in a residual network with 110 layers comes from paths that are only 10-34 layers deep. Our results reveal one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of very deep networks."
            },
            "slug": "Residual-Networks-Behave-Like-Ensembles-of-Shallow-Veit-Wilber",
            "title": {
                "fragments": [],
                "text": "Residual Networks Behave Like Ensembles of Relatively Shallow Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This work proposes a novel interpretation of residual networks showing that they can be seen as a collection of many paths of differing length, and reveals one of the key characteristics that seem to enable the training of very deep networks: Residual networks avoid the vanishing gradient problem by introducing short paths which can carry gradient throughout the extent of veryDeep networks."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2791531"
                        ],
                        "name": "Jiyan Yang",
                        "slug": "Jiyan-Yang",
                        "structuredName": {
                            "firstName": "Jiyan",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiyan Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9545803"
                        ],
                        "name": "Alex Gittens",
                        "slug": "Alex-Gittens",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Gittens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Gittens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16100393,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5a11a57a143d5b585b43eb564e9f7f812469820",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent years have demonstrated that using random feature maps can significantly decrease the training and testing times of kernel-based algorithms without significantly lowering their accuracy. Regrettably, because random features are target-agnostic, typically thousands of such features are necessary to achieve acceptable accuracies. In this work, we consider the problem of learning a small number of explicit polynomial features. Our approach, named Tensor Machines, finds a parsimonious set of features by optimizing over the hypothesis class introduced by Kar and Karnick for random feature maps in a target-specific manner. Exploiting a natural connection between polynomials and tensors, we provide bounds on the generalization error of Tensor Machines. Empirically, Tensor Machines behave favorably on several real-world datasets compared to other state-of-the-art techniques for learning polynomial features, and deliver significantly more parsimonious models."
            },
            "slug": "Tensor-machines-for-learning-target-specific-Yang-Gittens",
            "title": {
                "fragments": [],
                "text": "Tensor machines for learning target-specific polynomial features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work considers the problem of learning a small number of explicit polynomial features and finds a parsimonious set of features by optimizing over the hypothesis class introduced by Kar and Karnick for random feature maps in a target-specific manner, named Tensor Machines."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 175
                            }
                        ],
                        "text": "Deep neural networks (DNN) are able to learn non-trivial high-degree feature interactions due to embedding vectors and nonlinear activation functions. e recent success of the Residual Network [5] has enabled training of very deep networks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "\u008ce recent success of the Residual Network [5] has enabled training of very deep networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": false,
            "numCitedBy": 106571,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": "Batch normalization [6] was applied to the deep network and gradient clip norm was set at 100."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5808102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d376d6978dad0374edfa6709c9556b42d3594d3",
            "isKey": false,
            "numCitedBy": 31492,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."
            },
            "slug": "Batch-Normalization:-Accelerating-Deep-Network-by-Ioffe-Szegedy",
            "title": {
                "fragments": [],
                "text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2843982"
                        ],
                        "name": "Steffen Rendle",
                        "slug": "Steffen-Rendle",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Rendle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steffen Rendle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 29
                            }
                        ],
                        "text": "Factorization machines (FMs) [11, 12] project sparse features onto low-dimensional dense vectors and learn feature interactions from vector inner products."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5499886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e50f4d3316d13841c287dcdf5479d7820d593571",
            "isKey": false,
            "numCitedBy": 1084,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented.\n Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM."
            },
            "slug": "Factorization-Machines-with-libFM-Rendle",
            "title": {
                "fragments": [],
                "text": "Factorization Machines with libFM"
            },
            "venue": {
                "fragments": [],
                "text": "TIST"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145019134"
                        ],
                        "name": "Yu-Chin Juan",
                        "slug": "Yu-Chin-Juan",
                        "structuredName": {
                            "firstName": "Yu-Chin",
                            "lastName": "Juan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-Chin Juan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1975509"
                        ],
                        "name": "Damien Lefortier",
                        "slug": "Damien-Lefortier",
                        "structuredName": {
                            "firstName": "Damien",
                            "lastName": "Lefortier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Damien Lefortier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 42
                            }
                        ],
                        "text": "Field-aware factorization machines (FFMs) [7, 8] further allow each feature to learn several vectors where each vector is associated with a field."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13481275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7567efdb5d3ed312aaa40da0d4cf2d0809bae2f",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Predicting user response is one of the core machine learning tasks in computational advertising. Field-aware Factorization Machines (FFM) have recently been established as a state-of-the-art method for that problem and in particular won two Kaggle challenges. This paper presents some results from implementing this method in a production system that predicts click-through and conversion rates for display advertising and shows that this method it is not only effective to win challenges but is also valuable in a real-world prediction system. We also discuss some specific challenges and solutions to reduce the training time, namely the use of an innovative seeding algorithm and a distributed learning mechanism."
            },
            "slug": "Field-aware-Factorization-Machines-in-a-Real-world-Juan-Lefortier",
            "title": {
                "fragments": [],
                "text": "Field-aware Factorization Machines in a Real-world Online Advertising System"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper presents some results from implementing field-aware Factorization Machines in a production system that predicts click-through and conversion rates for display advertising and shows that this method it is not only effective to win challenges but is also valuable in a real-world prediction system."
            },
            "venue": {
                "fragments": [],
                "text": "WWW"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 66
                            }
                        ],
                        "text": "We applied mini-batch stochastic optimization with Adam optimizer [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": false,
            "numCitedBy": 98441,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3074096,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4cec122a08216fe8a3bc19b22e78fbaea096256",
            "isKey": false,
            "numCitedBy": 29217,
            "numCiting": 826,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine-learning technology powers many aspects of modern society: from web searches to content filtering on social networks to recommendations on e-commerce websites, and it is increasingly present in consumer products such as cameras and smartphones. Machine-learning systems are used to identify objects in images, transcribe speech into text, match news items, posts or products with users\u2019 interests, and select relevant results of search. Increasingly, these applications make use of a class of techniques called deep learning. Conventional machine-learning techniques were limited in their ability to process natural data in their raw form. For decades, constructing a pattern-recognition or machine-learning system required careful engineering and considerable domain expertise to design a feature extractor that transformed the raw data (such as the pixel values of an image) into a suitable internal representation or feature vector from which the learning subsystem, often a classifier, could detect or classify patterns in the input. Representation learning is a set of methods that allows a machine to be fed with raw data and to automatically discover the representations needed for detection or classification. Deep-learning methods are representation-learning methods with multiple levels of representation, obtained by composing simple but non-linear modules that each transform the representation at one level (starting with the raw input) into a representation at a higher, slightly more abstract level. With the composition of enough such transformations, very complex functions can be learned. For classification tasks, higher layers of representation amplify aspects of the input that are important for discrimination and suppress irrelevant variations. An image, for example, comes in the form of an array of pixel values, and the learned features in the first layer of representation typically represent the presence or absence of edges at particular orientations and locations in the image. The second layer typically detects motifs by spotting particular arrangements of edges, regardless of small variations in the edge positions. The third layer may assemble motifs into larger combinations that correspond to parts of familiar objects, and subsequent layers would detect objects as combinations of these parts. The key aspect of deep learning is that these layers of features are not designed by human engineers: they are learned from data using a general-purpose learning procedure. Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years. It has turned out to be very good at discovering intricate structures in high-dimensional data and is therefore applicable to many domains of science, business and government. In addition to beating records in image recognition and speech recognition, it has beaten other machine-learning techniques at predicting the activity of potential drug molecules, analysing particle accelerator data, reconstructing brain circuits, and predicting the effects of mutations in non-coding DNA on gene expression and disease. Perhaps more surprisingly, deep learning has produced extremely promising results for various tasks in natural language understanding, particularly topic classification, sentiment analysis, question answering and language translation. We think that deep learning will have many more successes in the near future because it requires very little engineering by hand, so it can easily take advantage of increases in the amount of available computation and data. New learning algorithms and architectures that are currently being developed for deep neural networks will only accelerate this progress."
            },
            "slug": "Deep-Learning-Goodfellow-Bengio",
            "title": {
                "fragments": [],
                "text": "Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Deep learning is making major advances in solving problems that have resisted the best attempts of the artificial intelligence community for many years, and will have many more successes in the near future because it requires very little engineering by hand and can easily take advantage of increases in the amount of available computation and data."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 68
                            }
                        ],
                        "text": "We train the cross network jointly with a deep neural network (DNN) [10, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11715509,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "193edd20cae92c6759c18ce93eeea96afd9528eb",
            "isKey": false,
            "numCitedBy": 12412,
            "numCiting": 977,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deep-learning-in-neural-networks:-An-overview-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Deep learning in neural networks: An overview"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27257992"
                        ],
                        "name": "Mathieu Blondel",
                        "slug": "Mathieu-Blondel",
                        "structuredName": {
                            "firstName": "Mathieu",
                            "lastName": "Blondel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathieu Blondel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34433830"
                        ],
                        "name": "Akinori Fujino",
                        "slug": "Akinori-Fujino",
                        "structuredName": {
                            "firstName": "Akinori",
                            "lastName": "Fujino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Akinori Fujino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735221"
                        ],
                        "name": "N. Ueda",
                        "slug": "N.-Ueda",
                        "structuredName": {
                            "firstName": "Naonori",
                            "lastName": "Ueda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ueda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784290"
                        ],
                        "name": "Masakazu Ishihata",
                        "slug": "Masakazu-Ishihata",
                        "structuredName": {
                            "firstName": "Masakazu",
                            "lastName": "Ishihata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masakazu Ishihata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 51
                            }
                        ],
                        "text": "\u008cere have been work extending FMs to higher orders [1, 18], but one downside lies in their large number of parameters which yields undesirable computational cost."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2543489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f3b3dc86415ebda1043d2be55e75a29ffe2bd95",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks."
            },
            "slug": "Higher-Order-Factorization-Machines-Blondel-Fujino",
            "title": {
                "fragments": [],
                "text": "Higher-Order Factorization Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The first generic yet efficient algorithms for training arbitrary-order higher-orderFactorization machines (HOFMs) are presented and new variants of HOFMs with shared parameters are presented, which greatly reduce model size and prediction times while maintaining similar accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2690730"
                        ],
                        "name": "Eren Manavoglu",
                        "slug": "Eren-Manavoglu",
                        "structuredName": {
                            "firstName": "Eren",
                            "lastName": "Manavoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eren Manavoglu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144956884"
                        ],
                        "name": "R\u00f3mer Rosales",
                        "slug": "R\u00f3mer-Rosales",
                        "structuredName": {
                            "firstName": "R\u00f3mer",
                            "lastName": "Rosales",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R\u00f3mer Rosales"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "Linear models [3] are simple, interpretable and easy to scale; however, they are limited in their expressive power."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3182689,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "62312d957c63d85e9ba446bdbfa4143e8fbd13d3",
            "isKey": false,
            "numCitedBy": 295,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Clickthrough and conversation rates estimation are two core predictions tasks in display advertising. We present in this article a machine learning framework based on logistic regression that is specifically designed to tackle the specifics of display advertising. The resulting system has the following characteristics: It is easy to implement and deploy, it is highly scalable (we have trained it on terabytes of data), and it provides models with state-of-the-art accuracy."
            },
            "slug": "Simple-and-Scalable-Response-Prediction-for-Display-Chapelle-Manavoglu",
            "title": {
                "fragments": [],
                "text": "Simple and Scalable Response Prediction for Display Advertising"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A machine learning framework based on logistic regression that is specifically designed to tackle the specifics of display advertising and provides models with state-of-the-art accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Trans. Intell. Syst. Technol."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2843982"
                        ],
                        "name": "Steffen Rendle",
                        "slug": "Steffen-Rendle",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Rendle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steffen Rendle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 29
                            }
                        ],
                        "text": "Factorization machines (FMs) [11, 12] project sparse features onto low-dimensional dense vectors and learn feature interactions from vector inner products."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17265929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df93596d4ed71d2863532c063c4c693711216abf",
            "isKey": false,
            "numCitedBy": 1819,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models."
            },
            "slug": "Factorization-Machines-Rendle",
            "title": {
                "fragments": [],
                "text": "Factorization Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Factorization Machines (FM) are introduced which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models and can mimic these models just by specifying the input data (i.e. the feature vectors)."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Data Mining"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738395"
                        ],
                        "name": "Alexandr Andoni",
                        "slug": "Alexandr-Andoni",
                        "structuredName": {
                            "firstName": "Alexandr",
                            "lastName": "Andoni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandr Andoni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679451"
                        ],
                        "name": "R. Panigrahy",
                        "slug": "R.-Panigrahy",
                        "structuredName": {
                            "firstName": "Rina",
                            "lastName": "Panigrahy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Panigrahy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806083"
                        ],
                        "name": "G. Valiant",
                        "slug": "G.-Valiant",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Valiant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Valiant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152831498"
                        ],
                        "name": "Li Zhang",
                        "slug": "Li-Zhang",
                        "structuredName": {
                            "firstName": "Li",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 23
                            }
                        ],
                        "text": "\u008cere has been research [16, 17] showing that DNNs are able to approximate an arbitrary function under certain smoothness assumptions to an arbitrary accuracy, given sufficiently many hidden units or hidden layers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10427590,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c8f73baeeeb9dc189753f3c2a275303b242187cf",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the effectiveness of learning low degree polynomials using neural networks by the gradient descent method. While neural networks have been shown to have great expressive power, and gradient descent has been widely used in practice for learning neural networks, few theoretical guarantees are known for such methods. In particular, it is well known that gradient descent can get stuck at local minima, even for simple classes of target functions. In this paper, we present several positive theoretical results to support the effectiveness of neural networks. We focus on twolayer neural networks where the bottom layer is a set of non-linear hidden nodes, and the top layer node is a linear function, similar to Barron (1993). First we show that for a randomly initialized neural network with sufficiently many hidden units, the generic gradient descent algorithm learns any low degree polynomial, assuming we initialize the weights randomly. Secondly, we show that if we use complex-valued weights (the target function can still be real), then under suitable conditions, there are no \"robust local minima\": the neural network can always escape a local minimum by performing a random perturbation. This property does not hold for real-valued weights. Thirdly, we discuss whether sparse polynomials can be learned with small neural networks, with the size dependent on the sparsity of the target function."
            },
            "slug": "Learning-Polynomials-with-Neural-Networks-Andoni-Panigrahy",
            "title": {
                "fragments": [],
                "text": "Learning Polynomials with Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper shows that for a randomly initialized neural network with sufficiently many hidden units, the generic gradient descent algorithm learns any low degree polynomial, assuming the authors initialize the weights randomly, and shows that if they use complex-valued weights, there are no \"robust local minima\"."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50012345"
                        ],
                        "name": "W. Rudin",
                        "slug": "W.-Rudin",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Rudin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Rudin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 50742905,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "88263b99ae7d88ae8719ad463e171058686349f0",
            "isKey": false,
            "numCitedBy": 6474,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Chapter 1: The Real and Complex Number Systems Introduction Ordered Sets Fields The Real Field The Extended Real Number System The Complex Field Euclidean Spaces Appendix Exercises Chapter 2: Basic Topology Finite, Countable, and Uncountable Sets Metric Spaces Compact Sets Perfect Sets Connected Sets Exercises Chapter 3: Numerical Sequences and Series Convergent Sequences Subsequences Cauchy Sequences Upper and Lower Limits Some Special Sequences Series Series of Nonnegative Terms The Number e The Root and Ratio Tests Power Series Summation by Parts Absolute Convergence Addition and Multiplication of Series Rearrangements Exercises Chapter 4: Continuity Limits of Functions Continuous Functions Continuity and Compactness Continuity and Connectedness Discontinuities Monotonic Functions Infinite Limits and Limits at Infinity Exercises Chapter 5: Differentiation The Derivative of a Real Function Mean Value Theorems The Continuity of Derivatives L'Hospital's Rule Derivatives of Higher-Order Taylor's Theorem Differentiation of Vector-valued Functions Exercises Chapter 6: The Riemann-Stieltjes Integral Definition and Existence of the Integral Properties of the Integral Integration and Differentiation Integration of Vector-valued Functions Rectifiable Curves Exercises Chapter 7: Sequences and Series of Functions Discussion of Main Problem Uniform Convergence Uniform Convergence and Continuity Uniform Convergence and Integration Uniform Convergence and Differentiation Equicontinuous Families of Functions The Stone-Weierstrass Theorem Exercises Chapter 8: Some Special Functions Power Series The Exponential and Logarithmic Functions The Trigonometric Functions The Algebraic Completeness of the Complex Field Fourier Series The Gamma Function Exercises Chapter 9: Functions of Several Variables Linear Transformations Differentiation The Contraction Principle The Inverse Function Theorem The Implicit Function Theorem The Rank Theorem Determinants Derivatives of Higher Order Differentiation of Integrals Exercises Chapter 10: Integration of Differential Forms Integration Primitive Mappings Partitions of Unity Change of Variables Differential Forms Simplexes and Chains Stokes' Theorem Closed Forms and Exact Forms Vector Analysis Exercises Chapter 11: The Lebesgue Theory Set Functions Construction of the Lebesgue Measure Measure Spaces Measurable Functions Simple Functions Integration Comparison with the Riemann Integral Integration of Complex Functions Functions of Class L2 Exercises Bibliography List of Special Symbols Index"
            },
            "slug": "Principles-of-mathematical-analysis-Rudin",
            "title": {
                "fragments": [],
                "text": "Principles of mathematical analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1964
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113979381"
                        ],
                        "name": "Xing Hao",
                        "slug": "Xing-Hao",
                        "structuredName": {
                            "firstName": "Xing",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xing Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8273966"
                        ],
                        "name": "Guigang Zhang",
                        "slug": "Guigang-Zhang",
                        "structuredName": {
                            "firstName": "Guigang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guigang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118869556"
                        ],
                        "name": "Shang Ma",
                        "slug": "Shang-Ma",
                        "structuredName": {
                            "firstName": "Shang",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shang Ma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1779661,
            "fieldsOfStudy": [
                "Education"
            ],
            "id": "4f8d648c52edf74e41b0996128aa536e13cc7e82",
            "isKey": false,
            "numCitedBy": 34883,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Deep-Learning-Hao-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Learning"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Semantic Comput."
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "We used Sibyl [2]\u2014a large-scale machine-learning system for distributed logistic regression."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "We used Sibyl [2]\u2014a large-scale machine-learning system for distributed logistic regression. e integer featureswere discretized on a log scale. e cross features were selected by a sophisticated feature selection tool."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Sibyl: A system for large scale supervised machine learning"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Talk"
            },
            "year": 2012
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 19,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Deep-&-Cross-Network-for-Ad-Click-Predictions-Wang-Fu/01d625baf1f2171b33e56d67213a7045b035da02?sort=total-citations"
}