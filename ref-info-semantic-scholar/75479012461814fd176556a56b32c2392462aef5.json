{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6183435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "309d5e05fb12df556237d58c8e844605f4d02073",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Current Neural Network learning algorithms are limited in their ability to model non-linear dynamical systems. Most supervised gradient-based recurrent neural networks (RNNs) suffer from a vanishing error signal that prevents learning from inputs far in the past. Those that do not, still have problems when there are numerous local minima. We introduce a general framework for sequence learning, EVOlution of recurrent systems with LINear outputs (Evolino). Evolino uses evolution to discover good RNN hidden node weights, while using methods such as linear regression or quadratic programming to compute optimal linear mappings from hidden state to output. Using the Long Short-Term Memory RNN Architecture, the method is tested in three very different problem domains: 1) context-sensitive languages, 2) multiple superimposed sine waves, and 3) the Mackey-Glass system. Evolino performs exceptionally well across all tasks, where other methods show notable deficiencies in some."
            },
            "slug": "Evolino:-Hybrid-Neuroevolution/Optimal-Linear-for-Schmidhuber-Wierstra",
            "title": {
                "fragments": [],
                "text": "Evolino: Hybrid Neuroevolution/Optimal Linear Search for Sequence Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A general framework for sequence learning, EVOlution of recurrent systems with LINear outputs (Evolino), which uses evolution to discover good RNN hidden node weights, while using methods such as linear regression or quadratic programming to compute optimal linear mappings from hidden state to output."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 786647,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a149d240ecde338e91ccb2f001074b792be070b2",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Existing Recurrent Neural Networks (RNNs) are limited in their ability to model dynamical systems with nonlinearities and hidden internal states. Here we use our general framework for sequence learning, EVOlution of recurrent systems with LINear Outputs (Evolino), to discover good RNN hidden node weights through evolution, while using linear regression to compute an optimal linear mapping from hidden state to output. Using the Long Short-Term Memory RNN Architecture, Evolino outperforms previous state-of-the-art methods on several tasks: 1) context-sensitive languages, 2) multiple superimposed sine waves."
            },
            "slug": "Modeling-systems-with-internal-state-using-evolino-Wierstra-Gomez",
            "title": {
                "fragments": [],
                "text": "Modeling systems with internal state using evolino"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work uses the general framework for sequence learning, EVOlution of recurrent systems with LINear Outputs (Evolino), to discover good RNN hidden node weights through evolution, while using linear regression to compute an optimal linear mapping from hidden state to output."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This approach can quickly learn to solve difficult reinforcement learning control tasks [5,6,22], including ones that require use of deep memory [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5241615,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "f04b240f1840cfeb2ede4bbcba835827acc30b10",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks are theoretically capable of learning complex temporal sequences, but training them through gradient-descent is too slow and unstable for practical use in reinforcement learning environments. Neuroevolution, the evolution of artificial neural networks using genetic algorithms, can potentially solve real-world reinforcement learning tasks that require deep use of memory, i.e. memory spanning hundreds or thousands of inputs, by searching the space of recurrent neural networks directly. In this paper, we introduce a new neuroevolution algorithm called Hierarchical Enforced SubPopulations that simultaneously evolves networks at two levels of granularity: full networks and network components or neurons. We demonstrate the method in two POMDP tasks that involve temporal dependencies of up to thousands of time-steps, and show that it is faster and simpler than the current best conventional reinforcement learning system on these tasks."
            },
            "slug": "Co-evolving-recurrent-neurons-learn-deep-memory-Gomez-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Co-evolving recurrent neurons learn deep memory POMDPs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new neuroevolution algorithm called Hierarchical Enforced SubPopulations that simultaneously evolves networks at two levels of granularity: full networks and network components or neurons is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3318461"
                        ],
                        "name": "M. Gagliolo",
                        "slug": "M.-Gagliolo",
                        "structuredName": {
                            "firstName": "Matteo",
                            "lastName": "Gagliolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gagliolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "We call this Evolino va riant EVOlution of systems with KErnel-based outputs (Evoke; [37])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9568795,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bbdd3f9497542ef7ca3d9bdc27a8010afe535d9",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional Support Vector Machines (SVMs) need pre-wired finite time windows to predict and classify time series. They do not have an internal state necessar y to deal with sequences involving arbitrary long-term dependencies. Here we introduce a new class of recurrent, truly sequential SVM-like devices with internal adaptive states, trained by a novel method called EVOlution of systems with KErnel-based outputs (Evoke), an instance of the recent Evolino class of methods [1, 2]. Evoke evolves recurrent neural networks to detect and represent temporal dependencies while using quadratic programming/support vector regression to produce precise outputs, in contrast t o our recent work [1, 2] which instead uses pseudoinverse regression. Evoke is the first SVM-based mechanis m learning to classify a context-sensitive language. It also outperforms recent state-of-the-art gra dient-based recurrent neural networks (RNNs) on various time series prediction tasks."
            },
            "slug": "Evolino-for-recurrent-support-vector-machines-Schmidhuber-Gagliolo",
            "title": {
                "fragments": [],
                "text": "Evolino for recurrent support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work introduces a new class of recurrent, truly sequential SVM-like devices with internal adaptive states, trained by a novel method called EVOlution of systems with KErnel-based outputs (Evoke), an instance of the recent Evolino class of methods."
            },
            "venue": {
                "fragments": [],
                "text": "ESANN"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780235"
                        ],
                        "name": "Fred Cummins",
                        "slug": "Fred-Cummins",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Cummins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred Cummins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous work on LSTM has focused on gradient-based G-LSTM [1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11598600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
            "isKey": false,
            "numCitedBy": 3338,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive forget gate that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way."
            },
            "slug": "Learning-to-Forget:-Continual-Prediction-with-LSTM-Gers-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning to Forget: Continual Prediction with LSTM"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work identifies a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset, and proposes a novel, adaptive forget gate that enables an LSTm cell to learn to reset itself at appropriate times, thus releasing internal resources."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We will see that it also outperforms recent state-of-the-art RNNs on certain tasks, including echo state networks (ESNs) (Jaeger, 2004a) and previous gradient descent RNNs (Hochreiter & Schmidhuber, 1997a; Pearlmutter, 1995; Robinson & Fallside, 1987; Rumelhart & McClelland, 1986; Werbos, 1974; Williams, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The drawback of ESNs is that the only truly computationally powerful, nonlinear part of the net does not learn, whereas previous supervised, gradient-based learning algorithms for sequence-processing RNNs (Pearlmutter, 1995; Robinson & Fallside, 1987; Schmidhuber, 1992; Werbos, 1988; Williams, 1989) adjust all weights of the net, not just the output weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Recurrent neural networks (RNNs; Pearlmutter, 1995; Robinson & Fallside, 1987; Rumelhart & McClelland, 1986; Werbos, 1974; Williams, 1989) are mathematical abstractions of biological nervous systems that can perform complex mappings from input sequences to output sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1400872,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32e97eef94beacace020e79322cef0e1e5a76ee0",
            "isKey": false,
            "numCitedBy": 607,
            "numCiting": 302,
            "paperAbstract": {
                "fragments": [],
                "text": "Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continues with some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. The author presents some simulations, and at the end, addresses issues of computational complexity and learning speed."
            },
            "slug": "Gradient-calculations-for-dynamic-recurrent-neural-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Gradient calculations for dynamic recurrent neural networks: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones and presents some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5631525"
                        ],
                        "name": "H. Jaeger",
                        "slug": "H.-Jaeger",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jaeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2759349"
                        ],
                        "name": "M. Luko\u0161evi\u010dius",
                        "slug": "M.-Luko\u0161evi\u010dius",
                        "structuredName": {
                            "firstName": "Mantas",
                            "lastName": "Luko\u0161evi\u010dius",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Luko\u0161evi\u010dius"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32384143"
                        ],
                        "name": "D. Popovici",
                        "slug": "D.-Popovici",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Popovici",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Popovici"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10707214"
                        ],
                        "name": "U. Siewert",
                        "slug": "U.-Siewert",
                        "structuredName": {
                            "firstName": "Udo",
                            "lastName": "Siewert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Siewert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17795421,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a10ec7cc6c42c7780ef631c038b16c49ed865038",
            "isKey": false,
            "numCitedBy": 650,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimization-and-applications-of-echo-state-with-Jaeger-Luko\u0161evi\u010dius",
            "title": {
                "fragments": [],
                "text": "Optimization and applications of echo state networks with leaky- integrator neurons"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2937141"
                        ],
                        "name": "Brian Yamauchi",
                        "slug": "Brian-Yamauchi",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Yamauchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Yamauchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760775"
                        ],
                        "name": "R. Beer",
                        "slug": "R.-Beer",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Beer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Beer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "more efficient and principled way is to search the space of RNN weight matrices [20,21,26,45,53,54] using evolutionary algorithms [13,31,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1167765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45eeee4e13639f33b6655795ac4929732faa9352",
            "isKey": false,
            "numCitedBy": 174,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This article explores the use of a real-valued modular genetic algorithm to evolve continuous-time recurrent neural networks capable of sequential behavior and learning. We evolve networks that can generate a fixed sequence of outputs in response to an external trigger occurring at varying intervals of time. We also evolve networks that can learn to generate one of a set of possible sequences based on reinforcement from the environment. Finally, we utilize concepts from dynamical systems theory to understand the operation of some of these evolved networks. A novel feature of our approach is that we assume neither an a priori discretization of states or time nor an a priori learning algorithm that explicitly modifies network parameters during learning. Rather, we merely expose dynamical neural networks to tasks that require sequential behavior and learning and allow the genetic algorithm to evolve network dynamics capable of accomplishing these tasks."
            },
            "slug": "Sequential-Behavior-and-Learning-in-Evolved-Neural-Yamauchi-Beer",
            "title": {
                "fragments": [],
                "text": "Sequential Behavior and Learning in Evolved Dynamical Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This article explores the use of a real-valued modular genetic algorithm to evolve continuous-time recurrent neural networks capable of sequential behavior and learning and utilizes concepts from dynamical systems theory to understand the operation of some of these evolved networks."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402348516"
                        ],
                        "name": "J. A. P\u00e9rez-Ortiz",
                        "slug": "J.-A.-P\u00e9rez-Ortiz",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "P\u00e9rez-Ortiz",
                            "middleNames": [
                                "Antonio"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. A. P\u00e9rez-Ortiz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2396681"
                        ],
                        "name": "D. Eck",
                        "slug": "D.-Eck",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Eck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous work on LSTM has focused on gradient-based G-LSTM [1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12588772,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c49019464e326899e76be358a86b8706ee20d0ef",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Kalman-filters-improve-LSTM-network-performance-in-P\u00e9rez-Ortiz-Gers",
            "title": {
                "fragments": [],
                "text": "Kalman filters improve LSTM network performance in problems unsolvable by traditional recurrent nets"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous work on LSTM has focused on gradient-based G-LSTM [1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 474078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "047655e733a9eed9a500afd916efa566915b9110",
            "isKey": false,
            "numCitedBy": 1280,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals."
            },
            "slug": "Learning-Precise-Timing-with-LSTM-Recurrent-Gers-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Learning Precise Timing with LSTM Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work finds that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous work on LSTM has focused on gradient-based G-LSTM [1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "A recent RNN called Long Short-Term Memory (LSTM; [11]), however, overcomes this fundamental problem through a specialized architecture that does not impose any unrealistic bias towards recent events by maintaining constant error flow back through time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We will see that it also outperforms recent state-of-the-art RNNs on certain tasks, including Echo State Networks (ESNs) [15] and previous gradient descent RNNs [11, 27, 32, 33, 49, 52]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 52390,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The drawback of ESNs is that the only truly computationally powerful, nonlinear part of the net does not learn, whereas previous supervised, gradient-based learning algorithms for sequence-processing RNNs [27, 32, 36, 50, 52] adjust all weights of the net, not just the output weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205118721,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "266e07d0dd9a75b61e3632e9469993dbaf063f1c",
            "isKey": false,
            "numCitedBy": 882,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Generalization-of-backpropagation-with-application-Werbos",
            "title": {
                "fragments": [],
                "text": "Generalization of backpropagation with application to a recurrent gas market model"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The G-LSTM results are taken from [1]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "To compare Evolino-based LSTM with published results for G-LSTM [1], we chose the language abc."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous work on LSTM has focused on gradient-based G-LSTM [1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Table 1 compares the results of Evolino-based LSTM, using Pseudoinverse as supervised learning module (PI-Evolino), with those of G-LSTM from [1]; \u201cStandard PI-Evolino\u201d uses parameter settings that are a compromise between discrete and continuous domains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In keeping with the setup in [1], we added a bias unit to the Forget gates and Output gates with values of +1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10192330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f828b401c86e0f8fddd8e77774e332dfd226cb05",
            "isKey": false,
            "numCitedBy": 587,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a(n)b(n)c(n)."
            },
            "slug": "LSTM-recurrent-networks-learn-simple-context-free-Gers-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "LSTM recurrent networks learn simple context-free and context-sensitive languages"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Long short-term memory (LSTM) variants are also the first RNNs to learn a simple context-sensitive language, namely a(n)b( n)c(n)."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859405"
                        ],
                        "name": "David E. Moriarty",
                        "slug": "David-E.-Moriarty",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Moriarty",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David E. Moriarty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, recent progress has been made with cooperatively coevolving recurrent neurons, each with its own rather small, local search space of possible weight vectors [6, 23, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This cooperative coevolutionary approach is an extension to Symbiotic, Adaptive Neuroevolution (SANE; [23]) which also evolves neurons, but in a single population."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 608769,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb01aa49fdd9b59127b8651b36fa187095097799",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a new reinforcement learning method called SANE (Symbiotic, Adaptive Neuro-Evolution), which evolves a population of neurons through genetic algorithms to form a neural network capable of performing a task. Symbiotic evolution promotes both cooperation and specialization, which results in a fast, efficient genetic search and discourages convergence to suboptimal solutions. In the inverted pendulum problem, SANE formed effective networks 9 to 16 times faster than the Adaptive Heuristic Critic and 2 times faster thanQ-learning and the GENITOR neuro-evolution approach without loss of generalization. Such efficient learning, combined with few domain assumptions, make SANE a promising approach to a broad range of reinforcement learning problems, including many real-world applications."
            },
            "slug": "Efficient-reinforcement-learning-through-symbiotic-Moriarty-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Efficient reinforcement learning through symbiotic evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A new reinforcement learning method called SANE (Symbiotic, Adaptive Neuro-Evolution), which evolves a population of neurons through genetic algorithms to form a neural network capable of performing a task, is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744439"
                        ],
                        "name": "J. Suykens",
                        "slug": "J.-Suykens",
                        "structuredName": {
                            "firstName": "Johan",
                            "lastName": "Suykens",
                            "middleNames": [
                                "A.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Suykens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704135"
                        ],
                        "name": "J. Vandewalle",
                        "slug": "J.-Vandewalle",
                        "structuredName": {
                            "firstName": "Joos",
                            "lastName": "Vandewalle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vandewalle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In a more sophisticated approach by Suykens and Vandewalle [46], a window of m previous output values is fed back as input to a recurrent model with a fixed kernel."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17321559,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "cf5a39d7ba0483683af2b010a13850d466f43229",
            "isKey": false,
            "numCitedBy": 322,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The method of support vector machines (SVM's) has been developed for solving classification and static function approximation problems. In this paper we introduce SVM's within the context of recurrent neural networks. Instead of Vapnik's epsilon insensitive loss function, we consider a least squares version related to a cost function with equality constraints for a recurrent network. Essential features of SVM's remain, such as Mercer's condition and the fact that the output weights are a Lagrange multiplier weighted sum of the data points. The solution to recurrent least squares (LS-SVM's) is characterized by a set of nonlinear equations. Due to its high computational complexity, we focus on a limited case of assigning the squared error an infinitely large penalty factor with early stopping as a form of regularization. The effectiveness of the approach is demonstrated on trajectory learning of the double scroll attractor in Chua's circuit."
            },
            "slug": "Recurrent-least-squares-support-vector-machines-Suykens-Vandewalle",
            "title": {
                "fragments": [],
                "text": "Recurrent least squares support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper introduces SVM's within the context of recurrent neural networks and considers a least squares version of Vapnik's epsilon insensitive loss function related to a cost function with equality constraints for a recurrent network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47966382"
                        ],
                        "name": "L. Dung",
                        "slug": "L.-Dung",
                        "structuredName": {
                            "firstName": "Le",
                            "lastName": "Dung",
                            "middleNames": [
                                "Tien"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Dung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47311339"
                        ],
                        "name": "T. Komeda",
                        "slug": "T.-Komeda",
                        "structuredName": {
                            "firstName": "Takashi",
                            "lastName": "Komeda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Komeda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49315614"
                        ],
                        "name": "M. Takagi",
                        "slug": "M.-Takagi",
                        "structuredName": {
                            "firstName": "Motoki",
                            "lastName": "Takagi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Takagi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27787454,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "194e63a44b0585c64ab3128180910eb6c786f3fc",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning (RL) has been widely used to solve problems with a little feedback from environment. Q learning can solve Markov decision processes (MDPs) quite well. For partially observable Markov decision processes (POMDPs), a recurrent neural network (RNN) can be used to approximate Q values. However, learning time for these problems is typically very long. We present a new combination of RL and RNN to find a good policy for POMDPs in a shorter learning time. This method contains two phases: firstly, state space is divided into two groups (fully observable state group and hidden state group); secondly, a Q value table is used to store values of fully observable states and an RNN is used to approximate values for hidden states. Results of experiments in two grid world problems show that the proposed method enables an agent to acquire a policy with better learning performance compared to the method using only a RNN."
            },
            "slug": "REINFORCEMENT-LEARNING-FOR-POMDP-USING-STATE-Dung-Komeda",
            "title": {
                "fragments": [],
                "text": "REINFORCEMENT LEARNING FOR POMDP USING STATE CLASSIFICATION"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new combination of RL and RNN is presented to find a good policy for POMDPs in a shorter learning time and results show that the proposed method enables an agent to acquire a policy with better learning performance compared to the method using only a RNN."
            },
            "venue": {
                "fragments": [],
                "text": "MLMTA"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1554231,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1d8c5d31ba295dcac4ec93ffdee31682c00c4b8",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "The success of evolutionary methods on standard control learning tasks has created a need for new benchmarks. The classic pole balancing problem is no longer difficult enough to serve as a viable yardstick for measuring the learning efficiency of these systems. The double pole case, where two poles connected to the cart must be balanced simultaneously is much more difficult, especially when velocity information is not available. In this article, we demonstrate a neuroevolution system, Enforced Sub-populations (ESP), that is used to evolve a controller for the standard double pole task and a much harder, non-Markovian version. In both cases, our results show that ESP is faster than other neuroevolution methods. In addition, we introduce an incremental method that evolves on a sequence of tasks, and utilizes a local search technique (Delta-Coding) to sustain diversity. This method enables the system to solve even more difficult versions of the task where direct evolution cannot."
            },
            "slug": "Solving-Non-Markovian-Control-Tasks-with-Gomez-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Solving Non-Markovian Control Tasks with Neuro-Evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article demonstrates a neuroevolution system, Enforced Sub-populations (ESP), that is used to evolve a controller for the standard double pole task and a much harder, non-Markovian version, and introduces an incremental method that evolves on a sequence of tasks, and utilizes a local search technique (Delta-Coding) to sustain diversity."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144815061"
                        ],
                        "name": "E. Maillard",
                        "slug": "E.-Maillard",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Maillard",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Maillard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2643840"
                        ],
                        "name": "D. Gu\u00e9riot",
                        "slug": "D.-Gu\u00e9riot",
                        "structuredName": {
                            "firstName": "Didier",
                            "lastName": "Gu\u00e9riot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Gu\u00e9riot"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The linear regression method used to compute the output weights (W in equation 2.2) is the Moore-Penrose pseudoinverse method, which is both fast and optimal in the sense that it minimizes the summed squared error (Penrose, 1955) (compare  Maillard & Gueriot, 1997,  for an application to feedforward RBF nets and Ishii et al., 2004, for an application to echo state networks)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This generalizes methods such as those of Maillard ( Maillard & Gueriot, 1997 ) and Ishii et al. (Ishii, van der Zant, Be\u02c7 canovi\u00b4c, & Pl\u00a8 oger, 2004; van der Zant, Be\u02c7 canovi\u00b4c, Ishii, Kobialka, & Pl\u00a8 oger, 2004) that evolve radial basis functions and ESNs, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14047522,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "41a651956cf2605a729e83aef5d13a831b4a832c",
            "isKey": true,
            "numCitedBy": 39,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The radial basis function (RBF) network is an efficient function approximator. Theoretical researches focus on the capabilities of the network to reach an optimal solution. Unfortunately, few results concerning the design, and training of the network are available. When dealing with a specific application, the performances of the network dramatically depend on the number of neurons and on the distribution of the hidden neurons in the input space. Generally, the network resulting from learning applied to a predetermined architecture, is either insufficient or over-complicated. In this study, we focus on genetic learning for the RBF network applied to prediction of chaotic time series. The centers and widths of the hidden layer neurons basis function-defined as the barycenter and distance between two input patterns-are coded into a chromosome. It is shown that the basis functions which are also coded as a parameter of the neurons provide an additional degree of freedom resulting in a smaller optimal network. A direct matrix inversion provides the weights between the hidden layer and the output layer and avoids the risk of getting stuck into a local minimum. The performances of a network with Gaussian basis functions is compared with those of a network with genetic determination of the basis functions on the Mackey-Glass delay differential equation."
            },
            "slug": "RBF-neural-network,-basis-functions-and-genetic-Maillard-Gu\u00e9riot",
            "title": {
                "fragments": [],
                "text": "RBF neural network, basis functions and genetic algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This study focuses on genetic learning for the RBF network applied to prediction of chaotic time series and it is shown that the basis functions which are also coded as a parameter of the neurons provide an additional degree of freedom resulting in a smaller optimal network."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "more efficient and principled way is to search the space of RNN weight matrices [20,21,26,45,53,54] using evolutionary algorithms [13,31,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 205966451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "add58c2acbe76ae430cc207356acb02ee6171c4e",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Research on potential interactions between connectionist learning systems, i.e., artificial neural networks (ANNs), and evolutionary search procedures, like genetic algorithms (GAs), has attracted a lot of attention recently. Evolutionary ANNs (EANNs) can be considered as the combination of ANNs and evolutionary search procedures. This article first distinguishes among three kinds of evolution in EANNs, i.e., the evolution of connection weights, of architectures, and of learning rules. Then it reviews each kind of evolution in detail and analyzes critical issues related to different evolutions. the review shows that although a lot of work has been done on the evolution of connection weights and architectures, few attempts have been made to understand the evolution of learning rules. Interactions among different evolutions are seldom mentioned in current research. However, the evolution of learning rules and its interactions with other kinds of evolution, play a vital role in EANNs. Finally, this article briefly describes a general framework for EANNs, which not only includes the aforementioned three kinds of evolution, but also considers interactions among them. \u00a9 1993 John Wiley & Sons, Inc."
            },
            "slug": "A-review-of-evolutionary-artificial-neural-networks-Yao",
            "title": {
                "fragments": [],
                "text": "A review of evolutionary artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This article distinguishes among three kinds of evolution in EANNs, i.e., the evolution of connection weights, of architectures, and of learning rules, and describes a general framework for E ANNs, which not only includes the aforementioned three types of evolution, but also considers interactions among them."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Intell. Syst."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "On the other hand, for large data sets such as those used in speech recognition we typically need much larger LSTM networks with on the order of 100,000 weights [ 8 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1856462,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f83f6e1afadf0963153974968af6b8342775d82",
            "isKey": false,
            "numCitedBy": 3324,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Framewise-phoneme-classification-with-bidirectional-Graves-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Framewise phoneme classification with bidirectional LSTM and other neural network architectures"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859405"
                        ],
                        "name": "David E. Moriarty",
                        "slug": "David-E.-Moriarty",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Moriarty",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David E. Moriarty"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 89
                            }
                        ],
                        "text": "This approach can quickly learn t o solve difficult reinforcement learning control tasks [5,6,22], including ones that require use of deep memory [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2943620,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6dbbc1c6a819978fb72a9baae157d7d2a19b5361",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 199,
            "paperAbstract": {
                "fragments": [],
                "text": "Sequential decision tasks appear in many practical situations ranging from robot navigation to stock market trading. Because of the complexity of such tasks, it is often difficult to perceive the direct consequences of individual decisions and even harder to generate examples of correct behavior. Consequently, difficult decision problems such as routing traffic, autonomous control, and resource allocation are often unautomated or are only semi-automated using \"rule-of-thumb\" strategies or simple heuristics. This dissertation proposes a general methodology for automating these tasks using techniques from machine learning. Specifically, this research studies the combination of evolutionary algorithms and artificial neural networks to learn and perform difficult decision tasks. Evolutionary algorithms provide an efficient search engine for building decision strategies and require only minimal reinforcement or direction from the environment. Neural networks provide an efficient storage mechanism for the decision policy and can generalize experiences from one situation to another. The learning system developed in this dissertation called SANE contains an evolutionary algorithm specifically tailored to sequential decision learning. Populations evolve faster than previous methods and rarely converge on suboptimal solutions. SANE is extensively evaluated and compared to existing decision learning systems and other evolutionary algorithms. SANE is shown to be significantly faster, more robust, and more adaptive in almost every situation. Moreover, SANE's efficient searches return more profitable decision strategies. The flexibility and scope of SANE is demonstrated in two real-world applications. First, SANE significantly improves the play of a world champion Othello program. Second, SANE successfully forms neural networks that guide a robot arm to target objects while avoiding randomly placed obstacles. The contributions of this research are twofold: a novel integration of evolutionary algorithms and neural networks and an efficient system for learning decision strategies in complex problems."
            },
            "slug": "Symbiotic-Evolution-of-Neural-Networks-in-Decision-Moriarty",
            "title": {
                "fragments": [],
                "text": "Symbiotic Evolution of Neural Networks in Sequential Decision Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This research studies the combination of evolutionary algorithms and artificial neural networks to learn and perform difficult decision tasks and develops an efficient system for learning decision strategies in complex problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30840575"
                        ],
                        "name": "K. Ishu",
                        "slug": "K.-Ishu",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Ishu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ishu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2126132300"
                        ],
                        "name": "T. Van Der Zant",
                        "slug": "T.-Van-Der-Zant",
                        "structuredName": {
                            "firstName": "Tamara",
                            "lastName": "Van Der Zant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Van Der Zant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735405"
                        ],
                        "name": "V. Becanovic",
                        "slug": "V.-Becanovic",
                        "structuredName": {
                            "firstName": "Vlatko",
                            "lastName": "Becanovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Becanovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3178853"
                        ],
                        "name": "P. Ploger",
                        "slug": "P.-Ploger",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Ploger",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ploger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We show that Evolino-based LSTM can solve tasks thatEcho State nets [15] cannot, and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30054105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0817fcfda01350b77d96f92c1f03b3e47f827f3a",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Echo State Networks (ESNs) use a recurrent artificial neural network as a reservoir. Finding a good one depends on choosing the right parameters for the generation of the reservoir, intuition and luck. The method proposed in this article eliminates the need for the tuning by hand by replacing it with a double evolutionary computation. First a broad search to find the right parameters which generate the reservoir is used. Then a search directly on the connectivity matrices fine-tunes the ESN. Both steps show improvements over other known methods for an experimental limit-cycle dataset of the Twin-Burger underwater robot."
            },
            "slug": "Identification-of-motion-with-echo-state-network-Ishu-Zant",
            "title": {
                "fragments": [],
                "text": "Identification of motion with echo state network"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The method proposed in this article eliminates the need for the tuning by hand by replacing it with a double evolutionary computation for an experimental limit-cycle dataset of the Twin-Burger underwater robot."
            },
            "venue": {
                "fragments": [],
                "text": "Oceans '04 MTS/IEEE Techno-Ocean '04 (IEEE Cat. No.04CH37600)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "This approach can quickly learn to solve difficult reinforcement learning control tasks [5,6,22], including ones that require use of deep memory [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1086771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9382c0ec9904ea93089f439479607f7fd0195505",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In practice, almost all control systems in use today implement some form of linear control. However, there are many tasks for which conventional control engineering methods are not directly applicable because there is not enough information about how the system should be controlled (i.e. reinforcement learning problems). In this paper, we explore an approach to such problems that evolves fast-weight neural networks. These networks, although capable of implementing arbitrary non-linear mappings, can more easily exploit the piecewise linearity inherent in most systems, in order to produce simpler and more comprehensible controllers. The method is tested on 2D mobile robot version of the pole balancing task where the controller must learn to switch between two operating modes, one using a single pole and the other using a jointed pole version that has not before been solved."
            },
            "slug": "Evolving-Modular-Fast-Weight-Networks-for-Control-Gomez-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Evolving Modular Fast-Weight Networks for Control"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach to problems that evolves fast-weight neural networks, although capable of implementing arbitrary non-linear mappings, can more easily exploit the piecewise linearity inherent in most systems, in order to produce simpler and more comprehensible controllers."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777920"
                        ],
                        "name": "M. A. Potter",
                        "slug": "M.-A.-Potter",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Potter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Potter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145502027"
                        ],
                        "name": "K. D. Jong",
                        "slug": "K.-D.-Jong",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Jong",
                            "middleNames": [
                                "A.",
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. D. Jong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In particular, recent progress has been made with cooperatively coevolving recurrent neurons, each with its own rather small, local search space of possible weight vectors [6, 23, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14464450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "773ee6c77651bbc15560dd2feba5649a0dafaeb6",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a coevolutionary architecture for solving decomposable problems and apply it to the evolution of artificial neural networks. Although this work is preliminary in nature it has a number of advantages over non-coevolutionary approaches. The coevolutionary approach utilizes a divide-and-conquer technique in which species representing simpler subtasks are evolved in separate instances of a genetic algorithm executing in parallel. Collaborations among the species are formed representing complete solutions. Species are created dynamically as needed. Results are presented in which the coevolutionary architecture produces higher quality solutions in fewer evolutionary trials when compared with an alternative noncoevolutionary approach on the problem of evolving cascade networks for parity computation."
            },
            "slug": "EVOLVING-NEURAL-NETWORKS-WITH-COLLABORATIVE-SPECIES-Potter-Jong",
            "title": {
                "fragments": [],
                "text": "EVOLVING NEURAL NETWORKS WITH COLLABORATIVE SPECIES"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A coevolutionary architecture for solving decomposable problems and apply it to the evolution of artificial neural networks that produces higher quality solutions in fewer evolutionary trials when compared with an alternative noncoevolutionaries approach on the problem of evolving cascade networks for parity computation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The display prevents the task from being solved by guessing the network weights [12], and makes the error gradient very rough."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "As we showed [12, 39], many RNN problems involving long-term dependencies that were considered challenging benchmarks in the 1990s, turned out to be trivial in that they could be solved by random weight guessing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7452865,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b158a006bebb619e2ea7bf0a22c27d45c5d19004",
            "isKey": false,
            "numCitedBy": 629,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard recurrent nets cannot deal with long minimal time lags between relevant signals. Several recent NIPS papers propose alternative methods. We first show: problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms. We then use LSTM, our own recent algorithm, to solve a hard problem that can neither be quickly solved by random search nor by any other recurrent net algorithm we are aware of."
            },
            "slug": "LSTM-can-Solve-Hard-Long-Time-Lag-Problems-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "LSTM can Solve Hard Long Time Lag Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that problems used to promote various previous algorithms can be solved more quickly by random weight guessing than by the proposed algorithms, and uses LSTM, its own recent algorithm, to solve a hard problem."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5631525"
                        ],
                        "name": "H. Jaeger",
                        "slug": "H.-Jaeger",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Jaeger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jaeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144547639"
                        ],
                        "name": "H. Haas",
                        "slug": "H.-Haas",
                        "structuredName": {
                            "firstName": "Harald",
                            "lastName": "Haas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Haas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Jaeger (2004b) reports that echo state networks are unable to learn functions composed of even two superimposed oscillators, in particular sin(0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "We used the same setup in our experiments as in Jaeger (2004a). Networks were evolved in the following way."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2184251,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d073966e48ffb6dccde1e4eb3f0380c10c6a766",
            "isKey": false,
            "numCitedBy": 2523,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a method for learning nonlinear systems, echo state networks (ESNs). ESNs employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains. The learning method is computationally efficient and easy to use. On a benchmark task of predicting a chaotic time series, accuracy is improved by a factor of 2400 over previous techniques. The potential for engineering applications is illustrated by equalizing a communication channel, where the signal error rate is improved by two orders of magnitude."
            },
            "slug": "Harnessing-Nonlinearity:-Predicting-Chaotic-Systems-Jaeger-Haas",
            "title": {
                "fragments": [],
                "text": "Harnessing Nonlinearity: Predicting Chaotic Systems and Saving Energy in Wireless Communication"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A method for learning nonlinear systems, echo state networks (ESNs), which employ artificial recurrent neural networks in a way that has recently been proposed independently as a learning mechanism in biological brains is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 88
                            }
                        ],
                        "text": "This approach can quickly learn to solve difficult reinforcement learning control tasks (Gomez & Schmidhuber, 2005a; Gomez, 2003; Moriarty, 1997), including ones that require use of deep memory (Gomez & Schmidhuber, 2005b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 240,
                                "start": 172
                            }
                        ],
                        "text": "In particular, recent progress has been made with cooperatively coevolving recurrent neurons, each with its own rather small, local search space of possible weight vectors (Gomez, 2003; Moriarty & Miikkulainen, 1996; Potter & De Jong, 1995)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62752350,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "9cdb78c0c6d237122d714885e1248080589a869f",
            "isKey": false,
            "numCitedBy": 181,
            "numCiting": 167,
            "paperAbstract": {
                "fragments": [],
                "text": "Many complex control problems require sophisticated solutions that are not amenable to traditional controller design. Not only is it difficult to model real world systems, but often it is unclear what kind of behavior is required to solve the task. Reinforcement learning approaches have made progress in such problems, but have so far not scaled well. Neuroevolution, has improved upon conventional reinforcement learning, but has still not been successful in full-scale, non-linear control problems. This dissertation develops a methodology for solving real world control tasks consisting of three components: (1)\u00a0an efficient neuroevolution algorithm that solves difficult non-linear control tasks by coevolving neurons, (2)\u00a0an incremental evolution method to scale the algorithm to the most challenging tasks, and (3)\u00a0a technique for making controllers robust so that they can transfer from simulation to the real world. The method is faster than other approaches on a set of difficult learning benchmarks, and is used in two full-scale control tasks demonstrating its applicability to real world problems."
            },
            "slug": "Robust-non-linear-control-through-neuroevolution-Gomez-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Robust non-linear control through neuroevolution"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This dissertation develops a methodology for solving real world control tasks consisting of an efficient neuroevolution algorithm that solves difficult non-linear control tasks by coevolving neurons, an incremental evolution method to scale the algorithm to the most challenging tasks, and a technique for making controllers robust so that they can transfer from simulation to the real world."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145675811"
                        ],
                        "name": "Sayan Mukherjee",
                        "slug": "Sayan-Mukherjee",
                        "structuredName": {
                            "firstName": "Sayan",
                            "lastName": "Mukherjee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sayan Mukherjee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1781874"
                        ],
                        "name": "E. Osuna",
                        "slug": "E.-Osuna",
                        "structuredName": {
                            "firstName": "Edgar",
                            "lastName": "Osuna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Osuna"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804489"
                        ],
                        "name": "F. Girosi",
                        "slug": "F.-Girosi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Girosi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Girosi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A limited way of applying existing SVMs to time series prediction [24, 25] or classification [34] is to build a training set either by transforming the sequential input into some static domain (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16950792,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d81512c6c2582fa91fe151efdaf80a867f66d12a",
            "isKey": false,
            "numCitedBy": 550,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel method for regression has been recently proposed by Vapnik et al. (1995, 1996). The technique, called support vector machine (SVM), is very well founded from the mathematical point of view and seems to provide a new insight in function approximation. We implemented the SVM and tested it on a database of chaotic time series previously used to compare the performances of different approximation techniques, including polynomial and rational approximation, local polynomial techniques, radial basis functions, and neural networks. The SVM performs better than the other approaches. We also study, for a particular time series, the variability in performance with respect to the few free parameters of SVM."
            },
            "slug": "Nonlinear-prediction-of-chaotic-time-series-using-Mukherjee-Osuna",
            "title": {
                "fragments": [],
                "text": "Nonlinear prediction of chaotic time series using support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The SVM is implemented and tested on a database of chaotic time series previously used to compare the performances of different approximation techniques, including polynomial and rational approximation, localPolynomial techniques, radial basis functions, and neural networks; the SVM performs better than the other approaches."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VII. Proceedings of the 1997 IEEE Signal Processing Society Workshop"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The drawback of ESNs is that the only truly computationally powerful, nonlinear part of the net does not learn, whereas previous supervised, gradient-based learning algorithms for sequence-processing RNNs [27, 32,  36 , 50, 52] adjust all weights of the net, not just the output weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11761172,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89b9a181801f32bf62c4237c4265ba036a79f9dc",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The real-time recurrent learning (RTRL) algorithm (Robinson and Fallside 1987; Williams and Zipser 1989) requires O(n4) computations per time step, where n is the number of noninput units. I describe a method suited for on-line learning that computes exactly the same gradient and requires fixed-size storage of the same order but has an average time complexity per time step of O(n3)."
            },
            "slug": "A-Fixed-Size-Storage-O(n3)-Time-Complexity-Learning-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "A Fixed Size Storage O(n3) Time Complexity Learning Algorithm for Fully Recurrent Continually Running Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A method suited for on-line learning that computes exactly the same gradient and requires fixed-size storage of the same order but has an average time complexity per time step of O(n3)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2396681"
                        ],
                        "name": "D. Eck",
                        "slug": "D.-Eck",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Eck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Previous work on LSTM has focused on gradient-based G-LSTM [1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[1\u20133, 8, 11, 29, 38]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 30459046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ec6a10ba7d671128d2a72a4bcd308661dbdd277",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "In response to Rodriguez's recent article (2001), we compare the performance of simple recurrent nets and long short-term memory recurrent nets on context-free and context-sensitive languages."
            },
            "slug": "Learning-Nonregular-Languages:-A-Comparison-of-and-Schmidhuber-Gers",
            "title": {
                "fragments": [],
                "text": "Learning Nonregular Languages: A Comparison of Simple Recurrent Networks and LSTM"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "In response to Rodriguez's recent article, this work compares the performance of simple recurrent nets and long short-term memory recurrent nets on context-free and context-sensitive languages."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145034054"
                        ],
                        "name": "K. M\u00fcller",
                        "slug": "K.-M\u00fcller",
                        "structuredName": {
                            "firstName": "Klaus-Robert",
                            "lastName": "M\u00fcller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. M\u00fcller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152597562"
                        ],
                        "name": "Gunnar R\u00e4tsch",
                        "slug": "Gunnar-R\u00e4tsch",
                        "structuredName": {
                            "firstName": "Gunnar",
                            "lastName": "R\u00e4tsch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gunnar R\u00e4tsch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707625"
                        ],
                        "name": "B. Sch\u00f6lkopf",
                        "slug": "B.-Sch\u00f6lkopf",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Sch\u00f6lkopf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Sch\u00f6lkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633352"
                        ],
                        "name": "J. Kohlmorgen",
                        "slug": "J.-Kohlmorgen",
                        "structuredName": {
                            "firstName": "Jens",
                            "lastName": "Kohlmorgen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kohlmorgen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5398743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f43840dc1638a18eb6178f1060dc5f41af1c5ac7",
            "isKey": false,
            "numCitedBy": 978,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Support Vector Machines are used for time series prediction and compared to radial basis function networks. We make use of two different cost functions for Support Vectors: training with (i) an e insensitive loss and (ii) Huber's robust loss function and discuss how to choose the regularization parameters in these models. Two applications are considered: data from (a) a noisy (normal and uniform noise) Mackey Glass equation and (b) the Santa Fe competition (set D). In both cases Support Vector Machines show an excellent performance. In case (b) the Support Vector approach improves the best known result on the benchmark by a factor of 29%."
            },
            "slug": "Predicting-Time-Series-with-Support-Vector-Machines-M\u00fcller-Smola",
            "title": {
                "fragments": [],
                "text": "Predicting Time Series with Support Vector Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Two different cost functions for Support Vectors are made use: training with an e insensitive loss and Huber's robust loss function and how to choose the regularization parameters in these models are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48890329"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Miller",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14024873"
                        ],
                        "name": "P. Todd",
                        "slug": "P.-Todd",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Todd",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Todd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983880"
                        ],
                        "name": "Shailesh U. Hegde",
                        "slug": "Shailesh-U.-Hegde",
                        "structuredName": {
                            "firstName": "Shailesh",
                            "lastName": "Hegde",
                            "middleNames": [
                                "U."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shailesh U. Hegde"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "more efficient and principled way is to search the space of RNN weight matrices [20,21,26,45,53,54] using evolutionary algorithms [13,31,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23166548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e89cb97bc83badf8c6cc0e2439ee4a035cba72d9",
            "isKey": false,
            "numCitedBy": 934,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "RoboCup has come a long way since it\u2019s creation in \u201997 [1] and is a respected place for machine learning researchers to try out new algorithms in a competitive fashion. RoboCup is now an international competition that draws many teams and respected researchers looking for a chance to create the best team. Originally we set out to create a team to compete in RoboCup. This was an ambitious project, and we had hopes to finish within the next year. For this semester, we chose to scale down the RoboCup team towards a smaller research area to try our learning algorithm on. The scaled down version of the RoboCup soccer environment is known as the \u201dKeepaway Testbed\u201d and was started by Peter Stone, University of Texas [2]. Here the task is simple, you have two teams on the field each with the same number of players. Instead of trying to score a goal on the opponent the teams are given tasks, and one team is labeled the keepers and the other is labeled the takers. It is the task of the keepers to maintain possesion of the ball and it is the task of the takers to take the ball. The longer the keepers are able to maintain possesion of the ball the better the team. There are several advantages to this environment. First, it provides some of the essential characteristics of a real soccer game. Typically it is believed that if a team is able to maintain possesion of the ball for long periods of time they will win the match. Secondly, it provides realistic behavior much the same as the original RoboCup server. This is accomplished by introducing noise into the system similar to the original RoboCup, and similar to what would be received by real robots. Finally, when you want to go through the learning process this environment is capable of stopping play once the takers have touched the ball, and the environment is capable of starting a new trial based on that occurrence. Although the RoboCup Keepaway Machine Learning testbed provided an excellent environment to train our agents, we still needed to scale down the problem in order to do a feasibility study. Based on the Keepaway testbed, we created a simulation world with one simple task. One agent is placed into the world and has to locate the position of the goal. This can be thought of as an agent in a soccer environment needing to locate either the ball or another teammate. It was in this environment where we tested our methods for learning autonomous agents."
            },
            "slug": "Designing-Neural-Networks-using-Genetic-Algorithms-Miller-Todd",
            "title": {
                "fragments": [],
                "text": "Designing Neural Networks using Genetic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This semester, the RoboCup Keepaway Machine Learning testbed provided an excellent environment to train the authors' agents, but still needed to scale down the problem in order to do a feasibility study."
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3296970"
                        ],
                        "name": "T. V. D. Zant",
                        "slug": "T.-V.-D.-Zant",
                        "structuredName": {
                            "firstName": "Tijn",
                            "lastName": "Zant",
                            "middleNames": [
                                "van",
                                "der"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. V. D. Zant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735405"
                        ],
                        "name": "V. Becanovic",
                        "slug": "V.-Becanovic",
                        "structuredName": {
                            "firstName": "Vlatko",
                            "lastName": "Becanovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Becanovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145418778"
                        ],
                        "name": "K. Ishii",
                        "slug": "K.-Ishii",
                        "structuredName": {
                            "firstName": "Kazuo",
                            "lastName": "Ishii",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ishii"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70054166"
                        ],
                        "name": "H. Kobialka",
                        "slug": "H.-Kobialka",
                        "structuredName": {
                            "firstName": "Hans-Ulrich",
                            "lastName": "Kobialka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kobialka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775016"
                        ],
                        "name": "P. Pl\u00f6ger",
                        "slug": "P.-Pl\u00f6ger",
                        "structuredName": {
                            "firstName": "Paul-Gerhard",
                            "lastName": "Pl\u00f6ger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Pl\u00f6ger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We show that Evolino-based LSTM can solve tasks thatEcho State nets [15] cannot, and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59832410,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ebe9413574d68fc866cd062cfcb33346eea2b302",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Finding-good-Echo-State-Networks-to-control-an-Zant-Becanovic",
            "title": {
                "fragments": [],
                "text": "Finding good Echo State Networks to control an underwater robot using evolutionary computations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We show that Evolino-based LSTM can solve tasks thatEcho State nets [15] cannot, and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17278462,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "aed054834e2c696807cc8b227ac7a4197196e211",
            "isKey": false,
            "numCitedBy": 1588,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "D3EGF(FIH)J KMLONPEGQSRPETN UCV.WYX(Z R.[ V R6\\M[ X N@]_^O\\`JaNcb V RcQ W d EGKeL(^(QgfhKeLOE?i)^(QSj ETNPfPQkRl[ V R)m\"[ X ^(KeLOEG^ npo qarpo m\"[ X ^(KeLOEG^tsAu EGNPb V ^ v wyx zlwO{(|(}<~O\u007fC}\u0081\u0080(\u0082(xp{a\u0083y\u0084.~A}\u0086\u0085\u0088\u0087_~ \u0089C\u008al\u00833\u0089#|<\u0080Az\u0086w#|l\u00806\u0087 \u008b(| \u008c JpfhL X\u008dV\u008f\u008e EG^O\u0090 QgJ \u0091 ETFOR\u0086\u0092\u0093] ^O\\\u0094J\u0095NPb V RcQ\u0097\u0096 X E)ETR \u00986EGKeLOETNcKMLOE\u009a\u0099 F\u0088\u009b ETN V RcQgJp^(^OE ZgZ E i ^(Qkj EGNPfhQSRO\u009b E \u009cOE2m1Jp^ RcNY\u009b E V\u0095Z sO\u009d\u009f\u009e! \u008d\u00a1 q.n sCD X KGKa\u00928\u009d\u00a2EG^ RPNhE\u00a4\u00a3 \u00a5\u00a6Q ZgZ E\u0095s m\u00a7J\u0095^ RPNO\u009b E V\u0095Z s( \u0308 X \u009b EG\u00a9#E\u0081Kas#\u009d V ^ V \u009c V s(H a \u009d\u00aba\u0095\u00ac3\u00ad \u00ae#|.\u0080Y \u0304y} xa\u00b0O\u007fC}l{\u008dx\u0093\u0087 \u0089 \u0083yxl\u0080Y~3{\u008d| \u0084 \u00b12\u0087Pz \u0084 \u009e V J Z J U N V fhKTJp^(Q \u0091 ETFOR\u0086\u0092 J\u0095\\ D vYf3RPEGb \u0301f V ^(\u009c\u00a7\u009d\u0088Jpb\u008fF X RPETN@D KTQ\u0097EG^(KTE i ^(QSjpEGNPfhQSR4v\u03bcJ\u0095\\ U\u00b6Z JaNPEG^(K\u00b7E jYQ V \u009c(Q \u0327D V ^ R V m V N3R V aOs#1 o \u00a1Ga r U Q\u0097NhE\u0081^OoTE1\u20444\u00bb,] R V\u0095Z vC1\u20442 3\u20444 \u0084 x \u00b1 x \u007f \u008b#\u00bf }\u00c0\u0087 \u00893\u0080t}l\u0082C}2\u0087P}<~ \u00act[ X NP\u0090\u0095E\u0081^\u00a7D KeL(b \u0301Qg\u009c(L X \u00a9yETN ] \u0091 DY]_\u00c1 \u009d\u0088J\u0095NPfhJ\u00c3\u00c2 Z j ETo\u0081Q V a\u0095 rpopo2\u00c4 X \u0090 V ^(J(sCD \u00c5)QSRPoTEGN ZgV ^(\u009c \u00c6 \u0089#|\u0095{3 \u0304\u008d|.\u0080(\u007fC}.\u008bC\u00bfY}p\u0084 \u0087Pz\u0086w"
            },
            "slug": "Gradient-Flow-in-Recurrent-Nets:-the-Difficulty-of-Hochreiter-Bengio",
            "title": {
                "fragments": [],
                "text": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404817"
                        ],
                        "name": "J. Holland",
                        "slug": "J.-Holland",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holland",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Holland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We show that Evolino-based LSTM can solve tasks thatEcho State nets [15] cannot, and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58781161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b4279db68b16e20fbc56f9d41980a950191d30a",
            "isKey": false,
            "numCitedBy": 38845,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Name of founding work in the area. Adaptation is key to survival and evolution. Evolution implicitly optimizes organisims. AI wants to mimic biological optimization { Survival of the ttest { Exploration and exploitation { Niche nding { Robust across changing environments (Mammals v. Dinos) { Self-regulation,-repair and-reproduction 2 Artiicial Inteligence Some deenitions { \"Making computers do what they do in the movies\" { \"Making computers do what humans (currently) do best\" { \"Giving computers common sense; letting them make simple deci-sions\" (do as I want, not what I say) { \"Anything too new to be pidgeonholed\" Adaptation and modiication is root of intelligence Some (Non-GA) branches of AI: { Expert Systems (Rule based deduction)"
            },
            "slug": "Adaptation-in-natural-and-artificial-systems-Holland",
            "title": {
                "fragments": [],
                "text": "Adaptation in natural and artificial systems"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Names of founding work in the area of Adaptation and modiication, which aims to mimic biological optimization, and some (Non-GA) branches of AI."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743344"
                        ],
                        "name": "H. Schwefel",
                        "slug": "H.-Schwefel",
                        "structuredName": {
                            "firstName": "Hans-Paul",
                            "lastName": "Schwefel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schwefel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60033733,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d54583482f6108a3b972d09ba17c815237f8379d",
            "isKey": false,
            "numCitedBy": 699,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nWith the publication of this book, Hans-Paul Schwefel has responded to rapidly growing interest in Evolutionary Computation, a field that originated, in part, with his pioneering work in the early 1970s. Evolution and Optimum Seeking offers a systematic overview of both new and classical approaches to computer-aided optimum system design methods, including the new class of Evolutionary Algorithms and other \"Parallel Problem Solving from Nature\" (PPSN) methods. It presents numerical optimization methods and algorithms to computer calculations which will be particularly useful for massively parallel computers. It is the only book in the field that offers in-depth comparisons between classical direct optimization methods and the newer methods. Dr. Schwefel's method consists essentially of the adaptation of simple evolutionary rules to a computer procedure in the search for optimal parameters within a simulation model of a technical device. In addition to its historical and practical value, Evolution and Optimum Seeking will stimulate further research into PPSN and interdisciplinary thinking about multi-agent self-organization in natural and artificial environments. These developments have been accelerated by fortunate changes in the computational environment, especially with respect to new architectures. MIMD (Multiple Instructions Multiple Data) machines with many processors working in parallel on one task seem to lend themselves to inherently parallel problem solving concepts like Evolution Strategies. The most comprehensive work of its kind, Evolution and Optimum Seeking offers a state-of-the-art perspective on the field for researchers in computer-aided design, planning, control, systems analysis, computational intelligence, and artificial life. Its range and depth make it a virtual handbook for practitioners: epistemological introduction to the concepts and strategies of optimum seeking; taxonomy of optimization tasks and solution principles (material found n"
            },
            "slug": "Evolution-and-Optimum-Seeking:-The-Sixth-Generation-Schwefel",
            "title": {
                "fragments": [],
                "text": "Evolution and Optimum Seeking: The Sixth Generation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The most comprehensive work of its kind, Evolution and Optimum Seeking offers a state-of-the-art perspective on the field for researchers in computer-aided design, planning, control, systems analysis, computational intelligence, and artificial life."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15291527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff2c2e3e83d1e8828695484728393c76ee07a101",
            "isKey": false,
            "numCitedBy": 15736,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The fundamental principles, basic mechanisms, and formal analyses involved in the development of parallel distributed processing (PDP) systems are presented in individual chapters contributed by leading experts. Topics examined include distributed representations, PDP models and general issues in cognitive science, feature discovery by competitive learning, the foundations of harmony theory, learning and relearning in Boltzmann machines, and learning internal representations by error propagation. Consideration is given to linear algebra in PDP, the logic of additive functions, resource requirements of standard and programmable nets, and the P3 parallel-network simulating system."
            },
            "slug": "Parallel-distributed-processing:-explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing: explorations in the microstructure of cognition, vol. 1: foundations"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2267481"
                        ],
                        "name": "O. Miglino",
                        "slug": "O.-Miglino",
                        "structuredName": {
                            "firstName": "Orazio",
                            "lastName": "Miglino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Miglino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35019669"
                        ],
                        "name": "H. Lund",
                        "slug": "H.-Lund",
                        "structuredName": {
                            "firstName": "Henrik",
                            "lastName": "Lund",
                            "middleNames": [
                                "Hautop"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Lund"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015062"
                        ],
                        "name": "S. Nolfi",
                        "slug": "S.-Nolfi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Nolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nolfi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 80
                            }
                        ],
                        "text": "more efficient and principled way is to search the space of RNN weight matrices [20,21,26,45,53,54] using evolutionary algorithms [1 3,31,41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1655099,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d9bfd304e29c7265c078e3188f76c9fb32d6996",
            "isKey": false,
            "numCitedBy": 210,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of the validity of simulation is particularly relevant for methodologies that use machine learning techniques to develop control systems for autonomous robots, as, for instance, the artificial life approach known as evolutionary robotics. In fact, although it has been demonstrated that training or evolving robots in real environments is possible, the number of trials needed to test the system discourages the use of physical robots during the training period. By evolving neural controllers for a Khepera robot in computer simulations and then transferring the agents obtained to the real environment we show that (a) an accurate model of a particular robot-environment dynamics can be built by sampling the real world through the sensors and the actuators of the robot; (b) the performance gap between the obtained behaviors in simulated and real environments may be significantly reduced by introducing a conservative form of noise; (c) if a decrease in performance is observed when the system is transferred to a real environment, successful and robust results can be obtained by continuing the evolutionary process in the real environment for a few generations."
            },
            "slug": "Evolving-Mobile-Robots-in-Simulated-and-Real-Miglino-Lund",
            "title": {
                "fragments": [],
                "text": "Evolving Mobile Robots in Simulated and Real Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "By evolving neural controllers for a Khepera robot in computer simulations and then transferring the agents obtained to the real environment, it is shown that an accurate model of a particular robot-environment dynamics can be built by sampling the real world through the sensors and the actuators of the robot."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Life"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391181439"
                        ],
                        "name": "K. Sims",
                        "slug": "K.-Sims",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Sims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 80
                            }
                        ],
                        "text": "more efficient and principled way is to search the space of RNN weight matrices [20,21,26,45,53,54] using evolutionary algorithms [1 3,31,41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 28435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "10c9569eff157f91365ee312df4fd693f7e3e23b",
            "isKey": false,
            "numCitedBy": 1176,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a novel system for creating virtual creatures that move and behave in simulated three-dimensional physical worlds. The morphologies of creatures and the neural systems for controlling their muscle forces are both generated automatically using genetic algorithms. Different fitness evaluation functions are used to direct simulated evolutions towards specific behaviors such as swimming, walking, jumping, and following. A genetic language is presented that uses nodes and connections as its primitive elements to represent directed graphs, which are used to describe both the morphology and the neural circuitry of these creatures. This genetic language defines a hyperspace containing an indefinite number of possible creatures with behaviors, and when it is searched using optimization techniques, a variety of successful and interesting locomotion strategies emerge, some of which would be difficult to invent or built by design."
            },
            "slug": "Evolving-virtual-creatures-Sims",
            "title": {
                "fragments": [],
                "text": "Evolving virtual creatures"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A genetic language is presented that uses nodes and connections as its primitive elements to represent directed graphs, which are used to describe both the morphology and the neural circuitry of creatures that move and behave in simulated three-dimensional physical worlds."
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015062"
                        ],
                        "name": "S. Nolfi",
                        "slug": "S.-Nolfi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Nolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nolfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2267481"
                        ],
                        "name": "O. Miglino",
                        "slug": "O.-Miglino",
                        "structuredName": {
                            "firstName": "Orazio",
                            "lastName": "Miglino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Miglino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727799"
                        ],
                        "name": "F. Mondada",
                        "slug": "F.-Mondada",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Mondada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Mondada"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "more efficient and principled way is to search the space of RNN weight matrices [20,21,26,45,53,54] using evolutionary algorithms [13,31,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2830773,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80945dd0388824492d0024ef7d1e827aa67a1b7f",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "A methodology for evolving the control systems of autonomous robots has not yet been well established. In this paper we will show different examples of applications of evolutionary robotics to real robots by describing three different approaches to develop neural controllers for mobile robots. In all the experiments described real robots are involved and are indeed the ultimate means of evaluating the success and the results of the procedures employed. Each approach will be compared with the others and the relative advantages and drawbacks will be discussed. Last, but not least, we will try to tackle a few important issues related to the design of the hardware and of the evolutionary conditions in which the control system of the autonomous agent should evolve."
            },
            "slug": "How-to-Evolve-Autonomous-Robots:-Different-in-Nolfi-Floreano",
            "title": {
                "fragments": [],
                "text": "How to Evolve Autonomous Robots: Different Approaches in Evolutionary Robotics"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Different examples of applications of evolutionary robotics to real robots are shown by describing three different approaches to develop neural controllers for mobile robots by compared with each other."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701373"
                        ],
                        "name": "H. Shimodaira",
                        "slug": "H.-Shimodaira",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Shimodaira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shimodaira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2074295865"
                        ],
                        "name": "Ken-ichi Noma",
                        "slug": "Ken-ichi-Noma",
                        "structuredName": {
                            "firstName": "Ken-ichi",
                            "lastName": "Noma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ken-ichi Noma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102026477"
                        ],
                        "name": "M. Nakai",
                        "slug": "M.-Nakai",
                        "structuredName": {
                            "firstName": "Mitsuru",
                            "lastName": "Nakai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Nakai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734761"
                        ],
                        "name": "S. Sagayama",
                        "slug": "S.-Sagayama",
                        "structuredName": {
                            "firstName": "Shigeki",
                            "lastName": "Sagayama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sagayama"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "One alternative presented in [ 43 ] is to average kernel distance between elements of input sequences aligned to m points."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2156663,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7b17f038bf233bc33f429b8aa7f7b883ae1fd43",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A new class of Support Vector Machine (SVM) that is applicable to sequential-pattern recognition such as speech recognition is developed by incorporating an idea of non-linear time alignment into the kernel function. Since the time-alignment operation of sequential pattern is embedded in the new kernel function, standard SVM training and classification algorithms can be employed without further modifications. The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition. Preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs)."
            },
            "slug": "Dynamic-Time-Alignment-Kernel-in-Support-Vector-Shimodaira-Noma",
            "title": {
                "fragments": [],
                "text": "Dynamic Time-Alignment Kernel in Support Vector Machine"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The proposed SVM (DTAK-SVM) is evaluated in speaker-dependent speech recognition experiments of hand-segmented phoneme recognition and preliminary experimental results show comparable recognition performance with hidden Markov models (HMMs)."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055652029"
                        ],
                        "name": "J\u00fcrgen Schmidhuber",
                        "slug": "J\u00fcrgen-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J\u00fcrgen Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 28776107,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "676c922b522bb1099e04c4a984ec44d1c6bac403",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A system for the lateral guidance of agricultural machines, especially harvesting machines to be guided along a line determined by a crop row or a stand of crop along a previously cut swath, has a deflectible sensor for feeling the position of the crop along the guide line and means for automatically steering the vehicle to maintain the latter along a predetermined path established by this line. The improvement of the invention comprises a deflector fixed rearwardly of the sensor to prevent excessive displacement thereof by individual stalks which may be out-of-place and thereby preclude an excessive response of the automatic steering system because of occasional out-of-place stalks."
            },
            "slug": "Dynamische-neuronale-Netze-und-das-fundamentale-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Dynamische neuronale Netze und das fundamentale raumzeitliche Lernproblem"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The improvement of the invention comprises a deflector fixed rearwardly of the sensor to prevent excessive displacement by individual stalks which may be out-of-place and thereby preclude an excessive response of the automatic steering system because of occasional out- of-place stalks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50555375"
                        ],
                        "name": "J. Salomon",
                        "slug": "J.-Salomon",
                        "structuredName": {
                            "firstName": "Jesper",
                            "lastName": "Salomon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Salomon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783569"
                        ],
                        "name": "Simon King",
                        "slug": "Simon-King",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "King",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057788"
                        ],
                        "name": "M. Osborne",
                        "slug": "M.-Osborne",
                        "structuredName": {
                            "firstName": "Miles",
                            "lastName": "Osborne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Osborne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "A limited way of applying existing SVMs to time series prediction [24, 25] or classification [34] is to build a training set either by transforming the sequential input into some static domain (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11866743,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce24e40c3dad87aafe03947eff701c26fa7ea011",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe the use of SupportVector Machines for phonetic classification on the TIMIT corpus. Unlike previous work, in which entire phonemes are classified, our system operates in a framewise manner and is intended for use as the front-end of a hybrid system similar toABBOT. We therefore avoid the problems of classifying variable-length vectors. Our framelevel phone classification accuracy on the complete TIMIT test set is competitive with other results from the literature. In addition, we address the serious problem of scaling Support Vector Machines by using the Kernel Fisher Discriminant."
            },
            "slug": "Framewise-phone-classification-using-support-vector-Salomon-King",
            "title": {
                "fragments": [],
                "text": "Framewise phone classification using support vector machines"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The authors' framelevel phone classification accuracy on the complete TIMIT test set is competitive with other results from the literature, and the serious problem of scaling Support Vector Machines by using the Kernel Fisher Discriminant is addressed."
            },
            "venue": {
                "fragments": [],
                "text": "INTERSPEECH"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2797623"
                        ],
                        "name": "H. Siegelmann",
                        "slug": "H.-Siegelmann",
                        "structuredName": {
                            "firstName": "Hava",
                            "lastName": "Siegelmann",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Siegelmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790264"
                        ],
                        "name": "Eduardo Sontag",
                        "slug": "Eduardo-Sontag",
                        "structuredName": {
                            "firstName": "Eduardo",
                            "lastName": "Sontag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eduardo Sontag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "In particular, they can approximate any dynam ic l system with arbitrary precision [44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5909565,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7509b472cbe7b1fe71a8fccf60f34cc873d1ab63",
            "isKey": false,
            "numCitedBy": 365,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Turing-computability-with-neural-nets-Siegelmann-Sontag",
            "title": {
                "fragments": [],
                "text": "Turing computability with neural nets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "a general framework for training RNNs called EVOlution of recurrent systems with LINear Outputs (Evolino) [40,51]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 59823722,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "a56a3c3d45001a47fb94a968888e6fbd8d25e051",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "An oxygen absorbent comprising a metal powder and a metal halide coated thereon is disclosed."
            },
            "slug": "Evolino:-Hybrid-Neuroevolution-/-Optimal-Linear-for-Schmidhuber-Wierstra",
            "title": {
                "fragments": [],
                "text": "Evolino: Hybrid Neuroevolution / Optimal Linear Search for Sequence Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "An oxygen absorbent comprising a metal powder and a metal halide coated thereon is disclosed and its application in selective separation of cadmium dioxide and carbon dioxide is confirmed."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI 2005"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368678"
                        ],
                        "name": "Jozef Barun\u00edk",
                        "slug": "Jozef-Barun\u00edk",
                        "structuredName": {
                            "firstName": "Jozef",
                            "lastName": "Barun\u00edk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jozef Barun\u00edk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2230630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "12dc93da4f6209bf1d6b54f8a5fd4cb0d94db128",
            "isKey": false,
            "numCitedBy": 1373,
            "numCiting": 248,
            "paperAbstract": {
                "fragments": [],
                "text": "This thesis examines the computational complexity of transmission network expansion planning which is defined as follows. Given a transmission grid of generators and consumers (nodes) connected by transmission lines (edges) with certain flow capacities, where should new lines be added at minimum cost such that all given capacity constraints are fulfilled? In its original mixed-integer non-linear programming formulation this is NP-hard since it contains the subproblem Steiner trees, the minimum cost connection of an initially unconnected set of both mandatory and optional nodes. By using electrical network theory is is shown why NP-hardness still holds when this subproblem is omitted by considering (highly) connected networks only and focussing on the satisfaction of the flow constraints. This refers to the more realistic case of extending a long working transmission grid for increased future demand. It will be achieved by showing that this case is computationally equivalent to 3-SAT, the classic satisfiability of Boolean formulas. Additionally, is is evaluated how much effort in computation and implementation is really necessary to solve realistic scenarios in practice. The original mathematical formulation is evaluated by using an appropriate state-of-the-art mixed-integer non-linear programming solver that can guarantee to find a global solution."
            },
            "slug": "Diploma-thesis-Barun\u00edk",
            "title": {
                "fragments": [],
                "text": "Diploma thesis"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown why NP-hardness still holds when this subproblem Steiner trees is omitted by considering (highly) connected networks only and focussing on the satisfaction of the flow constraints, and how much effort in computation and implementation is really necessary to solve realistic scenarios in practice."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48107116"
                        ],
                        "name": "M. Mackey",
                        "slug": "M.-Mackey",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mackey",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mackey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145548576"
                        ],
                        "name": "L. Glass",
                        "slug": "L.-Glass",
                        "structuredName": {
                            "firstName": "Leon",
                            "lastName": "Glass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Glass"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "3 Mackey-Glass Time-Series Prediction The Mackey-Glass system (MGS; [18]) is a standard benchmark for chaotic time series prediction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42039623,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "e39c17da0a3a0e7f709ef3e785c912df5cf386df",
            "isKey": false,
            "numCitedBy": 3649,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "First-order nonlinear differential-delay equations describing physiological control systems are studied. The equations display a broad diversity of dynamical behavior including limit cycle oscillations, with a variety of wave forms, and apparently aperiodic or \"chaotic\" solutions. These results are discussed in relation to dynamical respiratory and hematopoietic diseases."
            },
            "slug": "Oscillation-and-chaos-in-physiological-control-Mackey-Glass",
            "title": {
                "fragments": [],
                "text": "Oscillation and chaos in physiological control systems."
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "First-order nonlinear differential-delay equations describing physiological control systems displaying a broad diversity of dynamical behavior including limit cycle oscillations, with a variety of wave forms, and apparently aperiodic or \"chaotic\" solutions are studied."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1977
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743344"
                        ],
                        "name": "H. Schwefel",
                        "slug": "H.-Schwefel",
                        "structuredName": {
                            "firstName": "Hans-Paul",
                            "lastName": "Schwefel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schwefel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "However, for the present Evolino variant, where fine local search is desirable, ESP uses Cauchy-distributed mutation to produce all new individuals, making the approach in effect an Evolution Strategy [42]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43418053,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9006a35af873bcbb181dff7041309456b2999889",
            "isKey": false,
            "numCitedBy": 2468,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Problems and Methods of Optimization Hill Climbing Strategies Random Strategies Evolution Strategies for Numerical Optimization Comparison of Direct Search Strategies for Parameter Optimization."
            },
            "slug": "Evolution-and-optimum-seeking-Schwefel",
            "title": {
                "fragments": [],
                "text": "Evolution and optimum seeking"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Problems and Methods of Optimization Hill Climbing Strategies Random Strategies Evolution Strategies for Numerical Optimization Comparison of Direct Search Strategies for Parameter Optimization."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth-generation computer technology series"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50560492"
                        ],
                        "name": "V. Vapnik",
                        "slug": "V.-Vapnik",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Vapnik",
                            "middleNames": [
                                "Naumovich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Vapnik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 70
                            }
                        ],
                        "text": "If we replace mean squared error with the maximum margin criterion of Support Vector Machines (SVMs) [48], the optimal linear output weights can be evaluated using e.g. quadratic programming, as in traditional SVMs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 106
                            }
                        ],
                        "text": "If we instead use quadratic programmingto maximize the margin, we obtain the first evolutionary recurrent Support Vector Machines."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 154
                            }
                        ],
                        "text": "Its algorithms for shaping not only the linear but also the nonlinear parts allow LSTM to learn to solve tasks unlearnable by standard feed-forward nets, Support Vector Machines, Hidden Markov Models, and previous RNNs. Previous work on LSTM has focused on gradient-based G-LSTM [1\u20133, 8, 11, 29,38]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "If we replace mean squared error with the maximum margin criterion of Support V ector Machines (SVMs) [48], the optimal linear output weights can be evalua ted using e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 220
                            }
                        ],
                        "text": "Evolino evolves wei ghts to the nonlinear, hidden nodes while computing optimal linear mappings from h idden state to output, using methods such as pseudo-inverse-based linear reg ression [28] or Support Vector Machines [48], depending on the notion of optimality employed."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "In the case of maximum margin classification problems [48] we may compute W by quadratic programming."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 193
                            }
                        ],
                        "text": "Evolino evolves weights to the nonlinear, hidden nodes while computing optimal linear mappings from hidden state to output, using methods such as pseudo-inverse-based linear regression [28] or Support Vector Machines [48], depending on the notion of optimalityemployed."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Support Vector Machines are powerful regressors and classifier that make predictions based on a linear combination of kernel basis functions."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7138354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8213dbed4db44e113af3ed17d6dad57471a0c048",
            "isKey": true,
            "numCitedBy": 38912,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Setting of the learning problem consistency of learning processes bounds on the rate of convergence of learning processes controlling the generalization ability of learning processes constructing learning algorithms what is important in learning theory?."
            },
            "slug": "The-Nature-of-Statistical-Learning-Theory-Vapnik",
            "title": {
                "fragments": [],
                "text": "The Nature of Statistical Learning Theory"
            },
            "venue": {
                "fragments": [],
                "text": "Statistics for Engineering and Information Science"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583597"
                        ],
                        "name": "J. Kolen",
                        "slug": "J.-Kolen",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kolen",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kolen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2492622"
                        ],
                        "name": "S. C. Kremer",
                        "slug": "S.-C.-Kremer",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kremer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. C. Kremer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58278442,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "dc0c915827068345a47e0f36582faba701470f30",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction Random Guessing (RG) Experiments Final Remarks Conclusion Acknowledgments ]]>"
            },
            "slug": "Evaluating-Benchmark-Problems-by-Random-Guessing-Kolen-Kremer",
            "title": {
                "fragments": [],
                "text": "Evaluating Benchmark Problems by Random Guessing"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This chapter contains sections titled: Introduction Random Guessing (RG) Experiments Final Remarks Conclusion Acknowledgments."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47378595"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 142281124,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fd68c2e9e69822f2f4b12acaab6f9269a1a61d74",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "FORGETTING AND REMEMBERINGWhen remembering runs amok, past pain can disrupt someone's present. New drugs, psychotherapeutic approaches, and other strategies might temper traumatic memories."
            },
            "slug": "Learning-to-Forget-Miller",
            "title": {
                "fragments": [],
                "text": "Learning to Forget"
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 311,
                                "start": 172
                            }
                        ],
                        "text": "We will see that it also outperforms recent state-of-the-art RNNs on certain tasks, including echo state networks (ESNs) (Jaeger, 2004a) and previous gradient descent RNNs (Hochreiter & Schmidhuber, 1997a; Pearlmutter, 1995; Robinson & Fallside, 1987; Rumelhart & McClelland, 1986; Werbos, 1974; Williams, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 313,
                                "start": 205
                            }
                        ],
                        "text": "The first two were chosen to highlight Evolino\u2019s ability to perform well in both discrete and continuous domains, and to solve tasks that neither ESNs (Jaeger, 2004a) nor traditional gradient-descent RNNs (Pearlmutter, 1995; Robinson, & Fallside, 1987; Rumelhart, & McClelland, 1986; Werbos, 1974; Williams, 1989) can solve well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 205
                            }
                        ],
                        "text": "The drawback of ESNs is that the only truly computationally powerful, nonlinear part of the net does not learn, whereas previous supervised, gradient-based learning algorithms for sequence-processing RNNs (Pearlmutter, 1995; Robinson & Fallside, 1987; Schmidhuber, 1992; Werbos, 1988; Williams, 1989) adjust all weights of the net, not just the output weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 26
                            }
                        ],
                        "text": "Recurrent neural networks (RNNs; Pearlmutter, 1995; Robinson & Fallside, 1987; Rumelhart & McClelland, 1986; Werbos, 1974; Williams, 1989) are mathematical abstractions of biological nervous systems that can perform complex mappings from input sequences to output sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity of exact gradient computation algorithms for recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "(Tech. Rep. NU-CCS-89-27)"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88108619"
                        ],
                        "name": "W. Vent",
                        "slug": "W.-Vent",
                        "structuredName": {
                            "firstName": "Walter",
                            "lastName": "Vent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Vent"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 85086435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0acf50ce5c4e1268742f31e98ed294b8c967b829",
            "isKey": false,
            "numCitedBy": 1327,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Rechenberg,-Ingo,-Evolutionsstrategie-\u2014-Optimierung-Vent",
            "title": {
                "fragments": [],
                "text": "Rechenberg, Ingo, Evolutionsstrategie \u2014 Optimierung technischer Systeme nach Prinzipien der biologischen Evolution. 170 S. mit 36 Abb. Frommann\u2010Holzboog\u2010Verlag. Stuttgart 1973. Broschiert"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743344"
                        ],
                        "name": "H. Schwefel",
                        "slug": "H.-Schwefel",
                        "structuredName": {
                            "firstName": "Hans-Paul",
                            "lastName": "Schwefel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schwefel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62073400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "228046654120a58eda382b35da3daeac988fd2dd",
            "isKey": false,
            "numCitedBy": 1420,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Numerical-optimization-of-computer-models-Schwefel",
            "title": {
                "fragments": [],
                "text": "Numerical optimization of computer models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2319833"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We will see that it also outperforms recent state-of-the-art RNNs on certain tasks, including echo state networks (ESNs) (Jaeger, 2004a) and previous gradient descent RNNs (Hochreiter & Schmidhuber, 1997a; Pearlmutter, 1995; Robinson & Fallside, 1987; Rumelhart & McClelland, 1986; Werbos, 1974; Williams, 1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "The first two were chosen to highlight Evolino\u2019s ability to perform well in both discrete and continuous domains, and to solve tasks that neither ESNs (Jaeger, 2004a) nor traditional gradient-descent RNNs (Pearlmutter, 1995; Robinson, & Fallside, 1987; Rumelhart, & McClelland, 1986; Werbos, 1974; Williams, 1989) can solve well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Recurrent neural networks (RNNs; Pearlmutter, 1995; Robinson & Fallside, 1987; Rumelhart & McClelland, 1986; Werbos, 1974; Williams, 1989) are mathematical abstractions of biological nervous systems that can perform complex mappings from input sequences to output sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 207975157,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "56623a496727d5c71491850e04512ddf4152b487",
            "isKey": false,
            "numCitedBy": 4481,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Beyond-Regression-:-\"New-Tools-for-Prediction-and-Werbos",
            "title": {
                "fragments": [],
                "text": "Beyond Regression : \"New Tools for Prediction and Analysis in the Behavioral Sciences"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69040387"
                        ],
                        "name": "I. Rechenberg",
                        "slug": "I.-Rechenberg",
                        "structuredName": {
                            "firstName": "Ingo",
                            "lastName": "Rechenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Rechenberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "more efficient and principled way is to search the space of RNN weight matrices [20,21,26,45,53,54] using evolutionary algorithms [13,31,41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60975248,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d04942a086f9cafbb1c6453b64ba188beeb03823",
            "isKey": false,
            "numCitedBy": 3173,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Evolutionsstrategie-:-Optimierung-technischer-nach-Rechenberg",
            "title": {
                "fragments": [],
                "text": "Evolutionsstrategie : Optimierung technischer Systeme nach Prinzipien der biologischen Evolution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1973
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066667188"
                        ],
                        "name": "Sepp Hochreiter",
                        "slug": "Sepp-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sepp Hochreiter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Unfortunately, early RNN architectures could not learn to look far back into the past because they made gradients either vanish or blow up exponentially with the size of the time lag [9, 10]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60091947,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f3d13e95c25a8f6a753e38dfce88885097cbd43",
            "isKey": false,
            "numCitedBy": 609,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Untersuchungen-zu-dynamischen-neuronalen-Netzen-Hochreiter",
            "title": {
                "fragments": [],
                "text": "Untersuchungen zu dynamischen neuronalen Netzen"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143673845"
                        ],
                        "name": "P. Slusallek",
                        "slug": "P.-Slusallek",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Slusallek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Slusallek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144033462"
                        ],
                        "name": "P. Shirley",
                        "slug": "P.-Shirley",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Shirley",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Shirley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1913999"
                        ],
                        "name": "W. Mark",
                        "slug": "W.-Mark",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Mark",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Mark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34574034"
                        ],
                        "name": "G. Stoll",
                        "slug": "G.-Stoll",
                        "structuredName": {
                            "firstName": "Gordon",
                            "lastName": "Stoll",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Stoll"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145357585"
                        ],
                        "name": "I. Wald",
                        "slug": "I.-Wald",
                        "structuredName": {
                            "firstName": "Ingo",
                            "lastName": "Wald",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Wald"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "In recent years, gradient-based LSTM recurrent neural networks (RNNs) solved many previously RNN-unlearnable tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 36180426,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "cf7d7684600d3ebe916ca093eda123a9dad41459",
            "isKey": false,
            "numCitedBy": 3130,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Parallel-&-distributed-processing-Slusallek-Shirley",
            "title": {
                "fragments": [],
                "text": "Parallel & distributed processing"
            },
            "venue": {
                "fragments": [],
                "text": "SIGGRAPH Courses"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 196
                            }
                        ],
                        "text": "The first two were cho sen to highlight Evolino\u2019s ability to perform well in both discrete and continuous domains, and to solve tasks that neither ESNs [15] nor traditional gra dient descent RNNs [27, 32, 33, 49, 52] can solve well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 33
                            }
                        ],
                        "text": "Recurrent Neural Networks (RNNS; [27,32,33,49,52]) are ma thematical abstractions of biological nervous systems that can perform comple x mappings from input sequences to output sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 217504299,
            "fieldsOfStudy": [],
            "id": "5ef87a84be5e94abfda2dc0c6480995f49b002b3",
            "isKey": false,
            "numCitedBy": 1693,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Editors",
            "title": {
                "fragments": [],
                "text": "Editors"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Robust nonlinear control through neuroevolution. Unpublished doctoral dissertation"
            },
            "venue": {
                "fragments": [],
                "text": "Robust nonlinear control through neuroevolution. Unpublished doctoral dissertation"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "This generalizes methods such as those of Maillard [19] and Ishii et a l."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 229
                            }
                        ],
                        "text": "The linear regression method used to compute the output weig hts (W in equation 2) is the Moore-Penrose pseudo-inverse method, which i s both fast and optimal in the sense that it minimizes the summed squared error [2 8]\u2014compare [19] for an application to feedforward RBF nets and [14] for an app lication to Echo State Networks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 42
                            }
                        ],
                        "text": "This generalizes methods such as those of Maillard [19] and Ishii et al. [14,47] that evolve radial basis functions and ESNs, respectively."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "RBF neural network"
            },
            "venue": {
                "fragments": [],
                "text": "basis functions and genetic algorithms. InIEEE International Conference on Neural Networks  , pages 2187\u20132190, Piscataway, NJ"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 80
                            }
                        ],
                        "text": "more efficient and principled way is to search the space of RNN weight matrices [20,21,26,45,53,54] using evolutionary algorithms [1 3,31,41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How to e  volve autonomous robots: Different approaches in evolutionary rob otics"
            },
            "venue": {
                "fragments": [],
                "text": "R. A. Brooks and P. Maes, editors,  Fourth International Workshop on the Synthesis and Simulation of Living Systems (Artificial Life IV)  , pages 190\u2013197. MIT"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 68
                            }
                        ],
                        "text": "3 Mackey-Glass Time-Series Prediction The Mackey-Glass system (MGS; [18]) is a standard benchmark for chaotic time series prediction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Oscillation and chaos in physi ological control systems.Science"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic timealignment kernel in support vector machine Advances in neural information processing systems"
            },
            "venue": {
                "fragments": [],
                "text": "Dynamic timealignment kernel in support vector machine Advances in neural information processing systems"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic timealignment kernel in support vector machine"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in neural information processing systems,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The echo state approach to recurrent neural networks. Available online at http://www.faculty.iu-bremen.de/hjaeger/courses/SeminarSpring04/ ESNStandardSlides.pdf"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 300,
                                "start": 205
                            }
                        ],
                        "text": "The drawback of ESNs is that the only truly computationally powerful, nonlinear part of the net does not learn, whereas previous supervised, gradient-based learning algorithms for sequence-processing RNNs (Pearlmutter, 1995; Robinson & Fallside, 1987; Schmidhuber, 1992; Werbos, 1988; Williams, 1989) adjust all weights of the net, not just the output weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fixed size storage"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The utility driven dynamic error propagation network (Tech"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We show that Evolino-based LSTM can solve tasks thatEcho State nets [15] cannot, and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeling non-linear dynamical systems with Evolino"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. GECCO 2005 GECCO best paper award in Learning Classifier Systems and Other Genetics-Based Machine Learning"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 196
                            }
                        ],
                        "text": "The first two were cho sen to highlight Evolino\u2019s ability to perform well in both discrete and continuous domains, and to solve tasks that neither ESNs [15] nor traditional gra dient descent RNNs [27, 32, 33, 49, 52] can solve well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 33
                            }
                        ],
                        "text": "Recurrent Neural Networks (RNNS; [27,32,33,49,52]) are ma thematical abstractions of biological nervous systems that can perform comple x mappings from input sequences to output sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 207
                            }
                        ],
                        "text": "The drawback of ESNs is that the only truly computationally p owerful, nonlinear part of the net does not learn, whereas previous super vised, gradient-based learning algorithms for sequence-processing RNNs [27, 32, 36, 50, 52] adjust all weights of the net, not just the output weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity of exact gradient computati on algorithms for recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report Technical Rep  ort NU-CCS-89- 27, Boston: Northeastern University, College of Computer S cience"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We show that Evolino-based LSTM can solve tasks thatEcho State nets [15] cannot, and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fresh look at real-time computation in generic recurrent neural circuits"
            },
            "venue": {
                "fragments": [],
                "text": "A fresh look at real-time computation in generic recurrent neural circuits"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 67
                            }
                        ],
                        "text": "A limited way of applying existing SVMs to t ime series prediction [24, 25] or classification [34] is to build a training set ei her by transforming the sequential input into some static domain (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear predic tion of chaotic time series using support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": "J. Principe, L. Gil es, N. Morgan, and E. Wilson, editors, IEEE Workshop on Neural Networks for Signal Processing VII  , page 511. IEEE Press"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 107
                            }
                        ],
                        "text": "a general framework for training RNNs called EVOlution of re current systems with LINear Outputs (Evolino) [40,51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Modeling n on-linear dynamical systems with Evolino"
            },
            "venue": {
                "fragments": [],
                "text": " Proc. GECCO 2005, Washington, D. C. "
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "A limited way of applying existing SVMs to t ime series prediction [24, 25] or classification [34] is to build a training set ei her by transforming the sequential input into some static domain (e."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Framewise phone cla  ssification using support vector machines"
            },
            "venue": {
                "fragments": [],
                "text": " Proceedings International Conference on Spoken Language Processing  , Denver"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Numerische Optimierung von Computer-M  odellen"
            },
            "venue": {
                "fragments": [],
                "text": "Dissertation"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity of exact gradient computation algorithms for recurrent neural networks (Tech. Rep. NU-CCS-89-27)"
            },
            "venue": {
                "fragments": [],
                "text": "Complexity of exact gradient computation algorithms for recurrent neural networks (Tech. Rep. NU-CCS-89-27)"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 69
                            }
                        ],
                        "text": "Recently,Echo State Networks(ESNs; [15]) and a very similar approach,Liquid State Machines[17], have attracted significant attention."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Recently,Echo State Networks (ESNs; [15]) and a very similar approach, Liquid State Machines [17], have attracted significant attention."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fresh look a t re l-time computation in generic recurrent neural circuits"
            },
            "venue": {
                "fragments": [],
                "text": "Technical r  eport, Institute for Theoretical Computer Science, TU Graz"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "A recent RNN called Long Short-Term Memory (LSTM; [11]), how ever, overcomes this fundamental problem through a specialized archi tecture that does not impose any unrealistic bias towards recent events by mainta in g constant error flow back through time."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Long short-term memo  ry"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation, 9(8):1735\u20131780"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "On the other hand, for large data sets suc h as those used in speech recognition we typically need much larger LSTM net works with on the order of 100,000 weights [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Framewise phoneme classifi cation with bidirectional LSTM and other neural network architectures"
            },
            "venue": {
                "fragments": [],
                "text": " Neural Networks  , 18:602\u2013610"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A fresh look at real-time computation in generic recurrent neural circuits (Tech. Rep.) Graz: Institute for Theoretical Computer Science, TU Graz"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 163
                            }
                        ],
                        "text": "On the other hand, for large data sets suc h as those used in speech recognition we typically need much larger LSTM net works with on the order of 100,000 weights [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Framewise phoneme classifi cation with bidirectional LSTM and other neural network architectures"
            },
            "venue": {
                "fragments": [],
                "text": " Neural Networks  , 18:602\u2013610"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We show that Evolino-based LSTM can solve tasks thatEcho State nets [15] cannot, and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "RBF neural network, basis functions and genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We show that Evolino-based LSTM can solve tasks thatEcho State nets [15] cannot, and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "In recent years, gradient-based LSTM recurrent neural networks (RNNs) solved many previously RNN-unlearnable tasks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The utility driven dynamic error propagation network"
            },
            "venue": {
                "fragments": [],
                "text": "The utility driven dynamic error propagation network"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 196
                            }
                        ],
                        "text": "The first two were cho sen to highlight Evolino\u2019s ability to perform well in both discrete and continuous domains, and to solve tasks that neither ESNs [15] nor traditional gra dient descent RNNs [27, 32, 33, 49, 52] can solve well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 33
                            }
                        ],
                        "text": "Recurrent Neural Networks (RNNS; [27,32,33,49,52]) are ma thematical abstractions of biological nervous systems that can perform comple x mappings from input sequences to output sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 207
                            }
                        ],
                        "text": "The drawback of ESNs is that the only truly computationally p owerful, nonlinear part of the net does not learn, whereas previous super vised, gradient-based learning algorithms for sequence-processing RNNs [27, 32, 36, 50, 52] adjust all weights of the net, not just the output weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The utility driven dynam  ic error propagation network"
            },
            "venue": {
                "fragments": [],
                "text": "Technical Report CUED/F-INFENG/TR.1, Cambridge University Engineering Department"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel distributed processing (vol"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 80
                            }
                        ],
                        "text": "more efficient and principled way is to search the space of RNN weight matrices [20,21,26,45,53,54] using evolutionary algorithms [1 3,31,41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Designing neural netwo  rks using genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "InProceedings of the 3rd International Conference on Genetic Algorithms, pages 379\u2013384. Morgan Kauffman"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Complexity of exact gradient computation algorithms for recurrent neural networks ( Tech"
            },
            "venue": {
                "fragments": [],
                "text": "Rep . NUCCS8927 )"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "One alternative presented in [43] is to average kern el distance between elements of input sequences aligned to m points."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dy namic timealignment kernel in support vector machine"
            },
            "venue": {
                "fragments": [],
                "text": "T. G. Dietter  ich, S. Becker, and Z. Ghahramani, editors,  Advances in Neural Information Processing Systems 14  , Cambridge, MA"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We show that Evolino-based LSTM can solve tasks thatEcho State nets [15] cannot, and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Numerische Optimierung von Computer-Modellen. Dissertation"
            },
            "venue": {
                "fragments": [],
                "text": "Numerische Optimierung von Computer-Modellen. Dissertation"
            },
            "year": 1974
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 59
                            }
                        ],
                        "text": "In a more sophisticated approach by Suykens and Vandewalle [46], a wi ndow ofm previous output values is fed back as input to a recurrent model with a fi xed kernel."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 36
                            }
                        ],
                        "text": "In a more sophisticated approach by Suykens and Vandewalle [46], a window ofm previous output values is fed back as input to a recurrent model with a fixed kernel."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recurrent least squares s upport vector machines.IEEE Transactions on Circuits and Systems-I"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 181
                            }
                        ],
                        "text": "However,for the present Evolino variant, where fine local search is desirable, ESP uses Cauchy-distributed mutation to produce all new individuals, making the approach in effect an Evolution Strategy [42]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 203
                            }
                        ],
                        "text": "However, for the present Evolino variant, where fine local search is desirable, ESP us es Cauchy-distributed mutation to produce all new individuals, making the approac h in effect an Evolution Strategy [42]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Schwefel.Evolution and Optimum Seeking"
            },
            "venue": {
                "fragments": [],
                "text": "Wiley Interscience,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 196
                            }
                        ],
                        "text": "The first two were cho sen to highlight Evolino\u2019s ability to perform well in both discrete and continuous domains, and to solve tasks that neither ESNs [15] nor traditional gra dient descent RNNs [27, 32, 33, 49, 52] can solve well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 33
                            }
                        ],
                        "text": "Recurrent Neural Networks (RNNS; [27,32,33,49,52]) are ma thematical abstractions of biological nervous systems that can perform comple x mappings from input sequences to output sequences."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 207
                            }
                        ],
                        "text": "The drawback of ESNs is that the only truly computationally p owerful, nonlinear part of the net does not learn, whereas previous super vised, gradient-based learning algorithms for sequence-processing RNNs [27, 32, 36, 50, 52] adjust all weights of the net, not just the output weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gradient calculations for dynamic r  e urrent neural networks: A survey.IEEE Transactions on Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 383,
                                "start": 334
                            }
                        ],
                        "text": "One popular method that uses the advantage of random weight guessing in a more efficient and principled way is to search the space of RNN weight matrices (Miglino, Lund, & Nolfi, 1995; Miller, Todd, & Hedge, 1989; Nolfi, Floreano, Miglino, & Mondada, 1994; Sims, 1994; Yamauchi & Beer, 1994; Yao, 1993), using evolutionary algorithms (Holland, 1975; Rechenberg, 1973; Schwefel, 1977)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Numerische Optimierung von Computer-Modellen. Basel: Birkh\u00e4user"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1977
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The echo state approach to recurrent neural networks Available online at http://www.faculty.iu-bremen"
            },
            "venue": {
                "fragments": [],
                "text": "The echo state approach to recurrent neural networks Available online at http://www.faculty.iu-bremen"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 89
                            }
                        ],
                        "text": "This approach can quickly learn t o solve difficult reinforcement learning control tasks [5,6,22], including ones that require use of deep memory [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 174
                            }
                        ],
                        "text": "In particular, recent progress has been made with cooperati v ly coevolving recurrent neurons, each with its own rather small, local searc h space of possible weight vectors [6, 23, 30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gomez.Robust Nonlinear Control through Neuroevolution"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Department of Computer Sciences,"
            },
            "year": 2003
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 49,
            "methodology": 23,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 95,
        "totalPages": 10
    },
    "page_url": "https://www.semanticscholar.org/paper/Training-Recurrent-Networks-by-Evolino-Schmidhuber-Wierstra/75479012461814fd176556a56b32c2392462aef5?sort=total-citations"
}