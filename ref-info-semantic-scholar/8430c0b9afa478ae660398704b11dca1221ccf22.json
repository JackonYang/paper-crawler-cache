{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686506"
                        ],
                        "name": "A. Atiya",
                        "slug": "A.-Atiya",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Atiya",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Atiya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1838904"
                        ],
                        "name": "A. Parlos",
                        "slug": "A.-Parlos",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Parlos",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Parlos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "Several basic algorithms are known and have been refined in various directions (overviews in [3], [15], [2])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18072366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6bdabcdcde21d4d71321935e2e0332e32eda5366",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "How to efficiently train recurrent networks remains a challenging and active research topic. Most of the proposed training approaches are based on computational ways to efficiently obtain the gradient of the error function, and can be generally grouped into five major groups. In this study we present a derivation that unifies these approaches. We demonstrate that the approaches are only five different ways of solving a particular matrix equation. The second goal of this paper is develop a new algorithm based on the insights gained from the novel formulation. The new algorithm, which is based on approximating the error gradient, has lower computational complexity in computing the weight update than the competing techniques for most typical problems. In addition, it reaches the error minimum in a much smaller number of iterations. A desirable characteristic of recurrent network training algorithms is to be able to update the weights in an on-line fashion. We have also developed an on-line version of the proposed algorithm, that is based on updating the error gradient approximation in a recursive manner."
            },
            "slug": "New-results-on-recurrent-network-training:-unifying-Atiya-Parlos",
            "title": {
                "fragments": [],
                "text": "New results on recurrent network training: unifying the algorithms and accelerating convergence"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An on-line version of the proposed algorithm, which is based on approximating the error gradient, has lower computational complexity in computing the weight update than the competing techniques for most typical problems and reaches the error minimum in a much smaller number of iterations."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks Learn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700974"
                        ],
                        "name": "Barak A. Pearlmutter",
                        "slug": "Barak-A.-Pearlmutter",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Pearlmutter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak A. Pearlmutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Several basic algorithms are known and have been refined in various directions (overviews in [3], [15], [2])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1400872,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32e97eef94beacace020e79322cef0e1e5a76ee0",
            "isKey": false,
            "numCitedBy": 607,
            "numCiting": 302,
            "paperAbstract": {
                "fragments": [],
                "text": "Surveys learning algorithms for recurrent neural networks with hidden units and puts the various techniques into a common framework. The authors discuss fixed point learning algorithms, namely recurrent backpropagation and deterministic Boltzmann machines, and nonfixed point algorithms, namely backpropagation through time, Elman's history cutoff, and Jordan's output feedback architecture. Forward propagation, an on-line technique that uses adjoint equations, and variations thereof, are also discussed. In many cases, the unified presentation leads to generalizations of various sorts. The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones continues with some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks. The author presents some simulations, and at the end, addresses issues of computational complexity and learning speed."
            },
            "slug": "Gradient-calculations-for-dynamic-recurrent-neural-Pearlmutter",
            "title": {
                "fragments": [],
                "text": "Gradient calculations for dynamic recurrent neural networks: a survey"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The author discusses advantages and disadvantages of temporally continuous neural networks in contrast to clocked ones and presents some \"tricks of the trade\" for training, using, and simulating continuous time and recurrent neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780235"
                        ],
                        "name": "Fred Cummins",
                        "slug": "Fred-Cummins",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Cummins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred Cummins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 59
                            }
                        ],
                        "text": "A conspicuous example is \u201cLong Short Term Memory\u201d networks [7], which use specialized linear memory units with learnable read and write gates to achieve very long memory spans."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11598600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
            "isKey": false,
            "numCitedBy": 3338,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive forget gate that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way."
            },
            "slug": "Learning-to-Forget:-Continual-Prediction-with-LSTM-Gers-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning to Forget: Continual Prediction with LSTM"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work identifies a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset, and proposes a novel, adaptive forget gate that enables an LSTm cell to learn to reset itself at appropriate times, thus releasing internal resources."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145247053"
                        ],
                        "name": "W. Maass",
                        "slug": "W.-Maass",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Maass",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Maass"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792142"
                        ],
                        "name": "T. Natschl\u00e4ger",
                        "slug": "T.-Natschl\u00e4ger",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Natschl\u00e4ger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Natschl\u00e4ger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754307"
                        ],
                        "name": "H. Markram",
                        "slug": "H.-Markram",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Markram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Markram"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Stichwo\u0308rter: rekurrente neuronale Netze, u\u0308berwachtes Lernen"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Fig."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1045112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0535dedb8607d83cd2614317c99913378e89e26",
            "isKey": false,
            "numCitedBy": 2875,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "A key challenge for neural modeling is to explain how a continuous stream of multimodal input from a rapidly changing environment can be processed by stereotypical recurrent circuits of integrate-and-fire neurons in real time. We propose a new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks. It does not require a task-dependent construction of neural circuits. Instead, it is based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry. It is shown that the inherent transient dynamics of the high-dimensional dynamical system formed by a sufficiently large and heterogeneous neural circuit may serve as universal analog fading memory. Readout neurons can learn to extract in real time from the current state of such recurrent neural circuit information about current and past inputs that may be needed for diverse tasks. Stable internal states are not required for giving a stable output, since transient internal states can be transformed by readout neurons into stable target outputs due to the high dimensionality of the dynamical system. Our approach is based on a rigorous computational model, the liquid state machine, that, unlike Turing machines, does not require sequential transitions between well-defined discrete internal states. It is supported, as the Turing machine is, by rigorous mathematical results that predict universal computational power under idealized conditions, but for the biologically more realistic scenario of real-time processing of time-varying inputs. Our approach provides new perspectives for the interpretation of neural coding, the design of experiments and data analysis in neurophysiology, and the solution of problems in robotics and neurotechnology."
            },
            "slug": "Real-Time-Computing-Without-Stable-States:-A-New-on-Maass-Natschl\u00e4ger",
            "title": {
                "fragments": [],
                "text": "Real-Time Computing Without Stable States: A New Framework for Neural Computation Based on Perturbations"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new computational model for real-time computing on time-varying input that provides an alternative to paradigms based on Turing machines or attractor neural networks, based on principles of high-dimensional dynamical systems in combination with statistical learning theory and can be implemented on generic evolved or found recurrent circuitry."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145632693"
                        ],
                        "name": "M. Kimura",
                        "slug": "M.-Kimura",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kimura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1763517"
                        ],
                        "name": "R. Nakano",
                        "slug": "R.-Nakano",
                        "structuredName": {
                            "firstName": "Ryohei",
                            "lastName": "Nakano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Nakano"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The first is a primordeal form of temporal memory, the second, of static pattern memory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5183287,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "be373da84eb7dff8b2b265b8925a31628c81d694",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-dynamical-systems-by-recurrent-neural-from-Kimura-Nakano",
            "title": {
                "fragments": [],
                "text": "Learning dynamical systems by recurrent neural networks from orbits"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50216412"
                        ],
                        "name": "Douglas Eck",
                        "slug": "Douglas-Eck",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Eck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas Eck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 51
                            }
                        ],
                        "text": "Another good model, not contained in the survey of [6], was achieved in [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 106
                            }
                        ],
                        "text": "For comparison we consider first the work of [18], which contains the best results from a survey given in [6]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6378058,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28ce7a064544fc093724cb2fecd8dfaea6367349",
            "isKey": false,
            "numCitedBy": 342,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Long Short-Term Memory (LSTM) is able to solve many time series tasks unsolvable by feed-forward networks using fixed size time windows. Here we find that LSTM''s superiority does {\\em not} carry over to certain simpler time series prediction tasks solvable by time window approaches: the Mackey-Glass series and the Santa Fe FIR laser emission series (Set A). This suggests to use LSTM only when simpler traditional approaches fail."
            },
            "slug": "Applying-LSTM-to-Time-Series-Predictable-through-Gers-Eck",
            "title": {
                "fragments": [],
                "text": "Applying LSTM to Time Series Predictable through Time-Window Approaches"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is found that LSTM''s superiority does not carry over to certain simpler time series prediction tasks solvable by time window approaches: the Mackey-Glass series and the Santa Fe FIR laser emission series."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105353"
                        ],
                        "name": "K. Narendra",
                        "slug": "K.-Narendra",
                        "structuredName": {
                            "firstName": "Kumpati",
                            "lastName": "Narendra",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Narendra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2662634"
                        ],
                        "name": "S. Mukhopadhyay",
                        "slug": "S.-Mukhopadhyay",
                        "structuredName": {
                            "firstName": "Snehasis",
                            "lastName": "Mukhopadhyay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mukhopadhyay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The first is a primordeal form of temporal memory, the second, of static pattern memory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3301847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13ebb238d345b9932417e92c888ab88ae22419be",
            "isKey": false,
            "numCitedBy": 470,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The NARMA model is an exact representation of the input-output behavior of dynamical systems. However, it is not convenient for purposes of control. In this paper, the authors introduce two classes of models which are approximations to the NARMA model, and at the same time substantially simplifies the control problem."
            },
            "slug": "Adaptive-control-using-neural-networks-and-models-Narendra-Mukhopadhyay",
            "title": {
                "fragments": [],
                "text": "Adaptive control using neural networks and approximate models"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Two classes of models are introduced which are approximations to the NARMA model, and at the same time substantially simplifies the control problem."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 American Control Conference - ACC'95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8930772"
                        ],
                        "name": "J. Mcnames",
                        "slug": "J.-Mcnames",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Mcnames",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mcnames"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "In that approach, a local modelling technique was used (see [13] for an introduction to local modelling) in combination with a self-organizing feature map (SOM) which was used to generate prototype vectors for choosing the best local model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14729845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3236e93cfe32962984fddcf68f0ab89be971341f",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Local models have emerged as one of the leading methods of chaotic time series prediction. However, the accuracy of local models is sensitive to the choice of user-specified parameters, not unlike neural networks and other methods. This paper describes a method of optimizing these parameters so as to minimize the leave-one-out cross-validation error. This approach reduces the burden on the user to pick appropriate values and improves the prediction accuracy."
            },
            "slug": "Local-Modeling-Optimization-for-Time-Series-Mcnames",
            "title": {
                "fragments": [],
                "text": "Local Modeling Optimization for Time Series Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A method of optimizing parameters of local models so as to minimize the leave-one-out cross-validation error is described, which reduces the burden on the user to pick appropriate values and improves the prediction accuracy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293717"
                        ],
                        "name": "L. Abbott",
                        "slug": "L.-Abbott",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Abbott",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Abbott"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3049675"
                        ],
                        "name": "S. Nelson",
                        "slug": "S.-Nelson",
                        "structuredName": {
                            "firstName": "Sacha",
                            "lastName": "Nelson",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nelson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "While much is known about temporal learning phenomena at the synapse level (overview: [1]), not much is clear concerning the learning dynamics at the network level."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2048100,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "170d56c5934a71cb9ee989b26d280291a5873df7",
            "isKey": false,
            "numCitedBy": 1911,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Synaptic plasticity provides the basis for most models of learning, memory and development in neural circuits. To generate realistic results, synapse-specific Hebbian forms of plasticity, such as long-term potentiation and depression, must be augmented by global processes that regulate overall levels of neuronal and network activity. Regulatory processes are often as important as the more intensively studied Hebbian processes in determining the consequences of synaptic plasticity for network function. Recent experimental results suggest several novel mechanisms for regulating levels of activity in conjunction with Hebbian synaptic modification. We review three of them\u2014synaptic scaling, spike-timing dependent plasticity and synaptic redistribution\u2014and discuss their functional implications."
            },
            "slug": "Synaptic-plasticity:-taming-the-beast-Abbott-Nelson",
            "title": {
                "fragments": [],
                "text": "Synaptic plasticity: taming the beast"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work reviews three Hebbian forms of plasticity\u2014synaptic scaling, spike-timing dependent plasticity and synaptic redistribution\u2014and discusses their functional implications."
            },
            "venue": {
                "fragments": [],
                "text": "Nature Neuroscience"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8930772"
                        ],
                        "name": "J. Mcnames",
                        "slug": "J.-Mcnames",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Mcnames",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mcnames"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "Another good model, not contained in the survey of [6], was achieved in [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60008070,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab9c0ba4fbd96fb68587e3a643dfaf3e7cdf8393",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 137,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous studies have shown that local models are among the most accurate methods for predicting chaotic time series. This work discusses a number of improvements to local models that reduce computation, improve the model accuracy, or both. Local models are often criticized because they require much more computation than most global models to calculate the model outputs. Usually, most of this time is taken to find the nearest neighbors in the data set. This work introduces two new nearest neighbor algorithms that drastically reduce this time and enable local models to be evaluated very quickly. The two new algorithms are compared with fifteen other algorithms on a variety of benchmark problems. Local linear models are the most popular, and often the most accurate, type of local model. However, using an appropriate means of regularization to eliminate the effects of an ill-conditioned matrix inverse is crucial to producing accurate predictions. This work describes the two most popular types of regularization, ridge regression and principal components regression, and two new generalizations of each of these methods that enable more accurate models to be constructed. The accuracy of local models is sensitive to the values chosen for the model parameters. Most researchers pick the values for these parameters based on their experience and intuition. In this work, new optimization algorithms are introduced that improve the model accuracy by adjusting the initial parameter values provided by the user. These iterative algorithms take advantage of local models\u2019 ability to efficiently calculate the leave-one-out cross-validation error, an excellent measure of model accuracy that does not cause overfitting. When local models are used to predict chaotic time series, there are several improvements that can be made that use the properties of chaotic systems to generate more accurate"
            },
            "slug": "Innovations-in-local-modeling-for-time-series-Mcnames",
            "title": {
                "fragments": [],
                "text": "Innovations in local modeling for time series prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "New optimization algorithms are introduced that improve the model accuracy by adjusting the initial parameter values provided by the user in this work, which take advantage of local models\u2019 ability to efficiently calculate the leave-one-out cross-validation error."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398220613"
                        ],
                        "name": "B. Farhang-Boroujeny",
                        "slug": "B.-Farhang-Boroujeny",
                        "structuredName": {
                            "firstName": "Behrouz",
                            "lastName": "Farhang-Boroujeny",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Farhang-Boroujeny"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "After 1000 steps, we observe msetest \u2248 1.0 \u00d7 10 \u22125; in plots the two signals are still undistinguishable."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 58688997,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "e9500f1ddf1409ab84dd8d4bbc52c8f386f201aa",
            "isKey": true,
            "numCitedBy": 982,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nAdaptive filtering is an advanced and growing field in signal processing. A filter is a transmission network used in electronic circuits for the selective enhancement or reduction of specified components of an input signal. Filtering is achieved by selectively attenuating those components of the input signal which are undesired, relative to those which it is desired to enhance. This comprehensive book is both a valuable student resource and a useful technical reference for signal processing engineers in industry. The author is experienced in teaching graduates and practicing engineers and the text offers good theoretical coverage complemented by plenty of application examples."
            },
            "slug": "Adaptive-Filters:-Theory-and-Applications-Farhang-Boroujeny",
            "title": {
                "fragments": [],
                "text": "Adaptive Filters: Theory and Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This comprehensive book is both a valuable student resource and a useful technical reference for signal processing engineers in industry."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145170017"
                        ],
                        "name": "J. Stark",
                        "slug": "J.-Stark",
                        "structuredName": {
                            "firstName": "Jaroslav",
                            "lastName": "Stark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Stark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145768864"
                        ],
                        "name": "D. Broomhead",
                        "slug": "D.-Broomhead",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Broomhead",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Broomhead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145622261"
                        ],
                        "name": "M. Davies",
                        "slug": "M.-Davies",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Davies",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Davies"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2746773"
                        ],
                        "name": "J. Huke",
                        "slug": "J.-Huke",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Huke",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Huke"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "By Takens\u2019 theorem [16], very few (actually, 4) successive data points y(n), y(n + k), y(n + 2k), y(n + 3k) suffice to fully determine the attractor\u2019s state at time n + 3k."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 120171701,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "2af03632cbcfdbd076fe00f97292713ef19759c5",
            "isKey": false,
            "numCitedBy": 153,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Takens-embedding-theorems-for-forced-and-stochastic-Stark-Broomhead",
            "title": {
                "fragments": [],
                "text": "Takens embedding theorems for forced and stochastic systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "After 1000 steps, we observe msetest \u2248 1.0 \u00d7 10 \u22125; in plots the two signals are still undistinguishable."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recurrent neural networks: Supervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "The Handbook of Brain Theory and Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "For comparison we consider first the work of [18], which contains the best results from a survey given in [6]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 236440185,
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using the SOM and Local Models in Time-Series Prediction"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Original techreport received and put to printing Dec"
            },
            "venue": {
                "fragments": [],
                "text": "Original techreport received and put to printing Dec"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Nonlinear Systems (second edition)"
            },
            "venue": {
                "fragments": [],
                "text": "Nonlinear Systems (second edition)"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "the well-known Elman networks (overview in [10])."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Spatiotemporal connectionist neural networks: a taxonomy and review"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The first is a primordeal form of temporal memory, the second, of static pattern memory."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Enhanced multi-stream Kalman filter training for recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Nonlinear Modeling: Advanced Black-Box Techniques"
            },
            "year": 1998
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 10,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 18,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/The''echo-state''approach-to-analysing-and-training-Jaeger/8430c0b9afa478ae660398704b11dca1221ccf22?sort=total-citations"
}