{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684799"
                        ],
                        "name": "T. St\u00fctzle",
                        "slug": "T.-St\u00fctzle",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "St\u00fctzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. St\u00fctzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 210
                            }
                        ],
                        "text": "In particular, we automatically constructed different instantiations of the SPEAR algorithm and thereby substantially improved the state of the art for two sets of SAT-encoded industrial verification problems (Hutter et al., 2007a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 75
                            }
                        ],
                        "text": "113\n8 Applications II: Configuration of Various Target Algorithms based on PARAMILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 297,
                                "start": 275
                            }
                        ],
                        "text": "Indeed, we explored several optimization objectives in some of our published work: maximizing solution quality achieved in a given time, minimizing the runtime required to reach a given solution quality, and minimizing the runtime required to solve a single problem instance (Hutter et al., 2007b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 187
                            }
                        ],
                        "text": "Also thanks to Christian Bang, my office mate during my M.Sc. thesis, who helped me automate my experiments based on Ruby scripts; over time, these scripts grew into the first version of PARAMILS."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 60
                            }
                        ],
                        "text": "In particular, inspired by the mechanism used in FocusedILS (Hutter et al., 2007b), we maintain the invariant that we never choose an incumbent unless it is the parameter configuration with the most function evaluations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 108
                            }
                        ],
                        "text": "In joint work with Holger and Thomas during my PhD, we improved PARAMILS and first published it at AAAI-07 (Hutter et al., 2007b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 20
                            }
                        ],
                        "text": "An early version of PARAMILS goes back to an unpublished side project during my MSc. thesis (see Appendix A of Hutter, 2004), which was co-supervised by Thomas Stu\u0308tzle and Holger Hoos."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 47
                            }
                        ],
                        "text": "235\nxiii\nList of Algorithms and Procedures\n5.1 PARAMILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 11
                            }
                        ],
                        "text": "74 5.1 The PARAMILS framework . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 76
                            }
                        ],
                        "text": "4 Configuration of SAPS, GLS and SAT4J In our first publication on PARAMILS (Hutter et al., 2007b), we reported experiments on three target algorithms to demonstrate the effectiveness of the approach: SAPS, GLS+, and SAT4J."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 465,
                                "start": 457
                            }
                        ],
                        "text": "109 7.4 Speedup of FOCUSEDILS due to adaptive capping . . . . . . . . . . . . . . 111 7.5 Final evaluation of default vs configurations found with BASICILS and\nFOCUSEDILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 112\n8.1 Overview of our five CPLEX configuration scenarios . . . . . . . . . . . . . 116 8.2 Experimental results for our CPLEX configuration scenarios . . . . . . . . . 119 8.3 Parameters in the self-configuration of PARAMILS . . . . . . . . . . . . . . 122 8.4 Effect of self-configuration . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 100
                            }
                        ],
                        "text": "In fact, even the algorithm designers themselves often cannot solve this complex problem very well (Hutter et al., 2007a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 7
                            }
                        ],
                        "text": "76 5.2 PARAMILS . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 19
                            }
                        ],
                        "text": "The development of PARAMILS had a major impact on my PhD thesis."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 106
                            }
                        ],
                        "text": "An objective function While we optimized median performance in our first study on algorithm configuration (Hutter et al., 2007b), we have since found cases where optimizing median performance led to parameter configurations with good median but poor overall performance."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 68
                            }
                        ],
                        "text": "Adaptive capping is discussed in a comprehensive journal article on PARAMILS with Holger Hoos, Kevin Leyton-Brown, and Thomas Stu\u0308tzle, which also introduced the application to CPLEX and has been accepted for publication at JAIR (Hutter et al., 2009c)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 30
                            }
                        ],
                        "text": "116\n8.2 Self-Configuration of PARAMILS . . . . . . . . . . . . . . . . . . . . . . . 120 8.3 Applications of PARAMILS by Other Researchers . . . . . . . . . . . . . . . 124\n8.3.1 Configuration of SATenstein . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 65
                            }
                        ],
                        "text": "Many thanks to Chris Fawcett for taking over and maintaining the PARAMILS project and fixing some bugs to help external users."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 39
                            }
                        ],
                        "text": ", 2001) and overtuning in optimization (Birattari et al., 2002; Birattari, 2005; Hutter et al., 2007b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 74
                            }
                        ],
                        "text": "While we used medians in our first publication on algorithm configuration (Hutter et al., 2007b), we later found that the optimization of quantiles can result in parameter configurations with poor robustness."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 159
                            }
                        ],
                        "text": "\u2026optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al., 2003), protein folding (Thachuk et al., 2007), formal verification (Hutter et al., 2007a), and even in areas far outside of computer science, such as water resource management (Tolson and Shoemaker, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 105
                            }
                        ],
                        "text": "Chapter 6 is based on joint work with Domagoj Babic\u0301, Holger Hoos, and Alan Hu that appeared at FMCAD-07 (Hutter et al., 2007a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14512352,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4776b8295629c8e74eb7503b3d6364caa026bb6",
            "isKey": false,
            "numCitedBy": 311,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The determination of appropriate values for free algorithm parameters is a challenging and tedious task in the design of effective algorithms for hard problems. Such parameters include categorical choices (e.g., neighborhood structure in local search or variable/value ordering heuristics in tree search), as well as numerical parameters (e.g., noise or restart timing). In practice, tuning of these parameters is largely carried out manually by applying rules of thumb and crude heuristics, while more principled approaches are only rarely used. In this paper, we present a local search approach for algorithm configuration and prove its convergence to the globally optimal parameter configuration. Our approach is very versatile: it can, e.g., be used for minimising run-time in decision problems or for maximising solution quality in optimisation problems. It further applies to arbitrary algorithms, including heuristic tree search and local search algorithms, with no limitation on the number of parameters. Experiments in four algorithm configuration scenarios demonstrate that our automatically determined parameter settings always outperform the algorithm defaults, sometimes by several orders of magnitude. Our approach also shows better performance and greater flexibility than the recent CALIBRA system. Our ParamILS code, along with instructions on how to use it for tuning your own algorithms, is available on-line at http://www.cs.ubc.ca/labs/beta/Projects/ParamILS."
            },
            "slug": "Automatic-Algorithm-Configuration-Based-on-Local-Hutter-Hoos",
            "title": {
                "fragments": [],
                "text": "Automatic Algorithm Configuration Based on Local Search"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a local search approach for algorithm configuration and proves its convergence to the globally optimal parameter configuration, which can be used for minimising run-time in decision problems or for maximising solution quality in optimisation problems."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684799"
                        ],
                        "name": "T. St\u00fctzle",
                        "slug": "T.-St\u00fctzle",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "St\u00fctzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. St\u00fctzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 122
                            }
                        ],
                        "text": "Chapter 4 is based on joint work with Holger Hoos and Kevin Leyton-Brown, which is about to be submitted for publication (Hutter et al., 2009d)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 229
                            }
                        ],
                        "text": "Adaptive capping is discussed in a comprehensive journal article on PARAMILS with Holger Hoos, Kevin Leyton-Brown, and Thomas Stu\u0308tzle, which also introduced the application to CPLEX and has been accepted for publication at JAIR (Hutter et al., 2009c)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": "Chapters 9 and 10 are primarily based on a conference publication at GECCO-09 (Hutter et al., 2009e), co-authored by Holger Hoos, Kevin Leyton-Brown, and Kevin Murphy, as well as a book chapter with the same co-authors and Thomas Bartz-Beielstein (Hutter et al., 2009a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1034056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23105024da063b7ee62840e7c869721d12335e4a",
            "isKey": false,
            "numCitedBy": 923,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "The identification of performance-optimizing parameter settings is an important part of the development and application of algorithms. We describe an automatic framework for this algorithm configuration problem. More formally, we provide methods for optimizing a target algorithm's performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters. We review a family of local-search-based algorithm configuration procedures and present novel techniques for accelerating them by adaptively limiting the time spent for evaluating individual configurations. We describe the results of a comprehensive experimental evaluation of our methods, based on the configuration of prominent complete and incomplete algorithms for SAT. We also present what is, to our knowledge, the first published work on automatically configuring the CPLEX mixed integer programming solver. All the algorithms we considered had default parameter settings that were manually identified with considerable effort. Nevertheless, using our automated algorithm configuration procedures, we achieved substantial and consistent performance improvements."
            },
            "slug": "ParamILS:-An-Automatic-Algorithm-Configuration-Hutter-Hoos",
            "title": {
                "fragments": [],
                "text": "ParamILS: An Automatic Algorithm Configuration Framework"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An automatic framework for this algorithm configuration problem is described and methods for optimizing a target algorithm's performance on a given class of problem instances by varying a set of ordinal and/or categorical parameters are provided."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 214
                            }
                        ],
                        "text": "The research presented here preceded that development; it dates back to joint work with Holger Hoos, Kevin Leyton-Brown, and Thomas St\u00fctzle in summer 2007, which was presented and published at a doctoral symposium (Hutter, 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14960825,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5085979dbdeeeb2938fc94478d81a3cf4395fff9",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Design and implementation of efficient and robust algorithms are core topics of computer science and operations research, and the determination of appropriate values for free algorithm parameters is a challenging and tedious task in the design of effective algorithms for hard problems. Such parameters include categorical choices (e.g., neighborhood structure in local search or variable/value ordering heuristics in tree search), as well as numerical parameters (e.g., noise or restart timing). In practice, tuning of these parameters is largely carried out manually by applying rules of thumb and crude heuristics, while more principled approaches are only rarely used. In this paper, we study some tuning scenarios in more detail and demonstrate the large potential of even very simple automatic algorithm configuration approaches."
            },
            "slug": "On-the-Potential-of-Automatic-Algorithm-Hutter",
            "title": {
                "fragments": [],
                "text": "On the Potential of Automatic Algorithm Configuration"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper studies some tuning scenarios in more detail and demonstrates the large potential of even very simple automatic algorithm configuration approaches."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69372472"
                        ],
                        "name": "Prasanna Balaprakash",
                        "slug": "Prasanna-Balaprakash",
                        "structuredName": {
                            "firstName": "Prasanna",
                            "lastName": "Balaprakash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prasanna Balaprakash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690903"
                        ],
                        "name": "M. Birattari",
                        "slug": "M.-Birattari",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Birattari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Birattari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684799"
                        ],
                        "name": "T. St\u00fctzle",
                        "slug": "T.-St\u00fctzle",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "St\u00fctzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. St\u00fctzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 20
                            }
                        ],
                        "text": "2 Racing algorithms (Maron and Moore, 1994; Birattari et al., 2002; Birattari, 2005; Balaprakash et al., 2007) emphasize using as few problem instances as possible to reliably choose among a fixed set of parameter configurations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 45
                            }
                        ],
                        "text": ", 2002; Birattari, 2004) and iterated F-Race (Balaprakash et al., 2007)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9476027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6bb1a2a3285be50a6b703fea94878e08d1e0025",
            "isKey": false,
            "numCitedBy": 262,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Finding appropriate values for the parameters of an algorithm is a challenging, important, and time consuming task. While typically parameters are tuned by hand, recent studies have shown that automatic tuning procedures can effectively handle this task and often find better parameter settings. F-Race has been proposed specifically for this purpose and it has proven to be very effective in a number of cases. F-Race is a racing algorithm that starts by considering a number of candidate parameter settings and eliminates inferior ones as soon as enough statistical evidence arises against them. In this paper, we propose two modifications to the usual way of applying F-Race that on the one hand, make it suitable for tuning tasks with a very large number of initial candidate parameter settings and, on the other hand, allow a significant reduction of the number of function evaluations without any major loss in solution quality. We evaluate the proposed modifications on a number of stochastic local search algorithms and we show their effectiveness. \u00a9 Springer-Verlag Berlin Heidelberg 200"
            },
            "slug": "Improvement-Strategies-for-the-F-Race-Algorithm:-Balaprakash-Birattari",
            "title": {
                "fragments": [],
                "text": "Improvement Strategies for the F-Race Algorithm: Sampling Design and Iterative Refinement"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes two modifications to the usual way of applying F-Race that make it suitable for tuning tasks with a very large number of initial candidate parameter settings and allow a significant reduction of the number of function evaluations without any major loss in solution quality."
            },
            "venue": {
                "fragments": [],
                "text": "Hybrid Metaheuristics"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728524"
                        ],
                        "name": "Y. Hamadi",
                        "slug": "Y.-Hamadi",
                        "structuredName": {
                            "firstName": "Youssef",
                            "lastName": "Hamadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hamadi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 95
                            }
                        ],
                        "text": "My first publications on (per-instance) parameter optimization are jointly with Youssef Hamadi (Hutter and Hamadi, 2005), and with Youssef Hamadi, Holger Hoos, and Kevin Leyton-Brown (Hutter et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 45
                            }
                        ],
                        "text": "I would also like to express my gratitude to Youssef Hamadi for hiring me as a research intern at Microsoft Research in Cambridge, UK, in the summer of 2005."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 96
                            }
                        ],
                        "text": "My first publications on (per-instance) parameter optimization are jointly with Youssef Hamadi (Hutter and Hamadi, 2005), and with Youssef Hamadi, Holger Hoos, and Kevin Leyton-Brown (Hutter et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 581831,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfcac54ee9bc3e6b0f45bc7f23ffdea4c55965e4",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Tuning an algorithm\u2019s parameters for robust and high performance is a tedious and time-consuming task that often requires knowledge about both the domain and the algorithm of interest. Furthermore, the optimal parameter configuration to use may differ considerably across problem instances. In this report, we define and tackle the algorithm configuration problem, which is to automatically choose the optimal parameter configuration for a given algorithm on a per-instance base. We employ an indirect approach that predicts algorithm runtime for the problem instance at hand and each (continuous) parameter configuration, and then simply chooses the configuration that minimizes the prediction. This approach is based on similar work by Leyton-Brown et al. [LBNS02, NLBD04] who tackle the algorithm selection problem [Ric76] (given a problem instance, choose the best algorithm to solve it). While all previous studies for runtime prediction focussed on tree search algorithm, we demonstrate that it is possible to fairly accurately predict the runtime of SAPS [HTH02], one of the best-performing stochastic local search algorithms for SAT. We also show that our approach automatically picks parameter configurations that speed up SAPS by an average factor of more than two when compared to its default parameter configuration. Finally, we introduce sequential Bayesian learning to the problem of runtime prediction, enabling an incremental learning approach and yielding very informative estimates of predictive uncertainty."
            },
            "slug": "Parameter-Adjustment-Based-on-Performance-Towards-Hutter-Hamadi",
            "title": {
                "fragments": [],
                "text": "Parameter Adjustment Based on Performance Prediction: Towards an Instance-Aware Problem Solver"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that it is possible to fairly accurately predict the runtime of SAPS, one of the best-performing stochastic local search algorithms for SAT, and sequential Bayesian learning is introduced to the problem of runtime prediction, enabling an incremental learning approach and yielding very informative estimates of predictive uncertainty."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144444290"
                        ],
                        "name": "B. Srivastava",
                        "slug": "B.-Srivastava",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078229"
                        ],
                        "name": "Anupam Mediratta",
                        "slug": "Anupam-Mediratta",
                        "structuredName": {
                            "firstName": "Anupam",
                            "lastName": "Mediratta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anupam Mediratta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18964419,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00f828f44e17a10d872716565e292f24b739e49e",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Search-based algorithms, like planners, schedulers and satisfiability solvers, are notorious for having numerous parameters with a wide choice of values that can affect their performance drastically. As a result, the users of these algorithms, who may not be search experts, spend a significant time in tuning the values of the parameters to get acceptable performance on their particular problem domains. In this paper, we present a learning-based approach for automatic tuning of search-based algorithms to help such users. The benefit of our methodology is that it handles diverse parameter types, performs effectively for a broad range of systematic as well as non-systematic search based solvers (the selected parameters could make the algorithms solve up to 100% problems while the bad parameters would lead to none being solved), incorporates user-specified performance criteria (\u03a6) and is easy to implement. Moreover, the selected parameter will satisfy \u03a6 in the first try or the ranked candidates can be used along with \u03a6 to minimize the number of times the parameter settings need to he adjusted until a problem is solved."
            },
            "slug": "Domain-Dependent-Parameter-Selection-of-Algorithms-Srivastava-Mediratta",
            "title": {
                "fragments": [],
                "text": "Domain-Dependent Parameter Selection of Search-based Algorithms Compatible with User Performance Criteria"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A learning-based approach for automatic tuning of search-based algorithms to help users spend a significant time in tuning the values of the parameters to get acceptable performance on their particular problem domains."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728524"
                        ],
                        "name": "Y. Hamadi",
                        "slug": "Y.-Hamadi",
                        "structuredName": {
                            "firstName": "Youssef",
                            "lastName": "Hamadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hamadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 63
                            }
                        ],
                        "text": "1 Instance Features Existing work on empirical hardness models (Leyton-Brown et al., 2002; Nudelman et al., 2004; Hutter et al., 2006; Xu et al., 2007a; Leyton-Brown et al., 2009) convincingly demonstrated that it is possible to predict algorithm runtime based on features of the problem instance to be solved."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 247,
                                "start": 226
                            }
                        ],
                        "text": "We chose to study this algorithm because it is well known, it has relatively few parameters, we are intimately familiar with it, and we knew from earlier work that SAPS\u2019s parameters can have a strong impact on its performance (Hutter et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 49
                            }
                        ],
                        "text": "Joint publications with Lin Xu, Holger Hoos, and Kevin Leyton-Brown on per-instance algorithm selection (Xu et al., 2007b, 2008) fall into the same category."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 125
                            }
                        ],
                        "text": "We gained more experience with SAPS\u2019 parameters for more general problem classes in our early work on parameter optimization (Hutter et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 130
                            }
                        ],
                        "text": "Chapters 9 and 10 are primarily based on a conference publication at GECCO-09 (Hutter et al., 2009e), co-authored by Holger Hoos, Kevin Leyton-Brown, and Kevin Murphy, as well as a book chapter with the same co-authors and Thomas Bartz-Beielstein (Hutter et al., 2009a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 236,
                                "start": 215
                            }
                        ],
                        "text": "We have already applied an early version of the first approach for automatically setting two of SAPS\u2019s continuous parameters, as well as the single parameter of the WalkSAT variant Novelty+, on a per-instance basis (Hutter et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 85
                            }
                        ],
                        "text": "Chapters 11 through 13 are all based on yet-unpublished joint work with Holger Hoos, Kevin Leyton-Brown, and Kevin Murphy."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 95
                            }
                        ],
                        "text": "Adaptive capping is discussed in a comprehensive journal article on PARAMILS with Holger Hoos, Kevin Leyton-Brown, and Thomas Stu\u0308tzle, which also introduced the application to CPLEX and has been accepted for publication at JAIR (Hutter et al., 2009c)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 184
                            }
                        ],
                        "text": "My first publications on (per-instance) parameter optimization are jointly with Youssef Hamadi (Hutter and Hamadi, 2005), and with Youssef Hamadi, Holger Hoos, and Kevin Leyton-Brown (Hutter et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 55
                            }
                        ],
                        "text": ", 2008), or using per-instance algorithm configuration (Hutter et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 54
                            }
                        ],
                        "text": "Chapter 4 is based on joint work with Holger Hoos and Kevin Leyton-Brown, which is about to be submitted for publication (Hutter et al., 2009d)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 14
                            }
                        ],
                        "text": "Subsequently, Kevin Leyton-Brown got involved in the project and, notably, contributed the idea for the adaptive capping mechanism discussed in Chapter 7; thanks also to Kevin Murphy for valuable discussions of that topic."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 88
                            }
                        ],
                        "text": "I am deeply grateful to my three co-supervisors, Holger Hoos (my principal supervisor), Kevin Leyton-Brown, and Kevin Murphy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1471952,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56019659c24fcec61e80dfcf4f472f50091ac11b",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine learning can be used to build models that predict the run-time of search algorithms for hard combinatorial problems. Such empirical hardness models have previously been studied for complete, deterministic search algorithms. In this work, we demonstrate that such models can also make surprisingly accurate predictions of the run-time distributions of incomplete and randomized search methods, such as stochastic local search algorithms. We also show for the first time how information about an algorithm's parameter settings can be incorporated into a model, and how such models can be used to automatically adjust the algorithm's parameters on a per-instance basis in order to optimize its performance. Empirical results for Novelty+ and SAPS on structured and unstructured SAT instances show very good predictive performance and significant speedups of our automatically determined parameter settings when compared to the default and best fixed distribution-specific parameter settings."
            },
            "slug": "Performance-Prediction-and-Automated-Tuning-of-and-Hutter-Hamadi",
            "title": {
                "fragments": [],
                "text": "Performance Prediction and Automated Tuning of Randomized and Parametric Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated for the first time how information about an algorithm's parameter settings can be incorporated into a model, and how such models can be used to automatically adjust the algorithm's parameters on a per-instance basis in order to optimize its performance."
            },
            "venue": {
                "fragments": [],
                "text": "CP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 127
                            }
                        ],
                        "text": "This can be achieved by defining the parameter configuration space such that all allowed configurations have these guarantees (Hoos, 2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 150
                            }
                        ],
                        "text": "In order to motivate the automated configuration of such algorithms, we first describe the traditional, manual approach to algorithm design (see also Hoos, 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1433732,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d773e4c1ca8609f6e936a5b5ff3a57ff2979caa",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "High-performance algorithms play an important role in many areas of computer science and are core components of many software systems used in real-world applications. Traditionally, the creation of these algorithms requires considerable expertise and experience, often in combination with a substantial amount of trial and error. Here, we outline a new approach to the process of designing high-performance algorithms that is based on the use of automated procedures for exploring potentially very large spaces of candidate designs. We contrast this computer-aided design approach with the traditional approach and discuss why it can be expected to yield better performing, yet simpler algorithms. Finally, we sketch out the high-level design of a software environment that supports our new design approach. Existing work on algorithm portfolios, algorithm selection, algorithm configuration and parameter tuning, but also on general methods for discrete and continuous optimisation methods fits naturally into our design approach and can be integrated into the proposed software"
            },
            "slug": "Computer-Aided-Design-of-High-Performance-Hoos",
            "title": {
                "fragments": [],
                "text": "Computer-Aided Design of High-Performance Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work outlines a new approach to the process of designing high-performance algorithms that is based on the use of automated procedures for exploring potentially very large spaces of candidate designs, and contrasts this computer-aided design approach with the traditional approach."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393588417"
                        ],
                        "name": "T. Bartz-Beielstein",
                        "slug": "T.-Bartz-Beielstein",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Bartz-Beielstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bartz-Beielstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 155
                            }
                        ],
                        "text": ", 2009e), co-authored by Holger Hoos, Kevin Leyton-Brown, and Kevin Murphy, as well as a book chapter with the same co-authors and Thomas Bartz-Beielstein (Hutter et al., 2009a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 122
                            }
                        ],
                        "text": "Chapter 4 is based on joint work with Holger Hoos and Kevin Leyton-Brown, which is about to be submitted for publication (Hutter et al., 2009d)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 229
                            }
                        ],
                        "text": "Adaptive capping is discussed in a comprehensive journal article on PARAMILS with Holger Hoos, Kevin Leyton-Brown, and Thomas Stu\u0308tzle, which also introduced the application to CPLEX and has been accepted for publication at JAIR (Hutter et al., 2009c)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": "Chapters 9 and 10 are primarily based on a conference publication at GECCO-09 (Hutter et al., 2009e), co-authored by Holger Hoos, Kevin Leyton-Brown, and Kevin Murphy, as well as a book chapter with the same co-authors and Thomas Bartz-Beielstein (Hutter et al., 2009a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2527655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ac46d8563ec0fdf0107ca741575297fbf17d858",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "This work experimentally investigates model-based approaches for optimizing the performance of parameterized randomized algorithms. Such approaches build a response surface model and use this model for finding good parameter settings of the given algorithm. We evaluated two methods from the literature that are based on Gaussian process models: sequential parameter optimization (SPO) (Bartz-Beielstein et al, 2005) and sequential Kriging optimization (SKO) (Huang et al, 2006). SPO performed better \u201cout-of-the-box,\u201d whereas SKO was competitive when response values were log transformed. We then investigated key design decisions within the SPO paradigm, characterizing the performance consequences of each. Based on these findings, we propose a new version of SPO, dubbed SPO+, which extends SPO with a novel intensification procedure and a log-transformed objective function. In a domain for which performance results for other (model-free) parameter optimization approaches are available, we demonstrate that SPO+ achieves state-of-the-art performance. Finally, we compare this automated parameter tuning approach to an interactive, manual process that makes use of classical regression techniques. This interactive approach is particularly useful when only a relatively small number of parameter configurations can be evaluated. Because it can relatively quickly draw attention to important parameters and parameter interactions, it can help experts gain insights into the parameter response of a given algorithm and identify reasonable parameter settings."
            },
            "slug": "Sequential-Model-Based-Parameter-Optimisation:-an-Hutter-Bartz-Beielstein",
            "title": {
                "fragments": [],
                "text": "Sequential Model-Based Parameter Optimisation: an Experimental Investigation of Automated and Inte"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new version of SPO is proposed, dubbed SPO+, which extends SPO with a novel intensification procedure and a log-transformed objective function, and it is demonstrated that SPO+ achieves state-of-the-art performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684799"
                        ],
                        "name": "T. St\u00fctzle",
                        "slug": "T.-St\u00fctzle",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "St\u00fctzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. St\u00fctzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 164
                            }
                        ],
                        "text": "In particular, we automatically constructed different instantiations of the SPEAR algorithm and thereby substantially improved the state of the art for two sets of SAT-encoded industrial verification problems (Hutter et al., 2007a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "Both SPEAR and SATENSTEIN were configured using ParamILS, one of the automated configuration procedures we introduce in this thesis (see Chapter 5)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "123 8.5 Performance summary of SATenstein-LS; reproduced from (KhudaBukhsh\net al., 2009) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\nix\n9.1 Summary of notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 9.2 SPO and SPO+ algorithm parameters . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "81 5.3.3 Experimental Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 84 5.4 Configuration of SAPS, GLS+ and SAT4J . . . . . . . . . . . . . . . . . . . 86 5.5 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "99 6.6 MiniSAT 2.0 vs SPEAR configured for specific benchmark sets . . . . . . . . 100\n7.1 Speedup of RANDOMSEARCH due to adaptive capping . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "37\n3.1.1 Propostitional Satisfiability (SAT) . . . . . . . . . . . . . . . . . . . 38 3.1.2 Mixed Integer Programming (MIP) . . . . . . . . . . . . . . . . . . 38\n3.2 Target Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 829,
                                "start": 826
                            }
                        ],
                        "text": "In the context of multiple instances, these models can also be used to predict the cost distribution for a combination of a parameter configuration and an instance\nRMSE Root mean squared error, a measure of model quality \u2208 [0,\u221e) (the lower the better); RMSE measures the root of the squared differences between model predictions and validation costs, see Definition 14 on page 148\nEIC quality A measure of model quality \u2208 [\u22121, 1] (the higher the better); it measures the Spearman correlation coefficient between expected improvement based on model predictions, and validation costs, see Definition 13 on page 148\nQuality of predictive ranks A measure of model quality \u2208 [\u22121, 1] (the higher the better); it measures the Spearman correlation coefficient between model predictions and validation costs, see Definition 12 on page 148\nxvii\nSAT The propositional satisfiability problem; see Section 3.1.1\nSPO Sequential Parameter Optimization, a model-based framework for stochastic continuous blackbox optimization first described by Bartz-Beielstein et al. (2005)\nSKO Sequential Kriging Optimization, a model-based approach for stochastic continuous blackbox optimization described by Huang et al. (2006)\nStochastic Blackbox Optimization Problem Similar to a deterministic Blackbox Optimization Problem, but the observations we make at a query point, \u03b8, are samples of a cost distribution, P{\u03b8}, rather than deterministic values of a function, f(\u03b8); see Section 1.2.1\nTarget Algorithm A parameterized algorithm whose parameters are to be optimized\nTraining Performance of a Configurator at Time t Empirical cost estimate of the configurator\u2019s incumbent at time t, \u03b8inc(t), across the runs the configurator performed with \u03b8inc(t)\nTest Performance of a Configurator at Time t Empirical cost estimate of the configurator\u2019s incumbent at time t, \u03b8inc(t), for runs with \u03b8inc(t) on test set\nTraining Set Set of \u3008problem instance, pseudo-random number seed\u3009 combinations used during configuration\nTest Set Set of \u3008problem instance, pseudo-random number seed\u3009 combinations used for offline test purposes; disjoint from training set\nValidation Cost cvalid(\u03b8), empirical cost estimate of parameter configuration \u03b8 using all \u3008instance, seed\u3009 combinations in the training set\nxviii"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 191
                            }
                        ],
                        "text": "Using our procedures\u2014to the best of our knowledge still the only ones applicable to these complex configuration tasks\u2014we configured state-of-the-art tree search and local search algorithms for SAT, as well as CPLEX, the most widely-used commercial optimization tool for solving mixed integer programs (MIP)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "96 6.3 SPEAR default vs SPEAR configured for SAT competition, on BMC and SWV ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "86 5.4 Configuration of GLS+, SAPS, and SAT4J . . . . . . . . . . . . . . . . . . 87\n6.1 Summary of results for configuring SPEAR . . . . . . . . . . . . . . . . . . 101\n7.1 Speedup of RANDOMSEARCH due to adaptive capping . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "94 6.3.1 Parameterization of SPEAR . . . . . . . . . . . . . . . . . . . . . . . 94 6.3.2 Configuration for the 2007 SAT Competition . . . . . . . . . . . . . 95 6.3.3 Configuration for Specific Applications . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 167
                            }
                        ],
                        "text": "Note that categorical parameters can be used to select and combine discrete building blocks of an algorithm (e.g., preprocessing and variable ordering heuristics in a SAT solver)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "209\n13.1.1 SAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 13.1.2 MIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210 13.1.3 Generic Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 13.1.4 Principal Component Analysis (PCA) to Speed Up Learning . . . . . 213\n13.2 Modelling Cost Measures Defined Across Multiple Instances . . . . . . . . . 214 13.2.1 Gaussian Processes: Existing Work for Predicting Marginal Perfor-\nmance Across Instances . . . . . . . . . . . . . . . . . . . . . . . . 214 13.2.2 Random Forests: Prediction of Cost Measures Across Multiple Instances215\n13.3 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "88\n6.1 MiniSAT 2.0 vs SPEAR default . . . . . . . . . . . . . . . . . . . . . . . . . 93 6.2 SPEAR default vs SPEAR configured for SAT competition, on SAT competition instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "43 3.3 Benchmark Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.3.1 SAT Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 3.3.2 MIP Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.3.3 Test Functions for Continuous Global Optimization . . . . . . . . . . 48 3.4 Optimization Objective: Penalized Average Runtime . . . . . . . . . . . . . 48 3.5 Configuration Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n3.5.1 Set of Configuration Scenarios BLACKBOXOPT . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 196
                            }
                        ],
                        "text": "116\n8.2 Self-Configuration of PARAMILS . . . . . . . . . . . . . . . . . . . . . . . 120 8.3 Applications of PARAMILS by Other Researchers . . . . . . . . . . . . . . . 124\n8.3.1 Configuration of SATenstein . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "38 3.2.1 Target Algorithms for SAT . . . . . . . . . . . . . . . . . . . . . . . 39 3.2.2 Target Algorithm for MIP . . . . . . . . . . . . . . . . . . . . . . . 42 3.2.3 CMA-ES for Global Continuous Function Optimization . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "In particular, thanks to Dave Tompkins for help with UBCSAT, to Daniel Le Berre for help with SAT4J, to Belarmino Adenso-D\u0131\u0301az for providing the CALIBRA system and benchmark data, and to Theodore Allen for access to the SKO source code."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "We provide full details on the configuration of SPEAR in Chapter 6 and review the SATENSTEIN application in Section 8.3.1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The SATENSTEIN framework of local search algorithms for SAT by KhudaBukhsh et al. (2009) took this process to the extreme, combining a multitude of components from various existing local search algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 170
                            }
                        ],
                        "text": "Since PARAMILS performs an iterated local search using a one-exchange neighbourhood, it is very similar in spirit to local search methods for other problems, such as SAT (Selman et al., 1992; Hoos and St\u00fctzle, 2000; Schuurmans and Southey, 2001), CSP (Minton et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 193
                            }
                        ],
                        "text": "They often combine a multitude of approaches and feature a correspondingly large and structured parameter space (see, e.g., the aforementioned solver CPLEX or many state-of-the-art solvers for SAT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 227
                            }
                        ],
                        "text": "In many cases, we achieved improvements of orders of magnitude over the algorithm default, thereby substantially improving the state of the art in solving a broad range of problems, including industrially relevant instances of SAT and MIP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "206\n13.1 11 groups of SAT features; these were introduced by Nudelman et al. (2004) and Xu et al. (2008, 2009). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 13.2 Eight groups of features for the mixed integer programming problem. . . . . . . . . 212 13.3 RF model quality based on different features, for scenario SAPS-QCP . . . . . 218 13.4 RF model quality based on different features, for scenario SAPS-SWGCP . . . . 219 13.5 RF model quality based on different sets of instance features . . . . . . . . . 220 13.6 RF model quality based on different number of principal components . . . . 222 13.7 Predictions of runtime matrix . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The SAT instance features we use in Section 13.1 originated in this work.\nxxii\nPart I\nAlgorithm Configuration: The Problem\n\u2014in which we introduce and motivate the algorithm configuration problem and discuss related work\n1\nChapter 1\nIntroduction Civilization advances by extending the number of important operations which we can perform without thinking of them."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11291275,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "792a9ca259c0081f5715464f5554a2bb72a865c6",
            "isKey": true,
            "numCitedBy": 168,
            "numCiting": 130,
            "paperAbstract": {
                "fragments": [],
                "text": "Local search algorithms are among the standard methods for solving hard combinatorial problems from various areas of artificial intelligence and operations research. For SAT, some of the most successful and powerful algorithms are based on stochastic local search, and in the past 10 years a large number of such algorithms have been proposed and investigated. In this article, we focus on two particularly well-known families of local search algorithms for SAT, the GSAT and WalkSAT architectures. We present a detailed comparative analysis of these algorithms\" performance using a benchmark set that contains instances from randomized distributions as well as SAT-encoded problems from various domains. We also investigate the robustness of the observed performance characteristics as algorithm-dependent and problem-dependent parameters are changed. Our empirical analysis gives a very detailed picture of the algorithms\" performance for various domains of SAT problems; it also reveals a fundamental weakness in some of the best-performing algorithms and shows how this can be overcome."
            },
            "slug": "Local-Search-Algorithms-for-SAT:-An-Empirical-Hoos-St\u00fctzle",
            "title": {
                "fragments": [],
                "text": "Local Search Algorithms for SAT: An Empirical Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This article presents a detailed comparative analysis of two particularly well-known families of local search algorithms for SAT, the GSAT and WalkSAT architectures, using a benchmark set that contains instances from randomized distributions as well as SAT-encoded problems from various domains."
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Automated Reasoning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398717619"
                        ],
                        "name": "B. Adenso-D\u00edaz",
                        "slug": "B.-Adenso-D\u00edaz",
                        "structuredName": {
                            "firstName": "Belarmino",
                            "lastName": "Adenso-D\u00edaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Adenso-D\u00edaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144853029"
                        ],
                        "name": "M. Laguna",
                        "slug": "M.-Laguna",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Laguna",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Laguna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 177
                            }
                        ],
                        "text": "We compared the performance of the respective algorithm\u2019s default performance, and the performance with the configurations found by BASICILS, FOCUSEDILS, and the CALIBRA system (Adenso-Diaz and Laguna, 2006)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 66
                            }
                        ],
                        "text": "Many search algorithms, such as Multi-TAC (Minton, 1996), Calibra (Adenso-Diaz and Laguna, 2006), the mesh adaptive direct search algorithm (Audet and Orban, 2006) and BASICILS (Hutter et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8902300,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "d2d9b1bf324a124d27bfa2cc79be039d26e926b9",
            "isKey": false,
            "numCitedBy": 427,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Researchers and practitioners frequently spend more time fine-tuning algorithms than designing and implementing them. This is particularly true when developing heuristics and metaheuristics, where the right choice of values for search parameters has a considerable effect on the performance of the procedure. When testing metaheuristics, performance typically is measured considering both the quality of the solutions obtained and the time needed to find them. In this paper, we describe the development of CALIBRA, a procedure that attempts to find the best values for up to five search parameters associated with a procedure under study. Because CALIBRA uses Taguchis fractional factorial experimental designs coupled with a local search procedure, the best values found are not guaranteed to be optimal. We test CALIBRA on six existing heuristic-based procedures. These experiments show that CALIBRA is able to find parameter values that either match or improve the performance of the procedures resulting from using the parameter values suggested by their developers. The latest version of CALIBRA can be downloaded for free from the website that appears in the online supplement of this paper at http://or.pubs.informs.org/Pages.collect.html."
            },
            "slug": "Fine-Tuning-of-Algorithms-Using-Fractional-Designs-Adenso-D\u00edaz-Laguna",
            "title": {
                "fragments": [],
                "text": "Fine-Tuning of Algorithms Using Fractional Experimental Designs and Local Search"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The development of CALIBRA is described, a procedure that attempts to find the best values for up to five search parameters associated with a procedure under study and is able to find parameter values that either match or improve the performance of the procedures resulting from using the parameter values suggested by their developers."
            },
            "venue": {
                "fragments": [],
                "text": "Oper. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35198410"
                        ],
                        "name": "A. Fukunaga",
                        "slug": "A.-Fukunaga",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Fukunaga",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fukunaga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 282,
                                "start": 266
                            }
                        ],
                        "text": "In particular, research communities that have contributed techniques for algorithm configuration or parameter tuning include planning (Gratch and Dejong, 1992), evolutionary computation (Bartz-Beielstein, 2006), meta-heuristics (Birattari, 2005), genetic algorithms (Fukunaga, 2008), parallel computing (Brewer, 1995), and numerical optimization (Audet and Orban, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15304060,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "efcc8130f405aa055e31a7882b287119cb7ce93d",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The development of successful metaheuristic algorithms such as local search for a difficult problem such as satisfiability testing (SAT) is a challenging task. We investigate an evolutionary approach to automating the discovery of new local search heuristics for SAT. We show that several well-known SAT local search algorithms such as Walksat and Novelty are composite heuristics that are derived from novel combinations of a set of building blocks. Based on this observation, we developed CLASS, a genetic programming system that uses a simple composition operator to automatically discover SAT local search heuristics. New heuristics discovered by CLASS are shown to be competitive with the best Walksat variants, including Novelty. Evolutionary algorithms have previously been applied to directly evolve a solution for a particular SAT instance. We show that the heuristics discovered by CLASS are also competitive with these previous, direct evolutionary approaches for SAT. We also analyze the local search behavior of the learned heuristics using the depth, mobility, and coverage metrics proposed by Schuurmans and Southey."
            },
            "slug": "Automated-Discovery-of-Local-Search-Heuristics-for-Fukunaga",
            "title": {
                "fragments": [],
                "text": "Automated Discovery of Local Search Heuristics for Satisfiability Testing"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is shown that the heuristics discovered by CLASS are also competitive with these previous, direct evolutionary approaches for SAT, and the local search behavior of the learnedHeuristics is analyzed using the depth, mobility, and coverage metrics proposed by Schuurmans and Southey."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2835134"
                        ],
                        "name": "Enda Ridge",
                        "slug": "Enda-Ridge",
                        "structuredName": {
                            "firstName": "Enda",
                            "lastName": "Ridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enda Ridge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380005"
                        ],
                        "name": "D. Kudenko",
                        "slug": "D.-Kudenko",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kudenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kudenko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 80
                            }
                        ],
                        "text": "In order to reduce the variance in these comparisons, following common practice (see, e.g., Birattari et al., 2002; Ridge and Kudenko, 2006), we blocked on instances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14260353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51b4e1fbcabd595bb99265f425c6444f1fafed18",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a sequential experimentation approach for efficiently screening and tuning the parameters of a stochastic heuristic. Stochastic heuristics such as ant colony algorithms often use a large number of tuning parameters. Testing all combinations of these factors is prohibitive and inefficient. The sequential procedure recommended by this paper uses resolution IV fractional factorial designs with fold-over and centre points as an efficient way to screen the most important tuning parameters. The effects of the most important parameters are then modelled using a central composite design and optimised with standard numerical methods. All designs, their analyses and interpretation are illustrated using the Ant Colony System algorithm. The use of standard designs and methods has the benefit that the presented procedure can easily be followed with commercial software rather that relying on custom methodologies and tools that have only been developed in an academic context. Such a procedure has not been applied to ant colony algorithms before."
            },
            "slug": "Sequential-Experiment-Designs-for-Screening-and-of-Ridge-Kudenko",
            "title": {
                "fragments": [],
                "text": "Sequential Experiment Designs for Screening and Tuning Parameters of Stochastic Heuristics"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The sequential procedure recommended by this paper uses resolution IV fractional factorial designs with fold-over and centre points as an efficient way to screen the most important tuning parameters of a stochastic heuristic."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078339399"
                        ],
                        "name": "Andreas B\u00f6lte",
                        "slug": "Andreas-B\u00f6lte",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "B\u00f6lte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas B\u00f6lte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2526644"
                        ],
                        "name": "U. W. Thonemann",
                        "slug": "U.-W.-Thonemann",
                        "structuredName": {
                            "firstName": "Ulrich",
                            "lastName": "Thonemann",
                            "middleNames": [
                                "Wilhelm"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. W. Thonemann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56581379,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "cee08643a7274c9c7d8b0a7bb2e878ef50499525",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimizing-simulated-annealing-schedules-with-B\u00f6lte-Thonemann",
            "title": {
                "fragments": [],
                "text": "Optimizing simulated annealing schedules with genetic programming"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144659218"
                        ],
                        "name": "C. Gomes",
                        "slug": "C.-Gomes",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Gomes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gomes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744679"
                        ],
                        "name": "B. Selman",
                        "slug": "B.-Selman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Selman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Selman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 53
                            }
                        ],
                        "text": "They have been widely used for benchmarking purposes (Gomes and Selman, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10639512,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "417ed2405c04afa0dee56b8812b8ddd9e850177f",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent progress on search and reasoning procedures has been driven by experimentation on computationally hard problem instances. Hard random problem distributions are an important source of such instances. Challenge problems from the area of finite algebra have also stimulated research on search and reasoning procedures. Nevertheless, the relation of such problems to practical applications is somewhat unclear. Realistic problem instances clearly have more structure than the random problem instances, but, on the other hand, they are not as regular as the structured mathematical problems. We propose a new benchmark domain that bridges the gap between the purely random instances and the highly structured problems, by introducing perturbations into a structured domain. We will show how to obtain interesting search problems in this manner, and how such problems can be used to study the robustness of search control mechanisms. Our experiments demonstrate that the performance of search strategies designed to mimic direct constructive methods degrade surprisingly quickly in the presence of even minor perturbations."
            },
            "slug": "Problem-Structure-in-the-Presence-of-Perturbations-Gomes-Selman",
            "title": {
                "fragments": [],
                "text": "Problem Structure in the Presence of Perturbations"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a new benchmark domain that bridges the gap between the purely random instances and the highly structured problems, by introducing perturbations into a structured domain and demonstrates that the performance of search strategies designed to mimic direct constructive methods degrade surprisingly quickly in the presence of even minor perturbation."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759010"
                        ],
                        "name": "E. Brewer",
                        "slug": "E.-Brewer",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brewer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brewer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 127
                            }
                        ],
                        "text": ", 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 150
                            }
                        ],
                        "text": "\u2026algebra (Whaley et al., 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 317,
                                "start": 303
                            }
                        ],
                        "text": "In particular, research communities that have contributed techniques for algorithm configuration or parameter tuning include planning (Gratch and Dejong, 1992), evolutionary computation (Bartz-Beielstein, 2006), meta-heuristics (Birattari, 2005), genetic algorithms (Fukunaga, 2008), parallel computing (Brewer, 1995), and numerical optimization (Audet and Orban, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4522735,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "daab3fcf0e7300bfba1ea588f0681f451020311f",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We develop the use of statistical modeling for portable high-level optimizations such as data layout and algorithm selection. We build the models automatically from profiling information, which ensures robust and accurate models that reflect all aspects of the target platform.\nWe use the models to select among several data layouts for an iterative PDE solver and to select among several sorting algorithms. The selection is correct more than 99% of the time on each of four platforms. In the few cases it selects suboptimally, the selected implementation performs nearly as well; that is, it always makes at least a very good choice. Correct selection is platform and workload dependent and can improve performance by nearly a factor of three.\nWe also use the models to optimize parameters of these applications automatically. In all cases, the models predicted the optimal parameter setting, resulting in improvements ranging up to a factor of three.\nFinally, we use the models to construct portable high-level libraries, which contain multiple implementations and support for automatic selection and parameter optimization of the fastest implementation for the target platform and workload."
            },
            "slug": "High-level-optimization-via-automated-statistical-Brewer",
            "title": {
                "fragments": [],
                "text": "High-level optimization via automated statistical modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This work develops the use of statistical modeling for portable high-level optimizations such as data layout and algorithm selection and builds the models automatically from profiling information, which ensures robust and accurate models that reflect all aspects of the target platform."
            },
            "venue": {
                "fragments": [],
                "text": "PPOPP '95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2530656,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65461d01c7950afbe9e1f9f421c6c39744fec8cb",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic local search algorithms based on the WalkSAT architecture are among the best known methods for solving hard and large instances of the propositional satisfiability problem (SAT). The performance and behaviour of these algorithms critically depends on the setting of the noise parameter, which controls the greediness of the search process. The optimal setting for the noise parameter varies considerably between different types and sizes of problem instances; consequently, considerable manual tuning is typically required to obtain peak performance. In this paper, we characterise the impact of the noise setting on the behaviour of WalkSAT and introduce a simple adaptive noise mechanism for WalkSAT that does not require manual adjustment for different problem instances. We present experimental results indicating that by using this self-tuning noise mechanism, various WalkSAT variants (including WalkSAT/SKC and Novelty<sup>+</sup>) achieve performance levels close to their peak performance for instance-specific, manually tuned noise settings."
            },
            "slug": "An-adaptive-noise-mechanism-for-walkSAT-Hoos",
            "title": {
                "fragments": [],
                "text": "An adaptive noise mechanism for walkSAT"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The impact of the noise setting on the behaviour of WalkSAT is characterised and a simple adaptive noise mechanism is introduced that does not require manual adjustment for different problem instances to achieve performance levels close to their peak performance for instance-specific, manually tuned noise settings."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784072"
                        ],
                        "name": "M. Lagoudakis",
                        "slug": "M.-Lagoudakis",
                        "structuredName": {
                            "firstName": "Michail",
                            "lastName": "Lagoudakis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lagoudakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12610697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79936bfab7ab464afba9e653f1921d147a883a2c",
            "isKey": false,
            "numCitedBy": 154,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Many computational problems can be solved by multiple algorithms, with different algorithms fastest for different problem sizes, input distributions, and hardware characteristics. We consider the problem of algorithm selection: dynamically choose an algorithm to attack an instance of a problem with the goal of minimizing the overall execution time. We formulate the problem as a kind of Markov decision process (MDP), and use ideas from reinforcement learning to solve it. This paper introduces a kind of MDP that models the algorithm selection problem by allowing multiple state transitions. The well known Q-learning algorithm is adapted for this case in a way that combines both Monte-Carlo and Temporal Difference methods. Also, this work uses, and extends in a way to control problems, the Least-Squares Temporal Difference algorithm (LSTD(0)) of Boyan. The experimental study focuses on the classic problems of order statistic selection and sorting. The encouraging results reveal the potential of applying learning methods to traditional computational problems."
            },
            "slug": "Algorithm-Selection-using-Reinforcement-Learning-Lagoudakis-Littman",
            "title": {
                "fragments": [],
                "text": "Algorithm Selection using Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A kind of MDP that models the algorithm selection problem by allowing multiple state transitions is introduced, and the well known Q-learning algorithm is adapted for this case in a way that combines both Monte-Carlo and Temporal Difference methods."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700994"
                        ],
                        "name": "R. Battiti",
                        "slug": "R.-Battiti",
                        "structuredName": {
                            "firstName": "Roberto",
                            "lastName": "Battiti",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Battiti"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2402592"
                        ],
                        "name": "M. Brunato",
                        "slug": "M.-Brunato",
                        "structuredName": {
                            "firstName": "Mauro",
                            "lastName": "Brunato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Brunato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48261835"
                        ],
                        "name": "F. Mascia",
                        "slug": "F.-Mascia",
                        "structuredName": {
                            "firstName": "Franco",
                            "lastName": "Mascia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Mascia"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16503027,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f33cc8b5998f2435a48bcedb64a7475982350da0",
            "isKey": false,
            "numCitedBy": 278,
            "numCiting": 191,
            "paperAbstract": {
                "fragments": [],
                "text": "Reactive Search integrates sub-symbolic machine learning techniques into search heuristics for solving complex optimization problems. By automatically adjusting the working parameters, a reactive search self-tunes and adapts, effectively learning by doing until a solution is found. Intelligent Optimization, a superset of Reactive Search, concerns online and off-line schemes based on the use of memory, adaptation, incremental development of models, experimental algorithms applied to optimization, intelligent tuning and design of heuristics. Reactive Search and Intelligent Optimization is an excellent introduction to the main principles of reactive search, as well as an attempt to develop some fresh intuition for the approaches. The book looks at different optimization possibilities with an emphasis on opportunities for learning and self-tuning strategies. While focusing more on methods than on problems, problems are introduced wherever they help make the discussion more concrete, or when a specific problem has been widely studied by reactive search and intelligent optimization heuristics. Individual chapters cover reacting on the neighborhood; reacting on the annealing schedule; reactive prohibitions; model-based search; reacting on the objective function; relationships between reactive search and reinforcement learning; and much more. Each chapter is structured to show basic issues and algorithms; the parameters critical for the success of the different methods discussed; and opportunities and schemes for the automated tuning of these parameters. Anyone working in decision making in business, engineering, economics or science will find a wealth of information here."
            },
            "slug": "Reactive-Search-and-Intelligent-Optimization-Battiti-Brunato",
            "title": {
                "fragments": [],
                "text": "Reactive Search and Intelligent Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Reactive Search and Intelligent Optimization is an excellent introduction to the main principles of reactive search, as well as an attempt to develop some fresh intuition for the approaches."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714467"
                        ],
                        "name": "P. Cowling",
                        "slug": "P.-Cowling",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Cowling",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cowling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145528658"
                        ],
                        "name": "G. Kendall",
                        "slug": "G.-Kendall",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Kendall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kendall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122483"
                        ],
                        "name": "E. Soubeiga",
                        "slug": "E.-Soubeiga",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Soubeiga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Soubeiga"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7229432,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "0736a57e9a0c991bfdd7127cf3610703f93c8f71",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The term hyperheuristic was introduced by the authors as a high-level heuristic that adaptively controls several low-level knowledgepoor heuristics so that while using only cheap, easy-to-implement low-level heuristics, we may achieve solution quality approaching that of an expensive knowledge-rich approach. For certain classes of problems, this allows us to rapidly produce effective solutions, in a fraction of the time needed for other approaches, and using a level of expertise common among non-academic IT professionals. Hyperheuristics have been successfully applied by the authors to a real-world problem of personnel scheduling. In this paper, the authors report another successful application of hyperheuristics to a rather different real-world problem of personnel scheduling occuring at a UK academic institution. Not only did the hyperheuristics produce results of a quality much superior to that of a manual solution but also these results were produced within a period of only three weeks due to the savings resulting from using the existing hyperheuristic software framework."
            },
            "slug": "Hyperheuristics:-A-Tool-for-Rapid-Prototyping-in-Cowling-Kendall",
            "title": {
                "fragments": [],
                "text": "Hyperheuristics: A Tool for Rapid Prototyping in Scheduling and Optimisation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reports another successful application of hyperheuristics to a rather different real-world problem of personnel scheduling occuring at a UK academic institution and results of a quality much superior to that of a manual solution are produced."
            },
            "venue": {
                "fragments": [],
                "text": "EvoWorkshops"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116431932"
                        ],
                        "name": "Xiaoming Li",
                        "slug": "Xiaoming-Li",
                        "structuredName": {
                            "firstName": "Xiaoming",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoming Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3106639"
                        ],
                        "name": "M. Garzar\u00e1n",
                        "slug": "M.-Garzar\u00e1n",
                        "structuredName": {
                            "firstName": "Mar\u00eda",
                            "lastName": "Garzar\u00e1n",
                            "middleNames": [
                                "Jes\u00fas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Garzar\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729097"
                        ],
                        "name": "D. Padua",
                        "slug": "D.-Padua",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Padua",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Padua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 87
                            }
                        ],
                        "text": "Other examples of parameterized algorithms can be found in areas as diverse as sorting (Li et al., 2005), linear algebra (Whaley et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 88
                            }
                        ],
                        "text": "Other examples of parameterized algorithms can be found in areas as diverse as sorting (Li et al., 2005), linear algebra (Whaley et al., 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5511441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4d8496527160a1c58314f727317e674aa8e05b7",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The growing complexity of modern processors has made the generation of highly efficient code increasingly difficult. Manual code generation is very time consuming, but it is often the only choice since the code generated by today's compiler technology often has much lower performance than the best hand-tuned codes. A promising code generation strategy, implemented by systems like ATLAS, FFTW, and SPIRAL, uses empirical search to find the parameter values of the implementation, such as the tile size and instruction schedules, that deliver near-optimal performance for a particular machine. However, this approach has only proven successful on scientific codes whose performance does not depend on the input data. In this paper we study machine learning techniques to extend empirical search to the generation of sorting routines, whose performance depends on the input characteristics and the architecture of the target machine. We build on a previous study that selects a \"pure\" sorting algorithm at the outset of the computation as a function of the standard deviation. The approach discussed in this paper uses genetic algorithms and a classifier system to build hierarchically-organized hybrid sorting algorithms capable of adapting to the input data. Our results show that such algorithms generated using the approach presented in this paper are quite effective at taking into account the complex interactions between architectural and input data characteristics and that the resulting code performs significantly better than conventional sorting implementations and the code generated by our earlier study. In particular, the routines generated using our approach perform better than all the commercial libraries that we tried including IBM ESSL, INTEL MKL and the C++ STL The best algorithm we have been able to generate is on the average 26% and 62% faster than the IBM ESSL in an IBM Power 3 and IBM Power 4, respectively."
            },
            "slug": "Optimizing-sorting-with-genetic-algorithms-Li-Garzar\u00e1n",
            "title": {
                "fragments": [],
                "text": "Optimizing sorting with genetic algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Algorithms generated using machine learning techniques to extend empirical search to the generation of sorting routines, whose performance depends on the input characteristics and the architecture of the target machine, are quite effective at taking into account the complex interactions between architectural and input data characteristics."
            },
            "venue": {
                "fragments": [],
                "text": "International Symposium on Code Generation and Optimization"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 313,
                                "start": 278
                            }
                        ],
                        "text": "4 GLS for the Most Probable Explanation (MPE) Problem GLS+ is a guided local search algorithm for solving the Most Probable Explanation (MPE) problem in discrete-valued graphical models, that is, the problem of finding the variable instantiation with maximal overall likelihood (Hutter et al., 2005; Hutter, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 111
                            }
                        ],
                        "text": "An early version of PARAMILS goes back to an unpublished side project during my MSc. thesis (see Appendix A of Hutter, 2004), which was co-supervised by Thomas Stu\u0308tzle and Holger Hoos."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123144171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bcce9a026ff863442cea6d1596bfe06ee02af32f",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 67,
            "paperAbstract": {
                "fragments": [],
                "text": "In this thesis, we develop and study novel Stochastic Local Search (SLS) algorithms for solving the Most Probable Explanation (MPE) problem in graphical models, that is, to find the most probable instantiation of all variables V in the model, given the observed values E = e of a subset E of V. SLS algorithms have been applied to the MPE problem before [KD99b, Par02], but none of the previous SLS algorithms pays sufficient attention to such important concerns as algorithmic complexity per search step, search stagnation, and thorough parameter tuning. We remove these shortcomings of previous SLS algorithms for MPE, improving their efficiency by up to six orders of magnitude. In a thorough experimental analysis, we demonstrate how each of the novel components of our algorithms substantially contributes to their high performance. A comparison with an anytime version of the prominent Mini-Buckets algorithm [DR03] and the exact algorithm Branchand-Bound with static Mini-Buckets heuristic (BBMB) [KD99a, MKD03] shows that our best algorithm outperforms these approaches on most MPE instances we study. We also show that our SLS algorithms scale much better in terms of a number of important instance characteristics, namely the number of variables, domain size, node degree, and induced width of the underlying graphical model."
            },
            "slug": "Stochastic-Local-Search-for-Solving-the-Most-in-Hutter",
            "title": {
                "fragments": [],
                "text": "Stochastic Local Search for Solving the Most Probable Explanation Problem in Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This thesis develops and studies novel Stochastic Local Search algorithms for solving the Most Probable Explanation (MPE) problem in graphical models, that is, to find the most probable instantiation of all variables V in the model, given the observed values E = e of a subset E of V."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064595436"
                        ],
                        "name": "Eric Horvitz",
                        "slug": "Eric-Horvitz",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Horvitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Horvitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760493"
                        ],
                        "name": "Y. Ruan",
                        "slug": "Y.-Ruan",
                        "structuredName": {
                            "firstName": "Yongshao",
                            "lastName": "Ruan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ruan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144659218"
                        ],
                        "name": "C. Gomes",
                        "slug": "C.-Gomes",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Gomes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gomes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690271"
                        ],
                        "name": "Henry A. Kautz",
                        "slug": "Henry-A.-Kautz",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Kautz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Henry A. Kautz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744679"
                        ],
                        "name": "B. Selman",
                        "slug": "B.-Selman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Selman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Selman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724065"
                        ],
                        "name": "D. M. Chickering",
                        "slug": "D.-M.-Chickering",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chickering",
                            "middleNames": [
                                "Maxwell"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Chickering"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 119
                            }
                        ],
                        "text": "This suggests splitting such heterogeneous sets of instances into more homogeneous subsets, using portfolio techniques (Gomes and Selman, 2001; Horvitz et al., 2001; Xu et al., 2008), or using per-instance algorithm configuration (Hutter et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 181055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a55d2b4c5895de6c7c760a2e20e8ee4328014c16",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Bayesian-Approach-to-Tackling-Hard-Computational-Horvitz-Ruan",
            "title": {
                "fragments": [],
                "text": "A Bayesian Approach to Tackling Hard Computational Problems (Preliminary Report)"
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Notes Discret. Math."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 114
                            }
                        ],
                        "text": "This chapter is based on joint work with Holger Hoos and Kevin Leyton-Brown about to be submitted for publication (Hutter et al., 2009d)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 122
                            }
                        ],
                        "text": "Chapter 4 is based on joint work with Holger Hoos and Kevin Leyton-Brown, which is about to be submitted for publication (Hutter et al., 2009d)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 229
                            }
                        ],
                        "text": "Adaptive capping is discussed in a comprehensive journal article on PARAMILS with Holger Hoos, Kevin Leyton-Brown, and Thomas Stu\u0308tzle, which also introduced the application to CPLEX and has been accepted for publication at JAIR (Hutter et al., 2009c)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": "Chapters 9 and 10 are primarily based on a conference publication at GECCO-09 (Hutter et al., 2009e), co-authored by Holger Hoos, Kevin Leyton-Brown, and Kevin Murphy, as well as a book chapter with the same co-authors and Thomas Bartz-Beielstein (Hutter et al., 2009a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11604917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ecc6362774732f01b633318af31a7e515e07512",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose an empirical analysis approach for characterizing tradeoffs between different methods for comparing a set of competing algorithm designs. Our approach can provide insight into performance variation both across candidate algorithms and across instances. It can also identify the best tradeoff between evaluating a larger number of candidate algorithm designs, performing these evaluations on a larger number of problem instances, and allocating more time to each algorithm run. We applied our approach to a study of the rich algorithm design spaces offered by three highly-parameterized, state-of-the-art algorithms for satisfiability and mixed integer programming, considering six different distributions of problem instances. We demonstrate that the resulting algorithm design scenarios differ in many ways, with important consequences for both automatic and manual algorithm design. We expect that both our methods and our findings will lead to tangible improvements in algorithm design methods."
            },
            "slug": "Tradeoffs-in-the-empirical-evaluation-of-competing-Hutter-Hoos",
            "title": {
                "fragments": [],
                "text": "Tradeoffs in the empirical evaluation of competing algorithm designs"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "An empirical analysis approach for characterizing tradeoffs between different methods for comparing a set of competing algorithm designs is applied to a study of the rich algorithm design spaces offered by three highly-parameterized, state-of-the-art algorithms for satisfiability and mixed integer programming."
            },
            "venue": {
                "fragments": [],
                "text": "Annals of Mathematics and Artificial Intelligence"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26602711"
                        ],
                        "name": "Steven N. Minton",
                        "slug": "Steven-N.-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven N. Minton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37994193"
                        ],
                        "name": "M. Johnston",
                        "slug": "M.-Johnston",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Johnston",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Johnston"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2648787"
                        ],
                        "name": "Andrew B. Philips",
                        "slug": "Andrew-B.-Philips",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Philips",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew B. Philips"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29961301"
                        ],
                        "name": "P. Laird",
                        "slug": "P.-Laird",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Laird",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Laird"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 67
                            }
                        ],
                        "text": ", 1992; Hoos and St\u00fctzle, 2000; Schuurmans and Southey, 2001), CSP (Minton et al., 1992), and MPE (Kask and Dechter, 1999; Hutter, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14830518,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "be41652987acff408e1b9d699a8e78ab3ae11932",
            "isKey": false,
            "numCitedBy": 996,
            "numCiting": 80,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Minimizing-Conflicts:-A-Heuristic-Repair-Method-for-Minton-Johnston",
            "title": {
                "fragments": [],
                "text": "Minimizing Conflicts: A Heuristic Repair Method for Constraint Satisfaction and Scheduling Problems"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393588417"
                        ],
                        "name": "T. Bartz-Beielstein",
                        "slug": "T.-Bartz-Beielstein",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Bartz-Beielstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bartz-Beielstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2565835"
                        ],
                        "name": "S. Markon",
                        "slug": "S.-Markon",
                        "structuredName": {
                            "firstName": "Sandor",
                            "lastName": "Markon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Markon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7041566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cee220298e83d4d04cce622067eac93b5552bff",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The optimization of complex real-world problems might benefit from well tuned algorithm's parameters. We propose a methodology that performs this tuning in an effective and efficient algorithmical manner. This approach combines methods from statistical design of experiments, regression analysis, design and analysis of computer experiments methods, and tree-based regression. It can also be applied to analyze the influence of different operators or to compare the performance of different algorithms. An evolution strategy and a simulated annealing algorithm that optimize an elevator supervisory group controller system are used to demonstrate the applicability of our approach to real-world optimization problems."
            },
            "slug": "Tuning-search-algorithms-for-real-world-a-tree-Bartz-Beielstein-Markon",
            "title": {
                "fragments": [],
                "text": "Tuning search algorithms for real-world applications: a regression tree based approach"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This approach combines methods from statistical design of experiments, regression analysis, design and analysis of computer experiments methods, and tree-based regression to perform algorithmical tuning of parameters of well tuned algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2004 Congress on Evolutionary Computation (IEEE Cat. No.04TH8753)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144659218"
                        ],
                        "name": "C. Gomes",
                        "slug": "C.-Gomes",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Gomes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gomes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744679"
                        ],
                        "name": "B. Selman",
                        "slug": "B.-Selman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Selman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Selman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 119
                            }
                        ],
                        "text": "This suggests splitting such heterogeneous sets of instances into more homogeneous subsets, using portfolio techniques (Gomes and Selman, 2001; Horvitz et al., 2001; Xu et al., 2008), or using per-instance algorithm configuration (Hutter et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 20814400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3686cc6a1b864b8912d6cf22d3af673709d99627",
            "isKey": false,
            "numCitedBy": 472,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Algorithm-portfolios-Gomes-Selman",
            "title": {
                "fragments": [],
                "text": "Algorithm portfolios"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690903"
                        ],
                        "name": "M. Birattari",
                        "slug": "M.-Birattari",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Birattari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Birattari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 112
                            }
                        ],
                        "text": "In particular, we would like to compare the methods studied in this thesis to racing algorithms, such as F-Race (Birattari et al., 2002; Birattari, 2004) and iterated F-Race (Balaprakash et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 38
                            }
                        ],
                        "text": "Prominent competitors, such as F-Race (Birattari et al., 2002; Birattari, 2004), do not admit such a result."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42180519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66f816f950a09e62523640d3d38944e1ad88c86f",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A metaheuristic is a generic algorithmic template that can be used for finding high quality solutions of hard combinatorial optimization problems. To arrive at a functioning algorithm, a metaheuristic needs to be configured: typically some modules need to be instantiated and some parameters need to be tuned. We call these two problems \"structural\" and \"parametric\" tuning, respectively. More generally, we refer to the combination of the two problems as \"tuning\". Tuning is crucial to metaheuristics optimization both in academic research and for practical applications. Nevertheless, a precise definition of the tuning problem is missing in the literature. In this thesis, we show that the problem of tuning a metaheuristic can be described and solved as a machine learning problem. Using the machine learning perspective, we are able to provide a formal definition of the tuning problem. Moreover, we propose F-Race, a generic metaheuristic tuning algorithm. Our machine learning perspective also allows us to highlight some flaws in current metaheuristics research methodologies. Based on this discussion, we propose some methodological guidelines for future empirical analysis in metaheuristics research. The thesis also contains an experimental analysis of F-Race and some examples of practical applications."
            },
            "slug": "The-problem-of-tuning-metaheuristics:-as-seen-from-Birattari",
            "title": {
                "fragments": [],
                "text": "The problem of tuning metaheuristics: as seen from the machine learning perspective"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown that the problem of tuning a metaheuristic can be described and solved as a machine learning problem, and some methodological guidelines for future empirical analysis in metaheuristics research are proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145438097"
                        ],
                        "name": "J. Gratch",
                        "slug": "J.-Gratch",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Gratch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gratch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50306987"
                        ],
                        "name": "Steve Ankuo Chien",
                        "slug": "Steve-Ankuo-Chien",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Chien",
                            "middleNames": [
                                "Ankuo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steve Ankuo Chien"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1570967,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "b1e97c2bf7d22055f2ac2eb3d54fa0b7b123c1c3",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Although most scheduling problems are NP-hard, domain specific techniques perform well in practice but are quite expensive to construct. In adaptive problem-solving, domain specific knowledge is acquired automatically for a general problem solver with a flexible control architecture. In this approach, a learning system explores a space of possible heuristic methods for one well-suited to the eccentricities of the given domain and problem distribution. In this article, we discuss an application of the approach to scheduling satellite communications. Using problem distributions based on actual mission requirements, our approach identifies strategies that not only decrease the amount of CPU time required to produce schedules, but also increase the percentage of problems that are solvable within computational resource limitations."
            },
            "slug": "Adaptive-Problem-solving-for-Large-scale-Scheduling-Gratch-Chien",
            "title": {
                "fragments": [],
                "text": "Adaptive Problem-solving for Large-scale Scheduling Problems: A Case Study"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This approach identifies strategies that not only decrease the amount of CPU time required to produce schedules, but also increase the percentage of problems that are solvable within computational resource limitations."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40076877"
                        ],
                        "name": "Ohad Shacham",
                        "slug": "Ohad-Shacham",
                        "structuredName": {
                            "firstName": "Ohad",
                            "lastName": "Shacham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ohad Shacham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3014279"
                        ],
                        "name": "E. Zarpas",
                        "slug": "E.-Zarpas",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Zarpas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Zarpas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5259243,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355a8de745bdbb4d7333da7cb30215abb33efb95",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Bounded model checking (BMC) techniques have been used for formal hardware verification, with the help of tools such as GRASP (generic search algorithm for satisfiability problem) and more recently zchaff. In order to cope with very large hardware designs, our work exploited the unique characteristics of bounded model checking to enhance the SAT algorithms used to solve our problems. In our work, we tuned the VSIDS (variable state independent decaying sum) decision heuristics embedded in zchaff (M.W. Moskewicz et al., 2001), in order to improve the efficiency of the DPLL SAT algorithm, which is especially effective for BMC problems. We also checked whether the conclusions reached by Strichman (2000) regarding the tuning of GRASP, are also appropriate and hold true for zchaff. Our experimental results on actual hardware designs prove, with a few exceptions, that there is no real improvement when the existing tuned algorithms are applied to zchaff. However, our further modifications to the tuning proved to significantly increase SAT efficiency for BMC problems."
            },
            "slug": "Tuning-the-VSIDS-decision-heuristic-for-bounded-Shacham-Zarpas",
            "title": {
                "fragments": [],
                "text": "Tuning the VSIDS decision heuristic for bounded model checking"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work tuned the VSIDS (variable state independent decaying sum) decision heuristics embedded in zchaff in order to improve the efficiency of the DPLL SAT algorithm, which is especially effective for BMC problems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 4th International Workshop on Microprocessor Test and Verification - Common Challenges and Solutions"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784072"
                        ],
                        "name": "M. Lagoudakis",
                        "slug": "M.-Lagoudakis",
                        "structuredName": {
                            "firstName": "Michail",
                            "lastName": "Lagoudakis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Lagoudakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2307657,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f93fb6dd39de8eaf5e8d80207d976b867681359c",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-to-Select-Branching-Rules-in-the-DPLL-for-Lagoudakis-Littman",
            "title": {
                "fragments": [],
                "text": "Learning to Select Branching Rules in the DPLL Procedure for Satisfiability"
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Notes Discret. Math."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214276"
                        ],
                        "name": "J. Boyan",
                        "slug": "J.-Boyan",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Boyan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Boyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1168975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef8ff78b71fdbb64f8d129240acd86ae1c4f2c14",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes algorithms that learn to improve search performance on large-scale optimization tasks. The main algorithm, STAGE, works by learning an evaluation function that predicts the outcome of a local search algorithm, such as hillclimbing or Walksat, from features of states visited during search. The learned evaluation function is then used to bias future search trajectories toward better optima on the same problem. Another algorithm, X-STAGE, transfers previously learned evaluation functions to new, similar optimization problems. Empirical results are provided on seven large-scale optimization domains: bin-packing, channel routing, Bayesian network structure-finding, radiotherapy treatment planning, cartogram design, Boolean satisfiability, and Boggle board setup."
            },
            "slug": "Learning-Evaluation-Functions-to-Improve-by-Local-Boyan-Moore",
            "title": {
                "fragments": [],
                "text": "Learning Evaluation Functions to Improve Optimization by Local Search"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "Algorithms that learn to improve search performance on large-scale optimization tasks, including STAGE, which works by learning an evaluation function that predicts the outcome of a local search algorithm from features of states visited during search."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393588417"
                        ],
                        "name": "T. Bartz-Beielstein",
                        "slug": "T.-Bartz-Beielstein",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Bartz-Beielstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bartz-Beielstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2278193"
                        ],
                        "name": "C. Lasarczyk",
                        "slug": "C.-Lasarczyk",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Lasarczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lasarczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1950379"
                        ],
                        "name": "M. Preuss",
                        "slug": "M.-Preuss",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Preuss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Preuss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 149
                            }
                        ],
                        "text": "\u2026problem; see Section 3.1.1\nSPO Sequential Parameter Optimization, a model-based framework for stochastic continuous blackbox optimization first described by Bartz-Beielstein et al. (2005)\nSKO Sequential Kriging Optimization, a model-based approach for stochastic continuous blackbox optimization\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 2
                            }
                        ],
                        "text": "3 (Bartz-Beielstein et al., 2005; Bartz-Beielstein, 2006; Bartz-Beielstein and Preuss, 2006) doubles the number of runs for subsequent function evaluations whenever this happens, SPO 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6815175,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a99e54554731bda87e72024bb58da8c902d9800",
            "isKey": true,
            "numCitedBy": 325,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Sequential parameter optimization is a heuristic that combines classical and modern statistical techniques to improve the performance of search algorithms. To demonstrate its flexibility, three scenarios are discussed: (1) no experience how to choose the parameter setting of an algorithm is available, (2) a comparison with other algorithms is needed, and (3) an optimization algorithm has to be applied effectively and efficiently to a complex real-world optimization problem. Although sequential parameter optimization relies on enhanced statistical techniques such as design and analysis of computer experiments, it can be performed algorithmically and requires basically the specification of the relevant algorithm's parameters"
            },
            "slug": "Sequential-parameter-optimization-Bartz-Beielstein-Lasarczyk",
            "title": {
                "fragments": [],
                "text": "Sequential parameter optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "Although sequential parameter optimization relies on enhanced statistical techniques such as design and analysis of computer experiments, it can be performed algorithmically and requires basically the specification of the relevant algorithm's parameters."
            },
            "venue": {
                "fragments": [],
                "text": "2005 IEEE Congress on Evolutionary Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8388443"
                        ],
                        "name": "Pascal Van Hentenryck",
                        "slug": "Pascal-Van-Hentenryck",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Hentenryck",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Van Hentenryck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707349"
                        ],
                        "name": "L. Michel",
                        "slug": "L.-Michel",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Michel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Michel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9313439,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0fcd9abced24814b39b46f45afdc0accb9db45a9",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The gap in automation between MIP/SAT solvers and those for constraint programming and constraint-based local search hinders experimentation and adoption of these technologies and slows down scientific progress. This paper addresses this important issue: It shows how effective local search procedures can be automatically synthesized from models expressed in a rich constraint language. The synthesizer analyzes the model and derives the local search algorithm for a specific meta-heuristic by exploiting the structure of the model and the constraint semantics. Experimental results suggest that the synthesized procedures only induce a small loss in efficiency on a variety of realistic applications in sequencing, resource allocation, and facility location."
            },
            "slug": "Synthesis-of-Constraint-Based-Local-Search-from-Hentenryck-Michel",
            "title": {
                "fragments": [],
                "text": "Synthesis of Constraint-Based Local Search Algorithms from High-Level Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experimental results suggest that the synthesized procedures only induce a small loss in efficiency on a variety of realistic applications in sequencing, resource allocation, and facility location."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3288675"
                        ],
                        "name": "F. Agakov",
                        "slug": "F.-Agakov",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Agakov",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Agakov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30561807"
                        ],
                        "name": "Edwin V. Bonilla",
                        "slug": "Edwin-V.-Bonilla",
                        "structuredName": {
                            "firstName": "Edwin",
                            "lastName": "Bonilla",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edwin V. Bonilla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1874233"
                        ],
                        "name": "J. Cavazos",
                        "slug": "J.-Cavazos",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cavazos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cavazos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2644436"
                        ],
                        "name": "Bj\u00f6rn Franke",
                        "slug": "Bj\u00f6rn-Franke",
                        "structuredName": {
                            "firstName": "Bj\u00f6rn",
                            "lastName": "Franke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bj\u00f6rn Franke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2043821"
                        ],
                        "name": "G. Fursin",
                        "slug": "G.-Fursin",
                        "structuredName": {
                            "firstName": "Grigori",
                            "lastName": "Fursin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Fursin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401533251"
                        ],
                        "name": "M. O\u2019Boyle",
                        "slug": "M.-O\u2019Boyle",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "O\u2019Boyle",
                            "middleNames": [
                                "F.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. O\u2019Boyle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153031524"
                        ],
                        "name": "J. Thomson",
                        "slug": "J.-Thomson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Thomson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Thomson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120284556"
                        ],
                        "name": "M. Toussaint",
                        "slug": "M.-Toussaint",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Toussaint",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Toussaint"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7891873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56b37f88b109fd455a389642e1747f766f1be471",
            "isKey": false,
            "numCitedBy": 440,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Iterative compiler optimization has been shown to outperform static approaches. This, however, is at the cost of large numbers of evaluations of the program. This paper develops a new methodology to reduce this number and hence speed up iterative optimization. It uses predictive modelling from the domain of machine learning to automatically focus search on those areas likely to give greatest performance. This approach is independent of search algorithm, search space or compiler infrastructure and scales gracefully with the compiler optimization space size. Off-line, a training set of programs is iteratively evaluated and the shape of the spaces and program features are modelled. These models are learnt and used to focus the iterative optimization of a new program. We evaluate two learnt models, an independent and Markov model, and evaluate their worth on two embedded platforms, the Texas Instrument C67I3 and the AMD Au1500. We show that such learnt models can speed up iterative search on large spaces by an order of magnitude. This translates into an average speedup of 1.22 on the TI C6713 and 1.27 on the AMD Au1500 in just 2 evaluations."
            },
            "slug": "Using-machine-learning-to-focus-iterative-Agakov-Bonilla",
            "title": {
                "fragments": [],
                "text": "Using machine learning to focus iterative optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new methodology is developed that uses predictive modelling from the domain of machine learning to automatically focus search on those areas likely to give greatest performance, independent of search algorithm, search space or compiler infrastructure and scales gracefully with the compiler optimization space size."
            },
            "venue": {
                "fragments": [],
                "text": "International Symposium on Code Generation and Optimization (CGO'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31453521"
                        ],
                        "name": "Susan L. Epstein",
                        "slug": "Susan-L.-Epstein",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Epstein",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan L. Epstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2606753"
                        ],
                        "name": "Eugene C. Freuder",
                        "slug": "Eugene-C.-Freuder",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Freuder",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene C. Freuder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6420741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "714784c35f2c1eacf7028027ea593e9b028bdad7",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Although constraint programming offers a wealth of strong, generalpurpose methods, in practice a complex, real application demands a person who selects, combines, and refines various available techniques for constraint satisfaction and optimization. Although such tuning produces efficient code, the scarcity of human experts slows commercialization. The necessary expertise is of two forms: constraint programming expertise and problem-domain expertise. The former is in short supply, and even experts can be reduced to trial and error prototyping; the latter is difficult to extract. The project described here seeks to automate both the application of constraint programming expertise and the extraction of domain-specific expertise. It applies FORR, an architecture for learning and problem-solving, to constraint solving. FORR develops expertise from multiple heuristics. A successful case study is presented on coloring problems."
            },
            "slug": "Collaborative-Learning-for-Constraint-Solving-Epstein-Freuder",
            "title": {
                "fragments": [],
                "text": "Collaborative Learning for Constraint Solving"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The project described here seeks to automate both the application of constraint programming expertise and the extraction of domain-specific expertise, and applies FORR, an architecture for learning and problem-solving, to constraint solving."
            },
            "venue": {
                "fragments": [],
                "text": "CP"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3291587"
                        ],
                        "name": "James E. Borrett",
                        "slug": "James-E.-Borrett",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Borrett",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James E. Borrett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144872811"
                        ],
                        "name": "E. Tsang",
                        "slug": "E.-Tsang",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Tsang",
                            "middleNames": [
                                "P.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Tsang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "118916603"
                        ],
                        "name": "N. R. Walsh",
                        "slug": "N.-R.-Walsh",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Walsh",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. R. Walsh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13172775,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "8e1adec8878f956b30979941c34021a8d6e1e500",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "The choice of a particular algorithm for solving a given class of constraint satisfaction problems is often confused by exceptional behaviour of algorithms. One method of reducing the impact of this exceptional behaviour is to adopt an adaptive philosophy to constraint satisfaction problem solving. In this report we describe one such adaptive algorithm, based on the principle of chaining. It is designed to avoid the phenomenon of exceptionally hard problem instances. Our algorithm shows how the speed of more naive algorithms can be utilised safe in the knowledge that the exceptional behaviour can be bounded. Our work clearly demonstrates the potential benefits of the adaptive approach and opens a new front of research for the constraint satisfaction community."
            },
            "slug": "Adaptive-Constraint-Satisfaction:-The-Quickest-Borrett-Tsang",
            "title": {
                "fragments": [],
                "text": "Adaptive Constraint Satisfaction: The Quickest First Principle"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This report describes one such adaptive algorithm, based on the principle of chaining, designed to avoid the phenomenon of exceptionally hard problem instances and shows how the speed of more naive algorithms can be utilised safe in the knowledge that the exceptional behaviour can be bounded."
            },
            "venue": {
                "fragments": [],
                "text": "ECAI"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2031548"
                        ],
                        "name": "S. Westfold",
                        "slug": "S.-Westfold",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Westfold",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Westfold"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111160089"
                        ],
                        "name": "Douglas R. Smith",
                        "slug": "Douglas-R.-Smith",
                        "structuredName": {
                            "firstName": "Douglas",
                            "lastName": "Smith",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Douglas R. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14129629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35c2e348cd3390174608e8a9a09ccd280a1a6018",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe the framework we have developed in KIDS (Kestrel Interactive Development System) for generating efficient constraint satisfaction programs. We have used KIDS to synthesise global search scheduling programs that have proved to be dramatically faster than other programs running the same data. We focus on the underlying ideas that lead to this efficiency. The key to the efficiency is the reduction of the size of the search space by an effective representation of sets of possible solutions (solution spaces) that allows efficient constraint propagation and pruning at the level of solution spaces. Moving to a solution space representation involves a problem reformulation. Having found a solution to the reformulated problem, an extraction phase extracts solutions to the original problem. We show how constraints from the original problem can be automatically reformulated and specialised in order to derive efficient propagation code automatically. Our solution methods exploit the semi-lattice structure of our solution spaces."
            },
            "slug": "Synthesis-of-efficient-constraint-satisfaction-Westfold-Smith",
            "title": {
                "fragments": [],
                "text": "Synthesis of efficient constraint-satisfaction programs"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This paper describes the framework developed in KIDS (Kestrel Interactive Development System) and shows how constraints from the original problem can be automatically reformulated and specialised in order to derive efficient propagation code automatically."
            },
            "venue": {
                "fragments": [],
                "text": "The Knowledge Engineering Review"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741137257"
                        ],
                        "name": "Helena Ramalhinho Dias Louren\u00e7o",
                        "slug": "Helena-Ramalhinho-Dias-Louren\u00e7o",
                        "structuredName": {
                            "firstName": "Helena",
                            "lastName": "Ramalhinho Dias Louren\u00e7o",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Helena Ramalhinho Dias Louren\u00e7o"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145640467"
                        ],
                        "name": "O. Martin",
                        "slug": "O.-Martin",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Martin",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72146012"
                        ],
                        "name": "T. Stutzle",
                        "slug": "T.-Stutzle",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Stutzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Stutzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5860935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c7caeb614cba94577949ce96d695621f1a3c03e0",
            "isKey": false,
            "numCitedBy": 1390,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": "Iterated Local Search has many of the desirable features of a metaheuristic: it is simple, easy to implement, robust, and highly effective. The essential idea of Iterated Local Search lies in focusing the search not on the full space of solutions but on a smaller subspace defined by the solutions that are locally optimal for a given optimization engine. The success of Iterated Local Search lies in the biased sampling of this set of local optima. How effective this approach turns out to be depends mainly on the choice of the local search, the perturbations, and the acceptance criterion. So far, in spite of its conceptual simplicity, it has lead to a number of state-of-the-art results without the use of too much problem- specific knowledge. But with further work so that the different modules are well adapted to the problem at hand, Iterated Local Search can often become a competitive or even state of the art algorithm. The purpose of this review is both to give a detailed description of this metaheuristic and to show where it stands in terms of performance."
            },
            "slug": "Iterated-Local-Search-Louren\u00e7o-Martin",
            "title": {
                "fragments": [],
                "text": "Iterated Local Search"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The purpose of this review is to give a detailed description of this metaheuristic and to show where it stands in terms of performance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 42
                            }
                        ],
                        "text": "Many search algorithms, such as Multi-TAC (Minton, 1996), Calibra (Adenso-Diaz and Laguna, 2006), the mesh adaptive direct search algorithm (Audet and Orban, 2006) and BASICILS (Hutter et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10719518,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "071498a5dc7dd88058d05182d7207795d6e376a7",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-tac is a learning system that synthesizes heuristic constraint satisfaction programs. The system takes a library of generic algorithms and heuristics and specializes them for a particular application. We present a detailed case study with three different distributions of a single combinatorial problem, \u201cMinimum Maximal Matching\u201d, and show that Muti-tac can synthesize programs for these different distributions that perform on par with hand-coded programs and that exceed the performance of some well-known satisfiability algorithms. In synthesizing a program, Multi-tac bases its choice of heuristics on an instance distribution, and we demonstrate that this capability has a significant impact on the results."
            },
            "slug": "Automatically-configuring-constraint-satisfaction-A-Minton",
            "title": {
                "fragments": [],
                "text": "Automatically configuring constraint satisfaction programs: A case study"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents a detailed case study with three different distributions of a single combinatorial problem, \u201cMinimum Maximal Matching\u201d, and shows that Muti-tac can synthesize programs for these different distributions that perform on par with hand-coded programs and that exceed the performance of some well-known satisfiability algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "Constraints"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393588417"
                        ],
                        "name": "T. Bartz-Beielstein",
                        "slug": "T.-Bartz-Beielstein",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Bartz-Beielstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bartz-Beielstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1950379"
                        ],
                        "name": "M. Preuss",
                        "slug": "M.-Preuss",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Preuss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Preuss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 2
                            }
                        ],
                        "text": "3 (Bartz-Beielstein et al., 2005; Bartz-Beielstein, 2006; Bartz-Beielstein and Preuss, 2006) doubles the number of runs for subsequent function evaluations whenever this happens, SPO 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1674032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d3c49c166c52cd21a2bcda0adc1d9e161b5d629",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Obviously, it is not a good idea to apply an optimization algorithm with wrongly specified parameter settings, a situation which can be avoided by applying algorithm tuning. Sequential tuning procedures are considered more efficient than single-stage procedures. [1] introduced a sequential approach for algorithm tuning that has been successfully applied to several real-world optimization tasks and experimental studies. The sequential procedure requires the specification of an initial sample size k. Small k values lead to poor models and thus poor predictions for the subsequent stages, whereas large values prevent an extensive search and local fine tuning. This study analyzes the interaction between global and local search in sequential tuning procedures and gives recommendations for an adequate budget allocation. Furthermore, the integration of hypothesis testing for increasing effectiveness of the latter phase is investigated."
            },
            "slug": "Considerations-of-Budget-Allocation-for-Sequential-Bartz-Beielstein-Preuss",
            "title": {
                "fragments": [],
                "text": "Considerations of Budget Allocation for Sequential Parameter Optimization (SPO)"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This study analyzes the interaction between global and local search in sequential tuning procedures and gives recommendations for an adequate budget allocation and the integration of hypothesis testing for increasing effectiveness of the latter phase is investigated."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744679"
                        ],
                        "name": "B. Selman",
                        "slug": "B.-Selman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Selman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Selman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143634377"
                        ],
                        "name": "H. Levesque",
                        "slug": "H.-Levesque",
                        "structuredName": {
                            "firstName": "Hector",
                            "lastName": "Levesque",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Levesque"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50066285"
                        ],
                        "name": "D. Mitchell",
                        "slug": "D.-Mitchell",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mitchell",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mitchell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 164
                            }
                        ],
                        "text": "In particular, we automatically constructed different instantiations of the SPEAR algorithm and thereby substantially improved the state of the art for two sets of SAT-encoded industrial verification problems (Hutter et al., 2007a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "Both SPEAR and SATENSTEIN were configured using ParamILS, one of the automated configuration procedures we introduce in this thesis (see Chapter 5)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "123 8.5 Performance summary of SATenstein-LS; reproduced from (KhudaBukhsh\net al., 2009) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\nix\n9.1 Summary of notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 9.2 SPO and SPO+ algorithm parameters . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "81 5.3.3 Experimental Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 84 5.4 Configuration of SAPS, GLS+ and SAT4J . . . . . . . . . . . . . . . . . . . 86 5.5 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "99 6.6 MiniSAT 2.0 vs SPEAR configured for specific benchmark sets . . . . . . . . 100\n7.1 Speedup of RANDOMSEARCH due to adaptive capping . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "37\n3.1.1 Propostitional Satisfiability (SAT) . . . . . . . . . . . . . . . . . . . 38 3.1.2 Mixed Integer Programming (MIP) . . . . . . . . . . . . . . . . . . 38\n3.2 Target Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 829,
                                "start": 826
                            }
                        ],
                        "text": "In the context of multiple instances, these models can also be used to predict the cost distribution for a combination of a parameter configuration and an instance\nRMSE Root mean squared error, a measure of model quality \u2208 [0,\u221e) (the lower the better); RMSE measures the root of the squared differences between model predictions and validation costs, see Definition 14 on page 148\nEIC quality A measure of model quality \u2208 [\u22121, 1] (the higher the better); it measures the Spearman correlation coefficient between expected improvement based on model predictions, and validation costs, see Definition 13 on page 148\nQuality of predictive ranks A measure of model quality \u2208 [\u22121, 1] (the higher the better); it measures the Spearman correlation coefficient between model predictions and validation costs, see Definition 12 on page 148\nxvii\nSAT The propositional satisfiability problem; see Section 3.1.1\nSPO Sequential Parameter Optimization, a model-based framework for stochastic continuous blackbox optimization first described by Bartz-Beielstein et al. (2005)\nSKO Sequential Kriging Optimization, a model-based approach for stochastic continuous blackbox optimization described by Huang et al. (2006)\nStochastic Blackbox Optimization Problem Similar to a deterministic Blackbox Optimization Problem, but the observations we make at a query point, \u03b8, are samples of a cost distribution, P{\u03b8}, rather than deterministic values of a function, f(\u03b8); see Section 1.2.1\nTarget Algorithm A parameterized algorithm whose parameters are to be optimized\nTraining Performance of a Configurator at Time t Empirical cost estimate of the configurator\u2019s incumbent at time t, \u03b8inc(t), across the runs the configurator performed with \u03b8inc(t)\nTest Performance of a Configurator at Time t Empirical cost estimate of the configurator\u2019s incumbent at time t, \u03b8inc(t), for runs with \u03b8inc(t) on test set\nTraining Set Set of \u3008problem instance, pseudo-random number seed\u3009 combinations used during configuration\nTest Set Set of \u3008problem instance, pseudo-random number seed\u3009 combinations used for offline test purposes; disjoint from training set\nValidation Cost cvalid(\u03b8), empirical cost estimate of parameter configuration \u03b8 using all \u3008instance, seed\u3009 combinations in the training set\nxviii"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 191
                            }
                        ],
                        "text": "Using our procedures\u2014to the best of our knowledge still the only ones applicable to these complex configuration tasks\u2014we configured state-of-the-art tree search and local search algorithms for SAT, as well as CPLEX, the most widely-used commercial optimization tool for solving mixed integer programs (MIP)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "96 6.3 SPEAR default vs SPEAR configured for SAT competition, on BMC and SWV ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "86 5.4 Configuration of GLS+, SAPS, and SAT4J . . . . . . . . . . . . . . . . . . 87\n6.1 Summary of results for configuring SPEAR . . . . . . . . . . . . . . . . . . 101\n7.1 Speedup of RANDOMSEARCH due to adaptive capping . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "94 6.3.1 Parameterization of SPEAR . . . . . . . . . . . . . . . . . . . . . . . 94 6.3.2 Configuration for the 2007 SAT Competition . . . . . . . . . . . . . 95 6.3.3 Configuration for Specific Applications . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 167
                            }
                        ],
                        "text": "Note that categorical parameters can be used to select and combine discrete building blocks of an algorithm (e.g., preprocessing and variable ordering heuristics in a SAT solver)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "209\n13.1.1 SAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 13.1.2 MIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210 13.1.3 Generic Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 13.1.4 Principal Component Analysis (PCA) to Speed Up Learning . . . . . 213\n13.2 Modelling Cost Measures Defined Across Multiple Instances . . . . . . . . . 214 13.2.1 Gaussian Processes: Existing Work for Predicting Marginal Perfor-\nmance Across Instances . . . . . . . . . . . . . . . . . . . . . . . . 214 13.2.2 Random Forests: Prediction of Cost Measures Across Multiple Instances215\n13.3 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "88\n6.1 MiniSAT 2.0 vs SPEAR default . . . . . . . . . . . . . . . . . . . . . . . . . 93 6.2 SPEAR default vs SPEAR configured for SAT competition, on SAT competition instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "43 3.3 Benchmark Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.3.1 SAT Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 3.3.2 MIP Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.3.3 Test Functions for Continuous Global Optimization . . . . . . . . . . 48 3.4 Optimization Objective: Penalized Average Runtime . . . . . . . . . . . . . 48 3.5 Configuration Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n3.5.1 Set of Configuration Scenarios BLACKBOXOPT . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 196
                            }
                        ],
                        "text": "116\n8.2 Self-Configuration of PARAMILS . . . . . . . . . . . . . . . . . . . . . . . 120 8.3 Applications of PARAMILS by Other Researchers . . . . . . . . . . . . . . . 124\n8.3.1 Configuration of SATenstein . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "38 3.2.1 Target Algorithms for SAT . . . . . . . . . . . . . . . . . . . . . . . 39 3.2.2 Target Algorithm for MIP . . . . . . . . . . . . . . . . . . . . . . . 42 3.2.3 CMA-ES for Global Continuous Function Optimization . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "In particular, thanks to Dave Tompkins for help with UBCSAT, to Daniel Le Berre for help with SAT4J, to Belarmino Adenso-D\u0131\u0301az for providing the CALIBRA system and benchmark data, and to Theodore Allen for access to the SKO source code."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "We provide full details on the configuration of SPEAR in Chapter 6 and review the SATENSTEIN application in Section 8.3.1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The SATENSTEIN framework of local search algorithms for SAT by KhudaBukhsh et al. (2009) took this process to the extreme, combining a multitude of components from various existing local search algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 170
                            }
                        ],
                        "text": "Since PARAMILS performs an iterated local search using a one-exchange neighbourhood, it is very similar in spirit to local search methods for other problems, such as SAT (Selman et al., 1992; Hoos and St\u00fctzle, 2000; Schuurmans and Southey, 2001), CSP (Minton et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 193
                            }
                        ],
                        "text": "They often combine a multitude of approaches and feature a correspondingly large and structured parameter space (see, e.g., the aforementioned solver CPLEX or many state-of-the-art solvers for SAT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 227
                            }
                        ],
                        "text": "In many cases, we achieved improvements of orders of magnitude over the algorithm default, thereby substantially improving the state of the art in solving a broad range of problems, including industrially relevant instances of SAT and MIP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "206\n13.1 11 groups of SAT features; these were introduced by Nudelman et al. (2004) and Xu et al. (2008, 2009). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 13.2 Eight groups of features for the mixed integer programming problem. . . . . . . . . 212 13.3 RF model quality based on different features, for scenario SAPS-QCP . . . . . 218 13.4 RF model quality based on different features, for scenario SAPS-SWGCP . . . . 219 13.5 RF model quality based on different sets of instance features . . . . . . . . . 220 13.6 RF model quality based on different number of principal components . . . . 222 13.7 Predictions of runtime matrix . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The SAT instance features we use in Section 13.1 originated in this work.\nxxii\nPart I\nAlgorithm Configuration: The Problem\n\u2014in which we introduce and motivate the algorithm configuration problem and discuss related work\n1\nChapter 1\nIntroduction Civilization advances by extending the number of important operations which we can perform without thinking of them."
                    },
                    "intents": []
                }
            ],
            "corpusId": 8540521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f8729c0cfc09dae60170b83e6112c19bde7c625",
            "isKey": true,
            "numCitedBy": 1477,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a greedy local search procedure called GSAT for solving propositional satisfiability problems. Our experiments show that this procedure can be used to solve hard, randomly generated problems that are an order of magnitude larger than those that can be handled by more traditional approaches such as the Davis-Putnam procedure or resolution. We also show that GSAT can solve structured satisfiability problems quickly. In particular, we solve encodings of graph coloring problems, N-queens, and Boolean induction. General application strategies and limitations of the approach are also discussed. \n \nGSAT is best viewed as a model-finding procedure. Its good performance suggests that it may be advantageous to reformulate reasoning tasks that have traditionally been viewed as theorem-proving problems as model-finding tasks."
            },
            "slug": "A-New-Method-for-Solving-Hard-Satisfiability-Selman-Levesque",
            "title": {
                "fragments": [],
                "text": "A New Method for Solving Hard Satisfiability Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "A greedy local search procedure called GSAT is introduced for solving propositional satisfiability problems and its good performance suggests that it may be advantageous to reformulate reasoning tasks that have traditionally been viewed as theorem-proving problems as model-finding tasks."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684799"
                        ],
                        "name": "T. St\u00fctzle",
                        "slug": "T.-St\u00fctzle",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "St\u00fctzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. St\u00fctzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 313,
                                "start": 278
                            }
                        ],
                        "text": "4 GLS for the Most Probable Explanation (MPE) Problem GLS+ is a guided local search algorithm for solving the Most Probable Explanation (MPE) problem in discrete-valued graphical models, that is, the problem of finding the variable instantiation with maximal overall likelihood (Hutter et al., 2005; Hutter, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 182
                            }
                        ],
                        "text": "As a brief reminder, GLS+ (with 1 binary and 4 numerical parameters) is a dynamic local search algorithm for solving the Most Probable Explanation (MPE) problem in Bayesian networks (Hutter et al., 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 240414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "313409d0bbc130cc5af19a86e39415f5531edbc2",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Finding most probable explanations (MPEs) in graphical models, such as Bayesian belief networks, is a fundamental problem in reasoning under uncertainty, and much effort has been spent on developing effective algorithms for this NP-hard problem. Stochastic local search (SLS) approaches to MPE solving have previously been explored, but were found to be not competitive with state-of-the-art branch & bound methods. In this work, we identify the shortcomings of earlier SLS algorithms for the MPE problem and demonstrate how these can be overcome, leading to an SLS algorithm that substantially improves the state-of-the-art in solving hard networks with many variables, large domain sizes, high degree, and, most importantly, networks with high induced width."
            },
            "slug": "Efficient-Stochastic-Local-Search-for-MPE-Solving-Hutter-Hoos",
            "title": {
                "fragments": [],
                "text": "Efficient Stochastic Local Search for MPE Solving"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work identifies the shortcomings of earlier SLS algorithms for the MPE problem and demonstrates how these can be overcome, leading to an SLS algorithm that substantially improves the state-of-the-art in solving hard networks with many variables, large domain sizes, high degree, and, most importantly, networks with high induced width."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690903"
                        ],
                        "name": "M. Birattari",
                        "slug": "M.-Birattari",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Birattari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Birattari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684799"
                        ],
                        "name": "T. St\u00fctzle",
                        "slug": "T.-St\u00fctzle",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "St\u00fctzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. St\u00fctzle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2036913"
                        ],
                        "name": "L. Paquete",
                        "slug": "L.-Paquete",
                        "structuredName": {
                            "firstName": "Lu\u00eds",
                            "lastName": "Paquete",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Paquete"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1961192"
                        ],
                        "name": "Klaus Varrentrapp",
                        "slug": "Klaus-Varrentrapp",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Varrentrapp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klaus Varrentrapp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 112
                            }
                        ],
                        "text": "In particular, we would like to compare the methods studied in this thesis to racing algorithms, such as F-Race (Birattari et al., 2002; Birattari, 2004) and iterated F-Race (Balaprakash et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 20
                            }
                        ],
                        "text": "2 Racing algorithms (Maron and Moore, 1994; Birattari et al., 2002; Birattari, 2005; Balaprakash et al., 2007) emphasize using as few problem instances as possible to reliably choose among a fixed set of parameter configurations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 38
                            }
                        ],
                        "text": "Prominent competitors, such as F-Race (Birattari et al., 2002; Birattari, 2004), do not admit such a result."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 39
                            }
                        ],
                        "text": ", 2001) and overtuning in optimization (Birattari et al., 2002; Birattari, 2005; Hutter et al., 2007b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5194351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "64247308dd5d534d37174feb6315043b233e49a8",
            "isKey": true,
            "numCitedBy": 597,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a racing procedure for finding, in a limited amount of time, a configuration of a metaheuristic that performs as good as possible on a given instance class of a combinatorial optimization problem. Taking inspiration from methods proposed in the machine learning literature for model selection through cross-validation, we propose a procedure that empirically evaluates a set of candidate configurations by discarding bad ones as soon as statistically sufficient evidence is gathered against them. We empirically evaluate our procedure using as an example the configuration of an ant colony optimization algorithm applied to the traveling salesman problem. The experimental results show that our procedure is able to quickly reduce the number of candidates, and allows to focus on the most promising ones."
            },
            "slug": "A-Racing-Algorithm-for-Configuring-Metaheuristics-Birattari-St\u00fctzle",
            "title": {
                "fragments": [],
                "text": "A Racing Algorithm for Configuring Metaheuristics"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A procedure that empirically evaluates a set of candidate configurations by discarding bad ones as soon as statistically sufficient evidence is gathered against them and allows to focus on the most promising ones is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726733"
                        ],
                        "name": "Ron Kohavi",
                        "slug": "Ron-Kohavi",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Kohavi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ron Kohavi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34929449"
                        ],
                        "name": "George H. John",
                        "slug": "George-H.-John",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "John",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George H. John"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 144
                            }
                        ],
                        "text": "\u2026(Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al., 2003), protein\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 199
                            }
                        ],
                        "text": ", 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7594891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "20c25424a20bc4f88accb35fbfd98d2ca7ffc525",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automatic-Parameter-Selection-by-Minimizing-Error-Kohavi-John",
            "title": {
                "fragments": [],
                "text": "Automatic Parameter Selection by Minimizing Estimated Error"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957256"
                        ],
                        "name": "W. Macready",
                        "slug": "W.-Macready",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Macready",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Macready"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 101
                            }
                        ],
                        "text": "One citation typically used in support of this argument is the so-called No Free Lunch (NFL) Theorem (Wolpert and Macready, 1997)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5553697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8315dff3d304baf47c025f4b33535b9d693350c1",
            "isKey": false,
            "numCitedBy": 9370,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of \"no free lunch\" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori \"head-to-head\" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms."
            },
            "slug": "No-free-lunch-theorems-for-optimization-Wolpert-Macready",
            "title": {
                "fragments": [],
                "text": "No free lunch theorems for optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving and a number of \"no free lunch\" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Evol. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 176
                            }
                        ],
                        "text": "Similar to lab experiments in the natural sciences, empirical studies can help build intuition and inspire new theoretical results, such as probabilistic algorithm guarantees (Hoos, 1999b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 618923,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "a4a086aaa6b84dcc21a3c787ee5d4e52c105205e",
            "isKey": false,
            "numCitedBy": 202,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic local search (SLS) algorithms for the propositional satisfiability problem (SAT) have been successfully applied to solve suitably encoded search problems from various domains. One drawback of these algorithms is that they are usually incomplete. We refine the notion of incompleteness for stochastic decision algorithms by introducing the notion of \"probabilistic asymptotic completeness\" (PAC) and prove for a number of well-known SLS algorithms whether or not they have this property. We also give evidence for the practical impact of the PAC property and show how to achieve the PAC property and significantly improved performance in practice for some of the most powerful SLS algorithms for SAT, using a simple and general technique called \"random walk extension\"."
            },
            "slug": "On-the-Run-time-Behaviour-of-Stochastic-Local-for-Hoos",
            "title": {
                "fragments": [],
                "text": "On the Run-time Behaviour of Stochastic Local Search Algorithms for SAT"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The notion of incompleteness for stochastic decision algorithms is refined by introducing the notion of \"probabilistic asymptotic completeness\" (PAC) and it is proved for a number of well-known SLS algorithms whether or not they have this property."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072163"
                        ],
                        "name": "Tom Carchrae",
                        "slug": "Tom-Carchrae",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Carchrae",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Carchrae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144778460"
                        ],
                        "name": "J. Christopher Beck",
                        "slug": "J.-Christopher-Beck",
                        "structuredName": {
                            "firstName": "J. Christopher",
                            "lastName": "Beck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Christopher Beck"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7953876,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fa7aac52fd53b65b96e5f6fcc3045692c42d30dd",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the question of allocating computational resources among a set of algorithms to achieve the best performance on scheduling problems. Our primary motivation in addressing this problem is to reduce the expertise needed to apply optimization technology. Therefore, we investigate algorithm control techniques that make decisions based only on observations of the improvement in solution quality achieved by each algorithm. We call our approach \u201clow knowledge\u201d since it does not rely on complex prediction models, either of the problem domain or of algorithm behavior. We show that a low\u2010knowledge approach results in a system that achieves significantly better performance than all of the pure algorithms without requiring additional human expertise. Furthermore the low\u2010knowledge approach achieves performance equivalent to a perfect high\u2010knowledge classification approach."
            },
            "slug": "APPLYING-MACHINE-LEARNING-TO-LOW\u2010KNOWLEDGE-CONTROL-Carchrae-Beck",
            "title": {
                "fragments": [],
                "text": "APPLYING MACHINE LEARNING TO LOW\u2010KNOWLEDGE CONTROL OF OPTIMIZATION ALGORITHMS"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper investigates algorithm control techniques that make decisions based only on observations of the improvement in solution quality achieved by each algorithm, and shows that a low knowledge approach results in a system that achieves significantly better performance than all of the pure algorithms without requiring additional human expertise."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Intell."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697528"
                        ],
                        "name": "Y. Diao",
                        "slug": "Y.-Diao",
                        "structuredName": {
                            "firstName": "Yixin",
                            "lastName": "Diao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Diao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3310240"
                        ],
                        "name": "F. Eskesen",
                        "slug": "F.-Eskesen",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Eskesen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Eskesen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34568978"
                        ],
                        "name": "S. Froehlich",
                        "slug": "S.-Froehlich",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Froehlich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Froehlich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716649"
                        ],
                        "name": "J. Hellerstein",
                        "slug": "J.-Hellerstein",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Hellerstein",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hellerstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2325003"
                        ],
                        "name": "L. Spainhower",
                        "slug": "L.-Spainhower",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Spainhower",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Spainhower"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1745484"
                        ],
                        "name": "M. Surendra",
                        "slug": "M.-Surendra",
                        "structuredName": {
                            "firstName": "Maheswaran",
                            "lastName": "Surendra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Surendra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 150
                            }
                        ],
                        "text": "\u2026learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al., 2003), protein folding (Thachuk et al., 2007), formal verification (Hutter et al., 2007a), and even in areas far outside of computer\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 358,
                                "start": 339
                            }
                        ],
                        "text": ", 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al., 2003), protein folding (Thachuk et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 24934246,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25acbf96a803b54c0252de0aff6f3565f0a13c1a",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimizing configuration parameters is time-consuming and skills-intensive. This paper proposes a generic approach to automating this task. By generic, we mean that the approach is relatively independent of the target system for which the optimization is done. Our approach uses online adjustment of configuration parameters to discover the system\u2019s performance characteristics. Doing so creates two challenges: (1) handling interdependencies between configuration parameters and (2) minimizing the deleterious effects on production workload while the optimization is underway. Our approach addresses (1) by including in the architecture a rule-based component that handles interdependencies between configuration parameters. For (2), we use a feedback mechanism for online optimization that searches the parameter space in a way that generally avoids poor performance at intermediate steps. Our studies of a DB2 Universal Database Server under an e-commerce workload indicate that our approach can be effective in practice."
            },
            "slug": "Generic-Online-Optimization-of-Multiple-Parameters-Diao-Eskesen",
            "title": {
                "fragments": [],
                "text": "Generic Online Optimization of Multiple Configuration Parameters with Application to a Database Server"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The approach uses online adjustment of configuration parameters to discover the system\u2019s performance characteristics and uses a feedback mechanism for online optimization that searches the parameter space in a way that generally avoids poor performance at intermediate steps."
            },
            "venue": {
                "fragments": [],
                "text": "DSOM"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770755"
                        ],
                        "name": "Kalev Kask",
                        "slug": "Kalev-Kask",
                        "structuredName": {
                            "firstName": "Kalev",
                            "lastName": "Kask",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kalev Kask"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751239"
                        ],
                        "name": "R. Dechter",
                        "slug": "R.-Dechter",
                        "structuredName": {
                            "firstName": "Rina",
                            "lastName": "Dechter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dechter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2057286,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2227c864c9408214b025e0a949a12077fd94e3ef",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper evaluates empirically the suitability of Stochastic Local Search algorithms (SLS) for nding most probable explanations in Bayesian networks. SLS algorithms (e.g., GSAT, WSAT [16]) have recently proven to be highly e ective in solving complex constraint-satisfaction and satis ability problems which cannot be solved by traditional search schemes. Our experiments investigate the applicability of this scheme to probabilistic optimization problems. Speci cally, we show that algorithms combining hill-climbing steps with stochastic steps (guided by the network's probability distribution) called G+StS, outperform pure hill-climbing search, pure stochastic simulation search, as well as simulated annealing. In addition, variants of G+StS that are augmented on top of alternative approximation methods are shown to be particularly e ective."
            },
            "slug": "Stochastic-Local-Search-for-Bayesian-Networks-Kask-Dechter",
            "title": {
                "fragments": [],
                "text": "Stochastic Local Search for Bayesian Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Algorithms combining hill-climbing steps with stochastic steps (guided by the network's probability distribution) called G+StS, outperform pure hill-Climbing search, pure Stochastic simulation search, as well as simulated annealing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3072163"
                        ],
                        "name": "Tom Carchrae",
                        "slug": "Tom-Carchrae",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Carchrae",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom Carchrae"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144778460"
                        ],
                        "name": "J. Christopher Beck",
                        "slug": "J.-Christopher-Beck",
                        "structuredName": {
                            "firstName": "J. Christopher",
                            "lastName": "Beck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Christopher Beck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14726125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50d3c1a1c37a7844ec9411eb7ad8d8f7623abd7f",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the question of allocating computational resources among a set of algorithms in order to achieve the best performance on a scheduling problem instance. Our primary motivation in addressing this problem is to reduce the expertise needed to apply constraint technology. Therefore, we investigate algorithm control techniques that make decision based only on observations of the improvement in solution quality achieved by each algorithm. We call our approach \"low-knowledge\" since it does not rely on complex prediction models. We show that such an approach results in a system that achieves significantly better performance than all of the pure algorithms without requiring additional human expertise. Furthermore the low knowledge approach achieves performance equivalent to a perfect high-knowledge classification approach."
            },
            "slug": "Low-Knowledge-Algorithm-Control-Carchrae-Beck",
            "title": {
                "fragments": [],
                "text": "Low-Knowledge Algorithm Control"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper investigates algorithm control techniques that make decision based only on observations of the improvement in solution quality achieved by each algorithm, and shows that such an approach results in a system that achieves significantly better performance than all of the pure algorithms without requiring additional human expertise."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118096690"
                        ],
                        "name": "Donald R. Jones",
                        "slug": "Donald-R.-Jones",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Jones",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald R. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815604"
                        ],
                        "name": "M. Schonlau",
                        "slug": "M.-Schonlau",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Schonlau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schonlau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145228938"
                        ],
                        "name": "W. Welch",
                        "slug": "W.-Welch",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Welch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Welch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 121
                            }
                        ],
                        "text": "On the model-based side, the most prominent approaches are variants of the efficient global optimization (EGO) algorithm (Jones et al., 1998), which we discussed in Section 9."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 124
                            }
                        ],
                        "text": "4, it is not compatible with transformations of the response variable, which are often crucial for strong model performance (Jones et al., 1998; Huang et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 29
                            }
                        ],
                        "text": "145\n10.1 Diagnostic plots of Jones et al. (1998) for scenario CMAES-SPHERE . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 42
                            }
                        ],
                        "text": ", 2002), negative inverse transformations (Jones et al., 1998), and square root or cube root transformations (Leyton-Brown, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 148
                            }
                        ],
                        "text": "\u2026a response surface model\nEGO Efficient Global Optimization, a model-based approach for deterministic continuous blackbox optimization described by Jones et al. (1998)\nEmpirical Cost Statistic c\u0302N (\u03b8), empirical statistic of the cost distribution c(\u03b8) based on N samples\nGP Model Regression model\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 156
                            }
                        ],
                        "text": "For example, they can be used to determine how much algorithm response depends on the setting of single parameters, and to which degree parameters interact (Jones et al., 1998; Santner et al., 2003; Bartz-Beielstein, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 87
                            }
                        ],
                        "text": "Transformations that have been suggested in the literature include log transformations (Jones et al., 1998; Williams et al., 2000; Huang et al., 2006; Leyton-Brown et al., 2002), negative inverse transformations (Jones et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13068209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "daa63f57c3fbe994c4356f8d986a22e696e776d2",
            "isKey": true,
            "numCitedBy": 5764,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome."
            },
            "slug": "Efficient-Global-Optimization-of-Expensive-Jones-Schonlau",
            "title": {
                "fragments": [],
                "text": "Efficient Global Optimization of Expensive Black-Box Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering and shows how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule."
            },
            "venue": {
                "fragments": [],
                "text": "J. Glob. Optim."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144486305"
                        ],
                        "name": "O. Mengshoel",
                        "slug": "O.-Mengshoel",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Mengshoel",
                            "middleNames": [
                                "Jakob"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Mengshoel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 73
                            }
                        ],
                        "text": "Since PARAMILS is a local search method, existing theoretical frameworks (see, e.g., Hoos, 2002b; Mengshoel, 2008), could in principle be used for its analysis."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1507354,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "015393a64c374072fbc16e9a88866745645089ee",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Understanding-the-role-of-noise-in-stochastic-local-Mengshoel",
            "title": {
                "fragments": [],
                "text": "Understanding the role of noise in stochastic local search: Analysis and experiments"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3348225"
                        ],
                        "name": "V. Cicirello",
                        "slug": "V.-Cicirello",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Cicirello",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Cicirello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111046427"
                        ],
                        "name": "Stephen F. Smith",
                        "slug": "Stephen-F.-Smith",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smith",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen F. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16954414,
            "fieldsOfStudy": [
                "Business",
                "Computer Science"
            ],
            "id": "d57bd1e113d7c47782c3933fdaf96dcbbf4b54e8",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "The success of stochastic algorithms is often due to their ability to effectively amplify the performance of search heuristics. This is certainly the case with stochastic sampling algorithms such as heuristic-biased stochastic sampling (HBSS) and value-biased stochastic sampling (VBSS), wherein a heuristic is used to bias a stochastic policy for choosing among alternative branches in the search tree. One complication in getting the most out of algorithms like HBSS and VBSS in a given problem domain is the need to identify the most effective search heuristic. In many domains, the relative performance of various heuristics tends to vary across different problem instances and no single heuristic dominates. In such cases, the choice of any given heuristic will be limiting and it would be advantageous to gain the collective power of several heuristics. Toward this goal, this paper describes a framework for integrating multiple heuristics within a stochastic sampling search algorithm. In its essence, the framework uses online-generated statistical models of the search performance of different base heuristics to select which to employ on each subsequent iteration of the search. To estimate the solution quality distribution resulting from repeated application of a strong heuristic within a stochastic search, we propose the use of models from extreme value theory (EVT). Our EVT-motivated approach is validated on the NP-Hard problem of resource-constrained project scheduling with time windows (RCPSP/max). Using VBSS as a base stochastic sampling algorithm, the integrated use of a set of project scheduling heuristics is shown to be competitive with the current best known heuristic algorithm for RCPSP/max and in some cases even improves upon best known solutions to difficult benchmark instances."
            },
            "slug": "Heuristic-Selection-for-Stochastic-Search-Modeling-Cicirello-Smith",
            "title": {
                "fragments": [],
                "text": "Heuristic Selection for Stochastic Search Optimization: Modeling Solution Quality by Extreme Value Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Using VBSS as a base stochastic sampling algorithm, the integrated use of a set of project scheduling heuristics is shown to be competitive with the current best known heuristic algorithm for RCPSP/max and in some cases even improves upon best known solutions to difficult benchmark instances."
            },
            "venue": {
                "fragments": [],
                "text": "CP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143746090"
                        ],
                        "name": "J. Rice",
                        "slug": "J.-Rice",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Rice",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rice"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 7554653,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "d8ae81043e96f0583501f7021abaa17db1a4be99",
            "isKey": false,
            "numCitedBy": 632,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Algorithm-Selection-Problem-Rice",
            "title": {
                "fragments": [],
                "text": "The Algorithm Selection Problem"
            },
            "venue": {
                "fragments": [],
                "text": "Adv. Comput."
            },
            "year": 1976
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2658890"
                        ],
                        "name": "Marius Muja",
                        "slug": "Marius-Muja",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Muja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marius Muja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35238678"
                        ],
                        "name": "D. Lowe",
                        "slug": "D.-Lowe",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Lowe",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lowe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 141
                            }
                        ],
                        "text": "\u2026optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou,\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 159
                            }
                        ],
                        "text": ", 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7317448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "35d81066cb1369acf4b6c5117fcbb862be2af350",
            "isKey": false,
            "numCitedBy": 2850,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "For many computer vision problems, the most time consuming component consists of nearest neighbor matching in high-dimensional spaces. There are no known exact algorithms for solving these high-dimensional problems that are faster than linear search. Approximate algorithms are known to provide large speedups with only minor loss in accuracy, but many such algorithms have been published with only minimal guidance on selecting an algorithm and its parameters for any given problem. In this paper, we describe a system that answers the question, \u201cWhat is the fastest approximate nearest-neighbor algorithm for my data?\u201d Our system will take any given dataset and desired degree of precision and use these to automatically determine the best algorithm and parameter values. We also describe a new algorithm that applies priority search on hierarchical k-means trees, which we have found to provide the best known performance on many datasets. After testing a range of alternatives, we have found that multiple randomized k-d trees provide the best performance for other datasets. We are releasing public domain code that implements these approaches. This library provides about one order of magnitude improvement in query time over the best previously available software and provides fully automated parameter selection."
            },
            "slug": "Fast-Approximate-Nearest-Neighbors-with-Automatic-Muja-Lowe",
            "title": {
                "fragments": [],
                "text": "Fast Approximate Nearest Neighbors with Automatic Algorithm Configuration"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A system that answers the question, \u201cWhat is the fastest approximate nearest-neighbor algorithm for my data?\u201d and a new algorithm that applies priority search on hierarchical k-means trees, which is found to provide the best known performance on many datasets."
            },
            "venue": {
                "fragments": [],
                "text": "VISAPP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48942559"
                        ],
                        "name": "E. Nudelman",
                        "slug": "E.-Nudelman",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Nudelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Nudelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701353"
                        ],
                        "name": "Y. Shoham",
                        "slug": "Y.-Shoham",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Shoham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shoham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 63
                            }
                        ],
                        "text": "1 Instance Features Existing work on empirical hardness models (Leyton-Brown et al., 2002; Nudelman et al., 2004; Hutter et al., 2006; Xu et al., 2007a; Leyton-Brown et al., 2009) convincingly demonstrated that it is possible to predict algorithm runtime based on features of the problem instance to be solved."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9223857,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2609f23364b0611e75d69ac5d311ebe6fcd75f44",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 113,
            "paperAbstract": {
                "fragments": [],
                "text": "Is it possible to predict how long an algorithm will take to solve a previously-unseen instance of an NP-complete problem? If so, what uses can be found for models that make such predictions? This article provides answers to these questions and evaluates the answers experimentally.\n We propose the use of supervised machine learning to build models that predict an algorithm's runtime given a problem instance. We discuss the construction of these models and describe techniques for interpreting them to gain understanding of the characteristics that cause instances to be hard or easy. We also present two applications of our models: building algorithm portfolios that outperform their constituent algorithms, and generating test distributions that emphasize hard problems.\n We demonstrate the effectiveness of our techniques in a case study of the combinatorial auction winner determination problem. Our experimental results show that we can build very accurate models of an algorithm's running time, interpret our models, build an algorithm portfolio that strongly outperforms the best single algorithm, and tune a standard benchmark suite to generate much harder problem instances."
            },
            "slug": "Empirical-hardness-models:-Methodology-and-a-case-Leyton-Brown-Nudelman",
            "title": {
                "fragments": [],
                "text": "Empirical hardness models: Methodology and a case study on combinatorial auctions"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The use of supervised machine learning is proposed to build models that predict an algorithm's runtime given a problem instance and techniques for interpreting them are described to gain understanding of the characteristics that cause instances to be hard or easy."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744679"
                        ],
                        "name": "B. Selman",
                        "slug": "B.-Selman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Selman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Selman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690271"
                        ],
                        "name": "Henry A. Kautz",
                        "slug": "Henry-A.-Kautz",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Kautz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Henry A. Kautz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055610874"
                        ],
                        "name": "Bram Cohen",
                        "slug": "Bram-Cohen",
                        "structuredName": {
                            "firstName": "Bram",
                            "lastName": "Cohen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bram Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 61
                            }
                        ],
                        "text": "SATENSTEIN draws on components from WalkSAT-based algorithms (Selman et al., 1996), dynamic local search algorithms (Hutter et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3215289,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "754b96e00671c74de0add9df1ef60dcf21160483",
            "isKey": false,
            "numCitedBy": 723,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "It has recently been shown that local search is surprisingly good at nding satisfying assignments for certain classes of CNF formulas 24]. In this paper we demonstrate that the power of local search for satissability testing can be further enhanced by employinga new strategy, called \\mixed random walk\", for escaping from local minima. We present experimental results showing how this strategy allows us to handle formulas that are substantially larger than those that can be solved with basic local search. We also present a detailed comparison of our random walk strategy with simulated annealing. Our results show that mixed random walk is the superior strategy on several classes of computationally diicult problem instances. Finally, we present results demonstrating the eeectiveness of local search with walk for solving circuit synthesis and diagnosis problems."
            },
            "slug": "Local-search-strategies-for-satisfiability-testing-Selman-Kautz",
            "title": {
                "fragments": [],
                "text": "Local search strategies for satisfiability testing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The power of local search for satissability testing can be further enhanced by employing a new strategy, called mixed random walk, for escaping from local minima, which allows us to handle formulas that are substantially larger than those that can be solved with basic local search."
            },
            "venue": {
                "fragments": [],
                "text": "Cliques, Coloring, and Satisfiability"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741789"
                        ],
                        "name": "E. Burke",
                        "slug": "E.-Burke",
                        "structuredName": {
                            "firstName": "Edmund",
                            "lastName": "Burke",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Burke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145528658"
                        ],
                        "name": "G. Kendall",
                        "slug": "G.-Kendall",
                        "structuredName": {
                            "firstName": "Graham",
                            "lastName": "Kendall",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kendall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39655711"
                        ],
                        "name": "J. Newall",
                        "slug": "J.-Newall",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Newall",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Newall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144988281"
                        ],
                        "name": "E. Hart",
                        "slug": "E.-Hart",
                        "structuredName": {
                            "firstName": "Emma",
                            "lastName": "Hart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708975"
                        ],
                        "name": "P. Ross",
                        "slug": "P.-Ross",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2265549"
                        ],
                        "name": "S. Schulenburg",
                        "slug": "S.-Schulenburg",
                        "structuredName": {
                            "firstName": "Sonia",
                            "lastName": "Schulenburg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Schulenburg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18097470,
            "fieldsOfStudy": [
                "Business",
                "Biology"
            ],
            "id": "3e19793ff253851d7d0a48b5edd71c596f010e14",
            "isKey": false,
            "numCitedBy": 743,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter introduces and overviews an emerging methodology in search and optimisation. One of the key aims of these new approaches, which have been termed hyperheuristics, is to raise the level of generality at which optimisation systems can operate. An objective is that hyper-heuristics will lead to more general systems that are able to handle a wide range of problem domains rather than current meta-heuristic technology which tends to be customised to a particular problem or a narrow class of problems. Hyper-heuristics are broadly concerned with intelligently choosing the right heuristic or algorithm in a given situation. Of course, a hyper-heuristic can be (often is) a (meta-)heuristic and it can operate on (meta-)heuristics. In a certain sense, a hyper-heuristic works at a higher level when compared with the typical application of meta-heuristics to optimisation problems, i.e., a hyper-heuristic could be thought of as a (meta)-heuristic which operates on lower level (meta-)heuristics. In this chapter we will introduce the idea and give a brief history of this emerging area. In addition, we will review some of the latest work to be published in the field."
            },
            "slug": "Hyper-Heuristics:-An-Emerging-Direction-in-Modern-Burke-Kendall",
            "title": {
                "fragments": [],
                "text": "Hyper-Heuristics: An Emerging Direction in Modern Search Technology"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This chapter introduces and overviews an emerging methodology in search and optimisation called hyperheuristics, which aims to raise the level of generality at which optimisation systems can operate and will lead to more general systems that are able to handle a wide range of problem domains."
            },
            "venue": {
                "fragments": [],
                "text": "Handbook of Metaheuristics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145689002"
                        ],
                        "name": "David A. McAllester",
                        "slug": "David-A.-McAllester",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "McAllester",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. McAllester"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744679"
                        ],
                        "name": "B. Selman",
                        "slug": "B.-Selman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Selman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Selman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690271"
                        ],
                        "name": "Henry A. Kautz",
                        "slug": "Henry-A.-Kautz",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Kautz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Henry A. Kautz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2574780,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "508790920303862aa12dbca12abbd9af8bea4ae5",
            "isKey": false,
            "numCitedBy": 448,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well known that the performance of a stochastic local search procedure depends upon the setting of its noise parameter, and that the optimal setting varies with the problem distribution. It is therefore desirable to develop general priniciples for tuning the procedures. We present two statistical measures of the local search process that allow one to quickly find the optimal noise settings. These properties are independent of the fine details of the local search strategies, and appear to be relatively independent of the structure of the problem domains. We applied these principles to the problem of evaluating new search heuristics, and discovered two promising new strategies."
            },
            "slug": "Evidence-for-Invariants-in-Local-Search-McAllester-Selman",
            "title": {
                "fragments": [],
                "text": "Evidence for Invariants in Local Search"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This work presents two statistical measures of the local search process that allow one to quickly find the optimal noise settings, and applies these principles to the problem of evaluating new search heuristics, and discovered two promising new strategies."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49830161"
                        ],
                        "name": "C. Fawcett",
                        "slug": "C.-Fawcett",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Fawcett",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Fawcett"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801772"
                        ],
                        "name": "Marco Chiarandini",
                        "slug": "Marco-Chiarandini",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Chiarandini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marco Chiarandini"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7902265,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fe67ab8de2bb78790ba7d58b82dede09dc31777",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Timetabling tasks form a widely studied type of resource scheduling problem, with important real-world applications in schools, universities and other educational settings. In this work, we focus on postenrollment course timetabling, the problem that was covered by Track 2 of the recent 2nd International Timetabling Competition (ITC2007). Following an approach that makes strong use of automated exploration of a large design space of modular and highly parameterised stochastic local search algorithms for this problem, we produced a solver that placed third in Track 2 of ITC2007. In subsequent work, we further improved both the solver framework and the automated algorithm design procedure, and obtained a solver that achieves consistently better performance than the top-ranked solver from the competition and represents a substantial improvement in the state of the art for post-enrollment course timetabling."
            },
            "slug": "An-automatically-configured-modular-algorithm-for-Fawcett-Hoos",
            "title": {
                "fragments": [],
                "text": "An automatically configured modular algorithm for post enrollment course timetabling"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work improves both the solver framework and the automated algorithm design procedure, and obtained a solver that achieves consistently better performance than the top-ranked solver from the competition and represents a substantial improvement in the state of the art for post-enrollment course timetabling."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716445"
                        ],
                        "name": "F. Southey",
                        "slug": "F.-Southey",
                        "structuredName": {
                            "firstName": "Finnegan",
                            "lastName": "Southey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Southey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 164
                            }
                        ],
                        "text": "In particular, we automatically constructed different instantiations of the SPEAR algorithm and thereby substantially improved the state of the art for two sets of SAT-encoded industrial verification problems (Hutter et al., 2007a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 15
                            }
                        ],
                        "text": "Both SPEAR and SATENSTEIN were configured using ParamILS, one of the automated configuration procedures we introduce in this thesis (see Chapter 5)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "123 8.5 Performance summary of SATenstein-LS; reproduced from (KhudaBukhsh\net al., 2009) . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 125\nix\n9.1 Summary of notation . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 136 9.2 SPO and SPO+ algorithm parameters . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 120
                            }
                        ],
                        "text": "81 5.3.3 Experimental Evaluation . . . . . . . . . . . . . . . . . . . . . . . . 84 5.4 Configuration of SAPS, GLS+ and SAT4J . . . . . . . . . . . . . . . . . . . 86 5.5 Chapter Summary . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "99 6.6 MiniSAT 2.0 vs SPEAR configured for specific benchmark sets . . . . . . . . 100\n7.1 Speedup of RANDOMSEARCH due to adaptive capping . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "37\n3.1.1 Propostitional Satisfiability (SAT) . . . . . . . . . . . . . . . . . . . 38 3.1.2 Mixed Integer Programming (MIP) . . . . . . . . . . . . . . . . . . 38\n3.2 Target Algorithms . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 829,
                                "start": 826
                            }
                        ],
                        "text": "In the context of multiple instances, these models can also be used to predict the cost distribution for a combination of a parameter configuration and an instance\nRMSE Root mean squared error, a measure of model quality \u2208 [0,\u221e) (the lower the better); RMSE measures the root of the squared differences between model predictions and validation costs, see Definition 14 on page 148\nEIC quality A measure of model quality \u2208 [\u22121, 1] (the higher the better); it measures the Spearman correlation coefficient between expected improvement based on model predictions, and validation costs, see Definition 13 on page 148\nQuality of predictive ranks A measure of model quality \u2208 [\u22121, 1] (the higher the better); it measures the Spearman correlation coefficient between model predictions and validation costs, see Definition 12 on page 148\nxvii\nSAT The propositional satisfiability problem; see Section 3.1.1\nSPO Sequential Parameter Optimization, a model-based framework for stochastic continuous blackbox optimization first described by Bartz-Beielstein et al. (2005)\nSKO Sequential Kriging Optimization, a model-based approach for stochastic continuous blackbox optimization described by Huang et al. (2006)\nStochastic Blackbox Optimization Problem Similar to a deterministic Blackbox Optimization Problem, but the observations we make at a query point, \u03b8, are samples of a cost distribution, P{\u03b8}, rather than deterministic values of a function, f(\u03b8); see Section 1.2.1\nTarget Algorithm A parameterized algorithm whose parameters are to be optimized\nTraining Performance of a Configurator at Time t Empirical cost estimate of the configurator\u2019s incumbent at time t, \u03b8inc(t), across the runs the configurator performed with \u03b8inc(t)\nTest Performance of a Configurator at Time t Empirical cost estimate of the configurator\u2019s incumbent at time t, \u03b8inc(t), for runs with \u03b8inc(t) on test set\nTraining Set Set of \u3008problem instance, pseudo-random number seed\u3009 combinations used during configuration\nTest Set Set of \u3008problem instance, pseudo-random number seed\u3009 combinations used for offline test purposes; disjoint from training set\nValidation Cost cvalid(\u03b8), empirical cost estimate of parameter configuration \u03b8 using all \u3008instance, seed\u3009 combinations in the training set\nxviii"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 191
                            }
                        ],
                        "text": "Using our procedures\u2014to the best of our knowledge still the only ones applicable to these complex configuration tasks\u2014we configured state-of-the-art tree search and local search algorithms for SAT, as well as CPLEX, the most widely-used commercial optimization tool for solving mixed integer programs (MIP)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "96 6.3 SPEAR default vs SPEAR configured for SAT competition, on BMC and SWV ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "86 5.4 Configuration of GLS+, SAPS, and SAT4J . . . . . . . . . . . . . . . . . . 87\n6.1 Summary of results for configuring SPEAR . . . . . . . . . . . . . . . . . . 101\n7.1 Speedup of RANDOMSEARCH due to adaptive capping . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 117
                            }
                        ],
                        "text": "94 6.3.1 Parameterization of SPEAR . . . . . . . . . . . . . . . . . . . . . . . 94 6.3.2 Configuration for the 2007 SAT Competition . . . . . . . . . . . . . 95 6.3.3 Configuration for Specific Applications . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 167
                            }
                        ],
                        "text": "Note that categorical parameters can be used to select and combine discrete building blocks of an algorithm (e.g., preprocessing and variable ordering heuristics in a SAT solver)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "209\n13.1.1 SAT . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 209 13.1.2 MIP . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 210 13.1.3 Generic Features . . . . . . . . . . . . . . . . . . . . . . . . . . . . 213 13.1.4 Principal Component Analysis (PCA) to Speed Up Learning . . . . . 213\n13.2 Modelling Cost Measures Defined Across Multiple Instances . . . . . . . . . 214 13.2.1 Gaussian Processes: Existing Work for Predicting Marginal Perfor-\nmance Across Instances . . . . . . . . . . . . . . . . . . . . . . . . 214 13.2.2 Random Forests: Prediction of Cost Measures Across Multiple Instances215\n13.3 Experimental Setup . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "88\n6.1 MiniSAT 2.0 vs SPEAR default . . . . . . . . . . . . . . . . . . . . . . . . . 93 6.2 SPEAR default vs SPEAR configured for SAT competition, on SAT competition instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 97
                            }
                        ],
                        "text": "43 3.3 Benchmark Sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 43 3.3.1 SAT Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 44 3.3.2 MIP Benchmarks . . . . . . . . . . . . . . . . . . . . . . . . . . . . 46 3.3.3 Test Functions for Continuous Global Optimization . . . . . . . . . . 48 3.4 Optimization Objective: Penalized Average Runtime . . . . . . . . . . . . . 48 3.5 Configuration Scenarios . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 49\n3.5.1 Set of Configuration Scenarios BLACKBOXOPT . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 196
                            }
                        ],
                        "text": "116\n8.2 Self-Configuration of PARAMILS . . . . . . . . . . . . . . . . . . . . . . . 120 8.3 Applications of PARAMILS by Other Researchers . . . . . . . . . . . . . . . 124\n8.3.1 Configuration of SATenstein . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 31
                            }
                        ],
                        "text": "38 3.2.1 Target Algorithms for SAT . . . . . . . . . . . . . . . . . . . . . . . 39 3.2.2 Target Algorithm for MIP . . . . . . . . . . . . . . . . . . . . . . . 42 3.2.3 CMA-ES for Global Continuous Function Optimization . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 56
                            }
                        ],
                        "text": "In particular, thanks to Dave Tompkins for help with UBCSAT, to Daniel Le Berre for help with SAT4J, to Belarmino Adenso-D\u0131\u0301az for providing the CALIBRA system and benchmark data, and to Theodore Allen for access to the SKO source code."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "We provide full details on the configuration of SPEAR in Chapter 6 and review the SATENSTEIN application in Section 8.3.1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The SATENSTEIN framework of local search algorithms for SAT by KhudaBukhsh et al. (2009) took this process to the extreme, combining a multitude of components from various existing local search algorithms."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 170
                            }
                        ],
                        "text": "Since PARAMILS performs an iterated local search using a one-exchange neighbourhood, it is very similar in spirit to local search methods for other problems, such as SAT (Selman et al., 1992; Hoos and St\u00fctzle, 2000; Schuurmans and Southey, 2001), CSP (Minton et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 193
                            }
                        ],
                        "text": "They often combine a multitude of approaches and feature a correspondingly large and structured parameter space (see, e.g., the aforementioned solver CPLEX or many state-of-the-art solvers for SAT)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 230,
                                "start": 227
                            }
                        ],
                        "text": "In many cases, we achieved improvements of orders of magnitude over the algorithm default, thereby substantially improving the state of the art in solving a broad range of problems, including industrially relevant instances of SAT and MIP."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 22
                            }
                        ],
                        "text": "206\n13.1 11 groups of SAT features; these were introduced by Nudelman et al. (2004) and Xu et al. (2008, 2009). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 13.2 Eight groups of features for the mixed integer programming problem. . . . . . . . . 212 13.3 RF model quality based on different features, for scenario SAPS-QCP . . . . . 218 13.4 RF model quality based on different features, for scenario SAPS-SWGCP . . . . 219 13.5 RF model quality based on different sets of instance features . . . . . . . . . 220 13.6 RF model quality based on different number of principal components . . . . 222 13.7 Predictions of runtime matrix . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The SAT instance features we use in Section 13.1 originated in this work.\nxxii\nPart I\nAlgorithm Configuration: The Problem\n\u2014in which we introduce and motivate the algorithm configuration problem and discuss related work\n1\nChapter 1\nIntroduction Civilization advances by extending the number of important operations which we can perform without thinking of them."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13432993,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "54a0826b1a909bf262a487b21e49671bbc777fd4",
            "isKey": true,
            "numCitedBy": 130,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Local-search-characteristics-of-incomplete-SAT-Schuurmans-Southey",
            "title": {
                "fragments": [],
                "text": "Local search characteristics of incomplete SAT procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15965671,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "e32bf05e1da65e7893aa0e5b3652b620858d2c5c",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic Local Search (SLS) algorithms are amongst the most effective approaches for solving hard and large propositional satisfiability (SAT) problems. Prominent and successful SLS algorithms for SAT, including many members of the WalkSAT and GSAT families of algorithms, tend to show highly regular behaviour when applied to hard SAT instances: The run-time distributions (RTDs) of these algorithms are closely approximated by exponential distributions. The deeper reasons for this regular behaviour are, however, essentially unknown. In this study we show that there are hard problem instances, <i>e.g.,</i> from the phase transition region of the widely studied class of Uniform Random 3-SAT instances, for which the RTDs for well-known SLS algorithms such as GWSAT or WalkSAT/SKC deviate substantially from exponential distributions. We investigate these irregular instances and show that the respective RTDs can be modelled using mixtures of exponential distributions. We present evidence that such mixture distributions reflect stagnation behaviour in the search process caused by \"traps\" in the underlying search spaces. This leads to the formulation of a new model of SLS behaviour as a simple Markov process. This model subsumes and extends earlier characterisations of SLS behaviour and provides plausible explanations for many empirical observations."
            },
            "slug": "A-mixture-model-for-the-behaviour-of-SLS-algorithms-Hoos",
            "title": {
                "fragments": [],
                "text": "A mixture-model for the behaviour of SLS algorithms for SAT"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that there are hard problem instances, e.g., from the phase transition region of the widely studied class of Uniform Random 3-SAT instances, for which the RTDs for well-known SLS algorithms such as GWSAT or WalkSAT/SKC deviate substantially from exponential distributions, and that the respective RTDs can be modelled using mixtures of exponential distributions."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152895903"
                        ],
                        "name": "Lin Xu",
                        "slug": "Lin-Xu",
                        "structuredName": {
                            "firstName": "Lin",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lin Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 88
                            }
                        ],
                        "text": "206\n13.1 11 groups of SAT features; these were introduced by Nudelman et al. (2004) and Xu et al. (2008, 2009). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 13.2 Eight groups of features for the mixed integer programming problem. . . . . . . . . 212 13.3 RF model quality\u2026"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 65
                            }
                        ],
                        "text": "Thus, the portfolio-based algorithm selection scheme of SATzilla (Xu et al., 2008) directly applies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "Joint publications with Lin Xu, Holger Hoos, and Kevin Leyton-Brown on per-instance algorithm selection (Xu et al., 2007b, 2008) fall into the same category."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 145
                            }
                        ],
                        "text": "Instead of applying PCA, we could have used feature selection methods (Guyon and Elisseeff, 2003), such as forward selection as done in SATzilla (Nudelman et al., 2004; Xu et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 182,
                                "start": 119
                            }
                        ],
                        "text": "This suggests splitting such heterogeneous sets of instances into more homogeneous subsets, using portfolio techniques (Gomes and Selman, 2001; Horvitz et al., 2001; Xu et al., 2008), or using per-instance algorithm configuration (Hutter et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 77
                            }
                        ],
                        "text": "In particular, per-instance algorithm selection techniques, such as SATzilla (Xu et al., 2008), and dynamic algorithm portfolios (Gagliolo and Schmidhuber, 2006) already use similar predictive models, but do not yet reason about the setting of algorithm parameters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10987043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72489f377f08ac720cd31d9eed20706abfed58bb",
            "isKey": false,
            "numCitedBy": 876,
            "numCiting": 99,
            "paperAbstract": {
                "fragments": [],
                "text": "It has been widely observed that there is no single \"dominant\" SAT solver; instead, different solvers perform best on different instances. Rather than following the traditional approach of choosing the best solver for a given class of instances, we advocate making this decision online on a per-instance basis. Building on previous work, we describe SATzilla, an automated approach for constructing per-instance algorithm portfolios for SAT that use so-called empirical hardness models to choose among their constituent solvers. This approach takes as input a distribution of problem instances and a set of component solvers, and constructs a portfolio optimizing a given objective function (such as mean runtime, percent of instances solved, or score in a competition). The excellent performance of SATzilla was independently verified in the 2007 SAT Competition, where our SATzilla07 solvers won three gold, one silver and one bronze medal. In this article, we go well beyond SATzilla07 by making the portfolio construction scalable and completely automated, and improving it by integrating local search solvers as candidate solvers, by predicting performance score instead of runtime, and by using hierarchical hardness models that take into account different types of SAT instances. We demonstrate the effectiveness of these new techniques in extensive experimental results on data sets including instances from the most recent SAT competition."
            },
            "slug": "SATzilla:-Portfolio-based-Algorithm-Selection-for-Xu-Hutter",
            "title": {
                "fragments": [],
                "text": "SATzilla: Portfolio-based Algorithm Selection for SAT"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "SATzilla is described, an automated approach for constructing per-instance algorithm portfolios for SAT that use so-called empirical hardness models to choose among their constituent solvers and is improved by integrating local search solvers as candidate solvers, by predicting performance score instead of runtime, and by using hierarchical hardness models that take into account different types of SAT instances."
            },
            "venue": {
                "fragments": [],
                "text": "J. Artif. Intell. Res."
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2835134"
                        ],
                        "name": "Enda Ridge",
                        "slug": "Enda-Ridge",
                        "structuredName": {
                            "firstName": "Enda",
                            "lastName": "Ridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Enda Ridge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2380005"
                        ],
                        "name": "D. Kudenko",
                        "slug": "D.-Kudenko",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Kudenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kudenko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15664433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30e26fa656a3a36f59b18c3e307b0b2300bf771d",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an in-depth Design of Experiments (DOE) methodology for the performance analysis of a stochastic heuristic. The heuristic under investigation is Max-Min Ant System (MMAS). for the Travelling Salesperson Problem (TSP). Specifically, the Response Surface Methodology is used to model and tune MMAS performance with regard to 10 tuning parameters, 2 problem characteristics and 2 performance metrics--solution quality and solution time. The accuracy of these predictions is methodically verified in a separate series of confirmation experiments. The two conflicting responses are simultaneously optimised using desirability functions. Recommendations on optimal parameter settings are made. The optimal parameters are methodically verified. The large number of degrees-of-freedom in the MMAS design are overcome with a Minimum Run Resolution V design. Publicly available algorithm and problem generator implementations are used throughout. The paper should therefore serve as an illustrative case study of the principled engineering of a stochastic heuristic."
            },
            "slug": "Tuning-the-Performance-of-the-MMAS-Heuristic-Ridge-Kudenko",
            "title": {
                "fragments": [],
                "text": "Tuning the Performance of the MMAS Heuristic"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "An in-depth Design of Experiments (DOE) methodology for the performance analysis of a stochastic heuristic for the Travelling Salesperson Problem (TSP) using the Response Surface Methodology."
            },
            "venue": {
                "fragments": [],
                "text": "SLS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 122
                            }
                        ],
                        "text": "Chapter 4 is based on joint work with Holger Hoos and Kevin Leyton-Brown, which is about to be submitted for publication (Hutter et al., 2009d)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 78
                            }
                        ],
                        "text": "Chapters 9 and 10 are primarily based on a conference publication at GECCO-09 (Hutter et al., 2009e), co-authored by Holger Hoos, Kevin Leyton-Brown, and Kevin Murphy, as well as a book chapter with the same co-authors and Thomas Bartz-Beielstein (Hutter et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 249,
                                "start": 229
                            }
                        ],
                        "text": "Adaptive capping is discussed in a comprehensive journal article on PARAMILS with Holger Hoos, Kevin Leyton-Brown, and Thomas Stu\u0308tzle, which also introduced the application to CPLEX and has been accepted for publication at JAIR (Hutter et al., 2009c)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 79
                            }
                        ],
                        "text": "Chapters 9 and 10 are primarily based on a conference publication at GECCO-09 (Hutter et al., 2009e), co-authored by Holger Hoos, Kevin Leyton-Brown, and Kevin Murphy, as well as a book chapter with the same co-authors and Thomas Bartz-Beielstein (Hutter et al., 2009a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1863673,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "eaa44ec117af3e45ffedd0028b3cc208469ab50f",
            "isKey": false,
            "numCitedBy": 96,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This work experimentally investigates model-based approaches for optimising the performance of parameterised randomised algorithms. We restrict our attention to procedures based on Gaussian process models, the most widely-studied family of models for this problem. We evaluated two approaches from the literature, and found that sequential parameter optimisation (SPO) [4] offered the most robust performance. We then investigated key design decisions within the SPO paradigm, characterising the performance consequences of each. Based on these findings, we propose a new version of SPO, dubbed SPO+, which extends SPO with a novel intensification procedure and log-transformed response values. Finally, in a domain for which performance results for other (model-free) parameter optimisation approaches are available, we demonstrate that SPO+ achieves state-of-the-art performance."
            },
            "slug": "An-experimental-investigation-of-model-based-SPO-Hutter-Hoos",
            "title": {
                "fragments": [],
                "text": "An experimental investigation of model-based parameter optimisation: SPO and beyond"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new version of SPO is proposed, dubbed SPO+, which extends SPO with a novel intensification procedure and log-transformed response values, and it is demonstrated that SPO+ achieves state-of-the-art performance."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1959925"
                        ],
                        "name": "J. Maturana",
                        "slug": "J.-Maturana",
                        "structuredName": {
                            "firstName": "Jorge",
                            "lastName": "Maturana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Maturana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1879539"
                        ],
                        "name": "\u00c1lvaro Fialho",
                        "slug": "\u00c1lvaro-Fialho",
                        "structuredName": {
                            "firstName": "\u00c1lvaro",
                            "lastName": "Fialho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c1lvaro Fialho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726003"
                        ],
                        "name": "F. Saubion",
                        "slug": "F.-Saubion",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Saubion",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Saubion"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69881991"
                        ],
                        "name": "Marc Schoenauer",
                        "slug": "Marc-Schoenauer",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Schoenauer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Schoenauer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69343681"
                        ],
                        "name": "M. Sebag",
                        "slug": "M.-Sebag",
                        "structuredName": {
                            "firstName": "Mich\u00e8le",
                            "lastName": "Sebag",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sebag"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 150
                            }
                        ],
                        "text": "In fact, such methods have already have been used to optimize the parameters of a dynamic multi-armed bandit approach for adaptive operator selection (Maturana et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15896429,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "8a9e72f12e9e59aed55a20d1356c4b4d2916b254",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of Adaptive Operator Selection is the on-line control of the choice of variation operators within Evolutionary Algorithms. The control process is based on two main components, the credit assignment, that defines the reward that will be used to evaluate the quality of an operator after it has been applied, and the operator selection mechanism, that selects one operator based on some operators qualities. Two previously developed Adaptive Operator Selection methods are combined here: Compass evaluates the performance of operators by considering not only the fitness improvements from parent to offspring, but also the way they modify the diversity of the population, and their execution time; Dynamic Multi-Armed Bandit proposes a selection strategy based on the well-known UCB algorithm, achieving a compromise between exploitation and exploration, while nevertheless quickly adapting to changes. Tests with the proposed method, called ExCoDyMAB, are carried out using several hard instances of the Satisfiability problem (SAT). Results show the good synergetic effect of combining both approaches."
            },
            "slug": "Extreme-compass-and-Dynamic-Multi-Armed-Bandits-for-Maturana-Fialho",
            "title": {
                "fragments": [],
                "text": "Extreme compass and Dynamic Multi-Armed Bandits for Adaptive Operator Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Two previously developed Adaptive Operator Selection methods are combined here: Compass evaluates the performance of operators by considering not only the fitness improvements from parent to offspring, but also the way they modify the diversity of the population, and their execution time."
            },
            "venue": {
                "fragments": [],
                "text": "2009 IEEE Congress on Evolutionary Computation"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2636214"
                        ],
                        "name": "Bryan A. Tolson",
                        "slug": "Bryan-A.-Tolson",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Tolson",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan A. Tolson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1817217"
                        ],
                        "name": "C. Shoemaker",
                        "slug": "C.-Shoemaker",
                        "structuredName": {
                            "firstName": "Christine",
                            "lastName": "Shoemaker",
                            "middleNames": [
                                "Annette"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Shoemaker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 268
                            }
                        ],
                        "text": "\u2026optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al., 2003), protein folding (Thachuk et al., 2007), formal verification (Hutter et al., 2007a), and even in areas far outside of computer science, such as water resource management (Tolson and Shoemaker, 2007)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 95
                            }
                        ],
                        "text": ", 2007a), and even in areas far outside of computer science, such as water resource management (Tolson and Shoemaker, 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 213965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca7fed61deacd767e07a43a49099a7eedf91e0ee",
            "isKey": false,
            "numCitedBy": 577,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "A new global optimization algorithm, dynamically dimensioned search (DDS), is introduced for automatic calibration of watershed simulation models. DDS is designed for calibration problems with many parameters, requires no algorithm parameter tuning, and automatically scales the search to find good solutions within the maximum number of user\u2010specified function (or model) evaluations. As a result, DDS is ideally suited for computationally expensive optimization problems such as distributed watershed model calibration. DDS performance is compared to the shuffled complex evolution (SCE) algorithm for multiple optimization test functions as well as real and synthetic SWAT2000 model automatic calibration formulations. Algorithms are compared for optimization problems ranging from 6 to 30 dimensions, and each problem is solved in 1000 to 10,000 total function evaluations per optimization trial. Results are presented so that future modelers can assess algorithm performance at a computational scale relevant to their modeling case study. In all four of the computationally expensive real SWAT2000 calibration formulations considered here (14, 14, 26, and 30 calibration parameters), results show DDS to be more efficient and effective than SCE. In two cases, DDS requires only 15\u201320% of the number of model evaluations used by SCE in order to find equally good values of the objective function. Overall, the results also show that DDS rapidly converges to good calibration solutions and easily avoids poor local optima. The simplicity of the DDS algorithm allows for easy recoding and subsequent adoption into any watershed modeling application framework."
            },
            "slug": "Dynamically-dimensioned-search-algorithm-for-model-Tolson-Shoemaker",
            "title": {
                "fragments": [],
                "text": "Dynamically dimensioned search algorithm for computationally efficient watershed model calibration"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "DDS performance is compared to the shuffled complex evolution (SCE) algorithm for multiple optimization test functions as well as real and synthetic SWAT2000 model automatic calibration formulations and results show DDS to be more efficient and effective than SCE."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143712289"
                        ],
                        "name": "Donald J. Patterson",
                        "slug": "Donald-J.-Patterson",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Patterson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald J. Patterson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690271"
                        ],
                        "name": "Henry A. Kautz",
                        "slug": "Henry-A.-Kautz",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Kautz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Henry A. Kautz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12584213,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1a9fcccfb162819ea0e9a1a258439ade09f7b33",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Auto-Walksat:-A-Self-Tuning-Implementation-of-Patterson-Kautz",
            "title": {
                "fragments": [],
                "text": "Auto-Walksat: A Self-Tuning Implementation of Walksat"
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Notes Discret. Math."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152895903"
                        ],
                        "name": "Lin Xu",
                        "slug": "Lin-Xu",
                        "structuredName": {
                            "firstName": "Lin",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lin Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "Joint publications with Lin Xu, Holger Hoos, and Kevin Leyton-Brown on per-instance algorithm selection (Xu et al., 2007b, 2008) fall into the same category."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 163
                            }
                        ],
                        "text": "First, it has previously been observed that instance features, such as the ones we used, are not very predictive for the type of SWGCP instances used in this case (Xu et al., 2007a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 63
                            }
                        ],
                        "text": "1 Instance Features Existing work on empirical hardness models (Leyton-Brown et al., 2002; Nudelman et al., 2004; Hutter et al., 2006; Xu et al., 2007a; Leyton-Brown et al., 2009) convincingly demonstrated that it is possible to predict algorithm runtime based on features of the problem instance to be solved."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2712451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b16bd46fe35e04669775f23ac7345b5fb1dd13b5",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Empirical hardness models predict a solver's runtime for a given instance of an NP-hard problem based on efficiently computable features. Previous research in the SAT domain has shown that better prediction accuracy and simpler models can be obtained when models are trained separately on satisfiable and unsatisfiable instances. We extend this work by training separate hardness models for each class, predicting the probability that a novel instance belongs to each class, and using these predictions to build a hierarchical hardness model using a mixture-of-experts approach. We describe and analyze classifiers and hardness models for four well-known distributions of SAT instances and nine high-performance solvers. We show that surprisingly accurate classifications can be achieved very efficiently. Our experiments show that hierarchical hardness models achieve higher prediction accuracy than the previous state of the art. Furthermore, the classifier's confidence correlates strongly with prediction error, giving a useful per-instance estimate of prediction error."
            },
            "slug": "Hierarchical-Hardness-Models-for-SAT-Xu-Hoos",
            "title": {
                "fragments": [],
                "text": "Hierarchical Hardness Models for SAT"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes and analyzes classifiers and hardness models for four well-known distributions of SAT instances and nine high-performance solvers, and shows that surprisingly accurate classifications can be achieved very efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "CP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50238440"
                        ],
                        "name": "W. Notz",
                        "slug": "W.-Notz",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Notz",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Notz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2011917"
                        ],
                        "name": "T. Santner",
                        "slug": "T.-Santner",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Santner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Santner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50475322"
                        ],
                        "name": "B. Williams",
                        "slug": "B.-Williams",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 87
                            }
                        ],
                        "text": "Transformations that have been suggested in the literature include log transformations (Jones et al., 1998; Williams et al., 2000; Huang et al., 2006; Leyton-Brown et al., 2002), negative inverse transformations (Jones et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10069227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9ef6e6f619e84ceb990fa62a3f57548b02c14c3a",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last ten to fifteen years many phenomena that could be studied only using physical experiments can now be studied by computer experiments. Advances in the mathematical modeling of many physical processes, in algorithms for solving mathematical systems, and in computer speeds, have combined to make it possible to augment or replace physical experiments with computer experiments. In a computer experiment, a response z( x), usually deterministic, is computed for each set of input variables, x, according to an experimental design strategy. This strategy is determined by the goal of the experiment and depends, for example, on whether response prediction at unsampled input sites or response optimization is of primary interest. \nWe are concerned with the commonly occuring situation in which there are two types of input variables: suppose x = ( xc, x e) where xc is a set of \u201ccontrol\u201d (manufacturing) variables and xe is a set of \u201cenvironmental\u201d (noise) variables. Manufacturing variables can be controlled while noise variables are not controllable but have values governed by some probability distribution. \nFor single response settings, we introduce a sequential experimental design for finding the optimum of e(x c) = E[z(x c, Xe)], where the expectation is taken over the distribution of the environmental variables. For bivariate response settings, we introduce a sequential experimental design for finding the constrained optimum of e1( xc)) = E[z( xc, X e)], subject to e2 (x c) = E[z2(x c, Xe)] \u2264 U. The approach is Bayesian; the prior information is that the responses are a draw from a stationary Gaussian stochastic process with correlation function belonging to a parametric family with unknown parameters. The idea of the methods is to compute the posterior expected \u201cimprovement\u201d over the current optimum for each untested site; the design selects the next site to maximize the expected improvement. Both procedures are illustrated by examples utilizing test functions from the numerical optimization literature."
            },
            "slug": "Sequential-design-of-computer-experiments-to-Notz-Santner",
            "title": {
                "fragments": [],
                "text": "Sequential design of computer experiments to minimize integrated response functions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The idea of the methods is to compute the posterior expected \u201cimprovement\u201d over the current optimum for each untested site; the design selects the next site to maximize the expected improvement."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731104"
                        ],
                        "name": "H. Zender",
                        "slug": "H.-Zender",
                        "structuredName": {
                            "firstName": "Hendrik",
                            "lastName": "Zender",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Zender"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708708"
                        ],
                        "name": "G. Kruijff",
                        "slug": "G.-Kruijff",
                        "structuredName": {
                            "firstName": "Geert-Jan",
                            "lastName": "Kruijff",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kruijff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401917019"
                        ],
                        "name": "Ivana Kruijff-Korbayov\u00e1",
                        "slug": "Ivana-Kruijff-Korbayov\u00e1",
                        "structuredName": {
                            "firstName": "Ivana",
                            "lastName": "Kruijff-Korbayov\u00e1",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivana Kruijff-Korbayov\u00e1"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 444975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4fd0fd8041b1230eb1fdbdefd649cd653fa5e428",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present an approach to the task of generating and resolving referring expressions (REs) for conversational mobile robots. It is based on a spatial knowledge base encompassing both robotand human-centric representations. Existing algorithms for the generation of referring expressions (GRE) try to find a description that uniquely identifies the referent with respect to other entities that are in the current context. Mobile robots, however, act in large-scale space, that is environments that are larger than what can be perceived at a glance, e.g. an office building with different floors, each containing several rooms and objects. One challenge when referring to elsewhere is thus to include enough information so that the interlocutors can extend their context appropriately. We address this challenge with a method for context construction that can be used for both generating and resolving REs \u2013 two previously disjoint aspects. Our approach is embedded in a bi-directional framework for natural language processing for robots."
            },
            "slug": "A-Portfolio-Approach-to-Algorithm-Selection-Zender-Kruijff",
            "title": {
                "fragments": [],
                "text": "A Portfolio Approach to Algorithm Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This paper presents an approach to the task of generating and resolving referring expressions (REs) for conversational mobile robots based on a spatial knowledge base encompassing both robot and human-centric representations and is embedded in a bi-directional framework for natural language processing for robots."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107327555"
                        ],
                        "name": "D. Huang",
                        "slug": "D.-Huang",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115429409"
                        ],
                        "name": "T. Allen",
                        "slug": "T.-Allen",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Allen",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Allen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50238440"
                        ],
                        "name": "W. Notz",
                        "slug": "W.-Notz",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Notz",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Notz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065868465"
                        ],
                        "name": "N. Zheng",
                        "slug": "N.-Zheng",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Zheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 124
                            }
                        ],
                        "text": "4, it is not compatible with transformations of the response variable, which are often crucial for strong model performance (Jones et al., 1998; Huang et al., 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 168
                            }
                        ],
                        "text": "The exact equations used in both SKO and the DACE toolbox implement methods to deal with ill conditioning; we refer the reader to the original publications for details (Huang et al., 2006; Bartz-Beielstein, 2006; Lophaven et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 348,
                                "start": 328
                            }
                        ],
                        "text": "7: SelectNewConfigurations(M,\u03b8inc,R) in SKO Input : Model,M; incumbent configuration, \u03b8inc; sequence of target algorithm runs, R Output: Sequence of parameter configurations to evaluate, here with one element, [\u03b8new] \u03b8new \u2190 the single parameter configuration found optimizing the augmented expected improvement 1 criterion from (Huang et al., 2006) with the Nelder-Mead simplex method."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 147
                            }
                        ],
                        "text": "\u2026by Bartz-Beielstein et al. (2005)\nSKO Sequential Kriging Optimization, a model-based approach for stochastic continuous blackbox optimization described by Huang et al. (2006)\nStochastic Blackbox Optimization Problem Similar to a deterministic Blackbox Optimization Problem, but the observations\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 87
                            }
                        ],
                        "text": "Transformations that have been suggested in the literature include log transformations (Jones et al., 1998; Williams et al., 2000; Huang et al., 2006; Leyton-Brown et al., 2002), negative inverse transformations (Jones et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14688276,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c51104c58c3e05f487d87ee94ce2e3b2d11dce6",
            "isKey": true,
            "numCitedBy": 584,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new method that extends the efficient global optimization to address stochastic black-box systems. The method is based on a kriging meta-model that provides a global prediction of the objective values and a measure of prediction uncertainty at every point. The criterion for the infill sample selection is an augmented expected improvement function with desirable properties for stochastic responses. The method is empirically compared with the revised simplex search, the simultaneous perturbation stochastic approximation, and the DIRECT methods using six test problems from the literature. An application case study on an inventory system is also documented. The results suggest that the proposed method has excellent consistency and efficiency in finding global optimal solutions, and is particularly useful for expensive systems."
            },
            "slug": "Global-Optimization-of-Stochastic-Black-Box-Systems-Huang-Allen",
            "title": {
                "fragments": [],
                "text": "Global Optimization of Stochastic Black-Box Systems via Sequential Kriging Meta-Models"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The proposed method is based on a kriging meta-model that provides a global prediction of the objective values and a measure of prediction uncertainty at every point and has excellent consistency and efficiency in finding global optimal solutions."
            },
            "venue": {
                "fragments": [],
                "text": "J. Glob. Optim."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3318461"
                        ],
                        "name": "M. Gagliolo",
                        "slug": "M.-Gagliolo",
                        "structuredName": {
                            "firstName": "Matteo",
                            "lastName": "Gagliolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gagliolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13962918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9d45e1b2dd69caac92592c0f264a928f2bf9a17",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Restart strategies are commonly used for minimizing the computational cost of randomized algorithms, but require prior knowledge of the run-time distribution in order to be effective. We propose a portfolio of two strategies, one fixed, with a provable bound on performance, the other based on a model of run-time distribution, updated as the two strategies are run on a sequence of problems. Computational resources are allocated probabilistically to the two strategies, based on their performances, using a well-known K-armed bandit problem solver. We present bounds on the performance of the resulting technique, and experiments with a satisfiability problem solver, showing rapid convergence to a near-optimal execution time."
            },
            "slug": "Learning-Restart-Strategies-Gagliolo-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning Restart Strategies"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work proposes a portfolio of two strategies, one fixed, with a provable bound on performance, the other based on a model of run-time distribution, updated as the two strategies are run on a sequence of problems."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152895903"
                        ],
                        "name": "Lin Xu",
                        "slug": "Lin-Xu",
                        "structuredName": {
                            "firstName": "Lin",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lin Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 128
                            }
                        ],
                        "text": "For SAT instances in the form of CNF formulae, we used the features that have been constructed for the 2009 version of SATzilla (Xu et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 88
                            }
                        ],
                        "text": "206\n13.1 11 groups of SAT features; these were introduced by Nudelman et al. (2004) and Xu et al. (2008, 2009). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 13.2 Eight groups of features for the mixed integer programming problem. . . . . . . . . 212 13.3 RF model quality\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 8234226,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2feb2d275e645da448ac0c3f91ffcc9a222d325d",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Empirical studies often observe that the performance of algorithms across problem domains can be quite uncorrelated. When this occurs, it seems practical to investigate the use of algorithm portfolios that draw on the strengths of multiple algorithms. SATzilla is such an algorithm portfolio for SAT problems; it was first deployed in the 2004 SAT competition [12], and recently an updated version, SATzilla2007, won a number of prizes in the 2007 SAT competition [21], including the gold medals for the SAT+UNSAT categories of both the random and handmade categories. SATzilla2008, submitted to the 2008 SAT Race, did not perform as well. We attribute this mainly to the lack of publicly available high-performance component solvers as well as to overheads in computing instance features for huge industrial instances; we addressed this latter point in SATzilla2009. SATzilla is based on empirical hardness models [10, 13], learned predictors that estimate each algorithm\u2019s performance on a given SAT instance. Over the years, we have added several features to SATzilla. We integrated regression methods based on partly censored data, probabilistic prediction of instance satisfiability, and hierarchical hardness models [21, 22]. We also almost entirely automated the portfolio construction process based on automatic procedures for selecting pre-solvers and candidate component solvers [23]. The new features in SATzilla2009 are as follows:"
            },
            "slug": "SATzilla2009:-an-Automatic-Algorithm-Portfolio-for-Xu-Hutter",
            "title": {
                "fragments": [],
                "text": "SATzilla2009: an Automatic Algorithm Portfolio for SAT"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "SATzilla is an algorithm portfolio for SAT problems that is based on empirical hardness models, learned predictors that estimate each algorithm\u2019s performance on a given SAT instance and almost entirely automated the portfolio construction process based on automatic procedures for selecting pre-solvers and candidate component solvers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393588417"
                        ],
                        "name": "T. Bartz-Beielstein",
                        "slug": "T.-Bartz-Beielstein",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Bartz-Beielstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bartz-Beielstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2278193"
                        ],
                        "name": "C. Lasarczyk",
                        "slug": "C.-Lasarczyk",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Lasarczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lasarczyk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1950379"
                        ],
                        "name": "M. Preuss",
                        "slug": "M.-Preuss",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Preuss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Preuss"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 83
                            }
                        ],
                        "text": "CMA-ES has been used as an application domain of parameter optimization algorithms (Bartz-Beielstein et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2741347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de0e45019862c0bb163957ff56ccfe86068fa41c",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The sequential parameter optimization toolbox (SPOT) is one possible implementation of the SPO framework introduced in Chap. 2. It has been successfully applied to numerous heuristics for practical and theoretical optimization problems. We describe the mechanics and interfaces employed by SPOT to enable users to plug in their own algorithms. Furthermore, two case studies are presented to demonstrate how SPOT can be applied in practice, followed by a discussion of alternative metamodels to be plugged into it.We conclude with some general guidelines."
            },
            "slug": "The-Sequential-Parameter-Optimization-Toolbox-Bartz-Beielstein-Lasarczyk",
            "title": {
                "fragments": [],
                "text": "The Sequential Parameter Optimization Toolbox"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The mechanics and interfaces employed by SPOT are described to enable users to plug in their own algorithms and to conclude with some general guidelines."
            },
            "venue": {
                "fragments": [],
                "text": "Experimental Methods for the Analysis of Optimization Algorithms"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2771092"
                        ],
                        "name": "S. Coy",
                        "slug": "S.-Coy",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Coy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Coy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761717"
                        ],
                        "name": "B. Golden",
                        "slug": "B.-Golden",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Golden",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Golden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2426318"
                        ],
                        "name": "G. Runger",
                        "slug": "G.-Runger",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Runger",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Runger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1807684"
                        ],
                        "name": "E. Wasil",
                        "slug": "E.-Wasil",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Wasil",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Wasil"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 35
                            }
                        ],
                        "text": "Like many other related approaches (see, e.g., Minton, 1996; Coy et al., 2001; AdensoDiaz and Laguna, 2006), BASICILS deals with the stochastic part of the optimization problem by using an estimate based on a fixed number, N , of training instances."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3117668,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "14584c9dbac190c4e7d05d3d8cb1b3e1ccef9fb2",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a procedure, based on statistical design of experiments and gradient descent, that finds effective settings for parameters found in heuristics. We develop our procedure using four experiments. We use our procedure and a small subset of problems to find parameter settings for two new vehicle routing heuristics. We then set the parameters of each heuristic and solve 19 capacity-constrained and 15 capacity-constrained and route-length-constrained vehicle routing problems ranging in size from 50 to 483 customers. We conclude that our procedure is an effective method that deserves serious consideration by both researchers and operations research practitioners."
            },
            "slug": "Using-Experimental-Design-to-Find-Effective-for-Coy-Golden",
            "title": {
                "fragments": [],
                "text": "Using Experimental Design to Find Effective Parameter Settings for Heuristics"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A procedure, based on statistical design of experiments and gradient descent, that finds effective settings for parameters found in heuristics that deserves serious consideration by both researchers and operations research practitioners is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Heuristics"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3331580"
                        ],
                        "name": "O. Maron",
                        "slug": "O.-Maron",
                        "structuredName": {
                            "firstName": "Oded",
                            "lastName": "Maron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Maron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760402"
                        ],
                        "name": "A. Moore",
                        "slug": "A.-Moore",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Moore",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Moore"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 20
                            }
                        ],
                        "text": "2 Racing algorithms (Maron and Moore, 1994; Birattari et al., 2002; Birattari, 2005; Balaprakash et al., 2007) emphasize using as few problem instances as possible to reliably choose among a fixed set of parameter configurations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 150
                            }
                        ],
                        "text": "\u20262006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 199
                            }
                        ],
                        "text": ", 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7930120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b31b23e0cb60fbc857728d6b16bc5d61f71a643",
            "isKey": false,
            "numCitedBy": 355,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Selecting a good model of a set of input points by cross validation is a computationally intensive process, especially if the number of possible models or the number of training points is high. Techniques such as gradient descent are helpful in searching through the space of models, but problems such as local minima, and more importantly, lack of a distance metric between various models reduce the applicability of these search methods. Hoeffding Races is a technique for finding a good model for the data by quickly discarding bad models, and concentrating the computational effort at differentiating between the better ones. This paper focuses on the special case of leave-one-out cross validation applied to memory-based learning algorithms, but we also argue that it is applicable to any class of model selection problems."
            },
            "slug": "Hoeffding-Races:-Accelerating-Model-Selection-for-Maron-Moore",
            "title": {
                "fragments": [],
                "text": "Hoeffding Races: Accelerating Model Selection Search for Classification and Function Approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper focuses on the special case of leave-one-out cross validation applied to memory-based learning algorithms, but it is argued that it is applicable to any class of model selection problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145438097"
                        ],
                        "name": "J. Gratch",
                        "slug": "J.-Gratch",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Gratch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gratch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802807"
                        ],
                        "name": "G. DeJong",
                        "slug": "G.-DeJong",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "DeJong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. DeJong"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 145
                            }
                        ],
                        "text": "Other search algorithms include mechanisms for adapting the set of instances used for evaluating parameter configurations; examples are Composer (Gratch and Dejong, 1992), SPO (Bartz-Beielstein, 2006, see also Section 9."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 134
                            }
                        ],
                        "text": "In particular, research communities that have contributed techniques for algorithm configuration or parameter tuning include planning (Gratch and Dejong, 1992), evolutionary computation (Bartz-Beielstein, 2006), meta-heuristics (Birattari, 2005), genetic algorithms (Fukunaga, 2008), parallel computing (Brewer, 1995), and numerical optimization (Audet and Orban, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9870367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d6ffb023aa22682afefc096142dd6fac51da4c3",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In machine learning there is considerable interest in techniques which improve planning ability. Initial investigations have identified a wide variety of techniques to address this issue. Progress has been hampered by the utility problem, a basic tradeoff between the benefit of learned knowledge and the cost to locate and apply relevant knowledge. In this paper we describe the COMPOSER system which embodies a probabilistic solution to the utility problem. We outline the statistical foundations of our approach and compare it against four other approaches which appear in the literature."
            },
            "slug": "COMPOSER:-A-Probabilistic-Solution-to-the-Utility-Gratch-DeJong",
            "title": {
                "fragments": [],
                "text": "COMPOSER: A Probabilistic Solution to the Utility Problem in Speed-Up Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The COMPOSER system is described, which embodies a probabilistic solution to the utility problem, and the statistical foundations of the approach are outlined and it is compared against four other approaches which appear in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1756353"
                        ],
                        "name": "Horst Samulowitz",
                        "slug": "Horst-Samulowitz",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Samulowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Horst Samulowitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710604"
                        ],
                        "name": "R. Memisevic",
                        "slug": "R.-Memisevic",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Memisevic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Memisevic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18675582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a4f8e4010d3787e64d09a9f1894558bbaccdca2",
            "isKey": false,
            "numCitedBy": 107,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach to solving Quantified Boolean Formulas (QBF) that combines a search-based QBF solver with marhine learning techniques. We show how classification methods can be used to predict run-times and to choose optimal heuristics both within a portfolio-based, and within a dynamic, online approach. In the dynamic method variables are set to a truth value according to a scheme that tries to maximize the probability of successfully solving the remaining sub-problem efficiently. Since each variable assignment can drastically change the problem-structure, new heuristics are chosen dynamically, and a classifier is used online to predict the usefulness of each heuristic. Experimental results on a large corpus of example problems show the usefulness of our approach in terms of run-time as well as the ability to solve previously unsolved problem instances."
            },
            "slug": "Learning-to-Solve-QBF-Samulowitz-Memisevic",
            "title": {
                "fragments": [],
                "text": "Learning to Solve QBF"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A novel approach to solving Quantified Boolean Formulas (QBF) that combines a search-based QBF solver with marhine learning techniques is presented and how classification methods can be used to predict run-times and to choose optimal heuristics both within a portfolio-based, and within a dynamic, online approach is shown."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152895903"
                        ],
                        "name": "Lin Xu",
                        "slug": "Lin-Xu",
                        "structuredName": {
                            "firstName": "Lin",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lin Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 105
                            }
                        ],
                        "text": "Joint publications with Lin Xu, Holger Hoos, and Kevin Leyton-Brown on per-instance algorithm selection (Xu et al., 2007b, 2008) fall into the same category."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9241958,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cafd6b340247fd04b947433637f6e590ccdab1c",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "It has been widely observed that there is no \"dominant\" SAT solver; instead, different solvers perform best on different instances. Rather than following the traditional approach of choosing the best solver for a given class of instances, we advocate making this decision online on a per-instance basis. Building on previous work, we describe a per-instance solver portfolio for SAT, SATzilla-07, which uses socalled empirical hardness models to choose among its constituent solvers. We leverage new model-building techniques such as censored sampling and hierarchical hardness models, and demonstrate the effectiveness of our techniques by building a portfolio of state-of-the-art SAT solvers and evaluating it on several widely-studied SAT data sets. Overall, we show that our portfolio significantly outperforms its constituent algorithms on every data set. Our approach has also proven itself to be effective in practice: in the 2007 SAT competition, SATzilla-07 won three gold medals, one silver, and one bronze; it is available online at http://www.cs.ubc.ca/labs/beta/Projects/SATzilla."
            },
            "slug": ":-The-Design-and-Analysis-of-an-Algorithm-Portfolio-Xu-Hutter",
            "title": {
                "fragments": [],
                "text": ": The Design and Analysis of an Algorithm Portfolio for SAT"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A per-instance solver portfolio for SAT, SATzilla-07, is described, which uses socalled empirical hardness models to choose among its constituent solvers, and shows that the portfolio significantly outperforms its constituent algorithms on every data set."
            },
            "venue": {
                "fragments": [],
                "text": "CP"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1775517"
                        ],
                        "name": "S. Seshia",
                        "slug": "S.-Seshia",
                        "structuredName": {
                            "firstName": "Sanjit",
                            "lastName": "Seshia",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Seshia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34673652"
                        ],
                        "name": "R. Bryant",
                        "slug": "R.-Bryant",
                        "structuredName": {
                            "firstName": "Randal",
                            "lastName": "Bryant",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bryant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62198485,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "306671fbe40c8ffdbcd6ecf88b8e1ecd198d3aa7",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 165,
            "paperAbstract": {
                "fragments": [],
                "text": "Decision procedures for first-order logics are widely applicable in design verification and static program analysis. However, existing procedures rarely scale to large systems, especially for verifying properties that depend on data or timing, in addition to control. \nThis thesis presents a new approach for building efficient, automated decision procedures for first-order logics involving arithmetic. In this approach, decision problems involving arithmetic are transformed to problems in the Boolean domain, such as Boolean satisfiability solving, thereby leveraging recent advances in that area. The transformation automatically detects and exploits problem structure based on new theoretical results and machine learning. The results of experimental evaluations show that our decision procedures can outperform other state-of-the-art procedures by several orders of magnitude. \nThe decision procedures form the computational engines for two verification systems, UCLID and TMV These systems have been applied to problems in computer security, electronic design automation, and software engineering that require efficient and precise analysis of system functionality and timing. This thesis describes two such applications: finding format-string exploits in software, and verifying circuits that operate under timing assumptions."
            },
            "slug": "Adaptive-eager-boolean-encoding-for-arithmetic-in-Seshia-Bryant",
            "title": {
                "fragments": [],
                "text": "Adaptive eager boolean encoding for arithmetic reasoning in verification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new approach for building efficient, automated decision procedures for first-order logics involving arithmetic is presented, where decision problems involving arithmetic are transformed to problems in the Boolean domain, such as Boolean satisfiability solving, thereby leveraging recent advances in that area."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31453521"
                        ],
                        "name": "Susan L. Epstein",
                        "slug": "Susan-L.-Epstein",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Epstein",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan L. Epstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2606753"
                        ],
                        "name": "Eugene C. Freuder",
                        "slug": "Eugene-C.-Freuder",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Freuder",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene C. Freuder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145144556"
                        ],
                        "name": "R. Wallace",
                        "slug": "R.-Wallace",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Wallace",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053569687"
                        ],
                        "name": "A. Morozov",
                        "slug": "A.-Morozov",
                        "structuredName": {
                            "firstName": "Anton",
                            "lastName": "Morozov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Morozov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059817826"
                        ],
                        "name": "Bruce Samuels",
                        "slug": "Bruce-Samuels",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Samuels",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bruce Samuels"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 436262,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc103f337cbf6409c834bca91a08790ead4293b2",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The Adaptive Constraint Engine (ACE) seeks to automate the application of constraint programming expertise and the extraction of domain-specific expertise. Under the aegis of FORR, an architecture for learning and problem-solving, ACE learns search-order heuristics from problem solving experience. This paper describes ACE?s approach, as well as new experimental results on specific problem classes. ACE is both a test-bed for CSP research and a discovery environment for new algorithms."
            },
            "slug": "The-Adaptive-Constraint-Engine-Epstein-Freuder",
            "title": {
                "fragments": [],
                "text": "The Adaptive Constraint Engine"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "The Adaptive Constraint Engine is described, both a test-bed for CSP research and a discovery environment for new algorithms, as well as new experimental results on specific problem classes."
            },
            "venue": {
                "fragments": [],
                "text": "CP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143854701"
                        ],
                        "name": "D. Tompkins",
                        "slug": "D.-Tompkins",
                        "structuredName": {
                            "firstName": "Dave",
                            "lastName": "Tompkins",
                            "middleNames": [
                                "A.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tompkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 41
                            }
                        ],
                        "text": ", 1996), dynamic local search algorithms (Hutter et al., 2002) and G2WSAT variants (Li and Huang, 2005), all combined in a highly parameterized framework solver with a total of 41 parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9775270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c2bf16428fa29cfb58e15f6c731e13e8c33d2bd",
            "isKey": false,
            "numCitedBy": 237,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we study the approach of dynamic local search for the SAT problem. We focus on the recent and promising Exponentiated Sub-Gradient (ESG) algorithm, and examine the factors determining the time complexity of its search steps. Basedon the insights gained from our analysis, we developed Scaling and Probabilistic Smoothing (SAPS), an efficient SAT algorithm that is conceptually closely related to ESG. We also introduce a reactive version of SAPS (RSAPS) that adaptively tunes one of the algorithm's important parameters. We show that for a broadra nge of standard benchmark problems for SAT, SAPS andR SAPS achieve significantly better performance than both ESG and the state-of-the-art WalkSAT variant, Novelty+."
            },
            "slug": "Scaling-and-Probabilistic-Smoothing:-Efficient-for-Hutter-Tompkins",
            "title": {
                "fragments": [],
                "text": "Scaling and Probabilistic Smoothing: Efficient Dynamic Local Search for SAT"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Scaling and Probabilistic Smoothing (SAPS), an efficient SAT algorithm that is conceptually closely related to ESG, is developed, and a reactive version of SAPS (RSAPS) is introduced that adaptively tunes one of the algorithm's important parameters."
            },
            "venue": {
                "fragments": [],
                "text": "CP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144874680"
                        ],
                        "name": "N. Gould",
                        "slug": "N.-Gould",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Gould",
                            "middleNames": [
                                "I.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Gould"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144256094"
                        ],
                        "name": "D. Orban",
                        "slug": "D.-Orban",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Orban",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Orban"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145758858"
                        ],
                        "name": "P. Toint",
                        "slug": "P.-Toint",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Toint",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Toint"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 211
                            }
                        ],
                        "text": "The optimization objective in their work was the runtime and number of function evaluations required by interior point methods for solving a set of large unconstrained regular problems from the CUTEr collection (Gould et al., 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6473453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e54937869fdb8c8af86bd46177bfb14d1722ec26",
            "isKey": false,
            "numCitedBy": 555,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "The initial release of CUTE, a widely used testing environment for optimization software, was described by Bongartz, et al. [1995]. A new version, now known as CUTEr, is presented. Features include reorganisation of the environment to allow simultaneous multi-platform installation, new tools for, and interfaces to, optimization packages, and a considerably simplified and entirely automated installation procedure for unix systems. The environment is fully backward compatible with its predecessor, and offers support for Fortran 90/95 and a general C/C++ Application Programming Interface. The SIF decoder, formerly a part of CUTE, has become a separate tool, easily callable by various packages. It features simple extensions to the SIF test problem format and the generation of files suited to automatic differentiation packages."
            },
            "slug": "CUTEr-and-SifDec:-A-constrained-and-unconstrained-Gould-Orban",
            "title": {
                "fragments": [],
                "text": "CUTEr and SifDec: A constrained and unconstrained testing environment, revisited"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A new version of CUTE, now known as CUTEr, is presented, which includes reorganisation of the environment to allow simultaneous multi-platform installation, new tools for, and interfaces to, optimization packages, and a considerably simplified and entirely automated installation procedure for unix systems."
            },
            "venue": {
                "fragments": [],
                "text": "TOMS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11727255"
                        ],
                        "name": "R. C. Whaley",
                        "slug": "R.-C.-Whaley",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Whaley",
                            "middleNames": [
                                "Clint"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Whaley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143976422"
                        ],
                        "name": "D. Whalley",
                        "slug": "D.-Whalley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Whalley",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Whalley"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 178
                            }
                        ],
                        "text": "Research in automatically optimizing such libraries has demonstrated that frequently-used routines can be optimized to run orders of magnitude faster than na\u0131\u0308ve implementations (Whaley, 2004)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18963646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcafbfb6690ae58dda6f03daf5f32eb42a0d05ba",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Using traditional methodologies and tools, the problem of keeping performance-critical kernels at high efficiency on hardware evolving at the incredible rates dictated by Moore's Law is almost intractable. On product lines where ISA compatibility is maintained through several generations of architecture, the growing gap between the machine that the software sees and the actual hardware exacerbates this problem considerably, as do the evolving software layers between the application in question and the ISA. To address this problem, we have utilized a relatively new technique, which we call AEOS (Automated Empirical Optimization of Software). In this paper, we describe the AEOS systems we have researched, implemented and tested. The first of these is ATLAS (Automatically Tuned Linear Algebra Software), which empirically optimizes key linear algebra kernels to arbitrary cache-based machines. Our latest research effort is instantiated in the iFKO (iterative Floating Point Kernel Optimizer) project, whose aim is to perform empirical optimization of relatively arbitrary kernels using a low-level iterative and empirical compilation framework."
            },
            "slug": "Automated-empirical-optimization-of-high-floating-Whaley-Whalley",
            "title": {
                "fragments": [],
                "text": "Automated empirical optimization of high performance floating point kernels"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The AEOS systems, which empirically optimizes key linear algebra kernels to arbitrary cache-based machines, are described and the latest research effort is instantiated in the iFKO (iterative Floating Point Kernel Optimizer) project, whose aim is to perform empirical optimization of relatively arbitrary kernels using a low-level iterative and empirical compilation framework."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144539890"
                        ],
                        "name": "N. Hansen",
                        "slug": "N.-Hansen",
                        "structuredName": {
                            "firstName": "Nikolaus",
                            "lastName": "Hansen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Hansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2094049161"
                        ],
                        "name": "Stefan Kern",
                        "slug": "Stefan-Kern",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kern",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Kern"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 168
                            }
                        ],
                        "text": "For optimizing target algorithms with only numerical parameters, we would also like to compare to prominent model-free blackbox optimization approaches, such as CMA-ES (Hansen and Ostermeier, 1996; Hansen and Kern, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 157
                            }
                        ],
                        "text": "133 9.3 Alternative GP fits to log-transformed data . . . . . . . . . . . . . . . . . . . 135 9.4 Comparison of SKO and three variants of SPO for optimizing CMA-ES . . . 144 9.5 Comparison of SKO and three variants of SPO for optimizing CMA-ES, with\nlog-transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 78
                            }
                        ],
                        "text": "In particular, it has been applied to optimize the solution quality of CMA-ES (Hansen and Ostermeier, 1996; Hansen and Kern, 2004), a prominent gradient-free global optimization algorithm for continuous functions (BartzBeielstein et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 170
                            }
                        ],
                        "text": "38 3.2.1 Target Algorithms for SAT . . . . . . . . . . . . . . . . . . . . . . . 39 3.2.2 Target Algorithm for MIP . . . . . . . . . . . . . . . . . . . . . . . 42 3.2.3 CMA-ES for Global Continuous Function Optimization . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 84
                            }
                        ],
                        "text": "This prominent gradient-free global optimization algorithm for continuous functions (Hansen and Ostermeier, 1996; Hansen and Kern, 2004) is based on an evolutionary strategy that uses a covariance matrix adaptation scheme."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 29
                            }
                        ],
                        "text": "2 In this comparison, CMA-ES (Hansen and Ostermeier, 1996; Hansen and Kern, 2004), which we describe in Section 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 34
                            }
                        ],
                        "text": "50 3.4 Experimental setup for the CMA-ES configuration scenarios . . . . . . . . . 50 3.5 Summary of our SINGLEINSTCONT scenarios . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15573950,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f27e096fb6f2aa0d6654238543af3139565ad2ed",
            "isKey": true,
            "numCitedBy": 781,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper the performance of the CMA evolution strategy with rank-\u03bc-update and weighted recombination is empirically investigated on eight multimodal test functions. In particular the effect of the population size \u03bb on the performance is investigated. Increasing the population size remarkably improves the performance on six of the eight test functions. The optimal population size takes a wide range of values, but, with one exception, scales sub-linearly with the problem dimension. The global optimum can be located in all but one function. The performance for locating the global optimum scales between linear and cubic with the problem dimension. In a comparison to state-of-the-art global search strategies the CMA evolution strategy achieves superior performance on multimodal, non-separable test functions without intricate parameter tuning."
            },
            "slug": "Evaluating-the-CMA-Evolution-Strategy-on-Multimodal-Hansen-Kern",
            "title": {
                "fragments": [],
                "text": "Evaluating the CMA Evolution Strategy on Multimodal Test Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "In this paper the performance of the CMA evolution strategy with rank-\u03bc-update and weighted recombination is empirically investigated on eight multimodal test functions and the effect of the population size \u03bb on the performance is investigated."
            },
            "venue": {
                "fragments": [],
                "text": "PPSN"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717349"
                        ],
                        "name": "D. Knuth",
                        "slug": "D.-Knuth",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Knuth",
                            "middleNames": [
                                "Ervin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Knuth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14416585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ecfe05be78751a6071424a09778ddcd4f12fd87d",
            "isKey": false,
            "numCitedBy": 313,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the chief difficulties associated with the so-called backtracking technique for combinatorial problems has been our inability to predict the efficiency of a given algorithm, or to compare the efficiencies of different approaches, without actually writing and running the programs. This paper presents a simple method which produces reasonable estimates for most applications, requiring only a modest amount of hand calculation. The method should prove to be of considerable utility in connection with D. H. Lehmer''s branch-and-bound approach to combinatorial optimization."
            },
            "slug": "Estimating-the-efficiency-of-backtrack-programs.-Knuth",
            "title": {
                "fragments": [],
                "text": "Estimating the efficiency of backtrack programs."
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper presents a simple method which produces reasonable estimates for most applications, requiring only a modest amount of hand calculation, and should prove to be of considerable utility in connection with D. H. Lehmer''s branch-and-bound approach to combinatorial optimization."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2440961"
                        ],
                        "name": "A. Bhalla",
                        "slug": "A.-Bhalla",
                        "structuredName": {
                            "firstName": "Ateet",
                            "lastName": "Bhalla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bhalla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1773888"
                        ],
                        "name": "I. Lynce",
                        "slug": "I.-Lynce",
                        "structuredName": {
                            "firstName": "In\u00eas",
                            "lastName": "Lynce",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Lynce"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145035060"
                        ],
                        "name": "J. Sousa",
                        "slug": "J.-Sousa",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Sousa",
                            "middleNames": [
                                "T.",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sousa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393656234"
                        ],
                        "name": "Joao Marques-Silva",
                        "slug": "Joao-Marques-Silva",
                        "structuredName": {
                            "firstName": "Joao",
                            "lastName": "Marques-Silva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joao Marques-Silva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 169
                            }
                        ],
                        "text": "Some examples from the SAT-solving world include variable and value selection, clause deletion, next watched literal selection, and initial variable ordering heuristics (see, e.g., Silva, 1999; Moskewicz et al., 2001; Bhalla et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2428461,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "60ebabcfaf7d4e4de98382929fa9762677995335",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years backtrack search SAT solvers have been the subject of dramatic improvements. These improvements allowed SAT solvers to successfully replace BDDs in many areas of formal verification, and also motivated the development of many new challenging problem instances, many of which too hard for the current generation of SAT solvers. As a result, further improvements to SAT technology are expected to have key consequences informal verification. The objective is to propose heuristic approaches to the backtrack step of backtrack search SAT solvers, with the goal of increasing the ability of the SAT solver to search different parts of the search space. The proposed heuristics to the backtrack step are inspired by the heuristics proposed in recent years for the branching step of SAT solvers, namely VSIDS and some of its improvements. The preliminary experimental results are promising, and motivate the integration of heuristic backtracking in state-of-the-art SAT solvers."
            },
            "slug": "Heuristic-backtracking-algorithms-for-SAT-Bhalla-Lynce",
            "title": {
                "fragments": [],
                "text": "Heuristic backtracking algorithms for SAT"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The objective is to propose heuristic approaches to the backtrack step of backtrack search SAT solvers, with the goal of increasing the ability of the SAT solver to search different parts of the search space."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings. 4th International Workshop on Microprocessor Test and Verification - Common Challenges and Solutions"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107603179"
                        ],
                        "name": "D. R. Jones",
                        "slug": "D.-R.-Jones",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Jones",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. R. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2563797"
                        ],
                        "name": "C. D. Perttunen",
                        "slug": "C.-D.-Perttunen",
                        "structuredName": {
                            "firstName": "Cary",
                            "lastName": "Perttunen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Perttunen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424124"
                        ],
                        "name": "B. Stuckman",
                        "slug": "B.-Stuckman",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Stuckman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Stuckman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 150
                            }
                        ],
                        "text": "(In experiments we omit due to their preliminary nature, we compared three methods for optimizing expected improvement inR17: random sampling, DIRECT (Jones et al., 1993), and CMA-ES (Hansen and Ostermeier, 1996)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123674634,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d7808f2ad77de7b71e83a2e79d27f2e3e12be8d5",
            "isKey": false,
            "numCitedBy": 1840,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new algorithm for finding the global minimum of a multivariate function subject to simple bounds. The algorithm is a modification of the standard Lipschitzian approach that eliminates the need to specify a Lipschitz constant. This is done by carrying out simultaneous searches using all possible constants from zero to infinity. On nine standard test functions, the new algorithm converges in fewer function evaluations than most competing methods.The motivation for the new algorithm stems from a different way of looking at the Lipschitz constant. In particular, the Lipschitz constant is viewed as a weighting parameter that indicates how much emphasis to place on global versus local search. In standard Lipschitzian methods, this constant is usually large because it must equal or exceed the maximum rate of change of the objective function. As a result, these methods place a high emphasis on global search and exhibit slow convergence. In contrast, the new algorithm carries out simultaneous searches using all possible constants, and therefore operates at both the global and local level. Once the global part of the algorithm finds the basin of convergence of the optimum, the local part of the algorithm quickly and automatically exploits it. This accounts for the fast convergence of the new algorithm on the test functions."
            },
            "slug": "Lipschitzian-optimization-without-the-Lipschitz-Jones-Perttunen",
            "title": {
                "fragments": [],
                "text": "Lipschitzian optimization without the Lipschitz constant"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144301481"
                        ],
                        "name": "R. C. Whaley",
                        "slug": "R.-C.-Whaley",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Whaley",
                            "middleNames": [
                                "Clinton"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. C. Whaley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719457"
                        ],
                        "name": "A. Petitet",
                        "slug": "A.-Petitet",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Petitet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Petitet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708869"
                        ],
                        "name": "J. Dongarra",
                        "slug": "J.-Dongarra",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Dongarra",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dongarra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 24
                            }
                        ],
                        "text": ", 2005), linear algebra (Whaley et al., 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 122
                            }
                        ],
                        "text": "Other examples of parameterized algorithms can be found in areas as diverse as sorting (Li et al., 2005), linear algebra (Whaley et al., 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 46
                            }
                        ],
                        "text": "Thus, the approach taken in the ATLAS project (Whaley et al., 2001) is to optimize algorithm performance on the end user side."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 159954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c649fed768cf39c83531eb4f17082a51bfedb70f",
            "isKey": false,
            "numCitedBy": 1158,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Automated-empirical-optimizations-of-software-and-Whaley-Petitet",
            "title": {
                "fragments": [],
                "text": "Automated empirical optimizations of software and the ATLAS project"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Comput."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145459111"
                        ],
                        "name": "Michael Stillger",
                        "slug": "Michael-Stillger",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stillger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Stillger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685612"
                        ],
                        "name": "M. Spiliopoulou",
                        "slug": "M.-Spiliopoulou",
                        "structuredName": {
                            "firstName": "Myra",
                            "lastName": "Spiliopoulou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Spiliopoulou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 149
                            }
                        ],
                        "text": "\u2026(Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al., 2003), protein folding (Thachuk et al., 2007), formal verification\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 275
                            }
                        ],
                        "text": ", 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16570730,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d38cf37f65e2fde04633c5643d944b21a4097a3d",
            "isKey": false,
            "numCitedBy": 48,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Database query optimization is a hard research problem. Exhaustive techniques are adequate for trivial instances only, while combinatorial optimization techniques are vulnerable to the peculiarities of specific instances. We propose a model based on genetic programming to address this problem, motivated by its robustness and efficiency in a wide area of search problems. We adapt the genetic programming paradigm to the requirements of the query optimization problem, showing that the nature of the problem makes genetic programming a particularly attractive approach to it."
            },
            "slug": "Genetic-programming-in-database-query-optimization-Stillger-Spiliopoulou",
            "title": {
                "fragments": [],
                "text": "Genetic programming in database query optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work adapts the genetic programming paradigm to the requirements of the query optimization problem, showing that the nature of the problem makes genetic programming a particularly attractive approach to it."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1874233"
                        ],
                        "name": "J. Cavazos",
                        "slug": "J.-Cavazos",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cavazos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cavazos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401533251"
                        ],
                        "name": "M. O\u2019Boyle",
                        "slug": "M.-O\u2019Boyle",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "O\u2019Boyle",
                            "middleNames": [
                                "F.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. O\u2019Boyle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1516905,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "366d02f9687a33b21079acc6d62ad755189a52f0",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Determining the best set of optimizations to apply to a program has been a long standing problem for compiler writers. To reduce the complexity of this task, existing approaches typically apply the same set of optimizations to all procedures within a program, without regard to their particular structure. This paper develops a new method-specific approach that automatically selects the best optimizations on a per method basis within a dynamic compiler. Our approach uses the machine learning technique of logistic regression to automatically derive a predictive model that determines which optimizations to apply based on the features of a method. This technique is implemented in the Jikes RVM Java JIT compiler. Using this approach we reduce the average total execution time of the SPECjvm98 benchmarks by 29%. When the same heuristic is applied to the DaCapo+ benchmark suite, we obtain an average 33% reduction over the default level O2 setting."
            },
            "slug": "Method-specific-dynamic-compilation-using-logistic-Cavazos-O\u2019Boyle",
            "title": {
                "fragments": [],
                "text": "Method-specific dynamic compilation using logistic regression"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper develops a new method-specific approach that automatically selects the best optimizations on a per method basis within a dynamic compiler using the machine learning technique of logistic regression to automatically derive a predictive model that determines which optimizations to apply based on the features of a method."
            },
            "venue": {
                "fragments": [],
                "text": "OOPSLA '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1874233"
                        ],
                        "name": "J. Cavazos",
                        "slug": "J.-Cavazos",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Cavazos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cavazos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401533251"
                        ],
                        "name": "M. O\u2019Boyle",
                        "slug": "M.-O\u2019Boyle",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "O\u2019Boyle",
                            "middleNames": [
                                "F.",
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. O\u2019Boyle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 79
                            }
                        ],
                        "text": ", 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 148
                            }
                        ],
                        "text": "\u2026diverse as sorting (Li et al., 2005), linear algebra (Whaley et al., 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10752763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "06a13b2b1a7f3fad30f252d7ae6df04d625b5c67",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Inlining improves the performance of programs by reducing the overhead of method invocation and increasing the opportunities for compiler optimization. Incorrect inlining decisions, however, can degrade both the running and compilation time of a program. This is especially important for a dynamically compiled language such as Java. Therefore, the heuristics that control inlining must be carefully tuned to achieve a good balance between these two costs to reduce overall total execution time. This paper develops a genetic algorithms based approach to automatically tune a dynamic compiler\u2019s internal inlining heuristic. We evaluate our technique within the Jikes RVM [1] compiler and show a 17% average reduction in total execution time on the SPECjvm98 benchmark suite on a Pentium-4. When applied to the DaCapo benchmark suite, our approach reduces total execution time by 37%% outperforming all existing techniques."
            },
            "slug": "Automatic-Tuning-of-Inlining-Heuristics-Cavazos-O\u2019Boyle",
            "title": {
                "fragments": [],
                "text": "Automatic Tuning of Inlining Heuristics"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper develops a genetic algorithms based approach to automatically tune a dynamic compiler\u2019s internal inlining heuristic and reduces total execution time by 37%% outperforming all existing techniques."
            },
            "venue": {
                "fragments": [],
                "text": "ACM/IEEE SC 2005 Conference (SC'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742664"
                        ],
                        "name": "R. Poli",
                        "slug": "R.-Poli",
                        "structuredName": {
                            "firstName": "Riccardo",
                            "lastName": "Poli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Poli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3169349"
                        ],
                        "name": "W. Langdon",
                        "slug": "W.-Langdon",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Langdon",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Langdon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1853744"
                        ],
                        "name": "N. McPhee",
                        "slug": "N.-McPhee",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "McPhee",
                            "middleNames": [
                                "Freitag"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. McPhee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6740056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2096d7a06ef7c8cd014a9f1b6a93890a3134d703",
            "isKey": false,
            "numCitedBy": 1928,
            "numCiting": 597,
            "paperAbstract": {
                "fragments": [],
                "text": "Genetic programming (GP) is a systematic, domain-independent method for getting computers to solve problems automatically starting from a high-level statement of what needs to be done. Using ideas from natural evolution, GP starts from an ooze of random computer programs, and progressively refines them through processes of mutation and sexual recombination, until high-fitness solutions emerge. All this without the user having to know or specify the form or structure of solutions in advance. GP has generated a plethora of human-competitive results and applications, including novel scientific discoveries and patentable inventions. This unique overview of this exciting technique is written by three of the most active scientists in GP. See www.gp-field-guide.org.uk for more information on the book."
            },
            "slug": "A-Field-Guide-to-Genetic-Programming-Poli-Langdon",
            "title": {
                "fragments": [],
                "text": "A Field Guide to Genetic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A unique overview of this exciting technique is written by three of the most active scientists in GP, which starts from an ooze of random computer programs, and progressively refines them through processes of mutation and sexual recombination until high-fitness solutions emerge."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35250057"
                        ],
                        "name": "D. Babic",
                        "slug": "D.-Babic",
                        "structuredName": {
                            "firstName": "Domagoj",
                            "lastName": "Babic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Babic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702346"
                        ],
                        "name": "M. Musuvathi",
                        "slug": "M.-Musuvathi",
                        "structuredName": {
                            "firstName": "Madan",
                            "lastName": "Musuvathi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Musuvathi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 122
                            }
                        ],
                        "text": "In addition, SPEAR has several enhancements for software verification, such as support for modular arithmetic constraints (Babi\u0107 and Musuvathi, 2005), incrementality to enable structural abstraction/refinement (Babi\u0107 and Hu, 2007b), and a technique for identifying context-insensitive invariants to speed up solving multiple queries that share common structure (Babi\u0107 and Hu, 2007a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14879881,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "45901503a02c63330695634712fca58cb43bdb6f",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "All integer data types in programs (such as int, short, byte) have an underlying finite representation in hardware. This finiteness can result in subtle integer-overflow errors that are hard to reason about both for humans and analysis tools alike. As a first step towards finding such errors automatically, we will describe two modular arithmetic decision procedures for reasoning about bounded integers. We show how to deal with modular arithmetic operations and inequalities for both linear and non-linear problems. Both procedures are suitable for integration with Nelson-Oppen framework [1, 2, 3]. The linear solver is composed of Muller-Seidl algorithm [4] and an arbitrary integer solver for solving preprocessed congruences and inequalities. For the non-linear problems we use Newton\u2019s p-adic iteration algorithm to progressively reason about the satisfiability of the input constraints modulo 2, for increasing k. We use a SAT solver only for the base case when k = 1. According to our knowledge, this is the first Nelson-Oppen decision procedure capable of reasoning about multiplication over bounded integers without converting the entire problem to a SAT instance."
            },
            "slug": "Modular-Arithmetic-Decision-Procedure-Babic-Musuvathi",
            "title": {
                "fragments": [],
                "text": "Modular Arithmetic Decision Procedure"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This is the first Nelson-Oppen decision procedure capable of reasoning about multiplication over bounded integers without converting the entire problem to a SAT instance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70244843"
                        ],
                        "name": "Chu Min Li",
                        "slug": "Chu-Min-Li",
                        "structuredName": {
                            "firstName": "Chu",
                            "lastName": "Li",
                            "middleNames": [
                                "Min"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chu Min Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5966969"
                        ],
                        "name": "Wenqi Huang",
                        "slug": "Wenqi-Huang",
                        "structuredName": {
                            "firstName": "Wenqi",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenqi Huang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 28
                            }
                        ],
                        "text": ", 2002) and G2WSAT variants (Li and Huang, 2005), all combined in a highly parameterized framework solver with a total of 41 parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5646851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56e0cde0a9daad9872231dc8041cbc1684c7d032",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "The choice of the variable to flip in the Walksat family procedures is always random in that it is selected from a randomly chosen unsatisfied clause c. This choice in Novelty or R-Novelty heuristics also contains some determinism in that the variable to flip is always limited to the two best variables in c. In this paper, we first propose a diversification parameter for Novelty (or R-Novelty) heuristic to break the determinism in Novelty and show its performance compared with the random walk parameter in Novelty+. Then we exploit promising decreasing paths in a deterministic fashion in local search using a gradient-based approach. In other words, when promising decreasing paths exist, the variable to flip is no longer selected from a randomly chosen unsatisfied clause but in a deterministic fashion to surely decrease the number of unsatisfied clauses. Experimental results show that the proposed diversification and the determinism allow to significantly improve Novelty (and Walksat)."
            },
            "slug": "Diversification-and-Determinism-in-Local-Search-for-Li-Huang",
            "title": {
                "fragments": [],
                "text": "Diversification and Determinism in Local Search for Satisfiability"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a diversification parameter for Novelty (or R-Novelty) heuristic to break the determinism in Novelty and shows its performance compared with the random walk parameter in novels and exploits promising decreasing paths in a deterministic fashion in local search using a gradient-based approach."
            },
            "venue": {
                "fragments": [],
                "text": "SAT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48942559"
                        ],
                        "name": "E. Nudelman",
                        "slug": "E.-Nudelman",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Nudelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Nudelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701353"
                        ],
                        "name": "Y. Shoham",
                        "slug": "Y.-Shoham",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Shoham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shoham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 87
                            }
                        ],
                        "text": "Transformations that have been suggested in the literature include log transformations (Jones et al., 1998; Williams et al., 2000; Huang et al., 2006; Leyton-Brown et al., 2002), negative inverse transformations (Jones et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 63
                            }
                        ],
                        "text": "1 Instance Features Existing work on empirical hardness models (Leyton-Brown et al., 2002; Nudelman et al., 2004; Hutter et al., 2006; Xu et al., 2007a; Leyton-Brown et al., 2009) convincingly demonstrated that it is possible to predict algorithm runtime based on features of the problem instance to be solved."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14498800,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5580b7fda70be1dacb2d9424d13b5f0ca2cd51c8",
            "isKey": false,
            "numCitedBy": 207,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new approach for understanding the algorithm-specific empiricalh ardness of NP-Hard problems. In this work we focus on the empirical hardness of the winner determination problem--an optimization problem arising in combinatorial auctions--when solved by ILOG's CPLEX software. We consider nine widely-used problem distributions and sample randomly from a continuum of parameter settings for each distribution. We identify a large number of distribution-nonspecific features of data instances and use statisticalregression techniques to learn, evaluate and interpret a function from these features to the predicted hardness of an instance."
            },
            "slug": "Learning-the-Empirical-Hardness-of-Optimization-The-Leyton-Brown-Nudelman",
            "title": {
                "fragments": [],
                "text": "Learning the Empirical Hardness of Optimization Problems: The Case of Combinatorial Auctions"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work focuses on the empirical hardness of the winner determination problem--an optimization problem arising in combinatorial auctions--when solved by ILOG's CPLEX software."
            },
            "venue": {
                "fragments": [],
                "text": "CP"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48942559"
                        ],
                        "name": "E. Nudelman",
                        "slug": "E.-Nudelman",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Nudelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Nudelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144339350"
                        ],
                        "name": "Galen Andrew",
                        "slug": "Galen-Andrew",
                        "structuredName": {
                            "firstName": "Galen",
                            "lastName": "Andrew",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Galen Andrew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075110009"
                        ],
                        "name": "Jim McFadden",
                        "slug": "Jim-McFadden",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "McFadden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jim McFadden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701353"
                        ],
                        "name": "Y. Shoham",
                        "slug": "Y.-Shoham",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Shoham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shoham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 11002294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99d8828311facb070084cf9b621062de57a6b6cc",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Although some algorithms are better than others on average, there is rarely a best algorithm for a given problem. Instead, different algorithms often perform well on different problem instances. Not surprisingly, this phenomenon is most pronounced among algorithms for solving NP-hard problems, when runtimes are highly variable from instance to instance. When algorithms exhibit high runtime variance, one is faced with the problem of deciding which algorithm to use for each particular instance; in 1976 Rice dubbed this the \"algorithm selection problem\" [8]. More recent work on this problem includes [5,4]."
            },
            "slug": "Boosting-as-a-Metaphor-for-Algorithm-Design-Leyton-Brown-Nudelman",
            "title": {
                "fragments": [],
                "text": "Boosting as a Metaphor for Algorithm Design"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Although some algorithms are better than others on average, there is rarely a best algorithm for a given problem; instead, different algorithms often perform well on different problem instances."
            },
            "venue": {
                "fragments": [],
                "text": "CP"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145634459"
                        ],
                        "name": "M. Garey",
                        "slug": "M.-Garey",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Garey",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Garey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150444582"
                        ],
                        "name": "David S. Johnson",
                        "slug": "David-S.-Johnson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Johnson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David S. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 121
                            }
                        ],
                        "text": "1 Propostitional Satisfiability (SAT) The propositional satisfiability (SAT) problem is the prototypical NP-hard problem (Garey and Johnson, 1979)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2211006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdede1e17c947540b50e6e2db9e8467ddc6e7336",
            "isKey": false,
            "numCitedBy": 47786,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Horn formulae play a prominent role in artificial intelligence and logic programming. In this paper we investigate the problem of optimal compression of propositional Horn production rule knowledge bases. The standard approach to this problem, consisting in the removal of redundant rules from a knowledge base, leads to an \"irredundant\" but not necessarily optimal knowledge base. We prove here that the number of rules in any irredundant Horn knowledge base involving n propositional variables is at most n 0 1 times the minimum possible number of rules. In order to formalize the optimal compression problem, we define a Boolean function of a knowledge base as being the function whose set of true points is the set of models of the knowledge base. In this way the optimal compression of production rule knowledge bases becomes a problem of Boolean function minimization. In this paper we prove that the minimization of Horn functions (i.e. Boolean functions associated to Horn knowledge bases) is..."
            },
            "slug": "Computers-and-Intractability:-A-Guide-to-the-Theory-Garey-Johnson",
            "title": {
                "fragments": [],
                "text": "Computers and Intractability: A Guide to the Theory of NP-Completeness"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This is the second edition of a quarterly column the purpose of which is to provide a continuing update to the list of problems (NP-complete and harder) presented by M. R. Garey and myself in the authors' book \u2018\u2018Computers and Intractability: A Guide to the Theory of NP-Completeness\u2019\u2019."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751239"
                        ],
                        "name": "R. Dechter",
                        "slug": "R.-Dechter",
                        "structuredName": {
                            "firstName": "Rina",
                            "lastName": "Dechter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dechter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109771"
                        ],
                        "name": "I. Rish",
                        "slug": "I.-Rish",
                        "structuredName": {
                            "firstName": "Irina",
                            "lastName": "Rish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Rish"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 142
                            }
                        ],
                        "text": "The binary parameter decides whether to initialize at random or by using a weight-bounded version of the Mini-Buckets approximation algorithm (Dechter and Rish, 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15943351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13fbf15dce48e9a4e00654b8d4e6f42718f75062",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 110,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a class of approximation algorithms that extend the idea of bounded-complexity inference, inspired by successful constraint propagation algorithms, to probabilistic inference and combinatorial optimization. The idea is to bound the dimensionality of dependencies created by inference algorithms. This yields a parameterized scheme, called mini-buckets, that offers adjustable trade-off between accuracy and efficiency. The mini-bucket approach to optimization problems, such as finding the most probable explanation (MPE) in Bayesian networks, generates both an approximate solution and bounds on the solution quality. We present empirical results demonstrating successful performance of the proposed approximation scheme for the MPE task, both on randomly generated problems and on realistic domains such as medical diagnosis and probabilistic decoding."
            },
            "slug": "Mini-buckets:-A-general-scheme-for-bounded-Dechter-Rish",
            "title": {
                "fragments": [],
                "text": "Mini-buckets: A general scheme for bounded inference"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A class of approximation algorithms that extend the idea of bounded-complexity inference, inspired by successful constraint propagation algorithms, to probabilistic inference and combinatorial optimization to bound the dimensionality of dependencies created by inference algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "JACM"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2150444582"
                        ],
                        "name": "David S. Johnson",
                        "slug": "David-S.-Johnson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Johnson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David S. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 215
                            }
                        ],
                        "text": "A central question in empirical comparisons of algorithms is whether one algorithm outperforms another one because it is fundamentally superior, or because its developers more successfully optimized its parameters (Johnson, 2002)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11281694,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "2db53cdcf4ba7c67321c022954c2b2be9c8a40bd",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an informal discussion of issues that arise when one attempts to analyze algorithms experimentally. It is based on lessons learned by the author over the course of more than a decade of experimentation, survey paper writing, refereeing, and lively discussions with other experimentalists. Although written from the perspective of a theoretical computer scientist, it is intended to be of use to researchers from all fields who want to study algorithms experimentally. It has two goals: first, to provide a useful guide to new experimentalists about how such work can best be performed and written up, and second, to challenge current researchers to think about whether their own work might be improved from a scientific point of view. With the latter purpose in mind, the author hopes that at least a few of his recommendations will be considered controversial."
            },
            "slug": "A-theoretician's-guide-to-the-experimental-analysis-Johnson",
            "title": {
                "fragments": [],
                "text": "A theoretician's guide to the experimental analysis of algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This paper presents an informal discussion of issues that arise when one attempts to analyze algorithms experimentally, based on lessons learned over the course of more than a decade of experimentation, survey paper writing, refereeing, and lively discussions with other experimentalists."
            },
            "venue": {
                "fragments": [],
                "text": "Data Structures, Near Neighbor Searches, and Methodology"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3318461"
                        ],
                        "name": "M. Gagliolo",
                        "slug": "M.-Gagliolo",
                        "structuredName": {
                            "firstName": "Matteo",
                            "lastName": "Gagliolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gagliolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 42
                            }
                        ],
                        "text": ", 2008), and dynamic algorithm portfolios (Gagliolo and Schmidhuber, 2006) already use similar predictive models, but do not yet reason about the setting of algorithm parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10510294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7b512f9e40f840464224d15a163946a0c1b2336",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional Meta-Learning requires long training times, and is often focused on optimizing performance quality, neglecting computational complexity. Algorithm Portfolios are more robust, but present similar limitations. We reformulate algorithm selection as a time allocation problem: all candidate algorithms are run in parallel, and their relative priorities are continually updated based on runtime information, with the aim of minimizing the time to reach a desired performance level. Each algorithm\u2019s priority is set based on its current time to solution, estimated according to a parametric model that is trained and used while solving a sequence of problems, gradually increasing its impact on the priority attribution. The use of censored sampling allows to train the model efficiently."
            },
            "slug": "Dynamic-Algorithm-Portfolios-Gagliolo-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Dynamic Algorithm Portfolios"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work reformulates algorithm selection as a time allocation problem: all candidate algorithms are run in parallel, and their relative priorities are continually updated based on runtime information, with the aim of minimizing the time to reach a desired performance level."
            },
            "venue": {
                "fragments": [],
                "text": "AI&M"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7977819"
                        ],
                        "name": "M. Streeter",
                        "slug": "M.-Streeter",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Streeter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Streeter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111046427"
                        ],
                        "name": "Stephen F. Smith",
                        "slug": "Stephen-F.-Smith",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smith",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen F. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15340098,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b51018f8088d7097d70562a5a9dfdbdaae387d4d",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The max k-armed bandit problem is a recently-introduced online optimization problem with practical applications to heuristic search. Given a set of k slot machines, each yielding payoff from a fixed (but unknown) distribution, we wish to allocate trials to the machines so as to maximize the maximum payoff received over a series of n trials. Previous work on the max k-armed bandit problem has assumed that payoffs are drawn from generalized extreme value (GEV) distributions. In this paper we present a simple algorithm, based on an algorithm for the classical k-armed bandit problem, that solves the max k-armed bandit problem effectively without making strong distributional assumptions. We demonstrate the effectiveness of our approach by applying it to the task of selecting among priority dispatching rules for the resource-constrained project scheduling problem with maximal time lags (RCPSP/max)."
            },
            "slug": "A-Simple-Distribution-Free-Approach-to-the-Max-Streeter-Smith",
            "title": {
                "fragments": [],
                "text": "A Simple Distribution-Free Approach to the Max k-Armed Bandit Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The effectiveness of this approach is demonstrated by applying it to the task of selecting among priority dispatching rules for the resource-constrained project scheduling problem with maximal time lags (RCPSP/max)."
            },
            "venue": {
                "fragments": [],
                "text": "CP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393656234"
                        ],
                        "name": "Joao Marques-Silva",
                        "slug": "Joao-Marques-Silva",
                        "structuredName": {
                            "firstName": "Joao",
                            "lastName": "Marques-Silva",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joao Marques-Silva"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 942499,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "c4d3992a801af2862011e03eeabfdaa5a926393f",
            "isKey": false,
            "numCitedBy": 211,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the practical impact of the branching heuristics used in Propositional Satisfiability (SAT) algorithms, when applied to solving real-world instances of SAT. In addition, different SAT algorithms are experimentally evaluated. The main conclusion of this study is that even though branching heuristics are crucial for solving SAT, other aspects of the organization of SAT algorithms are also essential. Moreover, we provide empirical evidence that for practical instances of SAT, the search pruning techniques included in the most competitive SAT algorithms may be of more fundamental significance than branching heuristics."
            },
            "slug": "The-Impact-of-Branching-Heuristics-in-Propositional-Marques-Silva",
            "title": {
                "fragments": [],
                "text": "The Impact of Branching Heuristics in Propositional Satisfiability Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Empirical evidence is provided that for practical instances of SAT, the search pruning techniques included in the most competitive SAT algorithms may be of more fundamental significance than branching heuristics."
            },
            "venue": {
                "fragments": [],
                "text": "EPIA"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3348225"
                        ],
                        "name": "V. Cicirello",
                        "slug": "V.-Cicirello",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Cicirello",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Cicirello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111046427"
                        ],
                        "name": "Stephen F. Smith",
                        "slug": "Stephen-F.-Smith",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smith",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen F. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5617683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "53b61519e4fa067036213badca926936c095cd1f",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The multiarmed bandit is often used as an analogy for the tradeoff between exploration and exploitation in search problems. The classic problem involves allocating trials to the arms of a multiarmed slot machine to maximize the expected sum of rewards. We pose a new variation of the multiarmed bandit--the Max K-Armed Bandit--in which trials must be allocated among the arms to maximize the expected best single sample reward of the series of trials. Motivation for the Max K-Armed Bandit is the allocation of restarts among a set of multistart stochastic search algorithms. We present an analysis of this Max K-Armed Bandit showing under certain assumptions that the optimal strategy allocates trials to the observed best arm at a rate increasing double exponentially relative to the other arms. This motivates an exploration strategy that follows a Boltzmann distribution with an exponentially decaying temperature parameter. We compare this exploration policy to policies that allocate trials to the observed best arm at rates faster (and slower) than double exponentially. The results confirm, for two scheduling domains, that the double exponential increase in the rate of allocations to the observed best heuristic outperfonns the other approaches."
            },
            "slug": "The-Max-K-Armed-Bandit:-A-New-Model-of-Exploration-Cicirello-Smith",
            "title": {
                "fragments": [],
                "text": "The Max K-Armed Bandit: A New Model of Exploration Applied to Search Heuristic Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An analysis of this Max K-Armed Bandit shows under certain assumptions that the optimal strategy allocates trials to the observed best arm at a rate increasing double exponentially relative to the other arms, which motivates an exploration strategy that follows a Boltzmann distribution with an exponentially decaying temperature parameter."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741101"
                        ],
                        "name": "Oren Etzioni",
                        "slug": "Oren-Etzioni",
                        "structuredName": {
                            "firstName": "Oren",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oren Etzioni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2933110"
                        ],
                        "name": "Ruth Etzioni",
                        "slug": "Ruth-Etzioni",
                        "structuredName": {
                            "firstName": "Ruth",
                            "lastName": "Etzioni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruth Etzioni"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7368763,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3124bf6eaaa233a8a799ed5080f4a9fa1c757674",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Speedup learning systems are typically evaluated by comparing their impact on a problem solver's performance. The impact is measured by running the problem solver, before and after learning, on a sample of problems randomly drawn from some distribution. Often, the experimenter imposes a bound on the CPU time the problem solver is allowed to spend on any individual problem. Segre et al. (1991) argue that the experimenter's choice of time bound can bias the results of the experiment. To address this problem, we present statistical hypothesis tests specifically designed to analyze speedup data and eliminate this bias. We apply the tests to the data reported by Etzioni (1990a) and show that most (but not all) of the speedups observed are statistically significant."
            },
            "slug": "Statistical-methods-for-analyzing-speedup-learning-Etzioni-Etzioni",
            "title": {
                "fragments": [],
                "text": "Statistical methods for analyzing speedup learning experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Statistical hypothesis tests specifically designed to analyze speedup data and eliminate bias are presented and show that most (but not all) of the speedups observed are statistically significant."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143854701"
                        ],
                        "name": "D. Tompkins",
                        "slug": "D.-Tompkins",
                        "structuredName": {
                            "firstName": "Dave",
                            "lastName": "Tompkins",
                            "middleNames": [
                                "A.",
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tompkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3118503,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a9a0c587df01ea7b78d0356592f19846b797fa1",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce UBCSAT, a new implementation and experimentation environment for Stochastic Local Search (SLS) algorithms for SAT and MAX-SAT. Based on a novel triggered procedure architecture, UBCSAT provides implementations of numerous well-known and widely used SLS algorithms for SAT and MAX-SAT, including GSAT, WalkSAT, and SAPS; these implementations generally match or exceed the efficiency of the respective original reference implementations. Through numerous reporting and statistical features, including the measurement of run-time distributions, UBCSAT facilitates the advanced empirical analysis of these algorithms. New algorithm variants, SLS algorithms, and reporting features can be added to UBCSAT in a straightforward and efficient way. UBCSAT is implemented in C and runs on numerous platforms and operating systems; it is publicly and freely available at www.satlib.org/ubcsat."
            },
            "slug": "UBCSAT:-An-Implementation-and-Experimentation-for-&-Tompkins-Hoos",
            "title": {
                "fragments": [],
                "text": "UBCSAT: An Implementation and Experimentation Environment for SLS Algorithms for SAT & MAX-SAT"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "UBCSAT provides implementations of numerous well-known and widely used SLS algorithms for SAT and MAX-SAT, including GSAT, WalkS AT, and SAPS; these implementations generally match or exceed the efficiency of the respective original reference implementations."
            },
            "venue": {
                "fragments": [],
                "text": "SAT"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2015529"
                        ],
                        "name": "L. D. Gaspero",
                        "slug": "L.-D.-Gaspero",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Gaspero",
                            "middleNames": [
                                "Di"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Gaspero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2135235"
                        ],
                        "name": "Andrea Schaerf",
                        "slug": "Andrea-Schaerf",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Schaerf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrea Schaerf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8847363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99df40a2cbef282b9c09667d1a074c28944d936c",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a software tool, called EASYSYN++, for the automatic synthesis of the source code for a set of stochastic local search (SLS) algorithms. EASYSYN++ uses C++ as object language and relies on EASYLOCAL++, a C++ framework for the development of SLS algorithms. EASYSYN++ is particularly suitable for the frequent case of having many neighborhood relations that are potentially useful."
            },
            "slug": "EasySyn++:-A-Tool-for-Automatic-Synthesis-of-Local-Gaspero-Schaerf",
            "title": {
                "fragments": [],
                "text": "EasySyn++: A Tool for Automatic Synthesis of Stochastic Local Search Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A software tool, called EASYSYN++, for the automatic synthesis of the source code for a set of stochastic local search (SLS) algorithms, which is particularly suitable for the frequent case of having many neighborhood relations that are potentially useful."
            },
            "venue": {
                "fragments": [],
                "text": "SLS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "114576747"
                        ],
                        "name": "Chun-Hung Chen",
                        "slug": "Chun-Hung-Chen",
                        "structuredName": {
                            "firstName": "Chun-Hung",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Hung Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2235704"
                        ],
                        "name": "Jianwu Lin",
                        "slug": "Jianwu-Lin",
                        "structuredName": {
                            "firstName": "Jianwu",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianwu Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3091207"
                        ],
                        "name": "E. Y\u00fccesan",
                        "slug": "E.-Y\u00fccesan",
                        "structuredName": {
                            "firstName": "Enver",
                            "lastName": "Y\u00fccesan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Y\u00fccesan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7759889"
                        ],
                        "name": "S. Chick",
                        "slug": "S.-Chick",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Chick",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chick"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17081791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89d5670c7fc763402f65cb6f8aac77486785cccd",
            "isKey": false,
            "numCitedBy": 630,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Ordinal Optimization has emerged as an efficient technique for simulation and optimization. Exponential convergence rates can be achieved in many cases. In this paper, we present a new approach that can further enhance the efficiency of ordinal optimization. Our approach determines a highly efficient number of simulation replications or samples and significantly reduces the total simulation cost. We also compare several different allocation procedures, including a popular two-stage procedure in simulation literature. Numerical testing shows that our approach is much more efficient than all compared methods. The results further indicate that our approach can obtain a speedup factor of higher than 20 above and beyond the speedup achieved by the use of ordinal optimization for a 210-design example."
            },
            "slug": "Simulation-Budget-Allocation-for-Further-Enhancing-Chen-Lin",
            "title": {
                "fragments": [],
                "text": "Simulation Budget Allocation for Further Enhancing the Efficiency of Ordinal Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a new approach that can further enhance the efficiency of ordinal optimization, which determines a highly efficient number of simulation replications or samples and significantly reduces the total simulation cost."
            },
            "venue": {
                "fragments": [],
                "text": "Discret. Event Dyn. Syst."
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2353862"
                        ],
                        "name": "N. E\u00e9n",
                        "slug": "N.-E\u00e9n",
                        "structuredName": {
                            "firstName": "Niklas",
                            "lastName": "E\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. E\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2998689"
                        ],
                        "name": "Niklas S\u00f6rensson",
                        "slug": "Niklas-S\u00f6rensson",
                        "structuredName": {
                            "firstName": "Niklas",
                            "lastName": "S\u00f6rensson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Niklas S\u00f6rensson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 2
                            }
                        ],
                        "text": "0 (E\u00e9n and S\u00f6rensson, 2003), the winner of the industrial category of the 2005 SAT Competition and of the 2006 SAT Race."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 2
                            }
                        ],
                        "text": "0 (E\u00e9n and S\u00f6rensson, 2003)), we ultimately achieved speedup factors of 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9774288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ccba505e7e719bd9dd3bf83cca7497d29ffb87c",
            "isKey": false,
            "numCitedBy": 3054,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we present a small, complete, and efficient SAT-solver in the style of conflict-driven learning, as exemplified by Chaff. We aim to give sufficient details about implementation to enable the reader to construct his or her own solver in a very short time.This will allow users of SAT-solvers to make domain specific extensions or adaptions of current state-of-the-art SAT-techniques, to meet the needs of a particular application area. The presented solver is designed with this in mind, and includes among other things a mechanism for adding arbitrary boolean constraints. It also supports solving a series of related SAT-problems efficiently by an incremental SAT-interface."
            },
            "slug": "An-Extensible-SAT-solver-E\u00e9n-S\u00f6rensson",
            "title": {
                "fragments": [],
                "text": "An Extensible SAT-solver"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "This article presents a small, complete, and efficient SAT-solver in the style of conflict-driven learning, as exemplified by Chaff, and includes among other things a mechanism for adding arbitrary boolean constraints."
            },
            "venue": {
                "fragments": [],
                "text": "SAT"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48942559"
                        ],
                        "name": "E. Nudelman",
                        "slug": "E.-Nudelman",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Nudelman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Nudelman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2883187"
                        ],
                        "name": "Alex Devkar",
                        "slug": "Alex-Devkar",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Devkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Devkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701353"
                        ],
                        "name": "Y. Shoham",
                        "slug": "Y.-Shoham",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Shoham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shoham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 63
                            }
                        ],
                        "text": "1 Instance Features Existing work on empirical hardness models (Leyton-Brown et al., 2002; Nudelman et al., 2004; Hutter et al., 2006; Xu et al., 2007a; Leyton-Brown et al., 2009) convincingly demonstrated that it is possible to predict algorithm runtime based on features of the problem instance to be solved."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 145
                            }
                        ],
                        "text": "Instead of applying PCA, we could have used feature selection methods (Guyon and Elisseeff, 2003), such as forward selection as done in SATzilla (Nudelman et al., 2004; Xu et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 61
                            }
                        ],
                        "text": "206\n13.1 11 groups of SAT features; these were introduced by Nudelman et al. (2004) and Xu et al. (2008, 2009). . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 211 13.2 Eight groups of features for the mixed integer programming problem. . . . . . . . . 212 13.3 RF model quality\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 6487916,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdaaff55fd2f1b63f8952dbc9bf81e8f5e706357",
            "isKey": true,
            "numCitedBy": 204,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well known that the ratio of the number of clauses to the number of variables in a random k-SAT instance is highly correlated with the instance's empirical hardness. We consider the problem of identifying such features of random SAT instances automatically using machine learning. We describe and analyze models for three SAT solvers - kcnfs, oksolver and satz - and for two different distributions of instances: uniform random 3-SAT with varying ratio of clauses-to-variables, and uniform random 3-SAT with fixed ratio of clauses-to-variables. We show that surprisingly accurate models can be built in all cases. Furthermore, we analyze these models to determine which features are most useful in predicting whether an instance will be hard to solve. Finally we discuss the use of our models to build SATzilla, an algorithm portfolio for SAT."
            },
            "slug": "Understanding-Random-SAT:-Beyond-the-Ratio-Nudelman-Leyton-Brown",
            "title": {
                "fragments": [],
                "text": "Understanding Random SAT: Beyond the Clauses-to-Variables Ratio"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work describes and analyzes models for three SAT solvers and for two different distributions of instances, showing that surprisingly accurate models can be built in all cases and determining which features are most useful in predicting whether an instance will be hard to solve."
            },
            "venue": {
                "fragments": [],
                "text": "CP"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34673652"
                        ],
                        "name": "R. Bryant",
                        "slug": "R.-Bryant",
                        "structuredName": {
                            "firstName": "Randal",
                            "lastName": "Bryant",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Bryant"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10385726,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39dc786a942284e293eab1440f0eccbffdf0a4bf",
            "isKey": false,
            "numCitedBy": 8202,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a new data structure for representing Boolean functions and an associated set of manipulation algorithms. Functions are represented by directed, acyclic graphs in a manner similar to the representations introduced by Lee [1] and Akers [2], but with further restrictions on the ordering of decision variables in the graph. Although a function requires, in the worst case, a graph of size exponential in the number of arguments, many of the functions encountered in typical applications have a more reasonable representation. Our algorithms have time complexity proportional to the sizes of the graphs being operated on, and hence are quite efficient as long as the graphs do not grow too large. We present experimental results from applying these algorithms to problems in logic design verification that demonstrate the practicality of our approach."
            },
            "slug": "Graph-Based-Algorithms-for-Boolean-Function-Bryant",
            "title": {
                "fragments": [],
                "text": "Graph-Based Algorithms for Boolean Function Manipulation"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "Experimental results from applying a new data structure for representing Boolean functions and an associated set of manipulation algorithms to problems in logic design verification demonstrate the practicality of this approach."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Computers"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725540"
                        ],
                        "name": "Mihai Oltean",
                        "slug": "Mihai-Oltean",
                        "structuredName": {
                            "firstName": "Mihai",
                            "lastName": "Oltean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mihai Oltean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 242
                            }
                        ],
                        "text": "In his experiments the automatically-evolved genetic algorithms outperformed standard implementations of genetic algorithms on a variety of tasks, such as function optimization, the travelling salesperson and the quadratic assignment problem (Oltean, 2005)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9346944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ffbcca888323c72c6f2c5819d4e7affd7e60a3c",
            "isKey": false,
            "numCitedBy": 127,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "A new model for evolving Evolutionary Algorithms is proposed in this paper. The model is based on the Linear Genetic Programming (LGP) technique. Every LGP chromosome encodes an EA which is used for solving a particular problem. Several Evolutionary Algorithms for function optimization, the Traveling Salesman Problem and the Quadratic Assignment Problem are evolved by using the considered model. Numerical experiments show that the evolved Evolutionary Algorithms perform similarly and sometimes even better than standard approaches for several well-known benchmarking problems."
            },
            "slug": "Evolving-Evolutionary-Algorithms-Using-Linear-Oltean",
            "title": {
                "fragments": [],
                "text": "Evolving Evolutionary Algorithms Using Linear Genetic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A new model for evolving Evolutionary Algorithms based on the Linear Genetic Programming technique is proposed in this paper and it is shown that the evolved EvolutionaryAlgorithms perform similarly and sometimes even better than standard approaches for several well-known benchmarking problems."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2824278"
                        ],
                        "name": "P. Kilby",
                        "slug": "P.-Kilby",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Kilby",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kilby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1814690"
                        ],
                        "name": "J. Slaney",
                        "slug": "J.-Slaney",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Slaney",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Slaney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685896"
                        ],
                        "name": "S. Thi\u00e9baux",
                        "slug": "S.-Thi\u00e9baux",
                        "structuredName": {
                            "firstName": "Sylvie",
                            "lastName": "Thi\u00e9baux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Thi\u00e9baux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733716"
                        ],
                        "name": "T. Walsh",
                        "slug": "T.-Walsh",
                        "structuredName": {
                            "firstName": "Toby",
                            "lastName": "Walsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Walsh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7431470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8ebeab86d17b42539317bbf2373e9f57682c6948",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose two new online methods for estimating the size of a backtracking search tree. The first method is based on a weighted sample of the branches visited by chronological backtracking. The second is a recursive method based on assuming that the unexplored part of the search tree will be similar to the part we have so far explored. We compare these methods against an old method due to Knuth based on random probing. We show that these methods can reliably estimate the size of search trees explored by both optimization and decision procedures. We also demonstrate that these methods for estimating search tree size can be used to select the algorithm likely to perform best on a particular problem instance."
            },
            "slug": "Estimating-Search-Tree-Size-Kilby-Slaney",
            "title": {
                "fragments": [],
                "text": "Estimating Search Tree Size"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "This work proposes two new online methods for estimating the size of a backtracking search tree based on a weighted sample of the branches visited by chronological backtracking and compares them against an old method due to Knuth based on random probing."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3149529"
                        ],
                        "name": "Jean-No\u00ebl Monette",
                        "slug": "Jean-No\u00ebl-Monette",
                        "structuredName": {
                            "firstName": "Jean-No\u00ebl",
                            "lastName": "Monette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-No\u00ebl Monette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31523977"
                        ],
                        "name": "Y. Deville",
                        "slug": "Y.-Deville",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Deville",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Deville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8388443"
                        ],
                        "name": "Pascal Van Hentenryck",
                        "slug": "Pascal-Van-Hentenryck",
                        "structuredName": {
                            "firstName": "Pascal",
                            "lastName": "Hentenryck",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pascal Van Hentenryck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 920913,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7d7775b0198b900914f43d7e6679200737a59e0",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the aeon system whose aim is to synthesize scheduling algorithms from high-level models. Aeon, which is entirely written in comet, receives as input a high-level model for a scheduling application which is then analyzed to generate a dedicated scheduling algorithm exploiting the structure of the model. Aeon provides a variety of synthesizers for generating complete or heuristic algorithms. Moreover, synthesizers are compositional, making it possible to generate complex hybrid algorithms naturally. Preliminary experimental results indicate that this approach may be competitive with state-of-the-art search algorithms."
            },
            "slug": "Aeon:-Synthesizing-Scheduling-Algorithms-from-Monette-Deville",
            "title": {
                "fragments": [],
                "text": "Aeon: Synthesizing Scheduling Algorithms from High-Level Models"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Aeon, which is entirely written in comet, receives as input a high-level model for a scheduling application which is then analyzed to generate a dedicated scheduling algorithm exploiting the structure of the model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684799"
                        ],
                        "name": "T. St\u00fctzle",
                        "slug": "T.-St\u00fctzle",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "St\u00fctzle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. St\u00fctzle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 134
                            }
                        ],
                        "text": "However, in the context of optimizing functions with many categorical parameters, simulated annealing often yields suboptimal results (Hoos and St\u00fctzle, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 128
                            }
                        ],
                        "text": "It also relies on Gaussian noise assumptions that are often not met by cost distributions in an algorithm configuration setting (Hoos and St\u00fctzle, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42920353,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7b58ba0c8571f36226060577e4640d05b5675e62",
            "isKey": false,
            "numCitedBy": 1538,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Prologue Part I. Foundations 1. Introduction 2. SLS Methods 3. Generalised Local Search Machines 4. Empirical Analysis of SLS Algorithms 5. Search Space Structure and SLS Performance Part II. Applications 6. SAT and Constraint Satisfaction 7. MAX-SAT and MAX-CSP 8. Travelling Salesman Problems 9. Scheduling Problems 10. Other Combinatorial Problems Epilogue Glossary"
            },
            "slug": "Stochastic-Local-Search:-Foundations-&-Applications-Hoos-St\u00fctzle",
            "title": {
                "fragments": [],
                "text": "Stochastic Local Search: Foundations & Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This prologue explains the background to SLS, and some examples of applications can be found in SAT and Constraint Satisfaction, as well as some of the algorithms used to solve these problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759010"
                        ],
                        "name": "E. Brewer",
                        "slug": "E.-Brewer",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brewer",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brewer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6987412,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3d44e0dd264aa2805b8f88ff3caeab91c5e74f7",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 114,
            "paperAbstract": {
                "fragments": [],
                "text": "Although there is some amount of portability across today\u2019s supercomputers, current systems cannot adapt to the wide variance in basic costs, such as communication overhead, bandwidth and synchronization. Such costs vary by orders of magnitude from platforms like the Alewife multiprocessor to networks of workstations connected via an ATM network. The huge range of costs implies that for many applications, no single algorithm or data layout is optimal across all platforms. The goal of this work is to provide high-level scientific libraries that provide portability with near optimal performance across the full range of scalable parallel computers. Towards this end, we have built a prototype high-level library \u201ccompiler\u201d that automatically selects and optimizes the best implementation for a library among a predefined set of parameterized implementations. The selection and optimization are based on simple models that are statistically fitted to profiling data for the target platform. These models encapsulate platform performance in a compact form, and are thus useful by themselves. The library designer provides the implementations and the structure of the models, but not the coefficients. Model calibration is automated and occurs only when the environment changes (such as for a new platform). We look at applications on four platforms with varying costs: the CM-5 and three simulated platforms. We use PROTEUS to simulate Alewife, Paragon, and a network of workstations connected via an ATM network. For a PDE application with more than 40,000 runs, the model-based selection correctly picks the best data layout more than 99% of the time on each platform. For a parallel sorting library, it achieves similar results, correctly selecting among sample sort and several versions of radix sort more than 99% of the time on all platforms. When it picks a suboptimal choice, the average penalty for the error is only about 2%. The benefit of the correct choice is often a factor of two, and in general can be an order of magnitude. The system can also determine the optimal value for implementation parameters, even though these values depend on the platform. In particular, for the stencil library, we show that the models can predict the optimal number of extra gridpoints to allocate to boundary processors, which eliminates the load imbalance due to their reduced communication. For radix sort, the optimizer reliably determines the best radix for the target platform and workload. Because the instantiation of the libraries is completely automatic, end users get portability with near optimal performance on each platform: that is, they get the best implementation with the best parameter settings for their target platform and workload. By automatically capturing the performance of the underlying system, we ensure that the selection and optimization decisions are robust across platforms and over time. Finally, we also present a high-performance communication layer, called Strata, that forms the implementation base for the libraries. Strata exploits several novel techniques that increase the performance and predictability of communication: these techniques achieve the full bandwidth of the CM-5 and can improve application performance by up to a factor of four. Strata also supports split-phase synchronization operations and provides substantial support for debugging."
            },
            "slug": "Portable-high-performance-superconducting:-Brewer",
            "title": {
                "fragments": [],
                "text": "Portable high-performance superconducting: high-level platform-dependent optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A prototype high-level library \u201ccompiler\u201d is built that automatically selects and optimizes the best implementation for a library among a predefined set of parameterized implementations, based on simple models that are statistically fitted to profiling data for the target platform."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053221855"
                        ],
                        "name": "Mark Pearson",
                        "slug": "Mark-Pearson",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Pearson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Pearson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701353"
                        ],
                        "name": "Y. Shoham",
                        "slug": "Y.-Shoham",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Shoham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shoham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 113
                            }
                        ],
                        "text": "Scenario Type of benchmark instances & citation # training # test CPLEX-REGIONS200 Combinatorial Auctions (CATS) (Leyton-Brown et al., 2000) 1 000 1 000 CPLEX-MJA Machine-Job Assignment (BCOL) (Akt\u00fcrk et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 100
                            }
                        ],
                        "text": "We created two instance sets using the generator provided with the Combinatorial Auction Test Suite (Leyton-Brown et al., 2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6114062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "102edd6b6ec078daa064ec8bda142aa7bc60431d",
            "isKey": false,
            "numCitedBy": 413,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "General combinatorial auctions\u2014auctions in which bidders place unrestricted bids for bundles of goods\u2014are the subject of increasing study. Much of this work has focused on algorithms for finding an optimal or approximately optimal set of winning bids. Comparatively little attention has been paid to methodical evaluation and comparison of these algorithms. In particular, there has not been a systematic discussion of appropriate data sets that can serve as universally accepted and well motivated benchmarks. In this paper we present a suite of distribution families for generating realistic, economically motivated combinatorial bids in five broad real-world domains. We hope that this work will yield many comments, criticisms and extensions, bringing the community closer to a universal combinatorial auction test suite."
            },
            "slug": "Towards-a-universal-test-suite-for-combinatorial-Leyton-Brown-Pearson",
            "title": {
                "fragments": [],
                "text": "Towards a universal test suite for combinatorial auction algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper presents a suite of distribution families for generating realistic, economically motivated combinatorial bids in five broad real-world domains, and hopes that this work will yield many comments, criticisms and extensions, bringing the community closer to a universal combinatorsial auction test suite."
            },
            "venue": {
                "fragments": [],
                "text": "EC '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2536307"
                        ],
                        "name": "C. Gebruers",
                        "slug": "C.-Gebruers",
                        "structuredName": {
                            "firstName": "Cormac",
                            "lastName": "Gebruers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gebruers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733142"
                        ],
                        "name": "Brahim Hnich",
                        "slug": "Brahim-Hnich",
                        "structuredName": {
                            "firstName": "Brahim",
                            "lastName": "Hnich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brahim Hnich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2975599"
                        ],
                        "name": "D. Bridge",
                        "slug": "D.-Bridge",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Bridge",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bridge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2606753"
                        ],
                        "name": "Eugene C. Freuder",
                        "slug": "Eugene-C.-Freuder",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Freuder",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene C. Freuder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10189860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77e8e8e8f40322ba99318462be35f0059284d4d9",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Constraint programming is a powerful paradigm that offers many different strategies for solving problems. Choosing a good strategy is difficult; choosing a poor strategy wastes resources and may result in a problem going unsolved. We show how Case-Based Reasoning can be used to select good strategies. We design experiments which demonstrate that, on two problems with quite different characteristics, CBR can outperform four other strategy selection techniques."
            },
            "slug": "Using-CBR-to-Select-Solution-Strategies-in-Gebruers-Hnich",
            "title": {
                "fragments": [],
                "text": "Using CBR to Select Solution Strategies in Constraint Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown how Case-Based Reasoning can be used to select good strategies and designed experiments which demonstrate that, on two problems with quite different characteristics, CBR can outperform four other strategy selection techniques."
            },
            "venue": {
                "fragments": [],
                "text": "ICCBR"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7977819"
                        ],
                        "name": "M. Streeter",
                        "slug": "M.-Streeter",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Streeter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Streeter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111046427"
                        ],
                        "name": "Stephen F. Smith",
                        "slug": "Stephen-F.-Smith",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Smith",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen F. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 19
                            }
                        ],
                        "text": "While earlier work (Cicirello and Smith, 2004, 2005; Streeter and Smith, 2006a) assumed generalized extreme value distributions of performance, Streeter and Smith (2006b) applied a distribution-free approach."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3003475,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8dc26a3bfee8472d7d38897b705d45c86c61747c",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an asymptotically optimal algorithm for the max variant of the k-armed bandit problem. Given a set of k slot machines, each yielding payoff from a fixed (but unknown) distribution, we wish to allocate trials to the machines so as to maximize the expected maximum payoff received over a series of n trials. Subject to certain distributional assumptions, we show that O(k ln(k/\u03b4) ln(n)2/e2) trials are sufficient to identify, with probability at least 1 - \u03b4, a machine whose expected maximum payoff is within e of optimal. This result leads to a strategy for solving the problem that is asymptotically optimal in the following sense: the gap between the expected maximum payoff obtained by using our strategy for n trials and that obtained by pulling the single best arm for all n trials approaches zero as n \u2192 \u221e."
            },
            "slug": "An-Asymptotically-Optimal-Algorithm-for-the-Max-Streeter-Smith",
            "title": {
                "fragments": [],
                "text": "An Asymptotically Optimal Algorithm for the Max k-Armed Bandit Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The gap between the expected maximum payoff obtained by using the asymptotically optimal strategy for n trials and that obtained by pulling the single best arm for all n trials approaches zero as n \u2192 \u221e."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690271"
                        ],
                        "name": "Henry A. Kautz",
                        "slug": "Henry-A.-Kautz",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Kautz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Henry A. Kautz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064595436"
                        ],
                        "name": "Eric Horvitz",
                        "slug": "Eric-Horvitz",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Horvitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Horvitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760493"
                        ],
                        "name": "Y. Ruan",
                        "slug": "Y.-Ruan",
                        "structuredName": {
                            "firstName": "Yongshao",
                            "lastName": "Ruan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ruan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144659218"
                        ],
                        "name": "C. Gomes",
                        "slug": "C.-Gomes",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Gomes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gomes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744679"
                        ],
                        "name": "B. Selman",
                        "slug": "B.-Selman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Selman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Selman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1426297,
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "id": "6611dd4f3adf99f173b9fa353118ef8661ac93dd",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe theoretical results and empirical study of context-sensitive restart policies for randomized search procedures. The methods generalize previous results on optimal restart policies by exploiting dynamically updated beliefs about the probability distribution for run time. Rather than assuming complete knowledge or zero knowledge about the run-time distribution, we formulate restart policies that consider real-time observations about properties of instances and the solver's activity. We describe background work on the application of Bayesian methods to build predictive models for run time, introduce an optimal policy for dynamic restarts that considers predictions about run time, and perform a comparative Study of traditional fixed versus dynamic restart policies."
            },
            "slug": "Dynamic-restart-policies-Kautz-Horvitz",
            "title": {
                "fragments": [],
                "text": "Dynamic restart policies"
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2953896"
                        ],
                        "name": "Alper Atamt\u00fcrk",
                        "slug": "Alper-Atamt\u00fcrk",
                        "structuredName": {
                            "firstName": "Alper",
                            "lastName": "Atamt\u00fcrk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alper Atamt\u00fcrk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143744732"
                        ],
                        "name": "J. Mu\u00f1oz",
                        "slug": "J.-Mu\u00f1oz",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Mu\u00f1oz",
                            "middleNames": [
                                "Carlos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mu\u00f1oz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 56
                            }
                        ],
                        "text": ", 2007) 172 171 CPLEX-CLS Capacitated Lot Sizing (BCOL) (Atamt\u00fcrk and Mu\u00f1oz, 2004) 50 50 CPLEX-MIK Mixed-integer knapsack (BCOL) (Atamt\u00fcrk, 2003) 60 60"
                    },
                    "intents": []
                }
            ],
            "corpusId": 6355570,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "ba5d3c36d7286707fd6167ccc7da50d09a09f0fb",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.The lot-sizing polytope is a fundamental structure contained in many practical production planning problems. Here we study this polytope and identify facet\u2013defining inequalities that cut off all fractional extreme points of its linear programming relaxation, as well as liftings from those facets. We give a polynomial\u2013time combinatorial separation algorithm for the inequalities when capacities are constant. We also report computational experiments on solving the lot\u2013sizing problem with varying cost and capacity characteristics."
            },
            "slug": "A-study-of-the-lot-sizing-polytope-Atamt\u00fcrk-Mu\u00f1oz",
            "title": {
                "fragments": [],
                "text": "A study of the lot-sizing polytope"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A polynomial\u2013time combinatorial separation algorithm is given for the inequalities of the lot-sizing polytope that cut off all fractional extreme points of its linear programming relaxation, as well as liftings from those facets."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52531606"
                        ],
                        "name": "Babi\u0107 Domagoj",
                        "slug": "Babi\u0107-Domagoj",
                        "structuredName": {
                            "firstName": "Babi\u0107",
                            "lastName": "Domagoj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Babi\u0107 Domagoj"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 76
                            }
                        ],
                        "text": "In particular, we automatically constructed different instantiations of the SPEAR algorithm and thereby substantially improved the state of the art for two sets of SAT-encoded industrial verification problems (Hutter et al., 2007a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 5
                            }
                        ],
                        "text": "Both SPEAR and SATENSTEIN were configured using ParamILS, one of the automated configuration procedures we introduce in this thesis (see Chapter 5)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 49
                            }
                        ],
                        "text": "Our target algorithm in this case study is SPEAR (Babi\u0107, 2008), a modular arithmetic decision procedure and SAT solver, developed by Domagoj Babi\u0107 in support of the CALYSTO static checker (Babi\u0107 and Hu, 2007b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 280
                            }
                        ],
                        "text": "90 6.2 Algorithm Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n6.2.1 Manual Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 6.2.2 Initial Performance Assessment . . . . . . . . . . . . . . . . . . . . 93\n6.3 Automated Configuration of SPEAR . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "99 6.6 MiniSAT 2.0 vs SPEAR configured for specific benchmark sets . . . . . . . . 100\n7.1 Speedup of RANDOMSEARCH due to adaptive capping . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "96 6.3 SPEAR default vs SPEAR configured for SAT competition, on BMC and SWV ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 124
                            }
                        ],
                        "text": "86 5.4 Configuration of GLS+, SAPS, and SAT4J . . . . . . . . . . . . . . . . . . 87\n6.1 Summary of results for configuring SPEAR . . . . . . . . . . . . . . . . . . 101\n7.1 Speedup of RANDOMSEARCH due to adaptive capping . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 29
                            }
                        ],
                        "text": "94 6.3.1 Parameterization of SPEAR . . . . . . . . . . . . . . . . . . . . . . . 94 6.3.2 Configuration for the 2007 SAT Competition . . . . . . . . . . . . . 95 6.3.3 Configuration for Specific Applications . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "97 6.4 SPEAR configured for general benchmark set vs SPEAR configured for specific benchmark sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 6.5 SPEAR default vs SPEAR configured for specific benchmark sets . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "88\n6.1 MiniSAT 2.0 vs SPEAR default . . . . . . . . . . . . . . . . . . . . . . . . . 93 6.2 SPEAR default vs SPEAR configured for SAT competition, on SAT competition instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 48
                            }
                        ],
                        "text": "We provide full details on the configuration of SPEAR in Chapter 6 and review the SATENSTEIN application in Section 8.3.1."
                    },
                    "intents": []
                }
            ],
            "corpusId": 56628722,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3fbb73f4bf4ad228d8312b6a87f14ce82cc9a98",
            "isKey": true,
            "numCitedBy": 35,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "Software bugs are expensive. Recent estimates by the US National Institute of Standards and Technology claim that the cost of software bugs to the US economy alone is approximately 60 billion USD annually. As society becomes increasingly software-dependent, bugs also reduce our productivity and threaten our safety and security. Decreasing these direct and indirect costs represents a significant research challenge as well as an opportunity for businesses. Automatic software bug-finding and verification tools have a potential to completely revolutionize the software engineering industry by improving reliability and decreasing development costs. Since software analysis is in general undecidable, automatic tools have to use various abstractions to make the analysis computationally tractable. Abstraction is a double-edged sword: coarse abstractions, in general, yield easier verification, but also less precise results. This thesis focuses on exploiting the structure of software for abstracting away irrelevant behavior. Programmers tend to organize code into objects and functions, which effectively represent natural abstraction boundaries. Humans use such structural abstractions to simplify their mental models of software and for constructing informal explanations of why a piece of code should work. A natural question to ask is: How can automatic bug-finding tools exploit the same natural abstractions? This thesis offers possible answers. More specifically, I present three novel ways to exploit structure at three different steps of the software analysis process. First, I show how symbolic execution can preserve the data-flow dependencies of the original code while constructing compact symbolic representations of programs. Second, I propose 1For details, see [1]."
            },
            "slug": "Exploiting-structure-for-scalable-software-Domagoj",
            "title": {
                "fragments": [],
                "text": "Exploiting structure for scalable software verification"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Three novel ways to exploit structure at three different steps of the software analysis process are presented, showing how symbolic execution can preserve the data-flow dependencies of the original code while constructing compact symbolic representations of programs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701353"
                        ],
                        "name": "Y. Shoham",
                        "slug": "Y.-Shoham",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Shoham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shoham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388404060"
                        ],
                        "name": "Kevin Leyton-Brown",
                        "slug": "Kevin-Leyton-Brown",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Leyton-Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Leyton-Brown"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 54
                            }
                        ],
                        "text": ", 1998), and square root or cube root transformations (Leyton-Brown, 2003)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 151251929,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "7084e3ab0d7418b08b6b81a99ce116af472822b8",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "In systems involving multiple autonomous agents, it is often necessary to decide how scarce resources should be allocated. When agents have competing interests, they may have incentive to deviate from protocols or to lie to other agents about their preferences. Due to the strategic nature of such interactions, there has been a recent surge of interest in addressing problems in competitive multiagent systems by bringing together techniques from computer science and game-theoretic economics. In some cases, the interesting issue is the application of ideas from computer science to make existing economic mechanisms practical. In other cases, selfish agents' conflicting demands of a computer system can best be understood and/or managed through game-theoretic analysis. This thesis addresses problems that fall into both cases. \nThe first part of this work considers game-theoretic issues in multiagent resource allocation: (1)\u00a0using incentives to diffuse temporally-focused usage of a resource on a computer network at the lowest possible cost; (2)\u00a0identifying a protocol to allow agents to cooperate in (single-good) first-price auctions, and showing that in equilibrium such collusion benefits both colluding and non-colluding agents at the auctioneer's expense; (3)\u00a0compactly representing strategic multiagent situations as local-effect games, a novel class of multi-player general-sum games for which pure-strategy Nash equilibria can often be proven to exist and be computed efficiently. \nThe second part considers computational issues in multiagent resource allocation, concentrating on the winner determination problem (WDP) in combinatorial auctions: (1)\u00a0solving several variants of the WDP using heuristic branch-and-bound search algorithms; (2)\u00a0building a benchmark suite for WDP algorithms by modeling real-world valuations described in the economics literature; (3)\u00a0modeling the (empirical) computational hardness of the WDP, analyzing these models, and applying them to construct algorithm portfolios and to generate harder problem instances."
            },
            "slug": "Resource-Allocation-in-Competitive-Multiagent-Shoham-Leyton-Brown",
            "title": {
                "fragments": [],
                "text": "Resource Allocation in Competitive Multiagent Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686671"
                        ],
                        "name": "L. Csat\u00f3",
                        "slug": "L.-Csat\u00f3",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Csat\u00f3",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Csat\u00f3"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 108
                            }
                        ],
                        "text": "In particular, also in need of approximate GP models, we experimented with sparse online Gaussian processes (Csat and Opper, 2003) and sparse pseudo-input Gaussian processes (Snelson and Ghahramani, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 117811322,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81510b7d57e85ead7dd755fd2974b4bb0ce3ce6c",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-gaussian-processes:-inference,-subspace-and-Csat\u00f3-Opper",
            "title": {
                "fragments": [],
                "text": "Sparse gaussian processes: inference, subspace identification and model selection"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2423230"
                        ],
                        "name": "L. Breiman",
                        "slug": "L.-Breiman",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Breiman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Breiman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 32
                            }
                        ],
                        "text": "1 Random Forests Random forests (Breiman, 2001) are a flexible tool for regression and classification."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 89141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13d4c2f76a7c1a4d0a71204e1d5d263a3f5a7986",
            "isKey": false,
            "numCitedBy": 65885,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Random forests are a combination of tree predictors such that each tree depends on the values of a random vector sampled independently and with the same distribution for all trees in the forest. The generalization error for forests converges a.s. to a limit as the number of trees in the forest becomes large. The generalization error of a forest of tree classifiers depends on the strength of the individual trees in the forest and the correlation between them. Using a random selection of features to split each node yields error rates that compare favorably to Adaboost (Y. Freund & R. Schapire, Machine Learning: Proceedings of the Thirteenth International conference, ***, 148\u2013156), but are more robust with respect to noise. Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the splitting. Internal estimates are also used to measure variable importance. These ideas are also applicable to regression."
            },
            "slug": "Random-Forests-Breiman",
            "title": {
                "fragments": [],
                "text": "Random Forests"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Internal estimates monitor error, strength, and correlation and these are used to show the response to increasing the number of features used in the forest, and are also applicable to regression."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3372057"
                        ],
                        "name": "O. Shtrichman",
                        "slug": "O.-Shtrichman",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Shtrichman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Shtrichman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62963250,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d41319a947845856571645b17b3bd058bd1287b2",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Bounded Model Checking based on SAT methods has recently been introduced as a complementary technique to BDD-based Symbolic Model Checking. The basic idea is to search for a counter example in executions whose length is bounded by some integer k. The BMC problem can be efficiently reduced to a propositional satisfiability problem, and can therefore be solved by SAT methods rather than BDDs. SAT procedures are based on general-purpose heuristics that are designed for any propositional formula. We show that the unique characteristics of BMC formulas can be exploited for a variety of optimizations in the SAT checking procedure. Experiments with these optimizations on real designs proved their efficiency in many of the hard test cases, comparing to both the standard SAT procedure and a BDD-based model checker."
            },
            "slug": "Tuning-SAT-Checkers-for-Bounded-Model-Checking-Shtrichman",
            "title": {
                "fragments": [],
                "text": "Tuning SAT Checkers for Bounded Model Checking"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that the unique characteristics of BMC formulas can be exploited for a variety of optimizations in the SAT checking procedure, and proved their efficiency in many of the hard test cases, comparing to both the standard SAT procedure and a BDD-based model checker."
            },
            "venue": {
                "fragments": [],
                "text": "CAV 2000"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145134489"
                        ],
                        "name": "M. Prasad",
                        "slug": "M.-Prasad",
                        "structuredName": {
                            "firstName": "Mukul",
                            "lastName": "Prasad",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Prasad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790450"
                        ],
                        "name": "Armin Biere",
                        "slug": "Armin-Biere",
                        "structuredName": {
                            "firstName": "Armin",
                            "lastName": "Biere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Armin Biere"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37131028"
                        ],
                        "name": "Aarti Gupta",
                        "slug": "Aarti-Gupta",
                        "structuredName": {
                            "firstName": "Aarti",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aarti Gupta"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2840997,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b51f1ee64aff6c6be443632adf1d54add0ea211",
            "isKey": false,
            "numCitedBy": 293,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "Dramatic improvements in SAT solver technology over the last decade and the growing need for more efficient and scalable verification solutions have fueled research in verification methods based on SAT solvers. This paper presents a survey of the latest developments in SAT-based formal verification, including incomplete methods such as bounded model checking and complete methods for model checking. We focus on how the surveyed techniques formulate the verification problem as a SAT problem and how they exploit crucial aspects of a SAT solver, such as application-specific heuristics and conflict-driven learning. Finally, we summarize the noteworthy achievements in this area so far and note the major challenges in making this technology more pervasive in industrial design verification flows."
            },
            "slug": "A-survey-of-recent-advances-in-SAT-based-formal-Prasad-Biere",
            "title": {
                "fragments": [],
                "text": "A survey of recent advances in SAT-based formal verification"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A survey of the latest developments in SAT-based formal verification, including incomplete methods such as bounded model checking and complete methods for model checking, focuses on how the surveyed techniques formulate the verification problem as a SAT problem and how they exploit crucial aspects of a SAT solver."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Software Tools for Technology Transfer"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144539890"
                        ],
                        "name": "N. Hansen",
                        "slug": "N.-Hansen",
                        "structuredName": {
                            "firstName": "Nikolaus",
                            "lastName": "Hansen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Hansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069398"
                        ],
                        "name": "A. Ostermeier",
                        "slug": "A.-Ostermeier",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Ostermeier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ostermeier"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 168
                            }
                        ],
                        "text": "For optimizing target algorithms with only numerical parameters, we would also like to compare to prominent model-free blackbox optimization approaches, such as CMA-ES (Hansen and Ostermeier, 1996; Hansen and Kern, 2004)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 157
                            }
                        ],
                        "text": "133 9.3 Alternative GP fits to log-transformed data . . . . . . . . . . . . . . . . . . . 135 9.4 Comparison of SKO and three variants of SPO for optimizing CMA-ES . . . 144 9.5 Comparison of SKO and three variants of SPO for optimizing CMA-ES, with\nlog-transformations . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 78
                            }
                        ],
                        "text": "In particular, it has been applied to optimize the solution quality of CMA-ES (Hansen and Ostermeier, 1996; Hansen and Kern, 2004), a prominent gradient-free global optimization algorithm for continuous functions (BartzBeielstein et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 170
                            }
                        ],
                        "text": "38 3.2.1 Target Algorithms for SAT . . . . . . . . . . . . . . . . . . . . . . . 39 3.2.2 Target Algorithm for MIP . . . . . . . . . . . . . . . . . . . . . . . 42 3.2.3 CMA-ES for Global Continuous Function Optimization . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 84
                            }
                        ],
                        "text": "This prominent gradient-free global optimization algorithm for continuous functions (Hansen and Ostermeier, 1996; Hansen and Kern, 2004) is based on an evolutionary strategy that uses a covariance matrix adaptation scheme."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 29
                            }
                        ],
                        "text": "2 In this comparison, CMA-ES (Hansen and Ostermeier, 1996; Hansen and Kern, 2004), which we describe in Section 3."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 34
                            }
                        ],
                        "text": "50 3.4 Experimental setup for the CMA-ES configuration scenarios . . . . . . . . . 50 3.5 Summary of our SINGLEINSTCONT scenarios . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                }
            ],
            "corpusId": 10836085,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d5cec2e4f53a0e749b9ea22697578917fdff00aa",
            "isKey": true,
            "numCitedBy": 1045,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "A new formulation for coordinate system independent adaptation of arbitrary normal mutation distributions with zero mean is presented. This enables the evolution strategy (ES) to adapt the correct scaling of a given problem and also ensures invariance with respect to any rotation of the fitness function (or the coordinate system). Especially rotation invariance, here resulting directly from the coordinate system independent adaptation of the mutation distribution, is an essential feature of the ES with regard to its general applicability to complex fitness functions. Compared to previous work on this subject, the introduced formulation facilitates an interpretation of the resulting mutation distribution, making sensible manipulation by the user possible (if desired). Furthermore it enables a more effective control of the overall mutation variance (expected step length)."
            },
            "slug": "Adapting-arbitrary-normal-mutation-distributions-in-Hansen-Ostermeier",
            "title": {
                "fragments": [],
                "text": "Adapting arbitrary normal mutation distributions in evolution strategies: the covariance matrix adaptation"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new formulation for coordinate system independent adaptation of arbitrary normal mutation distributions with zero mean enables the evolution strategy to adapt the correct scaling of a given problem and also ensures invariance with respect to any rotation of the fitness function (or the coordinate system)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Evolutionary Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50690761"
                        ],
                        "name": "Mirela Andronescu",
                        "slug": "Mirela-Andronescu",
                        "structuredName": {
                            "firstName": "Mirela",
                            "lastName": "Andronescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mirela Andronescu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143926762"
                        ],
                        "name": "A. Condon",
                        "slug": "A.-Condon",
                        "structuredName": {
                            "firstName": "Anne",
                            "lastName": "Condon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Condon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748734"
                        ],
                        "name": "D. Mathews",
                        "slug": "D.-Mathews",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Mathews",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Mathews"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056417995"
                        ],
                        "name": "K. Murphy",
                        "slug": "K.-Murphy",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Murphy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Murphy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 72
                            }
                        ],
                        "text": "Note that the quadratic programs from RNA energy parameter optimization (Andronescu et al., 2007) could be solved in polynomial time."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 79
                            }
                        ],
                        "text": "CPLEX-QP Quadratic programs from RNA energy 1 000 1 000 parameter optimization (Andronescu et al., 2007)"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 84
                            }
                        ],
                        "text": "QP This set of quadratic programs originated from RNA energy parameter optimization (Andronescu et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6159015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd0df67b1caf430d8c4267e2b30e7941ffc672b8",
            "isKey": true,
            "numCitedBy": 169,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "MOTIVATION\nAccurate prediction of RNA secondary structure from the base sequence is an unsolved computational challenge. The accuracy of predictions made by free energy minimization is limited by the quality of the energy parameters in the underlying free energy model. The most widely used model, the Turner99 model, has hundreds of parameters, and so a robust parameter estimation scheme should efficiently handle large data sets with thousands of structures. Moreover, the estimation scheme should also be trained using available experimental free energy data in addition to structural data.\n\n\nRESULTS\nIn this work, we present constraint generation (CG), the first computational approach to RNA free energy parameter estimation that can be efficiently trained on large sets of structural as well as thermodynamic data. Our CG approach employs a novel iterative scheme, whereby the energy values are first computed as the solution to a constrained optimization problem. Then the newly computed energy parameters are used to update the constraints on the optimization function, so as to better optimize the energy parameters in the next iteration. Using our method on biologically sound data, we obtain revised parameters for the Turner99 energy model. We show that by using our new parameters, we obtain significant improvements in prediction accuracy over current state of-the-art methods.\n\n\nAVAILABILITY\nOur CG implementation is available at http://www.rnasoft.ca/CG/."
            },
            "slug": "Efficient-parameter-estimation-for-RNA-secondary-Andronescu-Condon",
            "title": {
                "fragments": [],
                "text": "Efficient parameter estimation for RNA secondary structure prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents constraint generation (CG), the first computational approach to RNA free energy parameter estimation that can be efficiently trained on large sets of structural as well as thermodynamic data and achieves significant improvements in prediction accuracy over current state of-the-art methods."
            },
            "venue": {
                "fragments": [],
                "text": "ISMB/ECCB"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 124
                            }
                        ],
                        "text": "In most recent work on sequential model-based optimization, this model takes the form of a Gaussian stochastic process (GP) (Rasmussen and Williams, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 137
                            }
                        ],
                        "text": "The complexity for predictions with this PP approximation is O(p) for the mean and O(p2) for the variance of the predictive distribution (Rasmussen and Williams, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 106
                            }
                        ],
                        "text": "However, the former one was orders of magnitude slower than the standard implementation we ended up using (Rasmussen and Williams, 2006), and the latter was tied to a particular kernel function that we could not extend to categorical parameters (discussed in the next chapter)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1430472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82266f6103bade9005ec555ed06ba20b5210ff22",
            "isKey": true,
            "numCitedBy": 18076,
            "numCiting": 231,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received growing attention in the machine learning community over the past decade. The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises. Code and datasets can be obtained on the web. Appendices provide mathematical background and a discussion of Gaussian Markov processes."
            },
            "slug": "Gaussian-Processes-for-Machine-Learning-Rasmussen-Williams",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics, and deals with the supervised learning problem for both regression and classification."
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive computation and machine learning"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081889"
                        ],
                        "name": "Edward Snelson",
                        "slug": "Edward-Snelson",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Snelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Edward Snelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744700"
                        ],
                        "name": "Zoubin Ghahramani",
                        "slug": "Zoubin-Ghahramani",
                        "structuredName": {
                            "firstName": "Zoubin",
                            "lastName": "Ghahramani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zoubin Ghahramani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 174
                            }
                        ],
                        "text": "In particular, also in need of approximate GP models, we experimented with sparse online Gaussian processes (Csat and Opper, 2003) and sparse pseudo-input Gaussian processes (Snelson and Ghahramani, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 394337,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b6a2d80854651a56e0f023543131744f14f20ab4",
            "isKey": false,
            "numCitedBy": 1512,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new Gaussian process (GP) regression model whose co-variance is parameterized by the the locations of M pseudo-input points, which we learn by a gradient based optimization. We take M \u226a N, where N is the number of real data points, and hence obtain a sparse regression method which has O(M2N) training cost and O(M2) prediction cost per test case. We also find hyperparameters of the covariance function in the same joint optimization. The method can be viewed as a Bayesian regression model with particular input dependent noise. The method turns out to be closely related to several other sparse GP approaches, and we discuss the relation in detail. We finally demonstrate its performance on some large data sets, and make a direct comparison to other sparse GP methods. We show that our method can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime."
            },
            "slug": "Sparse-Gaussian-Processes-using-Pseudo-inputs-Snelson-Ghahramani",
            "title": {
                "fragments": [],
                "text": "Sparse Gaussian Processes using Pseudo-inputs"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "It is shown that this new Gaussian process (GP) regression model can match full GP performance with small M, i.e. very sparse solutions, and it significantly outperforms other approaches in this regime."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3444394"
                        ],
                        "name": "E. Ziegel",
                        "slug": "E.-Ziegel",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Ziegel",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Ziegel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 127
                            }
                        ],
                        "text": "This effect is notorious in machine learning, where it is well known that models fit on small datasets often generalize poorly (Hastie et al., 2001)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 54
                            }
                        ],
                        "text": "This effect is called overfitting in machine learning (Hastie et al., 2001) and overtuning in optimization (Birattari et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46701966,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "e41ba5dc12c79a64dfa905c0328f95976252ffe0",
            "isKey": false,
            "numCitedBy": 12747,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Chapter 11 includes more case studies in other areas, ranging from manufacturing to marketing research. Chapter 12 concludes the book with some commentary about the scienti\u008e c contributions of MTS. The Taguchi method for design of experiment has generated considerable controversy in the statistical community over the past few decades. The MTS/MTGS method seems to lead another source of discussions on the methodology it advocates (Montgomery 2003). As pointed out by Woodall et al. (2003), the MTS/MTGS methods are considered ad hoc in the sense that they have not been developed using any underlying statistical theory. Because the \u201cnormal\u201d and \u201cabnormal\u201d groups form the basis of the theory, some sampling restrictions are fundamental to the applications. First, it is essential that the \u201cnormal\u201d sample be uniform, unbiased, and/or complete so that a reliable measurement scale is obtained. Second, the selection of \u201cabnormal\u201d samples is crucial to the success of dimensionality reduction when OAs are used. For example, if each abnormal item is really unique in the medical example, then it is unclear how the statistical distance MD can be guaranteed to give a consistent diagnosis measure of severity on a continuous scale when the larger-the-better type S/N ratio is used. Multivariate diagnosis is not new to Technometrics readers and is now becoming increasingly more popular in statistical analysis and data mining for knowledge discovery. As a promising alternative that assumes no underlying data model, The Mahalanobis\u2013Taguchi Strategy does not provide suf\u008e cient evidence of gains achieved by using the proposed method over existing tools. Readers may be very interested in a detailed comparison with other diagnostic tools, such as logistic regression and tree-based methods. Overall, although the idea of MTS/MTGS is intriguing, this book would be more valuable had it been written in a rigorous fashion as a technical reference. There is some lack of precision even in several mathematical notations. Perhaps a follow-up with additional theoretical justi\u008e cation and careful case studies would answer some of the lingering questions."
            },
            "slug": "The-Elements-of-Statistical-Learning-Ziegel",
            "title": {
                "fragments": [],
                "text": "The Elements of Statistical Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "Chapter 11 includes more case studies in other areas, ranging from manufacturing to marketing research, and a detailed comparison with other diagnostic tools, such as logistic regression and tree-based methods."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35250057"
                        ],
                        "name": "D. Babic",
                        "slug": "D.-Babic",
                        "structuredName": {
                            "firstName": "Domagoj",
                            "lastName": "Babic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Babic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741933"
                        ],
                        "name": "A. Hu",
                        "slug": "A.-Hu",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Hu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 188
                            }
                        ],
                        "text": "Our target algorithm in this case study is SPEAR (Babi\u0107, 2008), a modular arithmetic decision procedure and SAT solver, developed by Domagoj Babi\u0107 in support of the CALYSTO static checker (Babi\u0107 and Hu, 2007b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 231,
                                "start": 210
                            }
                        ],
                        "text": "In addition, SPEAR has several enhancements for software verification, such as support for modular arithmetic constraints (Babi\u0107 and Musuvathi, 2005), incrementality to enable structural abstraction/refinement (Babi\u0107 and Hu, 2007b), and a technique for identifying context-insensitive invariants to speed up solving multiple queries that share common structure (Babi\u0107 and Hu, 2007a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 166
                            }
                        ],
                        "text": "The benchmark set was very limited and included several medium-sized BMC and some small software verification (SWV) instances generated by the CALYSTO static checker (Babi\u0107 and Hu, 2007b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 126
                            }
                        ],
                        "text": "SWV This set of SAT-encoded software verification instances comprises 604 instances generated with the CALYSTO static checker (Babi\u0107 and Hu, 2007b), used for the verification of five"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 187
                            }
                        ],
                        "text": "5: two benchmarks sets of industrial problem instances, namely 754 BMC instances from IBM created by Zarpas (2005) and 604 verification conditions generated by the CALYSTO static checker (Babi\u0107 and Hu, 2007b)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1649108,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4779792130b50e3739438df86bd52317f6abca8",
            "isKey": true,
            "numCitedBy": 75,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Precise software analysis and verification require tracking the exact path along which a statement is executed (path-sensitivity), the different contexts from which a function is called (context-sensitivity), and the bit-accurate operations performed. Previously, verification with such precision has been considered too inefficient to scale to large software. In this paper, we present a novel approach to solving such verification conditions, based on an automatic abstraction-checking-refinement framework that exploits natural abstraction boundaries present in software. Experimental results show that our approach easily scales to over 200,000 lines of real C/C++ code."
            },
            "slug": "Structural-Abstraction-of-Software-Verification-Babic-Hu",
            "title": {
                "fragments": [],
                "text": "Structural Abstraction of Software Verification Conditions"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel approach to solving verification conditions, based on an automatic abstraction-checking-refinement framework that exploits natural abstraction boundaries present in software, is presented."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726451"
                        ],
                        "name": "Ian P. Gent",
                        "slug": "Ian-P.-Gent",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Gent",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian P. Gent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702467"
                        ],
                        "name": "Patrick Prosser",
                        "slug": "Patrick-Prosser",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Prosser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick Prosser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733716"
                        ],
                        "name": "T. Walsh",
                        "slug": "T.-Walsh",
                        "structuredName": {
                            "firstName": "Toby",
                            "lastName": "Walsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Walsh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6341757,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9694f836677154b5788ebda81199a8b785892f5e",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a mechanism called \"morphing\" for introducing structure or randomness into a wide variety of problems. We illustrate the usefulness of morphing by performing several different experimental studies. These studies identify the impact of a \"small-world\" topology on the cost of coloring graphs, of asymmetry on the cost of finding the optimal TSP tour, and of the dimensionality of space on the cost of finding the optimal TSP tour. We predict that morphing will find many other uses."
            },
            "slug": "Morphing:-Combining-Structure-and-Randomness-Gent-Hoos",
            "title": {
                "fragments": [],
                "text": "Morphing: Combining Structure and Randomness"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "This work introduces a mechanism called \"morphing\" for introducing structure or randomness into a wide variety of problems by performing several different experimental studies, and predicts that morphing will find many other uses."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI/IAAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40774846"
                        ],
                        "name": "Laurent Simon",
                        "slug": "Laurent-Simon",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Simon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurent Simon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2690085"
                        ],
                        "name": "P. Chatalic",
                        "slug": "P.-Chatalic",
                        "structuredName": {
                            "firstName": "Philippe",
                            "lastName": "Chatalic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Chatalic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 551137,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "8c7192c46c0761d652f3b0140ccc26247b246e48",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "SatEx:-A-Web-based-Framework-for-SAT-Simon-Chatalic",
            "title": {
                "fragments": [],
                "text": "SatEx: A Web-based Framework for SAT Experimentation"
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Notes Discret. Math."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696575"
                        ],
                        "name": "J. Bentley",
                        "slug": "J.-Bentley",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Bentley",
                            "middleNames": [
                                "Louis"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bentley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145380801"
                        ],
                        "name": "M. McIlroy",
                        "slug": "M.-McIlroy",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "McIlroy",
                            "middleNames": [
                                "Douglas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. McIlroy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8822797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4bc96b34f935bf99ea8b6ea077f493e054e0fd2a",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We recount the history of a new qsortfunction for a C library. Our function is clearer, faster and more robust than existing sorts. It chooses partitioning elements by a new sampling scheme; it partitions by a novel solution to Dijkstra's Dutch National Flag problem; and it swaps efficiently. Its behavior was assessed with timing and debugging testbeds, and with a program to certify performance. The design techniques apply in domains beyond sorting."
            },
            "slug": "Engineering-a-sort-function-Bentley-McIlroy",
            "title": {
                "fragments": [],
                "text": "Engineering a sort function"
            },
            "tldr": {
                "abstractSimilarityScore": 54,
                "text": "A new qsortfunction for a C library that chooses partitioning elements by a new sampling scheme; it partitions by a novel solution to Dijkstra's Dutch National Flag problem; and it swaps efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "Softw. Pract. Exp."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144376533"
                        ],
                        "name": "P. Sanders",
                        "slug": "P.-Sanders",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Sanders",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Sanders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1830936"
                        ],
                        "name": "Dominik Schultes",
                        "slug": "Dominik-Schultes",
                        "structuredName": {
                            "firstName": "Dominik",
                            "lastName": "Schultes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dominik Schultes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 177
                            }
                        ],
                        "text": "Only the last decade has seen the development of (parameterized) algorithms that are both exact and empirically outperform Dijkstra\u2019s algorithm by up to a factor of one million (Sanders and Schultes, 2007)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11832920,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "b5fa055f3440fb77751371122f422fb4fab69c3d",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for route planning in transportation networks have recently undergone a rapid development, leading to methods that are up to one million times faster than Dijkstra's algorithm. We outline ideas, algorithms, implementations, and experimental methods behind this development. We also explain why the story is not over yet because dynamically changing networks, flexible objective functions, and new applications pose a lot of interesting challenges."
            },
            "slug": "Engineering-Fast-Route-Planning-Algorithms-Sanders-Schultes",
            "title": {
                "fragments": [],
                "text": "Engineering Fast Route Planning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Algorithms for route planning in transportation networks have recently undergone a rapid development, leading to methods that are up to one million times faster than Dijkstra's algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "WEA"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145323121"
                        ],
                        "name": "J. Nelder",
                        "slug": "J.-Nelder",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Nelder",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Nelder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144189443"
                        ],
                        "name": "R. Mead",
                        "slug": "R.-Mead",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Mead",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Mead"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 144
                            }
                        ],
                        "text": "SKO selects a single new parameter configuration by maximizing an augmented expected improvement criterion using the Nelder-Mead simplex method (Nelder and Mead, 1965)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2208295,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "017ddb7e815236defd0566bc46f6ed8401cc6ba6",
            "isKey": false,
            "numCitedBy": 25684,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n 41) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point. The simplex adapts itself to the local landscape, and contracts on to the final minimum. The method is shown to be effective and computationally compact. A procedure is given for the estimation of the Hessian matrix in the neighbourhood of the minimum, needed in statistical estimation problems."
            },
            "slug": "A-Simplex-Method-for-Function-Minimization-Nelder-Mead",
            "title": {
                "fragments": [],
                "text": "A Simplex Method for Function Minimization"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A method is described for the minimization of a function of n variables, which depends on the comparison of function values at the (n 41) vertices of a general simplex, followed by the replacement of the vertex with the highest value by another point."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. J."
            },
            "year": 1965
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144539890"
                        ],
                        "name": "N. Hansen",
                        "slug": "N.-Hansen",
                        "structuredName": {
                            "firstName": "Nikolaus",
                            "lastName": "Hansen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Hansen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13968591,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "95a1f9e009d16e141448fbfea33353b02e1e9335",
            "isKey": false,
            "numCitedBy": 1737,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Derived from the concept of self-adaptation in evolution strategies, the CMA (Covariance Matrix Adaptation) adapts the covariance matrix of a multi-variate normal search distribution. The CMA was originally designed to perform well with small populations. In this review, the argument starts out with large population sizes, reflecting recent extensions of the CMA algorithm. Commonalities and differences to continuous Estimation of Distribution Algorithms are analyzed. The aspects of reliability of the estimation, overall step size control, and independence from the coordinate system (invariance) become particularly important in small populations sizes. Consequently, performing the adaptation task with small populations is more intricate."
            },
            "slug": "The-CMA-Evolution-Strategy:-A-Comparing-Review-Hansen",
            "title": {
                "fragments": [],
                "text": "The CMA Evolution Strategy: A Comparing Review"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "In this review, the argument starts out with large population sizes, reflecting recent extensions of the CMA algorithm, and similarities and differences to continuous Estimation of Distribution Algorithms are analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "Towards a New Evolutionary Computation"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 176
                            }
                        ],
                        "text": "Similar to lab experiments in the natural sciences, empirical studies can help build intuition and inspire new theoretical results, such as probabilistic algorithm guarantees (Hoos, 1999b)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 270,
                                "start": 257
                            }
                        ],
                        "text": "Note that multiplicative noise is very common in randomized algorithms: the standard deviation is often roughly proportional to the mean of the runtime distribution (in fact, runtime distributions can often be approximated well by exponential distributions (Hoos, 1999a), for which mean and standard deviation are identical)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 154
                            }
                        ],
                        "text": "Good approximations of its distribution can often be achieved with exponential distributions or generalizations thereof, such as the Weibull distribution (Hoos, 1999a)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5135200,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "f5cc2fbeb379ad95a5fd79b19cbc16f84400e449",
            "isKey": false,
            "numCitedBy": 125,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A shift mechanism for a transmission of the type used in riding lawn mowers and the like is provided. The shift pattern is essentially of the lineal type and yet the transmission can be shifted from one gear to another without being shifted through all intermediate gears. For example, the transmission can be shifted from the third forward speed to reverse without progressing through the second and first forward speeds. The shift lever is moved lineally in a path transverse to its longitudinal extent to align the lever with a desired shift position. The lever is then pivoted in a vertical path to effect the desired shifting. The new shift also provides a higher mechanical advantage to make shifting easier for the operator."
            },
            "slug": "Stochastic-local-search-methods,-models,-Hoos",
            "title": {
                "fragments": [],
                "text": "Stochastic local search - methods, models, applications"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A shift mechanism for a transmission of the type used in riding lawn mowers and the like is provided and the new shift provides a higher mechanical advantage to make shifting easier for the operator."
            },
            "venue": {
                "fragments": [],
                "text": "DISKI"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "148052095"
                        ],
                        "name": "Selden B. Grary",
                        "slug": "Selden-B.-Grary",
                        "structuredName": {
                            "firstName": "Selden",
                            "lastName": "Grary",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Selden B. Grary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750307"
                        ],
                        "name": "C. Spera",
                        "slug": "C.-Spera",
                        "structuredName": {
                            "firstName": "Cosimo",
                            "lastName": "Spera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Spera"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 80
                            }
                        ],
                        "text": "Instead of rigidly following an experimental design strategy (e.g., outlined in Crary and Spera, 1996), the designs to be evaluated are typically chosen in an ad hoc fashion based on the intuition of the algorithm designer whenever a new feature is implemented."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37477539,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "85d2c0f2a8a49b976cd1c279da11153ccce8fa2e",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We discuss two experimental designs and show how to use them to evaluate difficult empirical combinatorial problems. We restrict our analysis here to the knapsack problem but comment more generally on the use of computational testing to analyze the performances of algorithms."
            },
            "slug": "Optimal-experimental-design-for-combinatorial-Grary-Spera",
            "title": {
                "fragments": [],
                "text": "Optimal experimental design for combinatorial problems"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This work discusses two experimental designs and shows how to use them to evaluate difficult empirical combinatorial problems and comments more generally on the use of computational testing to analyze the performances of algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35250057"
                        ],
                        "name": "D. Babic",
                        "slug": "D.-Babic",
                        "structuredName": {
                            "firstName": "Domagoj",
                            "lastName": "Babic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Babic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741933"
                        ],
                        "name": "A. Hu",
                        "slug": "A.-Hu",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Hu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 382,
                                "start": 361
                            }
                        ],
                        "text": "In addition, SPEAR has several enhancements for software verification, such as support for modular arithmetic constraints (Babi\u0107 and Musuvathi, 2005), incrementality to enable structural abstraction/refinement (Babi\u0107 and Hu, 2007b), and a technique for identifying context-insensitive invariants to speed up solving multiple queries that share common structure (Babi\u0107 and Hu, 2007a)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13291750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18b7864ab77060d0c33e2c5ce98cee7fe8d250dc",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Despite many advances, today's software model checkers and extended static checkers still do not scale well to large code bases, when verifying properties that depend on complex interprocedural flow of data. An obvious approach to improve performance is to exploit software structure. Although a tremendous amount of work has been done on exploiting structure at various levels of granularity, the fine-grained shared structure among multiple verification conditions has been largely ignored. In this paper, we formalize the notion of shared structure among verification conditions, propose a novel and efficient approach to exploit this sharing, and provide experimental results that this approach can significantly improve the performance of verification, even on pathand context-sensitive and dataflow-intensive properties."
            },
            "slug": "Exploiting-Shared-Structure-in-Software-Conditions-Babic-Hu",
            "title": {
                "fragments": [],
                "text": "Exploiting Shared Structure in Software Verification Conditions"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper formalizes the notion of shared structure among verification conditions, proposes a novel and efficient approach to exploit this sharing, and provides experimental results that this approach can significantly improve the performance of verification, even on pathand context-sensitive and dataflow-intensive properties."
            },
            "venue": {
                "fragments": [],
                "text": "Haifa Verification Conference"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786591"
                        ],
                        "name": "C. Thachuk",
                        "slug": "C.-Thachuk",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Thachuk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Thachuk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2584834"
                        ],
                        "name": "A. Shmygelska",
                        "slug": "A.-Shmygelska",
                        "structuredName": {
                            "firstName": "Alena",
                            "lastName": "Shmygelska",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shmygelska"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2470869"
                        ],
                        "name": "H. Hoos",
                        "slug": "H.-Hoos",
                        "structuredName": {
                            "firstName": "Holger",
                            "lastName": "Hoos",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Hoos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 25
                            }
                        ],
                        "text": ", 2003), protein folding (Thachuk et al., 2007), formal verification (Hutter et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 147
                            }
                        ],
                        "text": "\u2026and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al., 2003), protein folding (Thachuk et al., 2007), formal verification (Hutter et al., 2007a), and even in areas far outside of computer science, such as water resource\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12602262,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "576d23ed7f59000a8207de39c9be213117dce9e5",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "BackgroundThe ab initio protein folding problem consists of predicting protein tertiary structure from a given amino acid sequence by minimizing an energy function; it is one of the most important and challenging problems in biochemistry, molecular biology and biophysics. The ab initio protein folding problem is computationally challenging and has been shown to be NPMathType@MTEF@5@5@+=feaafiart1ev1aaatCvAUfKttLearuWrP9MDH5MBPbIqV92AaeXatLxBI9gBaebbnrfifHhDYfgasaacH8akY=wiFfYdH8Gipec8Eeeu0xXdbba9frFj0=OqFfea0dXdd9vqai=hGuQ8kuc9pgc9s8qqaq=dirpe0xb9q8qiLsFr0=vr0=vr0dc8meaabaqaciaacaGaaeqabaqabeGadaaakeaat0uy0HwzTfgDPnwy1egaryqtHrhAL1wy0L2yHvdaiqaacqWFneVtcqqGqbauaaa@3961@-hard even when conformations are restricted to a lattice. In this work, we implement and evaluate the replica exchange Monte Carlo (REMC) method, which has already been applied very successfully to more complex protein models and other optimization problems with complex energy landscapes, in combination with the highly effective pull move neighbourhood in two widely studied Hydrophobic Polar (HP) lattice models.ResultsWe demonstrate that REMC is highly effective for solving instances of the square (2D) and cubic (3D) HP protein folding problem. When using the pull move neighbourhood, REMC outperforms current state-of-the-art algorithms for most benchmark instances. Additionally, we show that this new algorithm provides a larger ensemble of ground-state structures than the existing state-of-the-art methods. Furthermore, it scales well with sequence length, and it finds significantly better conformations on long biological sequences and sequences with a provably unique ground-state structure, which is believed to be a characteristic of real proteins. We also present evidence that our REMC algorithm can fold sequences which exhibit significant interaction between termini in the hydrophobic core relatively easily.ConclusionWe demonstrate that REMC utilizing the pull move neighbourhood significantly outperforms current state-of-the-art methods for protein structure prediction in the HP model on 2D and 3D lattices. This is particularly noteworthy, since so far, the state-of-the-art methods for 2D and 3D HP protein folding \u2013 in particular, the pruned-enriched Rosenbluth method (PERM) and, to some extent, Ant Colony Optimisation (ACO) \u2013 were based on chain growth mechanisms. To the best of our knowledge, this is the first application of REMC to HP protein folding on the cubic lattice, and the first extension of the pull move neighbourhood to a 3D lattice."
            },
            "slug": "A-replica-exchange-Monte-Carlo-algorithm-for-in-the-Thachuk-Shmygelska",
            "title": {
                "fragments": [],
                "text": "A replica exchange Monte Carlo algorithm for protein folding in the HP model"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work implements and evaluates the replica exchange Monte Carlo (REMC) method, which has already been applied very successfully to more complex protein models and other optimization problems with complex energy landscapes, in combination with the highly effective pull move neighbourhood in two widely studied Hydrophobic Polar (HP) lattice models."
            },
            "venue": {
                "fragments": [],
                "text": "BMC Bioinformatics"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743797"
                        ],
                        "name": "I. Guyon",
                        "slug": "I.-Guyon",
                        "structuredName": {
                            "firstName": "Isabelle",
                            "lastName": "Guyon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Guyon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766703"
                        ],
                        "name": "A. Elisseeff",
                        "slug": "A.-Elisseeff",
                        "structuredName": {
                            "firstName": "Andr\u00e9",
                            "lastName": "Elisseeff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Elisseeff"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 70
                            }
                        ],
                        "text": "Instead of applying PCA, we could have used feature selection methods (Guyon and Elisseeff, 2003), such as forward selection as done in SATzilla (Nudelman et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 379259,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8384f7ef288d2d5cb267128471c5427fc98b54b",
            "isKey": false,
            "numCitedBy": 14116,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Variable and feature selection have become the focus of much research in areas of application for which datasets with tens or hundreds of thousands of variables are available. These areas include text processing of internet documents, gene expression array analysis, and combinatorial chemistry. The objective of variable selection is three-fold: improving the prediction performance of the predictors, providing faster and more cost-effective predictors, and providing a better understanding of the underlying process that generated the data. The contributions of this special issue cover a wide range of aspects of such problems: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods."
            },
            "slug": "An-Introduction-to-Variable-and-Feature-Selection-Guyon-Elisseeff",
            "title": {
                "fragments": [],
                "text": "An Introduction to Variable and Feature Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The contributions of this special issue cover a wide range of aspects of variable selection: providing a better definition of the objective function, feature construction, feature ranking, multivariate feature selection, efficient search methods, and feature validity assessment methods."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393588417"
                        ],
                        "name": "T. Bartz-Beielstein",
                        "slug": "T.-Bartz-Beielstein",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Bartz-Beielstein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bartz-Beielstein"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 156
                            }
                        ],
                        "text": "For example, they can be used to determine how much algorithm response depends on the setting of single parameters, and to which degree parameters interact (Jones et al., 1998; Santner et al., 2003; Bartz-Beielstein, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 168
                            }
                        ],
                        "text": "The exact equations used in both SKO and the DACE toolbox implement methods to deal with ill conditioning; we refer the reader to the original publications for details (Huang et al., 2006; Bartz-Beielstein, 2006; Lophaven et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 2
                            }
                        ],
                        "text": "3 (Bartz-Beielstein et al., 2005; Bartz-Beielstein, 2006; Bartz-Beielstein and Preuss, 2006) doubles the number of runs for subsequent function evaluations whenever this happens, SPO 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 186
                            }
                        ],
                        "text": "In particular, research communities that have contributed techniques for algorithm configuration or parameter tuning include planning (Gratch and Dejong, 1992), evolutionary computation (Bartz-Beielstein, 2006), meta-heuristics (Birattari, 2005), genetic algorithms (Fukunaga, 2008), parallel computing (Brewer, 1995), and numerical optimization (Audet and Orban, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57129918,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c871e59b63f625846185092843f1358f673bfdc",
            "isKey": true,
            "numCitedBy": 290,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This book introduces the new experimentalism in evolutionary computation, providing tools to understand algorithms and programs and their interaction with optimization problems. It develops and applies statistical techniques to analyze and compare modern search heuristics such as evolutionary algorithms and particle swarm optimization. The book bridges the gap between theory and experiment by providing a self-contained experimental methodology and many examples."
            },
            "slug": "Experimental-Research-in-Evolutionary-Computation-Bartz-Beielstein",
            "title": {
                "fragments": [],
                "text": "Experimental Research in Evolutionary Computation - The New Experimentalism"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This book introduces the new experimentalism in evolutionary computation, providing tools to understand algorithms and programs and their interaction with optimization problems, and provides a self-contained experimental methodology and many examples."
            },
            "venue": {
                "fragments": [],
                "text": "Natural Computing Series"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3234559"
                        ],
                        "name": "E. Dijkstra",
                        "slug": "E.-Dijkstra",
                        "structuredName": {
                            "firstName": "Edsger",
                            "lastName": "Dijkstra",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Dijkstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 73
                            }
                        ],
                        "text": "Even though this problem can be solved by Dijkstra\u2019s classical algorithm (Dijkstra, 1959) in time O(|E|+ |V | log(|V |)), until recently practitioners used heuristic solutions without correctness guarantees."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123284777,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "45786063578e814444b8247028970758bbbd0488",
            "isKey": false,
            "numCitedBy": 21819,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider n points (nodes), some or all pairs of which are connected by a branch; the length of each branch is given. We restrict ourselves to the case where at least one path exists between any two nodes. We now consider two problems. Problem 1. Constrnct the tree of minimum total length between the n nodes. (A tree is a graph with one and only one path between every two nodes.) In the course of the construction that we present here, the branches are subdivided into three sets: I. the branches definitely assignec~ to the tree under construction (they will form a subtree) ; II. the branches from which the next branch to be added to set I, will be selected ; III. the remaining branches (rejected or not yet considered). The nodes are subdivided into two sets: A. the nodes connected by the branches of set I, B. the remaining nodes (one and only one branch of set II will lead to each of these nodes), We start the construction by choosing an arbitrary node as the only member of set A, and by placing all branches that end in this node in set II. To start with, set I is empty. From then onwards we perform the following two steps repeatedly. Step 1. The shortest branch of set II is removed from this set and added to"
            },
            "slug": "A-note-on-two-problems-in-connexion-with-graphs-Dijkstra",
            "title": {
                "fragments": [],
                "text": "A note on two problems in connexion with graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A tree is a graph with one and only one path between every two nodes, where at least one path exists between any two nodes and the length of each branch is given."
            },
            "venue": {
                "fragments": [],
                "text": "Numerische Mathematik"
            },
            "year": 1959
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1418377362"
                        ],
                        "name": "M. S. Akturk",
                        "slug": "M.-S.-Akturk",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Akturk",
                            "middleNames": [
                                "Selim"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. S. Akturk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2953896"
                        ],
                        "name": "Alper Atamt\u00fcrk",
                        "slug": "Alper-Atamt\u00fcrk",
                        "structuredName": {
                            "firstName": "Alper",
                            "lastName": "Atamt\u00fcrk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alper Atamt\u00fcrk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46403849"
                        ],
                        "name": "S. G\u00fcrel",
                        "slug": "S.-G\u00fcrel",
                        "structuredName": {
                            "firstName": "Sinan",
                            "lastName": "G\u00fcrel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. G\u00fcrel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 60
                            }
                        ],
                        "text": ", 2000) 1 000 1 000 CPLEX-MJA Machine-Job Assignment (BCOL) (Akt\u00fcrk et al., 2007) 172 171 CPLEX-CLS Capacitated Lot Sizing (BCOL) (Atamt\u00fcrk and Mu\u00f1oz, 2004) 50 50 CPLEX-MIK Mixed-integer knapsack (BCOL) (Atamt\u00fcrk, 2003) 60 60"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2480257,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "53ce6b288faa12530fec75dca9a8ebd371cac737",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-strong-conic-quadratic-reformulation-for-with-Akturk-Atamt\u00fcrk",
            "title": {
                "fragments": [],
                "text": "A strong conic quadratic reformulation for machine-job assignment with controllable processing times"
            },
            "venue": {
                "fragments": [],
                "text": "Oper. Res. Lett."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2268748"
                        ],
                        "name": "M. Rothkopf",
                        "slug": "M.-Rothkopf",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Rothkopf",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rothkopf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762639"
                        ],
                        "name": "A. Pekec",
                        "slug": "A.-Pekec",
                        "structuredName": {
                            "firstName": "Aleksandar",
                            "lastName": "Pekec",
                            "middleNames": [
                                "Sasa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pekec"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2969615"
                        ],
                        "name": "Ronald M. Harstad",
                        "slug": "Ronald-M.-Harstad",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Harstad",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald M. Harstad"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 24
                            }
                        ],
                        "text": "This problem is NP-hard (Rothkopf et al., 1998) and instances of it can be easily encoded as MILPs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 166
                            }
                        ],
                        "text": "Whether manual or automated, effective configuration procedures are central in the development of heuristic algorithms for many classes of problems, particularly for NP-hard problems."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17196300,
            "fieldsOfStudy": [
                "Economics"
            ],
            "id": "20c5bd401d5c5ed6c086408e2017491acf7ebd8b",
            "isKey": false,
            "numCitedBy": 1058,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "There is interest in designing simultaneous auctions for situations such as the recent FCC radio spectrum auctions, in which the value of assets to a bidder depends on which other assets he or she wins. In such auctions, bidders may wish to submit bids for combinations of assets. When this is allowed, the problem of determining the revenue maximizing set of nonconflicting bids can be difficult. We analyze this problem, identifying several different structures of permitted combinational bids for which computational tractability is constructively demonstrated and some structures for which computational tractability cannot be guaranteed."
            },
            "slug": "Computationally-Manageable-Combinational-Auctions-Rothkopf-Pekec",
            "title": {
                "fragments": [],
                "text": "Computationally Manageable Combinational Auctions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3014279"
                        ],
                        "name": "E. Zarpas",
                        "slug": "E.-Zarpas",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "Zarpas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Zarpas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 87
                            }
                        ],
                        "text": "It is remarkable that for the SPEAR-IBM scenario, which emphasizes very hard instances (Zarpas, 2005), captimes as low as \u03bamax/100 = 3s actually yielded good results."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18724746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "28f4bdd0ddb49a11ad80bbbe2878f497625ff3c4",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern SAT solvers are highly dependent on heuristics. Therefore, benchmarking is of prime importance in evaluating the performances of different solvers. However, relevant benchmarking is not necessarily straightforward. We present our experiments using the IBM CNF Benchmark on several SAT solvers. Using the results, we attempt to define guidelines for a relevant benchmarking methodology, using SAT solvers for real life BMC applications."
            },
            "slug": "Benchmarking-SAT-Solvers-for-Bounded-Model-Checking-Zarpas",
            "title": {
                "fragments": [],
                "text": "Benchmarking SAT Solvers for Bounded Model Checking"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work attempts to define guidelines for a relevant benchmarking methodology, using SAT solvers for real life BMC applications, using the IBM CNF Benchmark on several SATsolvers."
            },
            "venue": {
                "fragments": [],
                "text": "SAT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2953896"
                        ],
                        "name": "Alper Atamt\u00fcrk",
                        "slug": "Alper-Atamt\u00fcrk",
                        "structuredName": {
                            "firstName": "Alper",
                            "lastName": "Atamt\u00fcrk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alper Atamt\u00fcrk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 129
                            }
                        ],
                        "text": ", 2007) 172 171 CPLEX-CLS Capacitated Lot Sizing (BCOL) (Atamt\u00fcrk and Mu\u00f1oz, 2004) 50 50 CPLEX-MIK Mixed-integer knapsack (BCOL) (Atamt\u00fcrk, 2003) 60 60"
                    },
                    "intents": []
                }
            ],
            "corpusId": 14683451,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "4421a6d4840d9047accc1e1c80bf36f97689c8f9",
            "isKey": false,
            "numCitedBy": 111,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract.We study the mixed\u2013integer knapsack polyhedron, that is, the convex hull of the mixed\u2013integer set defined by an arbitrary linear inequality and the bounds on the variables. We describe facet\u2013defining inequalities of this polyhedron that can be obtained through sequential lifting of inequalities containing a single integer variable. These inequalities strengthen and/or generalize known inequalities for several special cases. We report computational results on using the inequalities as cutting planes for mixed\u2013integer programming."
            },
            "slug": "On-the-facets-of-the-mixed\u2013integer-knapsack-Atamt\u00fcrk",
            "title": {
                "fragments": [],
                "text": "On the facets of the mixed\u2013integer knapsack polyhedron"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "Facet\u2013defining inequalities of this polyhedron are described that can be obtained through sequential lifting of inequalities containing a single integer variable that strengthen and/or generalize known inequalities for several special cases."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145132131"
                        ],
                        "name": "D. Jackson",
                        "slug": "D.-Jackson",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Jackson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jackson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 2
                            }
                        ],
                        "text": "3 (Jackson, 2000)) where the difference of floating point rounding errors between Intel\u2019s non-standard 80-bit and IEEE 64-bit precision resulted in an extremely large difference in the runtimes on the same processor."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10042959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c17d23af96d08acd99f518a075b5f13fe3d1cb5e",
            "isKey": false,
            "numCitedBy": 240,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "An automatic analysis method for first-order logic with sets and relations is described. A first-order formula is translated to a quantifier-free boolean formula, which has a model when the original formula has a model within a given scope (that is, involving no more than some finite number of atoms). Because the satisfiable formulas that occur in practice tend to have small models, a small scope usually suffices and the analysis is efficient.\nThe paper presents a simple logic and gives a compositional translation scheme. It also reports briefly on experience using the Alloy Analyzer, a tool that implements the scheme."
            },
            "slug": "Automating-first-order-relational-logic-Jackson",
            "title": {
                "fragments": [],
                "text": "Automating first-order relational logic"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The paper presents a simple logic and gives a compositional translation scheme and reports briefly on experience using the Alloy Analyzer, a tool that implements the scheme."
            },
            "venue": {
                "fragments": [],
                "text": "SIGSOFT '00/FSE-8"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35112676"
                        ],
                        "name": "S. Kuhnt",
                        "slug": "S.-Kuhnt",
                        "structuredName": {
                            "firstName": "Sonja",
                            "lastName": "Kuhnt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kuhnt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50676519"
                        ],
                        "name": "D. Steinberg",
                        "slug": "D.-Steinberg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Steinberg",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Steinberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 186
                            }
                        ],
                        "text": "For example, expected runtime or median solution cost\nCost Statistic See Empirical Cost Statistic\nConfigurator See Configuration Procedure\nDACE model A noise-free GP model introduced by Sacks et al. (1989); DACE is an acronym for \u201cDesign and Analysis of Computer Experiments\u201d."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15210862,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b617b85b5add24d55bdbd952cb5bde52b01e4869",
            "isKey": false,
            "numCitedBy": 2858,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The design and analysis of computer experiments as a relatively young research field is not only of high importance for many industrial areas but also presents new challenges and open questions for statisticians. This editorial introduces a special issue devoted to the topic. The included papers present an interesting mixture of recent developments in the field as they cover fundamental research on the design of experiments, models and analysis methods as well as more applied research connected to real-life applications."
            },
            "slug": "Design-and-analysis-of-computer-experiments-Kuhnt-Steinberg",
            "title": {
                "fragments": [],
                "text": "Design and analysis of computer experiments"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The included papers present an interesting mixture of recent developments in the field as they cover fundamental research on the design of experiments, models and analysis methods as well as more applied research connected to real-life applications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2353862"
                        ],
                        "name": "N. E\u00e9n",
                        "slug": "N.-E\u00e9n",
                        "structuredName": {
                            "firstName": "Niklas",
                            "lastName": "E\u00e9n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. E\u00e9n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790450"
                        ],
                        "name": "Armin Biere",
                        "slug": "Armin-Biere",
                        "structuredName": {
                            "firstName": "Armin",
                            "lastName": "Biere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Armin Biere"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 126
                            }
                        ],
                        "text": "More specifically, over 50% of the instances in set SWV could be solved by applying the polynomial-time preprocessor SATelite (E\u00e9n and Biere, 2005) (note that we already pointed out this large fraction of trivial SWV instances in our empirical analysis in Section 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 109
                            }
                        ],
                        "text": "we noticed that some instances could already be solved by applying the polynomial-time preprocessor SATelite (E\u00e9n and Biere, 2005)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3159518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6825642b6377689796c4d2d896f726f6152d285f",
            "isKey": false,
            "numCitedBy": 683,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Preprocessing SAT instances can reduce their size considerably. We combine variable elimination with subsumption and self-subsuming resolution, and show that these techniques not only shrink the formula further than previous preprocessing efforts based on variable elimination, but also decrease runtime of SAT solvers substantially for typical industrial SAT problems. We discuss critical implementation details that make the reduction procedure fast enough to be practical."
            },
            "slug": "Effective-Preprocessing-in-SAT-Through-Variable-and-E\u00e9n-Biere",
            "title": {
                "fragments": [],
                "text": "Effective Preprocessing in SAT Through Variable and Clause Elimination"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This work combines variable elimination with subsumption and self-subsuming resolution, and shows that these techniques not only shrink the formula further than previous preprocessing efforts based on variable elimination, but also decrease runtime of SAT solvers substantially for typical industrial SAT problems."
            },
            "venue": {
                "fragments": [],
                "text": "SAT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2134514"
                        ],
                        "name": "J. Schmee",
                        "slug": "J.-Schmee",
                        "structuredName": {
                            "firstName": "Josef",
                            "lastName": "Schmee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27466478"
                        ],
                        "name": "G. J. Hahn",
                        "slug": "G.-J.-Hahn",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Hahn",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. J. Hahn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 160
                            }
                        ],
                        "text": "We are also experimenting with Gaussian process models under censoring (Ertin, 2007) and with more generic methods to handle censored data in regression models (Schmee and Hahn, 1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123202538,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c0847a5ef655178cbf83431b7bcb461fc6ccf29d",
            "isKey": false,
            "numCitedBy": 184,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Problems requiring regression analysis of censored data arise frequently in practice. For example, in accelerated testing one wishes to relate stress and average time to failure from data including unfailed units, i.e., censored observations. Maximum likelihood is one method for obtaining the desired estimates; in this paper, we propose an alternative approach. An initial least squares fit is obtained treating the censored values as failures. Then, based upon this initial fit, the expected failure time for each censored observation is estimated. These estimates are then used, instead of the censoring times, to obtain a revised least squares fit and new expected failure times are estimated for the censored values. These are then used in a further least squares fit. The procedure is iterated until convergence is achieved. This method is simpler to implement and explain to non-statisticians than maximum likelihood and appears to have good statistical and convergence properties. The method is illustrated by a..."
            },
            "slug": "A-Simple-Method-for-Regression-Analysis-With-Data-Schmee-Hahn",
            "title": {
                "fragments": [],
                "text": "A Simple Method for Regression Analysis With Censored Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1941834"
                        ],
                        "name": "N. Meinshausen",
                        "slug": "N.-Meinshausen",
                        "structuredName": {
                            "firstName": "Nicolai",
                            "lastName": "Meinshausen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Meinshausen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28528,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "7333e127b62eb545d81830df2a66b98c0693a32b",
            "isKey": false,
            "numCitedBy": 1099,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Random forests were introduced as a machine learning tool in Breiman (2001) and have since proven to be very popular and powerful for high-dimensional regression and classification. For regression, random forests give an accurate approximation of the conditional mean of a response variable. It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean. Conditional quantiles can be inferred with quantile regression forests, a generalisation of random forests. Quantile regression forests give a non-parametric and accurate way of estimating conditional quantiles for high-dimensional predictor variables. The algorithm is shown to be consistent. Numerical examples suggest that the algorithm is competitive in terms of predictive power."
            },
            "slug": "Quantile-Regression-Forests-Meinshausen",
            "title": {
                "fragments": [],
                "text": "Quantile Regression Forests"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown here that random forests provide information about the full conditional distribution of the response variable, not only about the conditional mean, in order to be competitive in terms of predictive power."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2700298"
                        ],
                        "name": "J. Spall",
                        "slug": "J.-Spall",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Spall",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Spall"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59958747,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "115e9ae70393b0d26463cd1ebc4109e7a2f5eeb9",
            "isKey": false,
            "numCitedBy": 1278,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \n* Unique in its survey of the range of topics. \n* Contains a strong, interdisciplinary format that will appeal to both students and researchers. \n* Features exercises and web links to software and data sets."
            },
            "slug": "Introduction-to-Stochastic-Search-and-Optimization-Spall",
            "title": {
                "fragments": [],
                "text": "Introduction to Stochastic Search and Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A survey of the range of topics, in a strong, interdisciplinary format that will appeal to both students and researchers."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064595436"
                        ],
                        "name": "Eric Horvitz",
                        "slug": "Eric-Horvitz",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Horvitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Horvitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760493"
                        ],
                        "name": "Y. Ruan",
                        "slug": "Y.-Ruan",
                        "structuredName": {
                            "firstName": "Yongshao",
                            "lastName": "Ruan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ruan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144659218"
                        ],
                        "name": "C. Gomes",
                        "slug": "C.-Gomes",
                        "structuredName": {
                            "firstName": "Carla",
                            "lastName": "Gomes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gomes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690271"
                        ],
                        "name": "Henry A. Kautz",
                        "slug": "Henry-A.-Kautz",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Kautz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Henry A. Kautz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744679"
                        ],
                        "name": "B. Selman",
                        "slug": "B.-Selman",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Selman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Selman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724065"
                        ],
                        "name": "D. M. Chickering",
                        "slug": "D.-M.-Chickering",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Chickering",
                            "middleNames": [
                                "Maxwell"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. M. Chickering"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56805319,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab0f07802a5cb8654cb5cefc64f18bc2d1fd5361",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Branch-and-bound-algorithm-selection-by-performance-Horvitz-Ruan",
            "title": {
                "fragments": [],
                "text": "Branch and bound algorithm selection by performance prediction"
            },
            "venue": {
                "fragments": [],
                "text": "UAI 2001"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706573"
                        ],
                        "name": "Emre Ertin",
                        "slug": "Emre-Ertin",
                        "structuredName": {
                            "firstName": "Emre",
                            "lastName": "Ertin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Emre Ertin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 71
                            }
                        ],
                        "text": "We are also experimenting with Gaussian process models under censoring (Ertin, 2007) and with more generic methods to handle censored data in regression models (Schmee and Hahn, 1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14770845,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8be8b7f1457074b0935fb1c53ac5d7aaed1884bb",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Sensor data models are key components of the design and testing of sensor network applications. In addition to their utility in validation of applications and network services, they provide a theoretical basis for the design of algorithms for efficient sampling, compression and exfiltration of the sensor data. In this paper we introduce a novel modeling technique for constructing probabilistic models for censored sensor readings. The proposed technique is an extension of the Gaussian process regression and applies to continuous valued readings subject to censoring. We treat the censored variable as a mixture of binary and a normal random variable. The Gaussian process framework provides a natural way of integrating information from both types of observations to estimate the parameters of the underlying random process. We illustrate the performance of the proposed technique in modeling wireless propagation between nodes of a wireless sensor network. The model can capture the anisotropic nature of the propagation characteristics and utilizes the implicit information from the packet reception failures."
            },
            "slug": "Gaussian-Process-Models-for-Censored-Sensor-Ertin",
            "title": {
                "fragments": [],
                "text": "Gaussian Process Models for Censored Sensor Readings"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel modeling technique for constructing probabilistic models for censored sensor readings is introduced and can capture the anisotropic nature of the propagation characteristics and utilizes the implicit information from the packet reception failures."
            },
            "venue": {
                "fragments": [],
                "text": "2007 IEEE/SP 14th Workshop on Statistical Signal Processing"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5054589"
                        ],
                        "name": "K. Chaloner",
                        "slug": "K.-Chaloner",
                        "structuredName": {
                            "firstName": "Kathryn",
                            "lastName": "Chaloner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chaloner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816502"
                        ],
                        "name": "I. Verdinelli",
                        "slug": "I.-Verdinelli",
                        "structuredName": {
                            "firstName": "Isabella",
                            "lastName": "Verdinelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Verdinelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13676847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f79332b1361e9eaa9da3327f83f57dcac5cd11d",
            "isKey": false,
            "numCitedBy": 1718,
            "numCiting": 319,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reviews the literature on Bayesian experimental design, both for linear and nonlinear models. A uniied view of the topic is presented by putting experimental design in a decision theoretic framework. This framework justiies many optimality criteria, and opens new possibilities. Various design criteria become part of a single, coherent approach."
            },
            "slug": "Bayesian-Experimental-Design:-A-Review-Chaloner-Verdinelli",
            "title": {
                "fragments": [],
                "text": "Bayesian Experimental Design: A Review"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This paper reviews the literature on Bayesian experimental design, both for linear and nonlinear models, and presents a uniied view of the topic by putting experimental design in a decision theoretic framework."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113641402"
                        ],
                        "name": "Martin D. Davis",
                        "slug": "Martin-D.-Davis",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Davis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Martin D. Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3153689"
                        ],
                        "name": "G. Logemann",
                        "slug": "G.-Logemann",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Logemann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Logemann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3043218"
                        ],
                        "name": "D. Loveland",
                        "slug": "D.-Loveland",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Loveland",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Loveland"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 58
                            }
                        ],
                        "text": "2 Algorithm Development The core of SPEAR is a DPLL-style (Davis et al., 1962) SAT solver, but with several novelties."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15866917,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "feca806dc20be4a72a57b45b9ce7ebfcded08038",
            "isKey": false,
            "numCitedBy": 3387,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The programming of a proof procedure is discussed in connection with trial runs and possible improvements."
            },
            "slug": "A-machine-program-for-theorem-proving-Davis-Logemann",
            "title": {
                "fragments": [],
                "text": "A machine program for theorem-proving"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The programming of a proof procedure is discussed in connection with trial runs and possible improvements."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1962
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3013332"
                        ],
                        "name": "M. Segal",
                        "slug": "M.-Segal",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Segal",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Segal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60974957,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "297e6802cf71adc7a3e018a416ad0bcac86697da",
            "isKey": false,
            "numCitedBy": 448,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The regression-tree methodology is extended to right-censored response variables by replacing the conventional splitting rules with rules based on the Tarone-Ware or Harrington-Fleming classes of two-sample statistics. New pruning strategies for determining desirable tree size are also devised. Properties of this approach are developed and comparisons with existing procedures, in terms of practical problems, are discussed. Illustrative, real-world performances of the technique are presented."
            },
            "slug": "Regression-Trees-for-Censored-Data-Segal",
            "title": {
                "fragments": [],
                "text": "Regression Trees for Censored Data"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The regression-tree methodology is extended to right-censored response variables by replacing the conventional splitting rules with rules based on the Tarone-Ware or Harrington-Fleming classes of two-sample statistics."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3342740"
                        ],
                        "name": "J. Beasley",
                        "slug": "J.-Beasley",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Beasley",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Beasley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 82
                            }
                        ],
                        "text": "ORLIB This benchmark set includes a heterogeneous mix of 140 instances from ORLIB (Beasley, 1990): 59 set covering instances, 20 capacitated p-median problems, 37 capacitated warehouse location instances, and 24 airplane landing scheduling instances."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62214548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a89dc7bf47b17b54f3998616160047a3ae50b481",
            "isKey": false,
            "numCitedBy": 1925,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In this note we present a system (OR-Library) that distributes test problems by electronic mail (e-mail). This system currently has available test problems drawn from a number of different areas of operational research."
            },
            "slug": "OR-Library:-Distributing-Test-Problems-by-Mail-Beasley",
            "title": {
                "fragments": [],
                "text": "OR-Library: Distributing Test Problems by Electronic Mail"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A system (OR-Library) that distributes test problems by electronic mail (e-mail) that has available test problems drawn from a number of different areas of operational research."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2700298"
                        ],
                        "name": "J. Spall",
                        "slug": "J.-Spall",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Spall",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Spall"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119914272,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b8faedd513f1ef73886799ddbf9a29d30eaa1296",
            "isKey": false,
            "numCitedBy": 56,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "The sections in this article are \n \n \n1 \nRobbins\u2013Monro Stochastic Approximation \n \n2 \nStochastic Approximation with Gradient Approximations Based on Function Measurements \n \n3 \nSimultaneous Perturbation Stochastic Approximation \n \n4 \nSimulated Annealing \n \n5 \nConcluding Remarks \n \n6 \nAcknowledgements"
            },
            "slug": "Stochastic-Optimization,-Stochastic-Approximation-Spall",
            "title": {
                "fragments": [],
                "text": "Stochastic Optimization, Stochastic Approximation and Simulated Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "The sections in this article are Robbins\u2013Monro Stochastic Approximation with Gradient Approximations Based on Function Measurements, and Simultaneous Perturbation Stoch Plasticity and Simulated Annealing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064919756"
                        ],
                        "name": "Frank M. Hutter",
                        "slug": "Frank-M.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Frank M. Hutter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 76
                            }
                        ],
                        "text": "In particular, we automatically constructed different instantiations of the SPEAR algorithm and thereby substantially improved the state of the art for two sets of SAT-encoded industrial verification problems (Hutter et al., 2007a)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 5
                            }
                        ],
                        "text": "Both SPEAR and SATENSTEIN were configured using ParamILS, one of the automated configuration procedures we introduce in this thesis (see Chapter 5)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 285,
                                "start": 280
                            }
                        ],
                        "text": "90 6.2 Algorithm Development . . . . . . . . . . . . . . . . . . . . . . . . . . . . 91\n6.2.1 Manual Tuning . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 92 6.2.2 Initial Performance Assessment . . . . . . . . . . . . . . . . . . . . 93\n6.3 Automated Configuration of SPEAR . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "99 6.6 MiniSAT 2.0 vs SPEAR configured for specific benchmark sets . . . . . . . . 100\n7.1 Speedup of RANDOMSEARCH due to adaptive capping . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "96 6.3 SPEAR default vs SPEAR configured for SAT competition, on BMC and SWV ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 124
                            }
                        ],
                        "text": "86 5.4 Configuration of GLS+, SAPS, and SAT4J . . . . . . . . . . . . . . . . . . 87\n6.1 Summary of results for configuring SPEAR . . . . . . . . . . . . . . . . . . 101\n7.1 Speedup of RANDOMSEARCH due to adaptive capping . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 29
                            }
                        ],
                        "text": "94 6.3.1 Parameterization of SPEAR . . . . . . . . . . . . . . . . . . . . . . . 94 6.3.2 Configuration for the 2007 SAT Competition . . . . . . . . . . . . . 95 6.3.3 Configuration for Specific Applications . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "97 6.4 SPEAR configured for general benchmark set vs SPEAR configured for specific benchmark sets . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . 98 6.5 SPEAR default vs SPEAR configured for specific benchmark sets . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "88\n6.1 MiniSAT 2.0 vs SPEAR default . . . . . . . . . . . . . . . . . . . . . . . . . 93 6.2 SPEAR default vs SPEAR configured for SAT competition, on SAT competition instances . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . . ."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 48
                            }
                        ],
                        "text": "We provide full details on the configuration of SPEAR in Chapter 6 and review the SATENSTEIN application in Section 8.3.1."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 39
                            }
                        ],
                        "text": "5 We submitted three versions of SPEAR (Babi\u0107 and Hutter, 2007): the manually-configured default, version Satcomp and a newer version with further code optimizations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 122737526,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "b51d5303d8a0a2d52f8680300087a9883f0612fd",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "SPEAR is a bit-vector arithmetic theorem prover designed for proving software verification conditions. The core of the theore m prover is a fast and simple SAT solver, which is described in this paper."
            },
            "slug": "SPEAR-Theorem-Prover-Hutter",
            "title": {
                "fragments": [],
                "text": "SPEAR Theorem Prover"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "SPEAR is a bit-vector arithmetic theorem prover designed for proving software verification conditions and the core of the theore m prover is a fast and simple SAT solver, which is described in this paper."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401755685"
                        ],
                        "name": "Joaquin Qui\u00f1onero-Candela",
                        "slug": "Joaquin-Qui\u00f1onero-Candela",
                        "structuredName": {
                            "firstName": "Joaquin",
                            "lastName": "Qui\u00f1onero-Candela",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joaquin Qui\u00f1onero-Candela"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152746370"
                        ],
                        "name": "Cki Williams",
                        "slug": "Cki-Williams",
                        "structuredName": {
                            "firstName": "Cki",
                            "lastName": "Williams",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cki Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17132092"
                        ],
                        "name": "Bottou",
                        "slug": "Bottou",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730609"
                        ],
                        "name": "O. Chapelle",
                        "slug": "O.-Chapelle",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Chapelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Chapelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703049"
                        ],
                        "name": "D. DeCoste",
                        "slug": "D.-DeCoste",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "DeCoste",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. DeCoste"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145183709"
                        ],
                        "name": "J. Weston",
                        "slug": "J.-Weston",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Weston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Weston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13071261,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "e1c73641c28cfe17a277227ce450d557ea04845a",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "A wealth of computationally efficient approximation methods for Gaussian process regression have been recently proposed. We give a unifying \ufffd"
            },
            "slug": "Approximation-Methods-for-Gaussian-Process-Qui\u00f1onero-Candela-Rasmussen",
            "title": {
                "fragments": [],
                "text": "Approximation Methods for Gaussian Process Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A wealth of computationally efficient approximation methods for Gaussian process regression have been recently proposed, but a unifying approach to this problem is given."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3240893"
                        ],
                        "name": "Charles Audet",
                        "slug": "Charles-Audet",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Audet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles Audet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144256094"
                        ],
                        "name": "D. Orban",
                        "slug": "D.-Orban",
                        "structuredName": {
                            "firstName": "Dominique",
                            "lastName": "Orban",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Orban"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 32
                            }
                        ],
                        "text": ", 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009), machine learning (Maron and Moore, 1994; Kohavi and John, 1995), database query optimization (Stillger and Spiliopoulou, 1996), database server optimization (Diao et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 140
                            }
                        ],
                        "text": "Many search algorithms, such as Multi-TAC (Minton, 1996), Calibra (Adenso-Diaz and Laguna, 2006), the mesh adaptive direct search algorithm (Audet and Orban, 2006) and BASICILS (Hutter et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 369,
                                "start": 346
                            }
                        ],
                        "text": "In particular, research communities that have contributed techniques for algorithm configuration or parameter tuning include planning (Gratch and Dejong, 1992), evolutionary computation (Bartz-Beielstein, 2006), meta-heuristics (Birattari, 2005), genetic algorithms (Fukunaga, 2008), parallel computing (Brewer, 1995), and numerical optimization (Audet and Orban, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 137
                            }
                        ],
                        "text": "\u2026algorithms can be found in areas as diverse as sorting (Li et al., 2005), linear algebra (Whaley et al., 2001), numerical optimization (Audet and Orban, 2006), compiler optimization (Cavazos and O\u2019Boyle, 2005), parallel computing (Brewer, 1995), computer vision (Muja and Lowe, 2009),\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59830681,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6eb7ab4cbf3b5d72f5e41609954eb59e7733969b",
            "isKey": true,
            "numCitedBy": 34,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Finding-Optimal-Algorithmic-Parameters-Using-the-Audet-Orban",
            "title": {
                "fragments": [],
                "text": "Finding Optimal Algorithmic Parameters Using the Mesh Adaptive Direct Search Algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5510152"
                        ],
                        "name": "S. Lophaven",
                        "slug": "S.-Lophaven",
                        "structuredName": {
                            "firstName": "S\u00f8ren",
                            "lastName": "Lophaven",
                            "middleNames": [
                                "Nymand"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lophaven"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2597782"
                        ],
                        "name": "H. B. Nielsen",
                        "slug": "H.-B.-Nielsen",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Nielsen",
                            "middleNames": [
                                "Bruun"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. B. Nielsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145093273"
                        ],
                        "name": "J. S\u00f8ndergaard",
                        "slug": "J.-S\u00f8ndergaard",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "S\u00f8ndergaard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. S\u00f8ndergaard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 168
                            }
                        ],
                        "text": "The exact equations used in both SKO and the DACE toolbox implement methods to deal with ill conditioning; we refer the reader to the original publications for details (Huang et al., 2006; Bartz-Beielstein, 2006; Lophaven et al., 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64393872,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2329f0b8a5dfc2c38248a39381da86fb55dfb58",
            "isKey": false,
            "numCitedBy": 272,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Aspects-of-the-Matlab-toolbox-DACE-Lophaven-Nielsen",
            "title": {
                "fragments": [],
                "text": "Aspects of the Matlab toolbox DACE"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3220513"
                        ],
                        "name": "B. Beachkofski",
                        "slug": "B.-Beachkofski",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Beachkofski",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Beachkofski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2335763"
                        ],
                        "name": "R. Grandhi",
                        "slug": "R.-Grandhi",
                        "structuredName": {
                            "firstName": "Ramana",
                            "lastName": "Grandhi",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Grandhi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 386,
                                "start": 355
                            }
                        ],
                        "text": "Here, we study the effect of the method for choosing which 250 parameter configurations to include in the initial design, considering four methods: (1) a uniform random sample from the region of interest; (2) a random Latin hypercube design (LHD); (3) the LHD used in SPO; and (4) a more complex LHD based on iterated distributed hypercube sampling (IHS) (Beachkofski and Grandhi, 2002)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62041530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a805af3924709f38757d245c53f283a9df8f4ad",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Improved-Distributed-Hypercube-Sampling-Beachkofski-Grandhi",
            "title": {
                "fragments": [],
                "text": "Improved Distributed Hypercube Sampling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32518077"
                        ],
                        "name": "C. Nelson",
                        "slug": "C.-Nelson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Nelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Nelson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 76
                            }
                        ],
                        "text": ", 2005), or an automated theorem prover based on the Nelson-Oppen framework (Nelson, 1979)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62330816,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c099e7df7e2325e906c6ecb73628e2317016359",
            "isKey": false,
            "numCitedBy": 260,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Techniques-for-program-verification-Nelson",
            "title": {
                "fragments": [],
                "text": "Techniques for program verification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1979
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815604"
                        ],
                        "name": "M. Schonlau",
                        "slug": "M.-Schonlau",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Schonlau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schonlau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145228938"
                        ],
                        "name": "W. Welch",
                        "slug": "W.-Welch",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Welch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Welch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118096690"
                        ],
                        "name": "Donald R. Jones",
                        "slug": "Donald-R.-Jones",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Jones",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald R. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 58806889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa2555e746b4d157a52d9bc4c63c9b386a720016",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Global-versus-local-search-in-constrained-of-models-Schonlau-Welch",
            "title": {
                "fragments": [],
                "text": "Global versus local search in constrained optimization of computer models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1392366310"
                        ],
                        "name": "Christian Lasarczyk",
                        "slug": "Christian-Lasarczyk",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Lasarczyk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Lasarczyk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62338121,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b8cfbda45f985602d3e1f3eb4da4ac82b9b35d37",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Genetische-Programmierung-einer-algorithmischen-Lasarczyk",
            "title": {
                "fragments": [],
                "text": "Genetische Programmierung einer algorithmischen Chemie"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800681"
                        ],
                        "name": "Alberto Maria Segre",
                        "slug": "Alberto-Maria-Segre",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Segre",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alberto Maria Segre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064580670"
                        ],
                        "name": "C. Elkan",
                        "slug": "C.-Elkan",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Elkan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Elkan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059546066"
                        ],
                        "name": "Alexander Russell",
                        "slug": "Alexander-Russell",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Russell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexander Russell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 189902375,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "0c732338fec5da0231d251927eb348d8b0b67231",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A number of experimental evaluations ofexplanation-based learning (EBL) have been reported in the literature on machine learning. A close examination of the design of these experiments revelas certain methodological problems that could affect the conclusions drawn from the experiments. This article analyzes some of the more common methodological difficulties, and illustrates them using selected previous studies."
            },
            "slug": "A-critical-look-at-experimental-evaluations-of-EBL-Segre-Elkan",
            "title": {
                "fragments": [],
                "text": "A critical look at experimental evaluations of EBL"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This article analyzes some of the more common methodological difficulties of Explanation-based learning, and illustrates them using selected previous studies."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145744238"
                        ],
                        "name": "Julia Couto",
                        "slug": "Julia-Couto",
                        "structuredName": {
                            "firstName": "Julia",
                            "lastName": "Couto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julia Couto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42550563,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d558f4c4797ce2b5f8d974bd23375077d8b642f",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering categorical data is an important and challenging data analysis task. In this paper, we explore the use of kernel K-means to cluster categorical data. We propose a new kernel function based on Hamming distance to embed categorical data in a constructed feature space where the clustering is conducted. We experimentally evaluated the quality of the solutions produced by kernel K-means on real datasets. Results indicated the feasibility of kernel K-means using our proposed kernel function to discover clusters embedded in categorical data."
            },
            "slug": "Kernel-K-Means-for-Categorical-Data-Couto",
            "title": {
                "fragments": [],
                "text": "Kernel K-Means for Categorical Data"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a new kernel function based on Hamming distance to embed categorical data in a constructed feature space where the clustering is conducted and experimentally evaluated the quality of the solutions produced by kernel K-means on real datasets."
            },
            "venue": {
                "fragments": [],
                "text": "IDA"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2318023"
                        ],
                        "name": "M. Moskewicz",
                        "slug": "M.-Moskewicz",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Moskewicz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Moskewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081144864"
                        ],
                        "name": "Conor F. Madigan",
                        "slug": "Conor-F.-Madigan",
                        "structuredName": {
                            "firstName": "Conor",
                            "lastName": "Madigan",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Conor F. Madigan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118976646"
                        ],
                        "name": "Ying Zhao",
                        "slug": "Ying-Zhao",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1978393184"
                        ],
                        "name": "Lintao Zhang",
                        "slug": "Lintao-Zhang",
                        "structuredName": {
                            "firstName": "Lintao",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lintao Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144682958"
                        ],
                        "name": "S. Malik",
                        "slug": "S.-Malik",
                        "structuredName": {
                            "firstName": "Sharad",
                            "lastName": "Malik",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Malik"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 169
                            }
                        ],
                        "text": "Some examples from the SAT-solving world include variable and value selection, clause deletion, next watched literal selection, and initial variable ordering heuristics (see, e.g., Silva, 1999; Moskewicz et al., 2001; Bhalla et al., 2003)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9292941,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b456a8c8c9fafcfbcbb3d4a854c88cf2adbf98a",
            "isKey": false,
            "numCitedBy": 3603,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "Boolean satisfiability is probably the most studied of the combinatorial optimization/search problems. Significant effort has been devoted to trying to provide practical solutions to this problem for problem instances encountered in a range of applications in electronic design automation (EDA), as well as in artificial intelligence (AI). This study has culminated in the development of several SAT packages, both proprietary and in the public domain (e.g. GRASP, SATO) which find significant use in both research and industry. Most existing complete solvers are variants of the Davis-Putnam (DP) search algorithm. In this paper we describe the development of a new complete solver, Chaff which achieves significant performance gains through careful engineering of all aspects of the search-especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy. Chaff has been able to obtain one to two orders of magnitude performance improvement on difficult SAT benchmarks in comparison with other solvers (DP or otherwise), including GRASP and SATO."
            },
            "slug": "Chaff:-engineering-an-efficient-SAT-solver-Moskewicz-Madigan",
            "title": {
                "fragments": [],
                "text": "Chaff: engineering an efficient SAT solver"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The development of a new complete solver, Chaff, is described which achieves significant performance gains through careful engineering of all aspects of the search-especially a particularly efficient implementation of Boolean constraint propagation (BCP) and a novel low overhead decision strategy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 38th Design Automation Conference (IEEE Cat. No.01CH37232)"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145293454"
                        ],
                        "name": "Steven Minton",
                        "slug": "Steven-Minton",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Minton",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven Minton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3146817,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "d49567066f736a2b2369f5ff7f7d47d7af131a6b",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes how meta-level theories are used for analytic learning in MULTI-TAC. MULTI-TAC operationalizes generic heuristics for constraint-satisfaction problems, in order to create programs that are tailored to specific problems. For each of its generic heuristics, MULTI-TAC has a meta-theory specifically designed for operationalising that heuristic. We present examples of the specialisation process and discuss how the theories influence the tractability of the learning process. We also describe an empirical study showing that the specialised programs produced by MULTITAC compare favorably to hand-coded programs."
            },
            "slug": "An-Analytic-Learning-System-for-Specializing-Minton",
            "title": {
                "fragments": [],
                "text": "An Analytic Learning System for Specializing Heuristics"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This paper describes how meta-level theories are used for analytic learning in MULTI-TAC and describes an empirical study showing that the specialised programs produced by MULTITAC compare favorably to hand-coded programs."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399006250"
                        ],
                        "name": "H. Terashima-Mar\u00edn",
                        "slug": "H.-Terashima-Mar\u00edn",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Terashima-Mar\u00edn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Terashima-Mar\u00edn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708975"
                        ],
                        "name": "P. Ross",
                        "slug": "P.-Ross",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Ross",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ross"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398268554"
                        ],
                        "name": "M. Valenzuela-Rend\u00f3n",
                        "slug": "M.-Valenzuela-Rend\u00f3n",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Valenzuela-Rend\u00f3n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Valenzuela-Rend\u00f3n"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59689856,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "84a67f140e01c08119908f110c547e5d7d666f59",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an investigation of solving Examination Timetabling Problems (ETTPs) with Genetic Algorithms (GAs) using a non-direct chromosome representation based on evolving the configuration of Constraint Satisfaction methods. There are two aims. The first is to circumvent the problems posed by a direct chromosome representation for the ETTP that consists of an array of events in which each value represents the timeslot which the corresponding event is assigned to. The second is to show that the adaptation of particular features in both the instance of the problem to be solved and the strategies used to solve it provides encouraging results for real ETTPs. There is much scope for investigating such approaches further, not only for the ETTP, but also for other related scheduling problems."
            },
            "slug": "Evolution-of-Constraint-Satisfaction-strategies-in-Terashima-Mar\u00edn-Ross",
            "title": {
                "fragments": [],
                "text": "Evolution of Constraint Satisfaction strategies in examination timetabling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Benchmark Instances in .mps format. http://www.andrew.cmu.edu/user/anureets/mpsInstances.htm"
            },
            "venue": {
                "fragments": [],
                "text": "Version last visited on April"
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The application of bayesian methods for seeking the extremum"
            },
            "venue": {
                "fragments": [],
                "text": "Towards Global Optimisation, 2:117\u2013129. North Holland, Amsterdam."
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 148
                            }
                        ],
                        "text": "One example can be found in the training of restricted Boltzmann machines and deep belief networks, which relies on various implementation \u201ctricks\u201d (Sversky and Murphy, 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "How to train restricted boltzmann machines and deep belief networks"
            },
            "venue": {
                "fragments": [],
                "text": "Technical report, University of British Columbia, Department of Computer Science. In preparation."
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Test functions for unconstrained global optimization"
            },
            "venue": {
                "fragments": [],
                "text": "http://www-optima. amp.i.kyoto-u.ac.jp/member/student/hedar/Hedar files/TestGO files/Page364.htm. Version last visited on July 19, 2009."
            },
            "year": 2009
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 54,
            "methodology": 75,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 182,
        "totalPages": 19
    },
    "page_url": "https://www.semanticscholar.org/paper/Automated-configuration-of-algorithms-for-solving-Hutter/08c4fdd974d874c87ea87faa6b404a7b8eb72c73?sort=total-citations"
}