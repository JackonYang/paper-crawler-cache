{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158193072"
                        ],
                        "name": "Dong Chen",
                        "slug": "Dong-Chen",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145200131"
                        ],
                        "name": "C. L. Giles",
                        "slug": "C.-L.-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. L. Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148800040"
                        ],
                        "name": "G. Sun",
                        "slug": "G.-Sun",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Sun",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1769857"
                        ],
                        "name": "H. H. Chen",
                        "slug": "H.-H.-Chen",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Chen",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. H. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145040825"
                        ],
                        "name": "Y. C. Lee",
                        "slug": "Y.-C.-Lee",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Lee",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. C. Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788202"
                        ],
                        "name": "M. Goudreau",
                        "slug": "M.-Goudreau",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Goudreau",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goudreau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Constructive algorithms initially assume a simple network and add nodes and links as warranted [2-8], while destructive methods start with a large network and prune off superfluous components [9-12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "[7] explore only fully connected topologies."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62618832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8c249709b436fdad1bf7efc61919dca6f494e3f",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "It is difficult to determine the minimal neural network structure for a particular automaton. A large recurrent network in practice is very difficult to train. Constructive or destructive recurrent methods might offer a solution to this problem. It is proved that one current method, recurrent cascade correlation, has fundamental limitations in representation and thus in its learning capabilities. A preliminary approach to circumventing these limitations by devising a simple constructive training method that adds neurons during training while still preserving the powerful fully recurrent structure is given. Through simulations it is shown that such a method can learn many types of regular grammars which the recurrent cascade correlation method is unable to learn.<<ETX>>"
            },
            "slug": "Constructive-learning-of-recurrent-neural-networks-Chen-Giles",
            "title": {
                "fragments": [],
                "text": "Constructive learning of recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is proved that one current method, recurrent cascade correlation, has fundamental limitations in representation and thus in its learning capabilities, and a preliminary approach to circumventing these limitations by devising a simple constructive training method."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732302"
                        ],
                        "name": "J. Koza",
                        "slug": "J.-Koza",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Koza",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30705144"
                        ],
                        "name": "J. P. Rice",
                        "slug": "J.-P.-Rice",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rice",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. P. Rice"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 783933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac4fedb5efe488addead4ea30c8856d689b156b5",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Shows how to find both the weights and architecture for a neural network, including the number of layers, the number of processing elements per layer, and the connectivity between processing elements. This is accomplished by using a recently developed extension to the genetic algorithm which genetically breeds a population of LISP symbolic expressions of varying size and shape until the desired performance by the network is successfully evolved. The novel 'genetic programming' paradigm is applied to the problem of generating a neural network for a one-bit adder.<<ETX>>"
            },
            "slug": "Genetic-generation-of-both-the-weights-and-for-a-Koza-Rice",
            "title": {
                "fragments": [],
                "text": "Genetic generation of both the weights and architecture for a neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The novel 'genetic programming' paradigm is applied to the problem of generating a neural network for a one-bit adder by using a recently developed extension to the genetic algorithm which genetically breeds a population of LISP symbolic expressions of varying size and shape."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682655"
                        ],
                        "name": "R. Belew",
                        "slug": "R.-Belew",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Belew",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Belew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49268728"
                        ],
                        "name": "J. McInerney",
                        "slug": "J.-McInerney",
                        "structuredName": {
                            "firstName": "John G.",
                            "lastName": "McInerney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McInerney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60923273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5378dbf09e75d133cafdcb388bb5546f91b2a02",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "It is appealing to consider hybrids of neural-network learning algorithms with evolutionary search procedures, simply because Nature has so successfully done so. In fact, computational models of learning and evolution ooer theoretical biology new tools for addressing questions about Nature that have dogged that eld since Darwin Belew, 1990]. The concern of this paper, however, is strictly artiicial: Can hybrids of connectionist learning algorithms and genetic algorithms produce more eecient and eeective algorithms than either technique applied in isolation? The paper begins with a survey of recent work (by us and others) that combines Holland's Genetic Algorithm (GA) with con-nectionist techniques and delineates some of the basic design problems these hybrids share. This analysis suggests the dangers of overly literal representations of the network on the genome (e.g., encoding each weight explicitly). A preliminary set of experiments that use the GA to nd unusual but successful values for BP parameters (learning rate, momentum) are also reported. The focus of the report is a series of experiments that use the GA to explore the space of initial weight values , from which two diierent gradient techniques (conjugate gradient and back propagation) are then allowed to optimize. We nd that use of the GA provides much greater conndence in the face of the stochas-tic variation that can plague gradient techniques, and can also allow training times to be reduced by as much as two orders of magnitude. Computational trade-oos between BP and the GA are considered, including discussion of a software facility that exploits the parallelism inherent in GA/BP hybrids. This evidence leads us to conclude that the GA's global sampling characteristics compliment connectionist local search techniques well, leading to eecient and reliable hybrids."
            },
            "slug": "Evolving-networks:-using-the-genetic-algorithm-with-Belew-McInerney",
            "title": {
                "fragments": [],
                "text": "Evolving networks: using the genetic algorithm with connectionist learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A survey of recent work that combines Holland's Genetic Algorithm with con-nectionist techniques and delineates some of the basic design problems these hybrids share concludes that the GA's global sampling characteristics compliment connectionist local search techniques well, leading to eecient and reliable hybrids."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749822"
                        ],
                        "name": "N. Karunanithi",
                        "slug": "N.-Karunanithi",
                        "structuredName": {
                            "firstName": "Nachimuthu",
                            "lastName": "Karunanithi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Karunanithi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863023"
                        ],
                        "name": "R. Das",
                        "slug": "R.-Das",
                        "structuredName": {
                            "firstName": "Rajarshi",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145285040"
                        ],
                        "name": "L. D. Whitley",
                        "slug": "L.-D.-Whitley",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Whitley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Whitley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Addition or deletion of a link is slightly more complicated in that a parameter identifies the likelihood that the link will originate from an input node or terminate at an output node."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61063423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "342f6ffd9f285ae7bf820651e32d17a7f646240e",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Genetic cascade learning is a new constructive algorithm for connectionist learning which combines genetic algorithms and the architectural feature of the cascade-correlation learning algorithm. Like the cascade-correlation learning architecture, this new algorithm also starts with a minimal network and dynamically builds a suitable cascade structure by training and installing one hidden unit at a time until the problem is successfully learned. This step-wise constructive algorithm exhibits more scalability than existing genetic algorithms and is free of the competing conventions problem which results from the fact that functionally equivalent networks may have different assignments of functionality to individual hidden units. Initial tests of genetic cascade learning are carried out on a difficult supervised learning problem as well as a reinforcement learning control problem.<<ETX>>"
            },
            "slug": "Genetic-cascade-learning-for-neural-networks-Karunanithi-Das",
            "title": {
                "fragments": [],
                "text": "Genetic cascade learning for neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This step-wise constructive algorithm exhibits more scalability than existing genetic algorithms and is free of the competing conventions problem which results from the fact that functionally equivalent networks may have different assignments of functionality to individual hidden units."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145285040"
                        ],
                        "name": "L. D. Whitley",
                        "slug": "L.-D.-Whitley",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Whitley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Whitley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209955"
                        ],
                        "name": "T. Starkweather",
                        "slug": "T.-Starkweather",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Starkweather",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starkweather"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143882377"
                        ],
                        "name": "C. Bogart",
                        "slug": "C.-Bogart",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bogart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bogart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6273216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d52b087713f79b7b1d3fe2112e9cfa3bad221bb",
            "isKey": false,
            "numCitedBy": 742,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Genetic-algorithms-and-neural-networks:-optimizing-Whitley-Starkweather",
            "title": {
                "fragments": [],
                "text": "Genetic algorithms and neural networks: optimizing connections and connectivity"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Comput."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760775"
                        ],
                        "name": "R. Beer",
                        "slug": "R.-Beer",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Beer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Beer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34741198"
                        ],
                        "name": "J. Gallagher",
                        "slug": "J.-Gallagher",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Gallagher",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gallagher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 42196865,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b03528ebfa23050a744da5a80fe85b7552e37a4d",
            "isKey": false,
            "numCitedBy": 624,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We would like the behavior of the artificial agents that we construct to be as well-adapted to their environments as natural animals are to theirs. Unfortunately, designing controllers with these properties is a very difficult task. In this article, we demonstrate that continuous-time recurrent neural networks are a viable mechanism for adaptive agent control and that the genetic algorithm can be used to evolve effective neural controllers. A significant advantage of this approach is that one need specify only a measure of an agent's overall performance rather than the precise motor output trajectories by which it is achieved. By manipulating the performance evaluation, one can place selective pressure on the development of controllers with desired properties. Several novel controllers have been evolved, including a chemotaxis controller that switches between different strategies depending on environmental conditions, and a locomotion controller that takes advantage of sensory feedback if available but that can operate in its absence if necessary."
            },
            "slug": "Evolving-Dynamical-Neural-Networks-for-Adaptive-Beer-Gallagher",
            "title": {
                "fragments": [],
                "text": "Evolving Dynamical Neural Networks for Adaptive Behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that continuous-time recurrent neural networks are a viable mechanism for adaptive agent control and that the genetic algorithm can be used to evolve effective neural controllers."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39755829"
                        ],
                        "name": "G. Castellano",
                        "slug": "G.-Castellano",
                        "structuredName": {
                            "firstName": "Giovanna",
                            "lastName": "Castellano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Castellano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724997"
                        ],
                        "name": "A. Fanelli",
                        "slug": "A.-Fanelli",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Fanelli",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fanelli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8111020"
                        ],
                        "name": "M. Pelillo",
                        "slug": "M.-Pelillo",
                        "structuredName": {
                            "firstName": "Marcello",
                            "lastName": "Pelillo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pelillo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59928099,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9156ef9e7bc77bce678fd92095739babbcc2746c",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural networks are attracting considerable interest within the neural network domain especially because of their potential in such problems as pattern completion and temporal sequence processing (Almeida, 1987; Hertz et al., 1991). As for feed-forward networks, in virtually all problems of interest the proper number of hidden units is not known in advance, and usually this turns out to be a trade-off between generalization and learning abilities (Hertz et al., 1991). One popular way of solving this problem involves training an over-dimensioned network and then pruning excessive units (Sietsma and Dow, 1988)."
            },
            "slug": "Pruning-in-Recurrent-Neural-Networks-Castellano-Fanelli",
            "title": {
                "fragments": [],
                "text": "Pruning in Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Recurrent neural networks are attracting considerable interest within the neural network domain especially because of their potential in such problems as pattern completion and temporal sequence processing."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723475"
                        ],
                        "name": "D. Montana",
                        "slug": "D.-Montana",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Montana",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Montana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144286518"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6336712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09920800fa7841c84a551d70c6101d9510e6fcc8",
            "isKey": false,
            "numCitedBy": 1119,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayered feedforward neural networks possess a number of properties which make them particularly suited to complex pattern classification problems. However, their application to some realworld problems has been hampered by the lack of a training algonthm which reliably finds a nearly globally optimal set of weights in a relatively short time. Genetic algorithms are a class of optimization procedures which are good at exploring a large and complex space in an intelligent way to find values close to the global optimum. Hence, they are well suited to the problem of training feedforward networks. In this paper, we describe a set of experiments performed on data from a sonar image classification problem. These experiments both 1) illustrate the improvements gained by using a genetic algorithm rather than backpropagation and 2) chronicle the evolution of the performance of the genetic algorithm as we added more and more domain-specific knowledge into it."
            },
            "slug": "Training-Feedforward-Neural-Networks-Using-Genetic-Montana-Davis",
            "title": {
                "fragments": [],
                "text": "Training Feedforward Neural Networks Using Genetic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A set of experiments performed on data from a sonar image classification problem are described to illustrate the improvements gained by using a genetic algorithm rather than backpropagation and chronicle the evolution of the performance of the genetic algorithm as it added more and more domain-specific knowledge into it."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777920"
                        ],
                        "name": "M. A. Potter",
                        "slug": "M.-A.-Potter",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Potter",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Potter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some studies attempt to coevolve both the topology and weight values within the GA framework, but as in the connectionist systems described above, the network architectures are restricted (e.g., [ 24 - 26 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14385548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7bf511bab221c3422e2ce1642e13b3545578c150",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Gradient descent techniques such as backpropagation have been used effectively to train neural network connection weights; however, in some applications gradient information may not be available. Biologically inspired genetic algorithms provide an alternative. The paper explores an approach in which a traditional genetic algorithm using standard two-point crossover and mutation is applied within the cascade-correlation learning architecture to train neural network connection weights. In the cascade-correlation architecture the hidden unit feature detector mapping is static; therefore, the possibility of the crossover operator shifting genetic material out of its useful context is reduced.<<ETX>>"
            },
            "slug": "A-genetic-cascade-correlation-learning-algorithm-Potter",
            "title": {
                "fragments": [],
                "text": "A genetic cascade-correlation learning algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper explores an approach in which a traditional genetic algorithm using standard two-point crossover and mutation is applied within the cascade-correlation learning architecture to train neural network connection weights."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153170210"
                        ],
                        "name": "Clifford B. Miller",
                        "slug": "Clifford-B.-Miller",
                        "structuredName": {
                            "firstName": "Clifford",
                            "lastName": "Miller",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Clifford B. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158193072"
                        ],
                        "name": "Dong Chen",
                        "slug": "Dong-Chen",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115401300"
                        ],
                        "name": "Hsing-Hen Chen",
                        "slug": "Hsing-Hen-Chen",
                        "structuredName": {
                            "firstName": "Hsing-Hen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsing-Hen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552960"
                        ],
                        "name": "Yee-Chun Lee",
                        "slug": "Yee-Chun-Lee",
                        "structuredName": {
                            "firstName": "Yee-Chun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yee-Chun Lee"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2543653,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8bdba34dbd4940cafff419cf6430d03d79f21231",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Simple second-order recurrent networks are shown to readily learn small known regular grammars when trained with positive and negative strings examples. We show that similar methods are appropriate for learning unknown grammars from examples of their strings. The training algorithm is an incremental real-time, recurrent learning (RTRL) method that computes the complete gradient and updates the weights at the end of each string. After or during training, a dynamic clustering algorithm extracts the production rules that the neural network has learned. The methods are illustrated by extracting rules from unknown deterministic regular grammars. For many cases the extracted grammar outperforms the neural net from which it was extracted in correctly classifying unseen strings."
            },
            "slug": "Extracting-and-Learning-an-Unknown-Grammar-with-Giles-Miller",
            "title": {
                "fragments": [],
                "text": "Extracting and Learning an Unknown Grammar with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "Simple second-order recurrent networks are shown to readily learn small known regular grammars when trained with positive and negative strings examples and it is shown that similar methods are appropriate for learning unknowngrammars from examples of their strings."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060375648"
                        ],
                        "name": "R. Collins",
                        "slug": "R.-Collins",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Collins",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Collins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2527871"
                        ],
                        "name": "D. Jefferson",
                        "slug": "D.-Jefferson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Jefferson",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jefferson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The various parameter values for the program are set as described above unless otherwise noted."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11122100,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e92a756f67595205281145ddc38d42f791ebe0bc",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an artificial neural network (ANN) representation that supports the evolution of complex behaviors in artificial organisms. The strength and location of each connection in the network is specified by a connection descriptor. The connection descriptors are mapped directly into a bit-string to which a genetic algorithm is applied. We empirically compare this representation to other ANN-based representations in the complex AntFarm task."
            },
            "slug": "An-Artificial-Neural-Network-Representation-for-Collins-Jefferson",
            "title": {
                "fragments": [],
                "text": "An Artificial Neural Network Representation for Artificial Organisms"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "An artificial neural network (ANN) representation that supports the evolution of complex behaviors in artificial organisms and is empirically compared to other ANN-based representations in the complex AntFarm task."
            },
            "venue": {
                "fragments": [],
                "text": "PPSN"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2471881"
                        ],
                        "name": "J. McDonnell",
                        "slug": "J.-McDonnell",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "McDonnell",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McDonnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3123199"
                        ],
                        "name": "D. Waagen",
                        "slug": "D.-Waagen",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Waagen",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waagen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61121566,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e172fce6dd8a313026dcb30c87e9dec31a9756dc",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of evolutionary programming, a stochastic search technique, for determining connectivity in feedforward neural networks, is investigated. The method is capable of simultaneously evolving both the connection scheme and the network weights. The number of synapses is incorporated into an objective function so that network parameter optimization is done with respect to a connectivity cost as well as mean pattern error. Experimental results are shown using feedforward networks for simple binary mapping problems.<<ETX>>"
            },
            "slug": "Determining-neural-network-connectivity-using-McDonnell-Waagen",
            "title": {
                "fragments": [],
                "text": "Determining neural network connectivity using evolutionary programming"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "The application of evolutionary programming, a stochastic search technique, for determining connectivity in feedforward neural networks, is investigated and results are shown using feedforward networks for simple binary mapping problems."
            },
            "venue": {
                "fragments": [],
                "text": "[1992] Conference Record of the Twenty-Sixth Asilomar Conference on Signals, Systems & Computers"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075413715"
                        ],
                        "name": "Zheng Zeng",
                        "slug": "Zheng-Zeng",
                        "structuredName": {
                            "firstName": "Zheng",
                            "lastName": "Zeng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zheng Zeng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145135018"
                        ],
                        "name": "R. Goodman",
                        "slug": "R.-Goodman",
                        "structuredName": {
                            "firstName": "Rodney",
                            "lastName": "Goodman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50860274"
                        ],
                        "name": "Padhraic Smyth",
                        "slug": "Padhraic-Smyth",
                        "structuredName": {
                            "firstName": "Padhraic",
                            "lastName": "Smyth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Padhraic Smyth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 159635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d3b51f94a5934fdda8f1e728f1b56c9bfeb1cb6",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that recurrent neural networks have the ability to learn finite state automata from examples. In particular, networks using second-order units have been successful at this task. In studying the performance and learning behavior of such networks we have found that the second-order network model attempts to form clusters in activation space as its internal representation of states. However, these learned states become unstable as longer and longer test input strings are presented to the network. In essence, the network forgets where the individual states are in activation space. In this paper we propose a new method to force such a network to learn stable states by introducing discretization into the network and using a pseudo-gradient learning rule to perform training. The essence of the learning rule is that in doing gradient descent, it makes use of the gradient of a sigmoid function as a heuristic hint in place of that of the hard-limiting function, while still using the discretized value in the feedback update path. The new structure uses isolated points in activation space instead of vague clusters as its internal representation of states. It is shown to have similar capabilities in learning finite state automata as the original network, but without the instability problem. The proposed pseudo-gradient learning rule may also be used as a basis for training other types of networks that have hard-limiting threshold activation functions."
            },
            "slug": "Learning-Finite-State-Machines-With-Self-Clustering-Zeng-Goodman",
            "title": {
                "fragments": [],
                "text": "Learning Finite State Machines With Self-Clustering Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes a new method to force a recurrent neural network to learn stable states by introducing discretization into the network and using a pseudo-gradient learning rule to perform training, which has similar capabilities in learning finite state automata as the original network, but without the instability problem."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40073871"
                        ],
                        "name": "Marcus Frean",
                        "slug": "Marcus-Frean",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Frean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Frean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 95
                            }
                        ],
                        "text": "Constructive algorithms initially assume a simple network and add nodes and links as warranted [2-8], while destructive methods start with a large network and prune off superfluous components [9-12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17369445,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db408205c71237b3566c358ee3737b8bd0c4078a",
            "isKey": false,
            "numCitedBy": 584,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A general method for building and training multilayer perceptrons composed of linear threshold units is proposed. A simple recursive rule is used to build the structure of the network by adding units as they are needed, while a modified perceptron algorithm is used to learn the connection strengths. Convergence to zero errors is guaranteed for any boolean classification on patterns of binary variables. Simulations suggest that this method is efficient in terms of the numbers of units constructed, and the networks it builds can generalize over patterns not in the training set."
            },
            "slug": "The-Upstart-Algorithm:-A-Method-for-Constructing-Frean",
            "title": {
                "fragments": [],
                "text": "The Upstart Algorithm: A Method for Constructing and Training Feedforward Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Simulations suggest that this method for building and training multilayer perceptrons composed of linear threshold units is efficient in terms of the numbers of units constructed, and the networks it builds can generalize over patterns not in the training set."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1740471"
                        ],
                        "name": "C. Omlin",
                        "slug": "C.-Omlin",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Omlin",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Omlin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18813536,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21ab69ccabe99e70f36062677cd63bca55a3c181",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The experimental results in this paper demonstrate that a simple pruning/retraining method effectively improves the generalization performance of recurrent neural networks trained to recognize regular languages. The technique also permits the extraction of symbolic knowledge in the form of deterministic finite-state automata (DFA) which are more consistent with the rules to be learned. Weight decay has also been shown to improve a network's generalization performance. Simulations with two small DFA (/spl les/10 states) and a large finite-memory machine (64 states) demonstrate that the performance improvement due to pruning/retraining is generally superior to the improvement due to training with weight decay. In addition, there is no need to guess a 'good' decay rate.<<ETX>>"
            },
            "slug": "Pruning-recurrent-neural-networks-for-improved-Giles-Omlin",
            "title": {
                "fragments": [],
                "text": "Pruning recurrent neural networks for improved generalization performance"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "It is demonstrated that a simple pruning/retraining method effectively improves the generalization performance of recurrent neural networks trained to recognize regular languages and permits the extraction of symbolic knowledge in the form of deterministic finite-state automata which are more consistent with the rules to be learned."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400831576"
                        ],
                        "name": "M. Azimi-Sadjadi",
                        "slug": "M.-Azimi-Sadjadi",
                        "structuredName": {
                            "firstName": "Mahmood",
                            "lastName": "Azimi-Sadjadi",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Azimi-Sadjadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1831114"
                        ],
                        "name": "S. Sheedvash",
                        "slug": "S.-Sheedvash",
                        "structuredName": {
                            "firstName": "Sassan",
                            "lastName": "Sheedvash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sheedvash"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2950599"
                        ],
                        "name": "F. O. Trujillo",
                        "slug": "F.-O.-Trujillo",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Trujillo",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. O. Trujillo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 95
                            }
                        ],
                        "text": "Constructive algorithms initially assume a simple network and add nodes and links as warranted [2-8], while destructive methods start with a large network and prune off superfluous components [9-12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 698850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "96f38a8bde6e784d6959c0babafbe4e02f8842c0",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The derivations of a novel approach for simultaneous recursive weight adaptation and node creation in multilayer backpropagation neural networks are presented. The method uses time and order update formulations in the orthogonal projection method to derive a recursive weight updating procedure for the training process of the neural network and a recursive node creation algorithm for weight adjustment of a layer with added nodes during the training process. The proposed approach allows optimal dynamic node creation in the sense that the mean-squared error is minimized for each new topology. The effectiveness of the algorithm is demonstrated on several benchmark problems (the multiplexer and the decoder problems) as well as a real world application for detection and classification of buried dielectric anomalies using a microwave sensor."
            },
            "slug": "Recursive-dynamic-node-creation-in-multilayer-Azimi-Sadjadi-Sheedvash",
            "title": {
                "fragments": [],
                "text": "Recursive dynamic node creation in multilayer neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "The derivations of a novel approach for simultaneous recursive weight adaptation and node creation in multilayer backpropagation neural networks are presented and it is shown that the mean-squared error is minimized for each new topology."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "Fogel [14], [37] uses EP to induce three-layer fully-connected feedforward networks with a variable number of hidden units that employ good strategies for playing Tic-TacToe."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "Research in [14] and [37] uses the heuristic of adding or deleting at most a single fully connected node per structural mutation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 123
                            }
                        ],
                        "text": "Parametric mutations are accomplished by perturbing each weight w of a network\u03b7 with gaussian noise, a method motivated by [37, 14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 32
                            }
                        ],
                        "text": "The structural mutation used in [14, 37] adds or deletes a single hidden unit with equal probability"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58854504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bf85ed770121ccc91154b568ee28fe2d56d4ae6c",
            "isKey": true,
            "numCitedBy": 91,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of evolutionary programming for adapting the design and weights of a multi-layer feedforward perceptron in the context of machine learning is described. Specifically, it is desired to evolve the structure and weights of a single hidden layer perceptron such that it can achieve a high level of play in the game tic-tac-toe without the use of heuristics or credit assignment algorithms. Conclusions from the experiments are offered regarding the relative importance of specific mutation operations, the necessity for credit assignment procedures, and the efficiency and effectiveness of evolutionary search.<<ETX>>"
            },
            "slug": "Using-evolutionary-programing-to-create-neural-that-Fogel",
            "title": {
                "fragments": [],
                "text": "Using evolutionary programing to create neural networks that are capable of playing tic-tac-toe"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The use of evolutionary programming for adapting the design and weights of a multi-layer feedforward perceptron in the context of machine learning to achieve a high level of play in the game tic-tac-toe without the use of heuristics or credit assignment algorithms is described."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727849"
                        ],
                        "name": "S. Hanson",
                        "slug": "S.-Hanson",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Hanson",
                            "middleNames": [
                                "Jose"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hanson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 95
                            }
                        ],
                        "text": "Constructive algorithms initially assume a simple network and add nodes and links as warranted [2-8], while destructive methods start with a large network and prune off superfluous components [9-12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10024978,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f9d9a040c7c888f7d6233c0eba054decac8d3435",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "A central problem in connectionist modelling is the control of network and architectural resources during learning. In the present approach, weights reflect a coarse prediction history as coded by a distribution of values and parameterized in the mean and standard deviation of these weight distributions. Weight updates are a function of both the mean and standard deviation of each connection in the network and vary as a function of the error signal (\"stochastic delta rule\"; Hanson, 1990). Consequently, the weights maintain information on their central tendency and their \"uncertainty\" in prediction. Such information is useful in establishing a policy concerning the size of the nodal complexity of the network and growth of new nodes. For example, during problem solving the present network can undergo \"meiosis\", producing two nodes where there was one \"overtaxed\" node as measured by its coefficient of variation. It is shown in a number of benchmark problems that meiosis networks can find minimal architectures, reduce computational complexity, and overall increase the efficiency of the representation learning interaction."
            },
            "slug": "Meiosis-Networks-Hanson",
            "title": {
                "fragments": [],
                "text": "Meiosis Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown in a number of benchmark problems that meiosis networks can find minimal architectures, reduce computational complexity, and overall increase the efficiency of the representation learning interaction."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2876033"
                        ],
                        "name": "A. Wieland",
                        "slug": "A.-Wieland",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Wieland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wieland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62662319,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8f67d2fbeab8b97b264fde1ee6e70c8cfd04052a",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The author describes how genetic algorithms (GAs) were used to create recurrent neural networks to control a series of unstable systems. The systems considered are variations of the pole balancing problem: network controllers with two, one, and zero inputs, variable length pole, multiple poles on one cart, and a jointed pole. GAs were able to quickly evolve networks for the one- and two-input pole balancing problems. Networks with zero inputs were only able to valance poles for a few seconds of simulated time due to the network's inability to maintain accurate estimates of their position and pole angle. Also, work in progress on a two-legged walker is briefly described.<<ETX>>"
            },
            "slug": "Evolving-neural-network-controllers-for-unstable-Wieland",
            "title": {
                "fragments": [],
                "text": "Evolving neural network controllers for unstable systems"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "The author describes how genetic algorithms were used to create recurrent neural networks to control a series of unstable systems, including network controllers with two, one, and zero inputs, and variations of the pole balancing problem."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145738991"
                        ],
                        "name": "J. D. Schaffer",
                        "slug": "J.-D.-Schaffer",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Schaffer",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Schaffer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145285040"
                        ],
                        "name": "L. D. Whitley",
                        "slug": "L.-D.-Whitley",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Whitley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Whitley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3035326"
                        ],
                        "name": "L. Eshelman",
                        "slug": "L.-Eshelman",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Eshelman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Eshelman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60670877,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "afdf48aaf69a520ed6d5b4a50189facc0ebf4e37",
            "isKey": false,
            "numCitedBy": 621,
            "numCiting": 150,
            "paperAbstract": {
                "fragments": [],
                "text": "Various schemes for combining genetic algorithms and neural networks have been proposed and tested in recent years, but the literature is scattered among a variety of journals, proceedings and technical reports. Activity in this area is clearly increasing. The authors provide an overview of this body of literature drawing out common themes and providing, where possible, the emerging wisdom about what seems to work and what does not.<<ETX>>"
            },
            "slug": "Combinations-of-genetic-algorithms-and-neural-a-of-Schaffer-Whitley",
            "title": {
                "fragments": [],
                "text": "Combinations of genetic algorithms and neural networks: a survey of the state of the art"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An overview of this body of literature drawing out common themes and providing, where possible, the emerging wisdom about what seems to work and what does not is provided."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34922532"
                        ],
                        "name": "Guo-Zheng Sun",
                        "slug": "Guo-Zheng-Sun",
                        "structuredName": {
                            "firstName": "Guo-Zheng",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guo-Zheng Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115401300"
                        ],
                        "name": "Hsing-Hen Chen",
                        "slug": "Hsing-Hen-Chen",
                        "structuredName": {
                            "firstName": "Hsing-Hen",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsing-Hen Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2552960"
                        ],
                        "name": "Yee-Chun Lee",
                        "slug": "Yee-Chun-Lee",
                        "structuredName": {
                            "firstName": "Yee-Chun",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yee-Chun Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158193072"
                        ],
                        "name": "Dong Chen",
                        "slug": "Dong-Chen",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 25151650,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25c19c8c1d6778a16f8b27beac4d9c6a55357580",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A higher order single layer recursive network easily learns to simulate a deterministic finite state machine and recognize regular grammars. When an enhanced version of this neural net state machine is connected through a common error term to an external analog stack memory, the combination can be interpreted as a neural net pushdown automata. The neural net finite state machine is given the primitives, push and POP, and is able to read the top of the stack. Through a gradient descent learning rule derived from the common error function, the hybrid network learns to effectively use the stack actions to manipulate the stack memory and to learn simple contextfree grammars."
            },
            "slug": "Higher-Order-Recurrent-Networks-and-Grammatical-Giles-Sun",
            "title": {
                "fragments": [],
                "text": "Higher Order Recurrent Networks and Grammatical Inference"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A higher order single layer recursive network easily learns to simulate a deterministic finite state machine and recognize regular grammars and can be interpreted as a neural net pushdown automata."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122959051"
                        ],
                        "name": "T. Ash",
                        "slug": "T.-Ash",
                        "structuredName": {
                            "firstName": "Timur",
                            "lastName": "Ash",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ash"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 95
                            }
                        ],
                        "text": "Constructive algorithms initially assume a simple network and add nodes and links as warranted [2-8], while destructive methods start with a large network and prune off superfluous components [9-12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "For example, Ash [2] allows only feedforward networks; Fahlman [6] assumes a restricted form of recurrence, and Chen et al. [7] explore only fully connected topologies."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 17
                            }
                        ],
                        "text": "For example, Ash [2] allows only feedforward networks; Fahlman [6] assumes a restricted form of recurrence, and Chen et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "[2] T. Ash."
                    },
                    "intents": []
                }
            ],
            "corpusId": 40941575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "212a4ab68c4489eca22984ecd297e986693e5200",
            "isKey": true,
            "numCitedBy": 209,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary form only given. A novel method called dynamic node creation (DNC) that attacks issues of training large networks and of testing networks with different numbers of hidden layer units is presented. DNC sequentially adds nodes one at a time to the hidden layer(s) of the network until the desired approximation accuracy is achieved. Simulation results for parity, symmetry, binary addition, and the encoder problem are presented. The procedure was capable of finding known minimal topologies in many cases, and was always within three nodes of the minimum. Computational expense for finding the solutions was comparable to training normal backpropagation (BP) networks with the same final topologies. Starting out with fewer nodes than needed to solve the problem actually seems to help find a solution. The method yielded a solution for every problem tried. BP applied to the same large networks with randomized initial weights was unable, after repeated attempts, to replicate some minimum solutions found by DNC.<<ETX>>"
            },
            "slug": "Dynamic-node-creation-in-backpropagation-networks-Ash",
            "title": {
                "fragments": [],
                "text": "Dynamic node creation in backpropagation networks"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A novel method called dynamic node creation (DNC) that attacks issues of training large networks and of testing networks with different numbers of hidden layer units is presented, which yielded a solution for every problem tried."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "Unlike genetic algorithms, evolutionary programming (EP) [14,34] defines representationdependent mutation operators that create offspring within a specific locus of the parent (see Figure 3)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10751306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c06992e33866e83dc28fc96ef992481a19aa174",
            "isKey": false,
            "numCitedBy": 1552,
            "numCiting": 172,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural evolution is a population-based optimization process. Simulating this process on a computer results in stochastic optimization techniques that can often outperform classical methods of optimization when applied to difficult real-world problems. There are currently three main avenues of research in simulated evolution: genetic algorithms, evolution strategies, and evolutionary programming. Each method emphasizes a different facet of natural evolution. Genetic algorithms stress chromosomal operators. Evolution strategies emphasize behavioral changes at the level of the individual. Evolutionary programming stresses behavioral change at the level of the species. The development of each of these procedures over the past 35 years is described. Some recent efforts in these areas are reviewed."
            },
            "slug": "An-introduction-to-simulated-evolutionary-Fogel",
            "title": {
                "fragments": [],
                "text": "An introduction to simulated evolutionary optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The development of each of these procedures over the past 35 years is described and some recent efforts in these areas are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732302"
                        ],
                        "name": "J. Koza",
                        "slug": "J.-Koza",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Koza",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koza"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18777522,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "440fcfb49ae05fd7f72a47d55011d23e6219de50",
            "isKey": false,
            "numCitedBy": 158,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Research in the field of artificial life focuses on computer programs that exhibit some of the properties of biological life (e.g. self-reproducibility, evolutionary adaptation to an environment, etc.). In one area of artificial life research, human programmers write intentionally simple computer programs (often incorporating observed features of actual biological processes) and then study the \"emergent\" higher level behavior that may be exhibited by such seemingly simple programs. In this chapter, we consider a different problem, namely, \"How can computer programs be automatically written by the computer itself using only measurements of a given program's performance?\" In particular, this chapter describes the recently developed \"genetic programming paradigm\" which genetically breeds populations of computer programs in order to find a computer program that solves the given problem. In the genetic programming paradigm, the individuals in the population are hierarchical compositions of functions and arguments. The hierarchies are of various sizes and shapes. Increasingly fit hierarchies are then evolved in response to the problem environment using the genetic operations of fitness proportionate reproduction (Darwinian survival and reproduction of the fittest) and crossover (sexual recombination). In the genetic programming paradigm, the size and shape of the hierarchical solution to the problem is not specified in advance. Instead, the size and shape of the hierarchy, as well as the contents of the hierarchy, evolve in response to the Darwinian selective pressure exerted by the problem environment. This chapter also describes an extension of the genetic programming paradigm to the case where two (or more) populations of hierarchical computer programs simultaneously co-evolve. In co-evolution, each population acts as the environment for the other population. In particular, each individual of the first population is evaluated for \"relative fitness\" by testing it against each individual in the second population, and, simultaneously, each individual in the second population is evaluated for relative fitness by testing them against each individual in the first population. Over a period of many generations, individuals with high \"absolute fitness\" may 2 evolve as the two populations mutually bootstrap each other to increasingly high levels of fitness. The genetic programming paradigm is illustrated by genetically breeding a population of hierarchical computer programs to allow an \"artificial ant\" to traverse an irregular trail. In addition, we genetically breed a computer program controlling the behavior of an individual ant in an ant colony which, when simultaneously executed by a large number of ants, causes the emergence of \u2026"
            },
            "slug": "Genetic-evolution-and-co-evolution-of-computer-Koza",
            "title": {
                "fragments": [],
                "text": "Genetic evolution and co-evolution of computer programs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This chapter describes the recently developed \"genetic programming paradigm\" which genetically breeds populations of computer programs in order to find a computer program that solves the given problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718584"
                        ],
                        "name": "P. Angeline",
                        "slug": "P.-Angeline",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Angeline",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Angeline"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5456812,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "343a3e0af906a1718a34bce30c1a6feebf9a9095",
            "isKey": false,
            "numCitedBy": 364,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "In the typical genetic algorithm experiment, the fitness function is constructed to be independent of the contents of the population to provide a consistent objective measure. Such objectivity entails significant knowledge about the environment which suggests either the problem has previously been solved or other non-evolutionary techniques may be more efficient. Furthermore, for many complex tasks an independent fitness function is either impractical or impossible to provide. In this paper, we demonstrate that competitive fitness functions, i.e. fitness functions that are dependent on the constituents of the population, can provide a more robust training environment than independent fitness functions. We describe three differing methods for competitive fitness, and discuss their respective advantages."
            },
            "slug": "Competitive-Environments-Evolve-Better-Solutions-Angeline-Pollack",
            "title": {
                "fragments": [],
                "text": "Competitive Environments Evolve Better Solutions for Complex Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "It is demonstrated that competitive fitness functions, i.e. fitness functions that are dependent on the constituents of the population, can provide a more robust training environment than independent fitness functions."
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Constructive algorithms initially assume a simple network and add nodes and links as warranted [2-8], while destructive methods start with a large network and prune off superfluous components [9-12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, Ash [2] allows only feedforward networks; Fahlman [6] assumes a restricted form of recurrence, and Chen et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15720720,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e8cf03655d224b0994d0f9d4f5aa80bca07021a",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Cascade-Correlation (RCC) is a recurrent version of the Cascade-Correlation learning architecture of Fahlman and Lebiere [Fahlman, 1990]. RCC can learn from examples to map a sequence of inputs into a desired sequence of outputs. New hidden units with recurrent connections are added to the network as needed during training. In effect, the network builds up a finite-state machine tailored specifically for the current problem. RCC retains the advantages of Cascade-Correlation: fast learning, good generalization, automatic construction of a near-minimal multi-layered network, and incremental training."
            },
            "slug": "The-Recurrent-Cascade-Correlation-Architecture-Fahlman",
            "title": {
                "fragments": [],
                "text": "The Recurrent Cascade-Correlation Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Recurrent Cascade-Correlation is a recurrent version of the Cascade- Correlation learning architecture of Fahlman and Lebiere that can learn from examples to map a sequence of inputs into a desired sequence of outputs."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822055"
                        ],
                        "name": "Raymond L. Watrous",
                        "slug": "Raymond-L.-Watrous",
                        "structuredName": {
                            "firstName": "Raymond",
                            "lastName": "Watrous",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raymond L. Watrous"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2091434"
                        ],
                        "name": "G. Kuhn",
                        "slug": "G.-Kuhn",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Kuhn",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Kuhn"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16369582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2d86ff53e0cbf244eb0aac8189ced50b39196185",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Second-order recurrent networks that recognize simple finite state languages over {0,1}* are induced from positive and negative examples. Using the complete gradient of the recurrent network and sufficient training examples to constrain the definition of the language to be induced, solutions are obtained that correctly recognize strings of arbitrary length. A method for extracting a finite state automaton corresponding to an optimized network is demonstrated."
            },
            "slug": "Induction-of-Finite-State-Automata-Using-Recurrent-Watrous-Kuhn",
            "title": {
                "fragments": [],
                "text": "Induction of Finite-State Automata Using Second-Order Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "Second-order recurrent networks that recognize simple finite state languages over {0,1}* are induced from positive and negative examples and solutions are obtained that correctly recognize strings of arbitrary length."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736279"
                        ],
                        "name": "B. Hassibi",
                        "slug": "B.-Hassibi",
                        "structuredName": {
                            "firstName": "Babak",
                            "lastName": "Hassibi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Hassibi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586918"
                        ],
                        "name": "D. Stork",
                        "slug": "D.-Stork",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Stork",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Stork"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7057040,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a42954d4b9d0ccdf1036e0af46d87a01b94c3516",
            "isKey": false,
            "numCitedBy": 1594,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate the use of information from all second order derivatives of the error function to perform network pruning (i.e., removing unimportant weights from a trained network) in order to improve generalization, simplify networks, reduce hardware or storage requirements, increase the speed of further training, and in some cases enable rule extraction. Our method, Optimal Brain Surgeon (OBS), is Significantly better than magnitude-based methods and Optimal Brain Damage [Le Cun, Denker and Solla, 1990], which often remove the wrong weights. OBS permits the pruning of more weights than other methods (for the same error on the training set), and thus yields better generalization on test data. Crucial to OBS is a recursion relation for calculating the inverse Hessian matrix H-1 from training data and structural information of the net. OBS permits a 90%, a 76%, and a 62% reduction in weights over backpropagation with weight decay on three benchmark MONK's problems [Thrun et al., 1991]. Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case. Finally, whereas Sejnowski and Rosenberg [1987] used 18,000 weights in their NETtalk network, we used OBS to prune a network to just 1560 weights, yielding better generalization."
            },
            "slug": "Second-Order-Derivatives-for-Network-Pruning:-Brain-Hassibi-Stork",
            "title": {
                "fragments": [],
                "text": "Second Order Derivatives for Network Pruning: Optimal Brain Surgeon"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Of OBS, Optimal Brain Damage, and magnitude-based methods, only OBS deletes the correct weights from a trained XOR network in every case, and thus yields better generalization on test data."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715339"
                        ],
                        "name": "D. Goldberg",
                        "slug": "D.-Goldberg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "Fogel [14], [37] uses EP to induce three-layer fully-connected feedforward networks with a variable number of hidden units that employ good strategies for playing Tic-TacToe."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Parametric mutations alter the value of parameters (link weights) currently in the network, whereasst uctural mutations alter the number of hidden nodes and the presence of links in the network, thus altering the space of parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "By design, the dual representation scheme allows the GA to crossover the bit strings without any knowledge of their interpretation as networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38613589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e62d1345b340d5fda3b092c460264b9543bc4b5",
            "isKey": false,
            "numCitedBy": 58251,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. \n \nMajor concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required."
            },
            "slug": "Genetic-Algorithms-in-Search-Optimization-and-Goldberg",
            "title": {
                "fragments": [],
                "text": "Genetic Algorithms in Search Optimization and Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This book brings together the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404817"
                        ],
                        "name": "J. Holland",
                        "slug": "J.-Holland",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holland",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Holland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58781161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b4279db68b16e20fbc56f9d41980a950191d30a",
            "isKey": false,
            "numCitedBy": 38845,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Name of founding work in the area. Adaptation is key to survival and evolution. Evolution implicitly optimizes organisims. AI wants to mimic biological optimization { Survival of the ttest { Exploration and exploitation { Niche nding { Robust across changing environments (Mammals v. Dinos) { Self-regulation,-repair and-reproduction 2 Artiicial Inteligence Some deenitions { \"Making computers do what they do in the movies\" { \"Making computers do what humans (currently) do best\" { \"Giving computers common sense; letting them make simple deci-sions\" (do as I want, not what I say) { \"Anything too new to be pidgeonholed\" Adaptation and modiication is root of intelligence Some (Non-GA) branches of AI: { Expert Systems (Rule based deduction)"
            },
            "slug": "Adaptation-in-natural-and-artificial-systems-Holland",
            "title": {
                "fragments": [],
                "text": "Adaptation in natural and artificial systems"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Names of founding work in the area of Adaptation and modiication, which aims to mimic biological optimization, and some (Non-GA) branches of AI."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48890329"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Miller",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14024873"
                        ],
                        "name": "P. Todd",
                        "slug": "P.-Todd",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Todd",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Todd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983880"
                        ],
                        "name": "Shailesh U. Hegde",
                        "slug": "Shailesh-U.-Hegde",
                        "structuredName": {
                            "firstName": "Shailesh",
                            "lastName": "Hegde",
                            "middleNames": [
                                "U."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shailesh U. Hegde"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 23166548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e89cb97bc83badf8c6cc0e2439ee4a035cba72d9",
            "isKey": false,
            "numCitedBy": 934,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "RoboCup has come a long way since it\u2019s creation in \u201997 [1] and is a respected place for machine learning researchers to try out new algorithms in a competitive fashion. RoboCup is now an international competition that draws many teams and respected researchers looking for a chance to create the best team. Originally we set out to create a team to compete in RoboCup. This was an ambitious project, and we had hopes to finish within the next year. For this semester, we chose to scale down the RoboCup team towards a smaller research area to try our learning algorithm on. The scaled down version of the RoboCup soccer environment is known as the \u201dKeepaway Testbed\u201d and was started by Peter Stone, University of Texas [2]. Here the task is simple, you have two teams on the field each with the same number of players. Instead of trying to score a goal on the opponent the teams are given tasks, and one team is labeled the keepers and the other is labeled the takers. It is the task of the keepers to maintain possesion of the ball and it is the task of the takers to take the ball. The longer the keepers are able to maintain possesion of the ball the better the team. There are several advantages to this environment. First, it provides some of the essential characteristics of a real soccer game. Typically it is believed that if a team is able to maintain possesion of the ball for long periods of time they will win the match. Secondly, it provides realistic behavior much the same as the original RoboCup server. This is accomplished by introducing noise into the system similar to the original RoboCup, and similar to what would be received by real robots. Finally, when you want to go through the learning process this environment is capable of stopping play once the takers have touched the ball, and the environment is capable of starting a new trial based on that occurrence. Although the RoboCup Keepaway Machine Learning testbed provided an excellent environment to train our agents, we still needed to scale down the problem in order to do a feasibility study. Based on the Keepaway testbed, we created a simulation world with one simple task. One agent is placed into the world and has to locate the position of the goal. This can be thought of as an agent in a soccer environment needing to locate either the ball or another teammate. It was in this environment where we tested our methods for learning autonomous agents."
            },
            "slug": "Designing-Neural-Networks-using-Genetic-Algorithms-Miller-Todd",
            "title": {
                "fragments": [],
                "text": "Designing Neural Networks using Genetic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This semester, the RoboCup Keepaway Machine Learning testbed provided an excellent environment to train the authors' agents, but still needed to scale down the problem in order to do a feasibility study."
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715339"
                        ],
                        "name": "D. Goldberg",
                        "slug": "D.-Goldberg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 19303284,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "affd63b925a1daa487bd83748a1d66eb4c8d6a5c",
            "isKey": false,
            "numCitedBy": 293,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates the application of Walsh functions to the analysis of genetic algorithms operating on different codingfunct ion combinations. Although these analysis tools have been in existence for some time, they have not been widely used. To promote their understanding and use, this paper introduces Bethke's Walshschema transform through the Walsh polynomials. This form of the method provides an intuitive basis for visualizing the nonlinearities being considered, thereby permitting the consideration of a number of useful extensions to the theory in Part II."
            },
            "slug": "Genetic-Algorithms-and-Walsh-Functions:-Part-I,-A-Goldberg",
            "title": {
                "fragments": [],
                "text": "Genetic Algorithms and Walsh Functions: Part I, A Gentle Introduction"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Bethke's Walshschema transform through the Walsh polynomials is introduced, which provides an intuitive basis for visualizing the nonlinearities being considered, thereby permitting the consideration of a number of useful extensions to the theory in Part II."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714528"
                        ],
                        "name": "T. Sejnowski",
                        "slug": "T.-Sejnowski",
                        "structuredName": {
                            "firstName": "Terrence",
                            "lastName": "Sejnowski",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sejnowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140752"
                        ],
                        "name": "Charles R. Rosenberg",
                        "slug": "Charles-R.-Rosenberg",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Rosenberg",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles R. Rosenberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12926318,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de996c32045df6f7b404dda2a753b6a9becf3c08",
            "isKey": false,
            "numCitedBy": 1888,
            "numCiting": 229,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes NETtalk, a class of massively-parallel network systems that learn to convert English text to speech. The memory representations for pronunciations are learned by practice and are shared among many processing units. The performance of NETtalk has some similarities with observed human performance. (i) The learning follows a power law. (ii) The more words the network learns, the better it is at generalizing and correctly pronouncing new words, (iii) The performance of the network degrades very slowly as connections in the network are damaged: no single link or processing unit is essential. (iv) Relearning after damage is much faster than learning during the original training. (v) Distributed or spaced practice is more effective for long-term retention than massed practice. Network models can be constructed that have the same performance and learning characteristics on a particular task, but differ completely at the levels of synaptic strengths and single-unit responses. However, hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units. This suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "slug": "Parallel-Networks-that-Learn-to-Pronounce-English-Sejnowski-Rosenberg",
            "title": {
                "fragments": [],
                "text": "Parallel Networks that Learn to Pronounce English Text"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "H hierarchical clustering techniques applied to NETtalk reveal that these different networks have similar internal representations of letter-to-sound correspondences within groups of processing units, which suggests that invariant internal representations may be found in assemblies of neurons intermediate in size between highly localized and completely distributed representations."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 50027191,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3106e66537a0c8f53278e553bcb38f0b0992ec0e",
            "isKey": false,
            "numCitedBy": 1247,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Given a \u0302 network of simple computing elements and some entities to be represented, the most straightforward scheme is to use one computing element for each entity. This is called a local representation. It is easy to understand and easy to implement because the structure of the physical network mirrors the structure of the knowledge it contains. This report describes a different type of representation that is less familiar and harder to think about than local representations. Each entity is represented by a pattern of activity distributed over many computing elements, and each computing element is involved in representing many different entities. The strength of this more complicated kind of representation does not lie in its notational convenience or its ease of implementation in a conventional computer, but rather in the efficiency with which it makes use of the processing abilities of networks of simple, neuron-like computing elements. Every representational scheme has its good and bad points. Distributed representations are no exception. Some desirable properties like content-addressable memory and automatic generalization arise very naturally from the use of patterns of activity as representations. Other properties, like the ability to temporarily store a large set of arbitrary associations, are much harder to achieve. The best psychological evidence for distributed representations is the degree to which their strengths and weaknesses match those of the human mind. ^This research was supported by a grant from the System Development Foundation. I thank Jim Anderson, Dave Ackley Dana Ballard, Francis Crick, Scott Fahlman, Jerry Feldman, Christopher Longuet-Higgins, Don Norman, Terry Sejnowski, and Tim Shallice for helpful discussions. Jay McClelland and Dave Rumelhart helped me refine and rewrite many of the ideas presented here A substantially revised version of this report will appear as a chapter by Hinton, McClelland and Rumelhart in Parallel Distributed Processing: Explorations in the micro-structure of cognition, edited by McClelland and Rumelhart)"
            },
            "slug": "Distributed-Representations-Hinton-McClelland",
            "title": {
                "fragments": [],
                "text": "Distributed Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This report describes a different type of representation that is less familiar and harder to think about than local representations, which makes use of the processing abilities of networks of simple, neuron-like computing elements."
            },
            "venue": {
                "fragments": [],
                "text": "The Philosophy of Artificial Intelligence"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7785881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "isKey": false,
            "numCitedBy": 3518,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application."
            },
            "slug": "Optimal-Brain-Damage-LeCun-Denker",
            "title": {
                "fragments": [],
                "text": "Optimal Brain Damage"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A class of practical and nearly optimal schemes for adapting the size of a neural network by using second-derivative information to make a tradeoff between network complexity and training set error is derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748557"
                        ],
                        "name": "P. Smolensky",
                        "slug": "P.-Smolensky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Smolensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smolensky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17651092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a87953825b0bea2a5d52bfccf09d2518295c5053",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a means of using the knowledge in a network to determine the functionality or relevance of individual units, both for the purpose of understanding the network's behavior and improving its performance. The basic idea is to iteratively train the network to a certain performance criterion, compute a measure of relevance that identifies which input or hidden units are most critical to performance, and automatically trim the least relevant units. This skeletonization technique can be used to simplify networks by eliminating units that convey redundant information; to improve learning performance by first learning with spare hidden units and then trimming the unnecessary ones away, thereby constraining generalization; and to understand the behavior of networks in terms of minimal \"rules.\""
            },
            "slug": "Skeletonization:-A-Technique-for-Trimming-the-Fat-a-Mozer-Smolensky",
            "title": {
                "fragments": [],
                "text": "Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The basic idea is to iteratively train the network to a certain performance criterion, compute a measure of relevance that identifies which input or hidden units are most critical to performance, and automatically trim the least relevant units."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715339"
                        ],
                        "name": "D. Goldberg",
                        "slug": "D.-Goldberg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Crossover tends to be most effective in environments where the fitness of a member of the population is reasonably correlated with the expected ability of its representational components [ 27 ]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 42161430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c094ddc2f957d9939ce1d5678cba5a2e92fa5462",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Part I considered the application of Walsh functions to the analysis of genetic algorithms operating on different coding-function combinations. In this paper, those meth ods are extended to permit rigorous analysis of deception by considering the expect ed disruption to schema processing caused by different genetic operators. Algebraic extensions of these metho ds are considered, and a sensitivit y analysis is described."
            },
            "slug": "Genetic-Algorithms-and-Walsh-Functions:-Part-II,-Goldberg",
            "title": {
                "fragments": [],
                "text": "Genetic Algorithms and Walsh Functions: Part II, Deception and Its Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Algebraic extensions of Walsh functions are extended to permit rigorous analysis of deception by considering the expect ed disruption to schema processing caused by different genetic operators."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145847131"
                        ],
                        "name": "S. Kirkpatrick",
                        "slug": "S.-Kirkpatrick",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Kirkpatrick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kirkpatrick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5882723"
                        ],
                        "name": "C. D. Gelatt",
                        "slug": "C.-D.-Gelatt",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Gelatt",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Gelatt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "88645967"
                        ],
                        "name": "M. Vecchi",
                        "slug": "M.-Vecchi",
                        "structuredName": {
                            "firstName": "Michelle",
                            "lastName": "Vecchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vecchi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 84
                            }
                        ],
                        "text": "Following [46], each ant is controlled by a network with two input nodes and four output nodes (Figure 11)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 205939,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "dd5061631a4d11fa394f4421700ebf7e78dcbc59",
            "isKey": false,
            "numCitedBy": 39780,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a deep and useful connection between statistical mechanics (the behavior of systems with many degrees of freedom in thermal equilibrium at a finite temperature) and multivariate or combinatorial optimization (finding the minimum of a given function depending on many parameters). A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems. This connection to statistical mechanics exposes new information and provides an unfamiliar perspective on traditional optimization problems and methods."
            },
            "slug": "Optimization-by-Simulated-Annealing-Kirkpatrick-Gelatt",
            "title": {
                "fragments": [],
                "text": "Optimization by Simulated Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A detailed analogy with annealing in solids provides a framework for optimization of the properties of very large and complex systems."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737243"
                        ],
                        "name": "L. Fogel",
                        "slug": "L.-Fogel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Fogel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46400209"
                        ],
                        "name": "A. J. Owens",
                        "slug": "A.-J.-Owens",
                        "structuredName": {
                            "firstName": "Alvin",
                            "lastName": "Owens",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Owens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1446900976"
                        ],
                        "name": "M. J. Walsh",
                        "slug": "M.-J.-Walsh",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Walsh",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. J. Walsh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 82
                            }
                        ],
                        "text": "The algorithm, described in section 3, is an instance of evolutionary programming [13, 14], a class of evolutionary computation that has been shown to perform well at function optimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62252283,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a9e41a611b3b57b828775a45a7d74a1c75ed3f20",
            "isKey": false,
            "numCitedBy": 3238,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: References Artificial Intelligence through a Simulation of Evolution Natural Automata and Prosthetic Devices"
            },
            "slug": "Artificial-Intelligence-through-Simulated-Evolution-Fogel-Owens",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence through Simulated Evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This chapter contains sections titled: References Artificial Intelligence through a Simulation of Evolution Natural Automata and Prosthetic Devices and Artificial intelligence through a simulation of Evolution natural automata and prosthetic devices."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 48
                            }
                        ],
                        "text": "Training sets for the languages of Table 2 from [41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "For instance, Pollack [40] trains sequential cascaded networks (SCNs) over a test set of languages, provided in [41] and"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 158
                            }
                        ],
                        "text": "An explicit collection of positive and negative examples, shown in Table 3, that pose specific difficulties for inducing the intended languages is offered in [41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "Other studies using the training sets in [41] have investigated various network architectures and training methods, as well as algorithms for extracting FSAs from the trained architectures [42 45]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic construction of finite automata from examples using hill-climbing"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Fourth Annual Conference of the Cognitive Science Society"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "The particular network architecture in [46] uses Boolean threshold logic for the hidden units and an identity activation function for the output units."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "The experiment reported in [46] discovered a comparable network after about 17 generations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "FSA hand-crafted for the Tracker task in [46]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "As in the original study [46], no-op allows the ant to remain at a fixed position while activation flows along recurrent connections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Following [46], each ant is controlled by a network with two input nodes and four output nodes (Figure 11)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "Positions \u201cB\u201d and \u201cC\u201d indicate the only two positions along the trail where the ant discovered in run 1 behaves differently from the 5-state FSA of [46] (see Figure 13)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "3 The Ant Problem GNARL was tested on a complex search and collection task \u2013 the Tracker task described in [46], and further investigated in [47]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "In contrast, research in [46] uses a genetic algorithm on a population of 65,536 bit strings with a direct encoding to evolve only the weights of a neural network with five hidden units to solve this task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Given [46] used a population size of 65,536 and replaced 95% of the population each generation, the total number of network evaluations to acquire the equivalent network was 1,123,942."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 118
                            }
                        ],
                        "text": "To understand how the network traverses the path of food, consider the simple FSA shown in Figure 13, hand-crafted in [46] as an approximate solution to the problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolution as a theme in artificial life: The genesys/tracker system"
            },
            "venue": {
                "fragments": [],
                "text": "C. G. Langton, C. Taylor, J. D. Farmer, and S. Rasmussen, editors,  Artificial Life II: Proceedings of the Workshop on Artificial Life, pages 549\u2013577. Addison-Wesley"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060983236"
                        ],
                        "name": "D. E. Goldberg",
                        "slug": "D.-E.-Goldberg",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. E. Goldberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 25
                            }
                        ],
                        "text": "Genetic algorithms (GAs) [15, 16] are a popular form of evolutionary computation that rely chiefly on the reproduction heuristic of crossover ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 131
                            }
                        ],
                        "text": "Section 2 argues that this class of evolutionary computation is better suited for evolving neural networks than genetic algorithms [15, 16], a more popular class of evolutionary computation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 186
                            }
                        ],
                        "text": "Current theory suggests that crossover will tend to recombine short, connected substrings of the bit string representation that correspond to above-average task solutions when evaluated [16, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61406969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc2806a373aefe9ec9fd02e6369fdf9b6a6ff8e4",
            "isKey": true,
            "numCitedBy": 12663,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Genetic-Algorithms-in-Search-Goldberg",
            "title": {
                "fragments": [],
                "text": "Genetic Algorithms in Search"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50472018"
                        ],
                        "name": "W. Miller",
                        "slug": "W.-Miller",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Miller",
                            "middleNames": [
                                "Thomas"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699645"
                        ],
                        "name": "R. Sutton",
                        "slug": "R.-Sutton",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Sutton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Sutton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63874355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cfff16e3e414a8494ae1e2f3a4e69d8d0d840751",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-for-Control-Miller-Sutton",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning for Control"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730590"
                        ],
                        "name": "A. Barto",
                        "slug": "A.-Barto",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Barto",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Barto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "In its complete form, network induction entails both parametric andstructural learning [1], i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 104
                            }
                        ],
                        "text": "Network mutations are separated into two classes, corresponding with the types of learning discussed in [1]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60055440,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "637097ec8f9be315696c7af8a3ac3c42a594ca93",
            "isKey": false,
            "numCitedBy": 64,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-learning-for-control-Barto",
            "title": {
                "fragments": [],
                "text": "Connectionist learning for control"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145559418"
                        ],
                        "name": "J. Torreele",
                        "slug": "J.-Torreele",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Torreele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Torreele"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some studies attempt to coevolve both the topology and weight values within the GA framework, but as in the connectionist systems described above, the network architectures are restricted (e.g., [ 24 - 26 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52852842,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8003367117cc9f3982dfcf6dc5d10d3866e9e89",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Temporal-Processing-with-Recurrent-Networks:-An-Torreele",
            "title": {
                "fragments": [],
                "text": "Temporal Processing with Recurrent Networks: An Evolutionary Approach"
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "An FSA that defines the enable-trigger task [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "1 Williams\u2019 Trigger Problem As an initial test, GNARL induced a solution for the enable-trigger task proposed in [39]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17962678,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25d9530f5a1ae0630d451d754407fc9ab7ceb4b2",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Adaptive-state-representation-and-estimation-using-Williams",
            "title": {
                "fragments": [],
                "text": "Adaptive state representation and estimation using recurrent connectionist networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dynamic node creation in backpropagation"
            },
            "venue": {
                "fragments": [],
                "text": "MIT Press,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist leaming for control"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Connectionist leaming for control"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Control"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Constructive leaming of recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Internarional Conference on Neural Ne?works"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 56
                            }
                        ],
                        "text": "Environments where this is not true are calleddeceptive [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Genetic algorithms and Walsh functions: Part 1"
            },
            "venue": {
                "fragments": [],
                "text": "A gentle introduction. Complex Systems , 3:129\u2013152"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Saunders received the B.S. degree in mathematics in 1988 from Ohio State University, Columbus, Ohio"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "he recurrent cascade-correlation architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing System 3"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The various parameter values for the program are set as described above unless otherwise noted."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Distributed representations, \" in Parallel Distributed Processing: Explorations in the Microstructure of Cognition"
            },
            "venue": {
                "fragments": [],
                "text": "Distributed representations, \" in Parallel Distributed Processing: Explorations in the Microstructure of Cognition"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Genetic algorithms and Walsh functions : Part 1 , A gentle introduction"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Systems"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "As an initial test, GNARL induced a solution for theenable-trigger task proposed in [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A brief history of simulated evolution"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the First Annual Conference on Evolutionay Programming"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "Crossover tends to be most effective in environments where the fitness of a member of the population is reasonably correlated with the expected ability of its representational components [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Genetic algorithms and Walsh functions: Part 2"
            },
            "venue": {
                "fragments": [],
                "text": "Deception and its analysis. Complex Systems , 3:153\u2013171"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extracting and leaming an unknown grammar with recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The various parameter values for the program are set as described above unless otherwise noted."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Genetic algorithms and Walsh functions: Part 1, A gentle introduction"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Systems"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 95
                            }
                        ],
                        "text": "Constructive algorithms initially assume a simple network and add nodes and links as warranted [2-8], while destructive methods start with a large network and prune off superfluous components [9-12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The cascade-correlation architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 2, D. Touretzky"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "PbD. degree in 1987 from the University of Illinois and is now an assistant professor in the Computer and Information Sciences Department of The Ohio State University, where his interests span cogni"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Leaming finite state machines with self-clustering recurrent networks Neural Computation, in press Evolution as a theme in artificial life: The genesyshacker system"
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Life II: Proceedings of the Workshop on Artificial Life"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Extracting and leaming an unknown grammar with recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Vecchi . Optimization by simulated annealing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "received the B.S. degree in mathematics in 1984 from Camegie-Mellon university, and the M.S. degree in computer science from The Ohio State University"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Ohio State University July 16"
            },
            "venue": {
                "fragments": [],
                "text": "The Ohio State University July 16"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Leaming finite state machines with selfclustering recurrent networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The cascade-correlation architecture,"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The various parameter values for the program are set as described above unless otherwise noted."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Genetic algorithms and Walsh functions: Part 2, Deception and its analysis"
            },
            "venue": {
                "fragments": [],
                "text": "Genetic algorithms and Walsh functions: Part 2, Deception and its analysis"
            },
            "year": 1989
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 21,
            "methodology": 11
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 69,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/An-evolutionary-algorithm-that-constructs-recurrent-Angeline-Saunders/fba6007d482db5fbe8cd6c3af90fe0922453e1d2?sort=total-citations"
}