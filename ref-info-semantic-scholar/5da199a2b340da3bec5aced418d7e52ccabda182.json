{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1597383891"
                        ],
                        "name": "Manjeet Dahiya",
                        "slug": "Manjeet-Dahiya",
                        "structuredName": {
                            "firstName": "Manjeet",
                            "lastName": "Dahiya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Manjeet Dahiya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145083365"
                        ],
                        "name": "Sorav Bansal",
                        "slug": "Sorav-Bansal",
                        "structuredName": {
                            "firstName": "Sorav",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorav Bansal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28105062,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a97b91dbbdae39cb3d9f0dd0d981a5b3d664d94a",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on equivalence checking for synthesis and translation validation has usually verified programs across selected optimizations, disabling the ones that exploit undefined behaviour. On the other hand, modern compilers extensively exploit language level undefined behaviour for optimization. Previous work on equivalence checking for translation validation and synthesis yields poor results, when such optimizations relying on undefined behaviour are enabled."
            },
            "slug": "Modeling-Undefined-Behaviour-Semantics-for-Checking-Dahiya-Bansal",
            "title": {
                "fragments": [],
                "text": "Modeling Undefined Behaviour Semantics for Checking Equivalence Across Compiler Optimizations"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This paper verifies equivalence checking for translation validation and synthesis across selected optimizations, when such optimizations relying on undefined behaviour are enabled."
            },
            "venue": {
                "fragments": [],
                "text": "Haifa Verification Conference"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38628703"
                        ],
                        "name": "Anna Zaks",
                        "slug": "Anna-Zaks",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Zaks",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anna Zaks"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698938"
                        ],
                        "name": "A. Pnueli",
                        "slug": "A.-Pnueli",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Pnueli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pnueli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 2
                            }
                        ],
                        "text": ", [13,22,24,29,41,44]), and most have been evaluated on a variety of relatively smaller examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 72
                            }
                        ],
                        "text": "It has been used extensively in previous work on translation validation [14, 26, 28, 32, 41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "CoVaC was tested on smaller examples across a handful of transformations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 6
                            }
                        ],
                        "text": "Also, CoVaC relies on the satisfiability of the conjunction of edge conditions (viz. branch alignment) in the two TFGs, which is unlikely to work across several common transformations that alter the branch structure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "CoVaC relies on an oracular procedure called InvGen; we show a concrete implementation of PredicatesGuessAndCheck()."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 80
                            }
                        ],
                        "text": "In terms of the correlation algorithm, our approach is perhaps closest to CoVaC [41], in that we both construct the JTFG incrementally, and rely on an invariant generation procedure, while determining the correlations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "CoVaC relies on correlating types of operations (e.g., memory reads and writes are different types), which is similar to TVI\u2019s syntactic memory correlations, and is less general than our semantic treatment of memory."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9568105,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43c11eae3ceb570ef627e502a3f041f0cf9a0c06",
            "isKey": true,
            "numCitedBy": 89,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a deductive framework for proving program equivalence and its application to automatic verification of transformations performed by optimizing compilers. To leverage existing program analysis techniques, we reduce the equivalence checking problem to analysis of one system --- a cross-product of the two input programs. We show how the approach can be effectively used for checking equivalence of consonant (i.e., structurally similar) programs. Finally, we report on the prototype tool that applies the developed methodology to verify that a compiler optimization run preserves the program semantics. Unlike existing frameworks, CoVaC accommodates absence of compiler annotations and handles most of the classical intraprocedural optimizations such as constant folding, reassociation, common subexpression elimination, code motion, dead code elimination, branch optimizations, and others."
            },
            "slug": "CoVaC:-Compiler-Validation-by-Program-Analysis-of-Zaks-Pnueli",
            "title": {
                "fragments": [],
                "text": "CoVaC: Compiler Validation by Program Analysis of the Cross-Product"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A deductive framework for proving program equivalence and its application to automatic verification of transformations performed by optimizing compilers, which accommodates absence of compiler annotations and handles most of the classical intraprocedural optimizations."
            },
            "venue": {
                "fragments": [],
                "text": "FM"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403808897"
                        ],
                        "name": "A. Poetzsch-Heffter",
                        "slug": "A.-Poetzsch-Heffter",
                        "structuredName": {
                            "firstName": "Arnd",
                            "lastName": "Poetzsch-Heffter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Poetzsch-Heffter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057415"
                        ],
                        "name": "M. Gawkowski",
                        "slug": "M.-Gawkowski",
                        "structuredName": {
                            "firstName": "Marek",
                            "lastName": "Gawkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gawkowski"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 2
                            }
                        ],
                        "text": ", [13,22,24,29,41,44]), and most have been evaluated on a variety of relatively smaller examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8134001,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4e3d785f17ec0fd57003fc436a1577f06daae3ad",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Towards-Proof-Generating-Compilers-Poetzsch-Heffter-Gawkowski",
            "title": {
                "fragments": [],
                "text": "Towards Proof Generating Compilers"
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Notes Theor. Comput. Sci."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145474353"
                        ],
                        "name": "Shuvendu K. Lahiri",
                        "slug": "Shuvendu-K.-Lahiri",
                        "structuredName": {
                            "firstName": "Shuvendu",
                            "lastName": "Lahiri",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuvendu K. Lahiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145141520"
                        ],
                        "name": "Rohit Sinha",
                        "slug": "Rohit-Sinha",
                        "structuredName": {
                            "firstName": "Rohit",
                            "lastName": "Sinha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rohit Sinha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3095589"
                        ],
                        "name": "C. Hawblitzel",
                        "slug": "C.-Hawblitzel",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Hawblitzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hawblitzel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 8
                            }
                        ],
                        "text": "SymDiff [10, 15, 16] is an effort towards verifying compilers and regression verification, and works on assembly code."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1919349,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fb8137886c443da5bedd266bbed43453122e7de",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Equivalence checking of imperative programs has several applications including compiler validation and cross-version verification. Debugging equivalence failures can be tedious for large examples, especially for low-level binary programs. In this paper, we formalize a simple yet precise notion of verifiable rootcause for equivalence failures that leverages semantic similarity between two programs. Unlike existing works on program repair, our definition of rootcause avoids the need for a template of fixes or the need for a complete repair to ensure equivalence. We show progressively weaker checks for detecting rootcauses that can be applicable even when multiple fixes are required to make the two programs equivalent. We provide optimizations based on Maximum Satisfiability (MAXSAT) and binary search to prune the search space of such rootcauses. We have implemented the techniques in SymDiff and provide an evaluation on a set of real-world compiler validation binary benchmarks."
            },
            "slug": "Automatic-Rootcausing-for-Program-Equivalence-in-Lahiri-Sinha",
            "title": {
                "fragments": [],
                "text": "Automatic Rootcausing for Program Equivalence Failures in Binaries"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper formalizes a simple yet precise notion of verifiable rootcause for equivalence failures that leverages semantic similarity between two programs and shows progressively weaker checks for detecting rootcauses that can be applicable even when multiple fixes are required to make the two programs equivalent."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143914877"
                        ],
                        "name": "Vu Le",
                        "slug": "Vu-Le",
                        "structuredName": {
                            "firstName": "Vu",
                            "lastName": "Le",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vu Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36889034"
                        ],
                        "name": "M. Afshari",
                        "slug": "M.-Afshari",
                        "structuredName": {
                            "firstName": "Mehrdad",
                            "lastName": "Afshari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Afshari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38319925"
                        ],
                        "name": "Z. Su",
                        "slug": "Z.-Su",
                        "structuredName": {
                            "firstName": "Zhendong",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Su"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 103
                            }
                        ],
                        "text": "Our work also overlaps with previous work on verified compilation [21, 42, 43], compiler testing tools [17, 18, 40], undefined behaviour detection [39], and domain specific languages for coding and verifying compiler optimizations [19,20]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 3027731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79bbd54d5bdfd20980e5f9a65480f5e127fc1221",
            "isKey": false,
            "numCitedBy": 274,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers. Our key insight is to exploit the close interplay between (1) dynamically executing a program on some test inputs and (2) statically compiling the program to work on all possible inputs. Indeed, the test inputs induce a natural collection of the original program's EMI variants, which can help differentially test any compiler and specifically target the difficult-to-find miscompilations. To create a practical implementation of EMI for validating C compilers, we profile a program's test executions and stochastically prune its unexecuted code. Our extensive testing in eleven months has led to 147 confirmed, unique bug reports for GCC and LLVM alone. The majority of those bugs are miscompilations, and more than 100 have already been fixed. Beyond testing compilers, EMI can be adapted to validate program transformation and analysis systems in general. This work opens up this exciting, new direction."
            },
            "slug": "Compiler-validation-via-equivalence-modulo-inputs-Le-Afshari",
            "title": {
                "fragments": [],
                "text": "Compiler validation via equivalence modulo inputs"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work introduces equivalence modulo inputs (EMI), a simple, widely applicable methodology for validating optimizing compilers, and profiles a program's test executions and stochastically prune its unexecuted code to create a practical implementation."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI 2014"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33594132"
                        ],
                        "name": "Sudipta Kundu",
                        "slug": "Sudipta-Kundu",
                        "structuredName": {
                            "firstName": "Sudipta",
                            "lastName": "Kundu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sudipta Kundu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2272813"
                        ],
                        "name": "Zachary Tatlock",
                        "slug": "Zachary-Tatlock",
                        "structuredName": {
                            "firstName": "Zachary",
                            "lastName": "Tatlock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zachary Tatlock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145655689"
                        ],
                        "name": "Sorin Lerner",
                        "slug": "Sorin-Lerner",
                        "structuredName": {
                            "firstName": "Sorin",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorin Lerner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 45
                            }
                        ],
                        "text": "We also include an integer program from the SPEC CPU2006 integer benchmarks: sjeng. sjeng is one of the few C benchmarks in SPEC CPU2006 that is not already present in CPU2000, and has a low fraction of floating point operations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 256
                            }
                        ],
                        "text": "The programs that were compiled and checked, are listed in Table 1 along with their characteristics. ctests is a program taken from the CompCert testsuite and involves a variety of different C features and behaviour; the other programs are taken from the SPEC CPU2000 integer benchmarks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 5
                            }
                        ],
                        "text": "The SPEC benchmark programs do not include gcc and eon because their ELF executables files are very big, and our tool to harvest instruction sequences from executable files does not support such large ELF files."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In PEC\u2019s setting, the presence of meta-variables usually provides an easier correspondence between the two programs, greatly simplifying the correlation procedure; the relations (predicates relating variables in two programs) across meta-variables are also easier to determine in this setting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "The Correlate module of parameterized program equivalence checking (PEC) [14] computes simulation based equivalence for optimization patterns represented as parameterized programs containing meta-variables."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 72
                            }
                        ],
                        "text": "It has been used extensively in previous work on translation validation [14, 26, 28, 32, 41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 155
                            }
                        ],
                        "text": "Our tool can automatically generate proofs of equivalence, across O2/O3 compiler transformations, for 74% of the functions in C programs belonging to the SPEC benchmark suite across all four compilers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 67
                            }
                        ],
                        "text": "This prior work has largely employed a pass-by-pass based approach [14,26], where each pass is verified separately by the equivalence checker, and/or worked with a set of handpicked transformations [14, 26, 37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15060665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d9579e31aabd30b752ade4064b965de76e3ce77",
            "isKey": true,
            "numCitedBy": 108,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Translation validation is a technique for checking that, after an optimization has run, the input and output of the optimization are equivalent. Traditionally, translation validation has been used to prove concrete, fully specified programs equivalent. In this paper we present Parameterized Equivalence Checking (PEC), a generalization of translation validation that can prove the equivalence of parameterized programs. A parameterized program is a partially specified program that can represent multiple concrete programs. For example, a parameterized program may contain a section of code whose only known property is that it does not modify certain variables. By proving parameterized programs equivalent, PEC can prove the correctness of transformation rules that represent complex optimizations once and for all, before they are ever run. We implemented our PEC technique in a tool that can establish the equivalence of two parameterized programs. To highlight the power of PEC, we designed a language for implementing complex optimizations using many-to-many rewrite rules, and used this language to implement a variety of optimizations including software pipelining, loop unrolling, loop unswitching, loop interchange, and loop fusion. Finally, to demonstrate the effectiveness of PEC, we used our PEC implementation to verify that all the optimizations we implemented in our language preserve program behavior."
            },
            "slug": "Proving-optimizations-correct-using-parameterized-Kundu-Tatlock",
            "title": {
                "fragments": [],
                "text": "Proving optimizations correct using parameterized program equivalence"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Parameterized Equivalence Checking (PEC), a generalization of translation validation that can prove the equivalence of parameterized programs, is presented."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790411"
                        ],
                        "name": "G. Necula",
                        "slug": "G.-Necula",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Necula",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Necula"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 8
                            }
                        ],
                        "text": "Second, TVI\u2019s heuristics for branch and memory-access correlations at basic-block granularity are syntactic, and fail for a large number of compiler transformations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "There are several issues with TVI when applied to end-to-end (black-box and composed transformations) equivalence checking."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 186
                            }
                        ],
                        "text": "A value-graph approach is limited by the vocabulary of transformations that are supported by the translation validator, and thus seems less general than constraint-based approaches like TVI and ours."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "Third, TVI relies on weakest-precondition based inference of simulation relation predicates, which is both expensive and less robust than our guessing procedure."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 15
                            }
                        ],
                        "text": ", Necula\u2019s TVI [26] fails when a simple if-block is replaced by a conditional move instruction (cmov)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 164
                            }
                        ],
                        "text": "Translation validation for mature compilers on large and complex programs, has been reported in at least two previous works: Translation validation infrastructure (TVI) [26] for GCC, and Value-graph translation validation [34,38] for LLVM.\nTVI demonstrated the validation of the gcc-2."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 4
                            }
                        ],
                        "text": "The TVI paper reports around 87% validation success rates."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 162
                            }
                        ],
                        "text": "We also find that the underlying algorithms of previous techniques are not robust with respect to the transformations produced by modern compilers, e.g., Necula\u2019s TVI [26] fails when a simple if-block is replaced by a conditional move instruction (cmov)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 72
                            }
                        ],
                        "text": "It has been used extensively in previous work on translation validation [14, 26, 28, 32, 41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 67
                            }
                        ],
                        "text": "This prior work has largely employed a pass-by-pass based approach [14,26], where each pass is verified separately by the equivalence checker, and/or worked with a set of handpicked transformations [14, 26, 37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 9
                            }
                        ],
                        "text": "Finally, TVI was tested across five compiler passes, and did not address several transformations, including those relying on undefined behaviour."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 2
                            }
                        ],
                        "text": ", [26, 32]) which are usually weaker, and do not suffice for black-box compiler transformations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "Translation validation for mature compilers on large and complex programs, has been reported in at least two previous works: Translation validation infrastructure (TVI) [26] for GCC, and Value-graph translation validation [34,38] for LLVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "CoVaC relies on correlating types of operations (e.g., memory reads and writes are different types), which is similar to TVI\u2019s syntactic memory correlations, and is less general than our semantic treatment of memory."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 3
                            }
                        ],
                        "text": "In TVI, validation is performed across each IR pass, i.e., first the input IR is validated against the output of the first pass, then the output of the first pass is validated against the output of the second pass, and so on."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2448939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "011f7da0095ac8c0d4477eeda2728e5f80a35767",
            "isKey": true,
            "numCitedBy": 486,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a translation validation infrastructure for the GNU C compiler. During the compilation the infrastructure compares the intermediate form of the program before and after each compiler pass and verifies the preservation of semantics. We discuss a general framework that the optimizer can use to communicate to the validator what transformations were performed. Our implementation however does not rely on help from the optimizer and it is quite successful by using instead a few heuristics to detect the transformations that take place.\nThe main message of this paper is that a practical translation validation infrastructure, able to check the correctness of many of the transformations performed by a realistic compiler, can be implemented with about the effort typically required to implement one compiler pass. We demonstrate this in the context of the GNU C compiler for a number of its optimizations while compiling realistic programs such as the compiler itself or the Linux kernel. We believe that the price of such an infrastructure is small considering the qualitative increase in the ability to isolate compilation errors during compiler testing and maintenance."
            },
            "slug": "Translation-validation-for-an-optimizing-compiler-Necula",
            "title": {
                "fragments": [],
                "text": "Translation validation for an optimizing compiler"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A practical translation validation infrastructure, able to check the correctness of many of the transformations performed by a realistic compiler, can be implemented with about the effort typically required to implement one compiler pass."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688414"
                        ],
                        "name": "L. Zuck",
                        "slug": "L.-Zuck",
                        "structuredName": {
                            "firstName": "Lenore",
                            "lastName": "Zuck",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Zuck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698938"
                        ],
                        "name": "A. Pnueli",
                        "slug": "A.-Pnueli",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Pnueli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pnueli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144179390"
                        ],
                        "name": "B. Goldberg",
                        "slug": "B.-Goldberg",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Goldberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 2
                            }
                        ],
                        "text": ", [13,22,24,29,41,44]), and most have been evaluated on a variety of relatively smaller examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2895860,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cb8e965ae51ad0a2752d2bda5b8c25b68b43a80f",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "There is a growing awareness, both in industry and academia, of the crucial role of formally verifying the translation from high-level source-code into low-level ob- ject code that is typically performed by an optimizing compiler. Formally verifying an optimizing compiler, as one would verify any other large program, is not feasible due to its size, ongoing evolution and modification, and, possibly, proprietary considerations. Translation validation is a novel approach that offers an alternative to the verification of translators in general and compilers in particular: Rather than verifying the compiler itself, one constructs a validation tool which, after every run of the compiler, formally confirms that the target code produced in the run is a correct translation of the source program. The paper presents voc, a methodology for the translation validation of optimizing compilers. We distinguish between structure preserving optimizations, for which we establish a simulation relation between the source and target code based on computational induction, and structure modifying optimizations, for which we develop specialized \"permutation rules\". The paper also describes voc-64\u2014a prototype trans- lation validator tool that automatically produces verification conditions for the global optimizations of the SGI Pro-64 compiler."
            },
            "slug": "VOC:-A-Methodology-for-the-Translation-Validation-Zuck-Pnueli",
            "title": {
                "fragments": [],
                "text": "VOC: A Methodology for the Translation Validation of OptimizingCompilers"
            },
            "venue": {
                "fragments": [],
                "text": "J. Univers. Comput. Sci."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2594759"
                        ],
                        "name": "Aditya Kanade",
                        "slug": "Aditya-Kanade",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Kanade",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aditya Kanade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145922381"
                        ],
                        "name": "A. Sanyal",
                        "slug": "A.-Sanyal",
                        "structuredName": {
                            "firstName": "Amitabha",
                            "lastName": "Sanyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sanyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3210935"
                        ],
                        "name": "Uday P. Khedker",
                        "slug": "Uday-P.-Khedker",
                        "structuredName": {
                            "firstName": "Uday",
                            "lastName": "Khedker",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Uday P. Khedker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 2
                            }
                        ],
                        "text": ", [13,22,24,29,41,44]), and most have been evaluated on a variety of relatively smaller examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18174964,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "feab9a1a90e9472b0b87b6b8dd3450864d1201d1",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "The translation validation approach involves establishing semantics preservation of individual compilations. In this paper, we present a novel framework for translation validation of optimizers. We identify a comprehensive set of primitive program transformations that are commonly used in many optimizations. For each primitive, we define soundness conditions that guarantee that the transformation is semantics preserving. This framework of transformations and soundness conditions is independent of any particular compiler implementation and is formalized in PVS. An optimizer is instrumented to generate the trace of an optimization run in terms of the predefined transformation primitives. The validation succeeds if (1) the trace conforms to the optimization and (2) the soundness conditions of the individual transformations in the trace are satisfied. The first step eliminates the need to trust the instrumentation. The soundness conditions are defined in a temporal logic and therefore the second step involves model checking. Thus the scheme is completely automatable. We have applied this approach to several intraprocedural optimizations of RTL intermediate code in GNU Compiler Collection (GCC) v4.1.0, namely, loop invariant code motion, partial redundancy elimination, lazy code motion, code hoisting, and copy and constant propagation for sample programs written in a subset of the C language. The validation does not require information about program analyses performed by GCC. Therefore even though the GCC code base is quite large and complex, instrumentation could be achieved easily. The framework requires an estimated 21 lines of instrumentation code and 140 lines of PVS specifications for every 1000 lines of the GCC code considered for validation. Copyright \u00a9 2009 John Wiley & Sons, Ltd."
            },
            "slug": "Validation-of-GCC-optimizers-through-trace-Kanade-Sanyal",
            "title": {
                "fragments": [],
                "text": "Validation of GCC optimizers through trace generation"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A novel framework for translation validation of optimizers by identifying a comprehensive set of primitive program transformations that are commonly used in many optimizations and defining soundness conditions that guarantee that the transformation is semantics preserving."
            },
            "venue": {
                "fragments": [],
                "text": "Softw. Pract. Exp."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755724"
                        ],
                        "name": "Jean-Baptiste Tristan",
                        "slug": "Jean-Baptiste-Tristan",
                        "structuredName": {
                            "firstName": "Jean-Baptiste",
                            "lastName": "Tristan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Baptiste Tristan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032062"
                        ],
                        "name": "Paul Govereau",
                        "slug": "Paul-Govereau",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Govereau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Govereau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143649399"
                        ],
                        "name": "J. G. Morrisett",
                        "slug": "J.-G.-Morrisett",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Morrisett",
                            "middleNames": [
                                "Gregory"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. G. Morrisett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 101
                            }
                        ],
                        "text": "Value-graph translation validation for LLVM has been performed previously in two independent efforts [34, 38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[38], validation is performed across a known set of transformations, namely, dead-code elimination, global value numbering, sparse-condition constant propagation, loop-invariant code motion, loop deletion, loop unswitching, and dead-store elimination."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 222
                            }
                        ],
                        "text": "Translation validation for mature compilers on large and complex programs, has been reported in at least two previous works: Translation validation infrastructure (TVI) [26] for GCC, and Value-graph translation validation [34,38] for LLVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 966104,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b40c2883a0b9bef9be6c9fa56c9f8dd48e2c3909",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Translation validators are static analyzers that attempt to verify that program transformations preserve semantics. Normalizing translation validators do so by trying to match the value-graphs of an original function and its transformed counterpart. In this paper, we present the design of such a validator for LLVM's intra-procedural optimizations, a design that does not require any instrumentation of the optimizer, nor any rewriting of the source code to compile, and needs to run only once to validate a pipeline of optimizations. We present the results of our preliminary experiments on a set of benchmarks that include GCC, a perl interpreter, SQLite3, and other C programs."
            },
            "slug": "Evaluating-value-graph-translation-validation-for-Tristan-Govereau",
            "title": {
                "fragments": [],
                "text": "Evaluating value-graph translation validation for LLVM"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The design of a translation validator for LLVM's intra-procedural optimizations is presented, a design that does not require any instrumentation of the optimizer, nor any rewriting of the source code to compile, and needs to run only once to validate a pipeline of optimizations."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI '11"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145655689"
                        ],
                        "name": "Sorin Lerner",
                        "slug": "Sorin-Lerner",
                        "structuredName": {
                            "firstName": "Sorin",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorin Lerner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3105241"
                        ],
                        "name": "T. Millstein",
                        "slug": "T.-Millstein",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Millstein",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Millstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717790"
                        ],
                        "name": "C. Chambers",
                        "slug": "C.-Chambers",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Chambers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chambers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 231
                            }
                        ],
                        "text": "Our work also overlaps with previous work on verified compilation [21, 42, 43], compiler testing tools [17, 18, 40], undefined behaviour detection [39], and domain specific languages for coding and verifying compiler optimizations [19,20]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 6411424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c685c3c47d797b446472e17a87b27e93a399eb3a",
            "isKey": false,
            "numCitedBy": 131,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a technique for automatically proving compiler optimizations sound, meaning that their transformations are always semantics-preserving. We first present a domain-specific language, called Cobalt, for implementing optimizations as guarded rewrite rules. Cobalt optimizations operate over a C-like intermediate representation including unstructured control flow, pointers to local variables and dynamically allocated memory, and recursive procedures. Then we describe a technique for automatically proving the soundness of Cobalt optimizations. Our technique requires an automatic theorem prover to discharge a small set of simple, optimization-specific proof obligations for each optimization. We have written a variety of forward and backward intraprocedural dataflow optimizations in Cobalt, including constant propagation and folding, branch folding, full and partial redundancy elimination, full and partial dead assignment elimination, and simple forms of points-to analysis. We implemented our soundness-checking strategy using the Simplify automatic theorem prover, and we have used this implementation to automatically prove our optimizations correct. Our checker found many subtle bugs during the course of developing our optimizations. We also implemented an execution engine for Cobalt optimizations as part of the Whirlwind compiler infrastructure."
            },
            "slug": "Automatically-proving-the-correctness-of-compiler-Lerner-Millstein",
            "title": {
                "fragments": [],
                "text": "Automatically proving the correctness of compiler optimizations"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A technique for automatically proving compiler optimizations sound, meaning that their transformations are always semantics-preserving, and a domain-specific language, called Cobalt, is presented for implementing optimizations as guarded rewrite rules."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI '03"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145655689"
                        ],
                        "name": "Sorin Lerner",
                        "slug": "Sorin-Lerner",
                        "structuredName": {
                            "firstName": "Sorin",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorin Lerner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3105241"
                        ],
                        "name": "T. Millstein",
                        "slug": "T.-Millstein",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Millstein",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Millstein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39786986"
                        ],
                        "name": "Erika Rice",
                        "slug": "Erika-Rice",
                        "structuredName": {
                            "firstName": "Erika",
                            "lastName": "Rice",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erika Rice"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717790"
                        ],
                        "name": "C. Chambers",
                        "slug": "C.-Chambers",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Chambers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chambers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 238,
                                "start": 231
                            }
                        ],
                        "text": "Our work also overlaps with previous work on verified compilation [21, 42, 43], compiler testing tools [17, 18, 40], undefined behaviour detection [39], and domain specific languages for coding and verifying compiler optimizations [19,20]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 8742603,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "043899304f00ef1b544320837633366d472c46db",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We present Rhodium, a new language for writing compiler optimizations that can be automatically proved sound. Unlike our previous work on Cobalt, Rhodium expresses optimizations using explicit dataflow facts manipulated by local propagation and transformation rules. This new style allows Rhodium optimizations to be mutually recursively defined, to be automatically composed, to be interpreted in both flow-sensitive and -insensitive ways, and to be applied interprocedurally given a separate context-sensitivity strategy, all while retaining soundness. Rhodium also supports infinite analysis domains while guaranteeing termination of analysis. We have implemented a soundness checker for Rhodium and have specified and automatically proven the soundness of all of Cobalt's optimizations plus a variety of optimizations not expressible in Cobalt, including Andersen's points-to analysis, arithmetic-invariant detection, loop-induction-variable strength reduction, and redundant array load elimination."
            },
            "slug": "Automated-soundness-proofs-for-dataflow-analyses-Lerner-Millstein",
            "title": {
                "fragments": [],
                "text": "Automated soundness proofs for dataflow analyses and transformations via local rules"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Rhodium is presented, a new language for writing compiler optimizations that can be automatically proved sound, and has specified and automatically proven the soundness of all of Cobalt's optimizations plus a variety of optimizations not expressible in Cobalt."
            },
            "venue": {
                "fragments": [],
                "text": "POPL '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3336104"
                        ],
                        "name": "Alan Leung",
                        "slug": "Alan-Leung",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Leung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan Leung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3068687"
                        ],
                        "name": "Dimitar Bounov",
                        "slug": "Dimitar-Bounov",
                        "structuredName": {
                            "firstName": "Dimitar",
                            "lastName": "Bounov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitar Bounov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145655689"
                        ],
                        "name": "Sorin Lerner",
                        "slug": "Sorin-Lerner",
                        "structuredName": {
                            "firstName": "Sorin",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorin Lerner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 2
                            }
                        ],
                        "text": ", [13,22,24,29,41,44]), and most have been evaluated on a variety of relatively smaller examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2380035,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2bb10925ceb698bc2a4525f3c7d31193e0417929",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "To offset the high engineering cost of digital circuit design, hardware engineers are looking increasingly toward high-level languages such as C and C++ to implement their designs. To do this, they employ High-Level Synthesis (HLS) tools that translate their high-level specifications down to a hardware description language such as Verilog. Unfortunately, HLS tools themselves employ sophisticated optimization passes that may have bugs that silently introduce errors in realized hardware. The cost of such errors is high, as hardware is costly or impossible to repair if software patching is not an option. In this work, we present a translation validation approach for verifying the correctness of the HLS translation process. Given an initial C program and the generated Verilog code, our approach establishes their equivalence without relying on any intermediate results or representations produced by the HLS tool. We implemented our approach in a tool called VTV that is able to validate a body of programs compiled by the Xilinx Vivado HLS compiler."
            },
            "slug": "C-to-Verilog-translation-validation-Leung-Bounov",
            "title": {
                "fragments": [],
                "text": "C-to-Verilog translation validation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work presents a translation validation approach for verifying the correctness of the HLS translation process, and establishes their equivalence without relying on any intermediate results or representations produced by the H LS tool."
            },
            "venue": {
                "fragments": [],
                "text": "2015 ACM/IEEE International Conference on Formal Methods and Models for Codesign (MEMOCODE)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111333766"
                        ],
                        "name": "Rahul Sharma",
                        "slug": "Rahul-Sharma",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1945094"
                        ],
                        "name": "Eric Schkufza",
                        "slug": "Eric-Schkufza",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Schkufza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Schkufza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445900"
                        ],
                        "name": "Berkeley R. Churchill",
                        "slug": "Berkeley-R.-Churchill",
                        "structuredName": {
                            "firstName": "Berkeley",
                            "lastName": "Churchill",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berkeley R. Churchill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 27
                            }
                        ],
                        "text": "Compared head-to-head with DDEC, our algorithm is static (does not rely on execution traces), supports a richer set of constructs (stack/memory/global accesses, function calls, undefined behaviour), is more robust (tested on a much larger set of programs, and across a richer set of transformations), and more efficient (when compared head-to-head on the same programs)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "However, DDEC takes a radically different approach of relying on the availability of execution traces for high-coverage tests, an assumption that is not always practical in a general compiler optimization setting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "Most previous translation validation work (except DDEC) has been applied to IR."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "DDEC was tested on a smaller set of examples (around 18) of x86 assembly code generated using GCC and CompCert, and all DDEC test examples are a part of our ctests benchmark."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": "Data-driven equivalence checking (DDEC) [32] is an effort perhaps closest to our goals of checking equivalence on x86 assembly programs."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 72
                            }
                        ],
                        "text": "It has been used extensively in previous work on translation validation [14, 26, 28, 32, 41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 2
                            }
                        ],
                        "text": ", [26, 32]) which are usually weaker, and do not suffice for black-box compiler transformations."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Recent work on loop superoptimization for Google Native Client [3] extends DDEC by supporting inequality-based invariants; the evaluation however is limited to a small selection of test cases, and hence does not address several scalability and modeling issues that we tackle in our equivalence checker."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "While DDEC can infer linear equalities through execution traces, it cannot handle several other types of nonlinear invariants (e.g., inequalities) often required to prove equivalence across modern compiler transformations."
                    },
                    "intents": []
                }
            ],
            "corpusId": 995430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e52a2e8535509ab0111c0c5d89a88d3bb10b34c",
            "isKey": true,
            "numCitedBy": 66,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a data driven algorithm for equivalence checking of two loops. The algorithm infers simulation relations using data from test runs. Once a candidate simulation relation has been obtained, off-the-shelf SMT solvers are used to check whether the simulation relation actually holds. The algorithm is sound: insufficient data will cause the proof to fail. We demonstrate a prototype implementation, called DDEC, of our algorithm, which is the first sound equivalence checker for loops written in x86 assembly."
            },
            "slug": "Data-driven-equivalence-checking-Sharma-Schkufza",
            "title": {
                "fragments": [],
                "text": "Data-driven equivalence checking"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "A data driven algorithm for equivalence checking of two loops, which is the first sound equivalence checker for loops written in x86 assembly and demonstrates a prototype implementation of the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "OOPSLA 2013"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719385"
                        ],
                        "name": "H. Samet",
                        "slug": "H.-Samet",
                        "structuredName": {
                            "firstName": "Hanan",
                            "lastName": "Samet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Samet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "One of the earliest examples of a translation validator can be found in [30]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1341327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e954296ea8765a5f1e7f6a1717224c37da4af2bb",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A system for proving that programs written in a high level language are correctly translated to a low level language is described. A primary use of the system is as a postoptimization step in code generation. The low level language programs need not be generated by a compiler and in fact could be hand coded. Examples of the usefulness of such a system are given. Some interesting results are the ability to handle programs that implement recursion by bypassing the start of the program, and the detection and pinpointing of a wide class of errors in the low level language programs. The examples demonstrate that optimization of the genre of this paper can result in substantially faster operation and the saving of memory in terms of program and stack sizes."
            },
            "slug": "Proving-the-correctness-of-heuristically-optimized-Samet",
            "title": {
                "fragments": [],
                "text": "Proving the correctness of heuristically optimized code"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The examples demonstrate that optimization of the genre of this paper can result in substantially faster operation and the saving of memory in terms of program and stack sizes."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144509790"
                        ],
                        "name": "Nuno P. Lopes",
                        "slug": "Nuno-P.-Lopes",
                        "structuredName": {
                            "firstName": "Nuno",
                            "lastName": "Lopes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nuno P. Lopes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145582916"
                        ],
                        "name": "David Menendez",
                        "slug": "David-Menendez",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Menendez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Menendez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2375915"
                        ],
                        "name": "Santosh Nagarakatte",
                        "slug": "Santosh-Nagarakatte",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Nagarakatte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santosh Nagarakatte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783210"
                        ],
                        "name": "J. Regehr",
                        "slug": "J.-Regehr",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Regehr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Regehr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Alive [23] verifies acyclic peephole optimization patterns of the InstCombine pass of LLVM and models undefined behaviour involving undefined values, poison values and arithmetic overflow."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6972481,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37791336941a0d954e4a98c96b1a66ca7be43eb2",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Compilers should not miscompile. Our work addresses problems in developing peephole optimizations that perform local rewriting to improve the efficiency of LLVM code. These optimizations are individually difficult to get right, particularly in the presence of undefined behavior; taken together they represent a persistent source of bugs. This paper presents Alive, a domain-specific language for writing optimizations and for automatically either proving them correct or else generating counterexamples. Furthermore, Alive can be automatically translated into C++ code that is suitable for inclusion in an LLVM optimization pass. Alive is based on an attempt to balance usability and formal methods; for example, it captures---but largely hides---the detailed semantics of three different kinds of undefined behavior in LLVM. We have translated more than 300 LLVM optimizations into Alive and, in the process, found that eight of them were wrong."
            },
            "slug": "Provably-correct-peephole-optimizations-with-alive-Lopes-Menendez",
            "title": {
                "fragments": [],
                "text": "Provably correct peephole optimizations with alive"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Alive is presented, a domain-specific language for writing optimizations and for automatically either proving them correct or else generating counterexamples, and can be automatically translated into C++ code that is suitable for inclusion in an LLVM optimization pass."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145180765"
                        ],
                        "name": "Xuejun Yang",
                        "slug": "Xuejun-Yang",
                        "structuredName": {
                            "firstName": "Xuejun",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuejun Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144353532"
                        ],
                        "name": "Yang Chen",
                        "slug": "Yang-Chen",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39080210"
                        ],
                        "name": "E. Eide",
                        "slug": "E.-Eide",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Eide",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Eide"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1783210"
                        ],
                        "name": "J. Regehr",
                        "slug": "J.-Regehr",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Regehr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Regehr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 103
                            }
                        ],
                        "text": "Our work also overlaps with previous work on verified compilation [21, 42, 43], compiler testing tools [17, 18, 40], undefined behaviour detection [39], and domain specific languages for coding and verifying compiler optimizations [19,20]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 868674,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3b1bae14269d1e3bbb45f79bb471af3bd0bf4e1e",
            "isKey": false,
            "numCitedBy": 748,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Compilers should be correct. To improve the quality of C compilers, we created Csmith, a randomized test-case generation tool, and spent three years using it to find compiler bugs. During this period we reported more than 325 previously unknown bugs to compiler developers. Every compiler we tested was found to crash and also to silently generate wrong code when presented with valid input. In this paper we present our compiler-testing tool and the results of our bug-hunting study. Our first contribution is to advance the state of the art in compiler testing. Unlike previous tools, Csmith generates programs that cover a large subset of C while avoiding the undefined and unspecified behaviors that would destroy its ability to automatically find wrong-code bugs. Our second contribution is a collection of qualitative and quantitative results about the bugs we have found in open-source C compilers."
            },
            "slug": "Finding-and-understanding-bugs-in-C-compilers-Yang-Chen",
            "title": {
                "fragments": [],
                "text": "Finding and understanding bugs in C compilers"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "Csmith, a randomized test-case generation tool, is created and spent three years using it to find compiler bugs, and a collection of qualitative and quantitative results about the bugs it found are presented."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI '11"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2941500"
                        ],
                        "name": "Dennis Felsing",
                        "slug": "Dennis-Felsing",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Felsing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dennis Felsing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770610"
                        ],
                        "name": "S. Grebing",
                        "slug": "S.-Grebing",
                        "structuredName": {
                            "firstName": "Sarah",
                            "lastName": "Grebing",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Grebing"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789960"
                        ],
                        "name": "V. Klebanov",
                        "slug": "V.-Klebanov",
                        "structuredName": {
                            "firstName": "Vladimir",
                            "lastName": "Klebanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Klebanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739683"
                        ],
                        "name": "Philipp R\u00fcmmer",
                        "slug": "Philipp-R\u00fcmmer",
                        "structuredName": {
                            "firstName": "Philipp",
                            "lastName": "R\u00fcmmer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philipp R\u00fcmmer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34752863"
                        ],
                        "name": "Mattias Ulbrich",
                        "slug": "Mattias-Ulbrich",
                        "structuredName": {
                            "firstName": "Mattias",
                            "lastName": "Ulbrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mattias Ulbrich"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 125
                            }
                        ],
                        "text": "While our equivalence checker can correctly compute equivalence across all the examples presented in regression verification [7, 35], the converse is not true."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 41
                            }
                        ],
                        "text": "Previous work on regression verification [7,35] determines equivalence across structurally similar programs, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1369504,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9b52c5fd1b6ad0e84065f7801a771673f765f948",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Regression verification is an approach complementing regression testing with formal verification. The goal is to formally prove that two versions of a program behave either equally or differently in a precisely specified way. In this paper, we present a novel automatic approach for regression verification that reduces the equivalence of two related imperative integer programs to Horn constraints over uninterpreted predicates. Subsequently, state-of-the-art SMT solvers are used to solve the constraints. We have implemented the approach, and our experiments show non-trivial integer programs that can now be proved equivalent without further user input."
            },
            "slug": "Automating-regression-verification-Felsing-Grebing",
            "title": {
                "fragments": [],
                "text": "Automating regression verification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A novel automatic approach for regression verification is presented that reduces the equivalence of two related imperative integer programs to Horn constraints over uninterpreted predicates and shows non-trivial integer programs that can now be proved equivalent without further user input."
            },
            "venue": {
                "fragments": [],
                "text": "Software Engineering & Management"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6331083"
                        ],
                        "name": "R. Tate",
                        "slug": "R.-Tate",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Tate",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30073551"
                        ],
                        "name": "M. Stepp",
                        "slug": "M.-Stepp",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stepp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stepp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145655689"
                        ],
                        "name": "Sorin Lerner",
                        "slug": "Sorin-Lerner",
                        "structuredName": {
                            "firstName": "Sorin",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorin Lerner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 51
                            }
                        ],
                        "text": "Program synthesis and superoptimization techniques [1, 2, 25, 27, 31, 33, 36] rely on an equivalence checker (verifier) for correctness."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14416979,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f4ae4c8aa94225a003c287bb873ea3e7481b312",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an automated technique for generating compiler optimizations from examples of concrete programs before and after improvements have been made to them. The key technical insight of our technique is that a proof of equivalence between the original and transformed concrete programs informs us which aspects of the programs are important and which can be discarded. Our technique therefore uses these proofs, which can be produced by translation validation or a proof-carrying compiler, as a guide to generalize the original and transformed programs into broadly applicable optimization rules.\n We present a category-theoretic formalization of our proof generalization technique. This abstraction makes our technique applicable to logics besides our own. In particular, we demonstrate how our technique can also be used to learn query optimizations for relational databases or to aid programmers in debugging type errors.\n Finally, we show experimentally that our technique enables programmers to train a compiler with application-specific optimizations by providing concrete examples of original programs and the desired transformed programs. We also show how it enables a compiler to learn efficient-to-run optimizations from expensive-to-run super-optimizers."
            },
            "slug": "Generating-compiler-optimizations-from-proofs-Tate-Stepp",
            "title": {
                "fragments": [],
                "text": "Generating compiler optimizations from proofs"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "An automated technique for generating compiler optimizations from examples of concrete programs before and after improvements have been made to them, which enables programmers to train a compiler with application-specific optimizations by providing concrete examples of original programs and the desired transformed programs."
            },
            "venue": {
                "fragments": [],
                "text": "POPL '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49392754"
                        ],
                        "name": "X. Leroy",
                        "slug": "X.-Leroy",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Leroy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Leroy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "Our work also overlaps with previous work on verified compilation [21, 42, 43], compiler testing tools [17, 18, 40], undefined behaviour detection [39], and domain specific languages for coding and verifying compiler optimizations [19,20]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 13281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3369e43abcb499eea4d208f2239df00551b8d2dd",
            "isKey": false,
            "numCitedBy": 737,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports on the development and formal certification (proof of semantic preservation) of a compiler from Cminor (a C-like imperative language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its correctness. Such a certified compiler is useful in the context of formal methods applied to the certification of critical software: the certification of the compiler guarantees that the safety properties proved on the source code hold for the executable compiled code as well."
            },
            "slug": "Formal-certification-of-a-compiler-back-end-or:-a-a-Leroy",
            "title": {
                "fragments": [],
                "text": "Formal certification of a compiler back-end or: programming a compiler with a proof assistant"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "This paper reports on the development and formal certification of a compiler from Cminor (a C-like imperative language) to PowerPC assembly code, using the Coq proof assistant both for programming the compiler and for proving its correctness."
            },
            "venue": {
                "fragments": [],
                "text": "POPL '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144509790"
                        ],
                        "name": "Nuno P. Lopes",
                        "slug": "Nuno-P.-Lopes",
                        "structuredName": {
                            "firstName": "Nuno",
                            "lastName": "Lopes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nuno P. Lopes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35243870"
                        ],
                        "name": "J. Monteiro",
                        "slug": "J.-Monteiro",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Monteiro",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Monteiro"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 2
                            }
                        ],
                        "text": ", [13,22,24,29,41,44]), and most have been evaluated on a variety of relatively smaller examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13898279,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9bcf92501af0b1b8222d60817b3dfbeba2a16298",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "Proving equivalence of programs has several important applications, including algorithm recognition, regression checking, compiler optimization verification and validation, and information flow checking. Despite being a topic with so many important applications, program equivalence checking has seen little advances over the past decades due to its inherent (high) complexity. In this paper, we propose, to the best of our knowledge, the first semi-algorithm for the automatic verification of partial equivalence of two programs over the combined theory of uninterpreted function symbols and integer arithmetic (UF+IA). The proposed algorithm supports, in particular, programs with nested loops. The crux of the technique is a transformation of uninterpreted functions (UFs) applications into integer polynomials, which enables the precise summarization of loops with UF applications using recurrences. The equivalence checking algorithm then proceeds on loop-free, integer only programs. We implemented the proposed technique in CORK, a tool that automatically verifies the correctness of compiler optimizations, and we show that it can prove more optimizations correct than state-of-the-art techniques."
            },
            "slug": "Automatic-equivalence-checking-of-programs-with-and-Lopes-Monteiro",
            "title": {
                "fragments": [],
                "text": "Automatic equivalence checking of programs with uninterpreted functions and integer arithmetic"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes the first semi-algorithm for the automatic verification of partial equivalence of two programs over the combined theory of uninterpreted function symbols and integer arithmetic (UF+IA), and shows that it can prove more optimizations correct than state-of-the-art techniques."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Software Tools for Technology Transfer"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108249416"
                        ],
                        "name": "Xi Wang",
                        "slug": "Xi-Wang",
                        "structuredName": {
                            "firstName": "Xi",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xi Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789973"
                        ],
                        "name": "N. Zeldovich",
                        "slug": "N.-Zeldovich",
                        "structuredName": {
                            "firstName": "Nickolai",
                            "lastName": "Zeldovich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Zeldovich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681493"
                        ],
                        "name": "M. Kaashoek",
                        "slug": "M.-Kaashoek",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Kaashoek",
                            "middleNames": [
                                "Frans"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kaashoek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389870240"
                        ],
                        "name": "Armando Solar-Lezama",
                        "slug": "Armando-Solar-Lezama",
                        "structuredName": {
                            "firstName": "Armando",
                            "lastName": "Solar-Lezama",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Armando Solar-Lezama"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13041092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1d5153ebbc240858bac11fc0102b8976a33bb84",
            "isKey": false,
            "numCitedBy": 118,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies an emerging class of software bugs called optimization-unstable code: code that is unexpectedly discarded by compiler optimizations due to undefined behavior in the program. Unstable code is present in many systems, including the Linux kernel and the Postgres database. The consequences of unstable code range from incorrect functionality to missing security checks. To reason about unstable code, this paper proposes a novel model, which views unstable code in terms of optimizations that leverage undefined behavior. Using this model, we introduce a new static checker called Stack that precisely identifies unstable code. Applying Stack to widely used systems has uncovered 160 new bugs that have been confirmed and fixed by developers."
            },
            "slug": "Towards-optimization-safe-systems:-analyzing-the-of-Wang-Zeldovich",
            "title": {
                "fragments": [],
                "text": "Towards optimization-safe systems: analyzing the impact of undefined behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel model is proposed, which views unstable code in terms of optimizations that leverage undefined behavior, and a new static checker called Stack is introduced that precisely identifies unstable code."
            },
            "venue": {
                "fragments": [],
                "text": "SOSP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6331083"
                        ],
                        "name": "R. Tate",
                        "slug": "R.-Tate",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Tate",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30073551"
                        ],
                        "name": "M. Stepp",
                        "slug": "M.-Stepp",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stepp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stepp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2272813"
                        ],
                        "name": "Zachary Tatlock",
                        "slug": "Zachary-Tatlock",
                        "structuredName": {
                            "firstName": "Zachary",
                            "lastName": "Tatlock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zachary Tatlock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145655689"
                        ],
                        "name": "Sorin Lerner",
                        "slug": "Sorin-Lerner",
                        "structuredName": {
                            "firstName": "Sorin",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorin Lerner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 210,
                                "start": 198
                            }
                        ],
                        "text": "This prior work has largely employed a pass-by-pass based approach [14,26], where each pass is verified separately by the equivalence checker, and/or worked with a set of handpicked transformations [14, 26, 37]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2138086,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f09d36476445c7f44d46555a753eae446cfed180",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer."
            },
            "slug": "Equality-saturation:-a-new-approach-to-optimization-Tate-Stepp",
            "title": {
                "fragments": [],
                "text": "Equality saturation: a new approach to optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed way of structuring optimizers has a variety of benefits over previous approaches: it obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than the authors' own."
            },
            "venue": {
                "fragments": [],
                "text": "POPL '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3095589"
                        ],
                        "name": "C. Hawblitzel",
                        "slug": "C.-Hawblitzel",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Hawblitzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hawblitzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145474353"
                        ],
                        "name": "Shuvendu K. Lahiri",
                        "slug": "Shuvendu-K.-Lahiri",
                        "structuredName": {
                            "firstName": "Shuvendu",
                            "lastName": "Lahiri",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuvendu K. Lahiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34974621"
                        ],
                        "name": "Kshama Pawar",
                        "slug": "Kshama-Pawar",
                        "structuredName": {
                            "firstName": "Kshama",
                            "lastName": "Pawar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kshama Pawar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068619995"
                        ],
                        "name": "Hammad Hashmi",
                        "slug": "Hammad-Hashmi",
                        "structuredName": {
                            "firstName": "Hammad",
                            "lastName": "Hashmi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hammad Hashmi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776563"
                        ],
                        "name": "Sedar Gokbulut",
                        "slug": "Sedar-Gokbulut",
                        "structuredName": {
                            "firstName": "Sedar",
                            "lastName": "Gokbulut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sedar Gokbulut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47635485"
                        ],
                        "name": "Lakshan Fernando",
                        "slug": "Lakshan-Fernando",
                        "structuredName": {
                            "firstName": "Lakshan",
                            "lastName": "Fernando",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lakshan Fernando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145339508"
                        ],
                        "name": "D. Detlefs",
                        "slug": "D.-Detlefs",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Detlefs",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Detlefs"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068713822"
                        ],
                        "name": "Scott Wadsworth",
                        "slug": "Scott-Wadsworth",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Wadsworth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Wadsworth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 8
                            }
                        ],
                        "text": "SymDiff [10, 15, 16] is an effort towards verifying compilers and regression verification, and works on assembly code."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8712721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e74f5ba5c7174e3ecf6ab2581a5e745bb69dd54",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a cross-version compiler validator and measures its effectiveness on the CLR JIT compiler. The validator checks for semantically equivalent assembly language output from various versions of the compiler, including versions across a seven-month time period, across two architectures (x86 and ARM), across two compilation scenarios (JIT and MDIL), and across optimizations levels. For month-to-month comparisons, the validator achieves a false alarm rate of just 2.2%. To help understand reported semantic differences, the validator performs a root-cause analysis on the counterexample traces generated by the underlying automated theorem proving tools. This root-cause analysis groups most of the counterexamples into a small number of buckets, reducing the number of counterexamples analyzed by hand by anywhere from 53% to 96%. The validator ran on over 500,000 methods across a large suite of test programs, finding 12 previously unknown correctness and performance bugs in the CLR compiler."
            },
            "slug": "Will-you-still-compile-me-tomorrow-static-compiler-Hawblitzel-Lahiri",
            "title": {
                "fragments": [],
                "text": "Will you still compile me tomorrow? static cross-version compiler validation"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The validator ran on over 500,000 methods across a large suite of test programs, finding 12 previously unknown correctness and performance bugs in the CLR compiler."
            },
            "venue": {
                "fragments": [],
                "text": "ESEC/FSE 2013"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698938"
                        ],
                        "name": "A. Pnueli",
                        "slug": "A.-Pnueli",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Pnueli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pnueli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119614910"
                        ],
                        "name": "M. Siegel",
                        "slug": "M.-Siegel",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Siegel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Siegel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774977"
                        ],
                        "name": "Eli Singerman",
                        "slug": "Eli-Singerman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Singerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eli Singerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 72
                            }
                        ],
                        "text": "It has been used extensively in previous work on translation validation [14, 26, 28, 32, 41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14822655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8b4164fef65ffc7082a3c95b0a706e5c3aa38f9",
            "isKey": false,
            "numCitedBy": 515,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the notion of translation validation as a new approach to the veriication of translators (compilers, code generators). Rather than proving in advance that the compiler always produces a target code which correctly implements the source code (compiler verii-cation), each individual translation (i.e. a run of the compiler) is followed by a validation phase which veriies that the target code produced on this run correctly implements the submitted source program. Several ingredients are necessary to set up the { fully automatic { translation validation process, among which are: 1. A common semantic framework for the representation of the source code and the generated target code. 2. A formalization of the notion of \"correct implementation\" as a re-nement relation. 3. A syntactic simulation-based proof method which allows to automatically verify that one model of the semantic framework, representing the produced target code, correctly implements another model which represents the source. These, and other ingredients are elaborated in this paper, in which we illustrate the new approach in a most challenging case. We consider a translation (compilation) from the synchronous multi-clock data-ow language Signal to asynchronous (sequential) C-code."
            },
            "slug": "Translation-Validation-Pnueli-Siegel",
            "title": {
                "fragments": [],
                "text": "Translation Validation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper considers a translation (compilation) from the synchronous multi-clock data-ow language Signal to asynchronous (sequential) C-code and presents the notion of translation validation as a new approach to the veriication of translators (compilers, code generators)."
            },
            "venue": {
                "fragments": [],
                "text": "TACAS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145474353"
                        ],
                        "name": "Shuvendu K. Lahiri",
                        "slug": "Shuvendu-K.-Lahiri",
                        "structuredName": {
                            "firstName": "Shuvendu",
                            "lastName": "Lahiri",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuvendu K. Lahiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3095589"
                        ],
                        "name": "C. Hawblitzel",
                        "slug": "C.-Hawblitzel",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Hawblitzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hawblitzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706936"
                        ],
                        "name": "Ming Kawaguchi",
                        "slug": "Ming-Kawaguchi",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Kawaguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Kawaguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751228"
                        ],
                        "name": "H. Reb\u00ealo",
                        "slug": "H.-Reb\u00ealo",
                        "structuredName": {
                            "firstName": "Henrique",
                            "lastName": "Reb\u00ealo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Reb\u00ealo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 8
                            }
                        ],
                        "text": "SymDiff [10, 15, 16] is an effort towards verifying compilers and regression verification, and works on assembly code."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2470613,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0764b9142c211ec094543c447612dfe79da2662",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe SymDiff, a language-agnostic tool for equivalence checking and displaying semantic (behavioral) differences over imperative programs. The tool operates on an intermediate verification language Boogie, for which translations exist from various source languages such as C, C# and x86. We discuss the tool and the front-end interface to target various source languages. Finally, we provide a brief description of the front-end for C programs."
            },
            "slug": "SYMDIFF:-A-Language-Agnostic-Semantic-Diff-Tool-for-Lahiri-Hawblitzel",
            "title": {
                "fragments": [],
                "text": "SYMDIFF: A Language-Agnostic Semantic Diff Tool for Imperative Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "SymDiff is described, a language-agnostic tool for equivalence checking and displaying semantic (behavioral) differences over imperative programs that operates on an intermediate verification language Boogie."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111333766"
                        ],
                        "name": "Rahul Sharma",
                        "slug": "Rahul-Sharma",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1945094"
                        ],
                        "name": "Eric Schkufza",
                        "slug": "Eric-Schkufza",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Schkufza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Schkufza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445900"
                        ],
                        "name": "Berkeley R. Churchill",
                        "slug": "Berkeley-R.-Churchill",
                        "structuredName": {
                            "firstName": "Berkeley",
                            "lastName": "Churchill",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berkeley R. Churchill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 51
                            }
                        ],
                        "text": "Program synthesis and superoptimization techniques [1, 2, 25, 27, 31, 33, 36] rely on an equivalence checker (verifier) for correctness."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6237583,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f877c1425280bb58fe5c17c8c201db93080ec375",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "The aggressive optimization of heavily used kernels is an important problem in high-performance computing. However, both general purpose compilers and highly specialized tools such as superoptimizers often do not have sufficient static knowledge of restrictions on program inputs that could be exploited to produce the very best code. For many applications, the best possible code is conditionally correct: the optimized kernel is equal to the code that it replaces only under certain preconditions on the kernel's inputs. The main technical challenge in producing conditionally correct optimizations is in obtaining non-trivial and useful conditions and proving conditional equivalence formally in the presence of loops. We combine abstract interpretation, decision procedures, and testing to yield a verification strategy that can address both of these problems. This approach yields a superoptimizer for x86 that in our experiments produces binaries that are often multiple times faster than those produced by production compilers."
            },
            "slug": "Conditionally-correct-superoptimization-Sharma-Schkufza",
            "title": {
                "fragments": [],
                "text": "Conditionally correct superoptimization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work combines abstract interpretation, decision procedures, and testing to yield a verification strategy that yields a superoptimizer for x86 that in the experiments produces binaries that are often multiple times faster than those produced by production compilers."
            },
            "venue": {
                "fragments": [],
                "text": "OOPSLA"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144692078"
                        ],
                        "name": "C. Flanagan",
                        "slug": "C.-Flanagan",
                        "structuredName": {
                            "firstName": "Cormac",
                            "lastName": "Flanagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Flanagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145189240"
                        ],
                        "name": "K. Leino",
                        "slug": "K.-Leino",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Leino",
                            "middleNames": [
                                "Rustan",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Leino"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 65
                            }
                        ],
                        "text": "This is similar to previous work on invariant inference (Houdini [8]), except that we are inferring these invariants/predicates on the JTFG, while previous work used this strategy for inferring invariants of an individual program."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Like Houdini [8], we guess several predicates generated through a grammar, and run a fixed point procedure to retain only the provable predicates."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1534849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02b72a79f17d7d86bb7b1d1e8ff8f659ca2bb1f0",
            "isKey": false,
            "numCitedBy": 418,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A static program checker that performs modular checking can check one program module for errors without needing to analyze the entire program. Modular checking requires that each module be accompanied by annotations that specify the module. To help reduce the cost of writing specifications, this paper presents Houdini, an annotation assistant for the modular checker ESC/Java. To infer suitable ESC/Java annotations for a given program, Houdini generates a large number of candidate annotations and uses ESC/Java to verify or refute each of these annotations. The paper describes the design, implementation, and preliminary evaluation of Houdini."
            },
            "slug": "Houdini,-an-Annotation-Assistant-for-ESC/Java-Flanagan-Leino",
            "title": {
                "fragments": [],
                "text": "Houdini, an Annotation Assistant for ESC/Java"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Houdini is presented, an annotation assistant for the modular checker ESC/Java, which generates a large number of candidate annotations and uses ESC/ Java to verify or refute each of these annotations."
            },
            "venue": {
                "fragments": [],
                "text": "FME"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9489218"
                        ],
                        "name": "Jianzhou Zhao",
                        "slug": "Jianzhou-Zhao",
                        "structuredName": {
                            "firstName": "Jianzhou",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jianzhou Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2375915"
                        ],
                        "name": "Santosh Nagarakatte",
                        "slug": "Santosh-Nagarakatte",
                        "structuredName": {
                            "firstName": "Santosh",
                            "lastName": "Nagarakatte",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Santosh Nagarakatte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110607721"
                        ],
                        "name": "Milo M. K. Martin",
                        "slug": "Milo-M.-K.-Martin",
                        "structuredName": {
                            "firstName": "Milo",
                            "lastName": "Martin",
                            "middleNames": [
                                "M.",
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Milo M. K. Martin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751798"
                        ],
                        "name": "S. Zdancewic",
                        "slug": "S.-Zdancewic",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Zdancewic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Zdancewic"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 66
                            }
                        ],
                        "text": "Our work also overlaps with previous work on verified compilation [21, 42, 43], compiler testing tools [17, 18, 40], undefined behaviour detection [39], and domain specific languages for coding and verifying compiler optimizations [19,20]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 5808635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c8f20da78ebc7891141c175fecb7a5c026f3e7d",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents Vellvm (verified LLVM), a framework for reasoning about programs expressed in LLVM's intermediate representation and transformations that operate on it. Vellvm provides a mechanized formal semantics of LLVM's intermediate representation, its type system, and properties of its SSA form. The framework is built using the Coq interactive theorem prover. It includes multiple operational semantics and proves relations among them to facilitate different reasoning styles and proof techniques.\n To validate Vellvm's design, we extract an interpreter from the Coq formal semantics that can execute programs from LLVM test suite and thus be compared against LLVM reference implementations. To demonstrate Vellvm's practicality, we formalize and verify a previously proposed transformation that hardens C programs against spatial memory safety violations. Vellvm's tools allow us to extract a new, verified implementation of the transformation pass that plugs into the real LLVM infrastructure; its performance is competitive with the non-verified, ad-hoc original."
            },
            "slug": "Formalizing-the-LLVM-intermediate-representation-Zhao-Nagarakatte",
            "title": {
                "fragments": [],
                "text": "Formalizing the LLVM intermediate representation for verified program transformations"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Vellvm provides a mechanized formal semantics of LLVM's intermediate representation, its type system, and properties of its SSA form, which includes multiple operational semantics and proves relations among them to facilitate different reasoning styles and proof techniques."
            },
            "venue": {
                "fragments": [],
                "text": "POPL '12"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747890"
                        ],
                        "name": "O. Strichman",
                        "slug": "O.-Strichman",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Strichman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Strichman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987608"
                        ],
                        "name": "Benny Godlin",
                        "slug": "Benny-Godlin",
                        "structuredName": {
                            "firstName": "Benny",
                            "lastName": "Godlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benny Godlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 125
                            }
                        ],
                        "text": "While our equivalence checker can correctly compute equivalence across all the examples presented in regression verification [7, 35], the converse is not true."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 41
                            }
                        ],
                        "text": "Previous work on regression verification [7,35] determines equivalence across structurally similar programs, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2863537,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0107f9683b7cbbdde7643fb391387d7e39e7622",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "When considering the program verification challenge [8] one should not forget a lesson learned in the testing community: when it comes to industrial size programs, it is not realistic to expect programmers to formally specify their program beyond simple assertions. It is well known that large parts of real code cannot be described naturally with high level invariants or temporal properties, and further that it is often the case that the process of describing what a code segment should do is as difficult and at least as complicated as the coding itself. Indeed, high-level temporal property-based testing, although by now supported by commercial tools such as Temporal-Rover [4], is in very limited use. The industry typically attempts to circumvent this problem with Regression Testing, which is probably the most popular testing method for general computer programs. It is based on the idea of reasoning by induction: check an initial version of the software when it is still very simple, and then check that a newer version of the software produces the same output as the earlier one, given the same inputs. If this process results with a counterexample, the user is asked to check whether it is an error or a legitimate change. In the latter case the testing database is updated with the new 'correct' output value.Regression Testing does not require a formal specification of the investigated system nor a deep understanding of the code, which makes it highly suitable for accompanying the development process, especially if it involves more than one programmer.We propose to learn from this experience and develop techniques for Regression Verification.The underlying proof engine is still a certifying compiler as envisioned by the grand challenge, so this proposal should be thought of as another application of this technology that makes the verification picture more complete."
            },
            "slug": "Regression-Verification-A-Practical-Way-to-Verify-Strichman-Godlin",
            "title": {
                "fragments": [],
                "text": "Regression Verification - A Practical Way to Verify Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The underlying proof engine is still a certifying compiler as envisioned by the grand challenge, so this proposal should be thought of as another application of this technology that makes the verification picture more complete."
            },
            "venue": {
                "fragments": [],
                "text": "VSTTE"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145083365"
                        ],
                        "name": "Sorav Bansal",
                        "slug": "Sorav-Bansal",
                        "structuredName": {
                            "firstName": "Sorav",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorav Bansal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 51
                            }
                        ],
                        "text": "Program synthesis and superoptimization techniques [1, 2, 25, 27, 31, 33, 36] rely on an equivalence checker (verifier) for correctness."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15857056,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09a890d0ec90d80207cd6414c719f065a929a07d",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new scheme for performing binary translation that produces code comparable to or better than existing binary translators with much less engineering effort. Instead of hand-coding the translation from one instruction set to another, our approach automatically learns translation rules using superoptimization techniques. We have implemented a PowerPC-x86 binary translator and report results on small and large computeintensive benchmarks. When compared to the native compiler, our translated code achieves median performance of 67% on large benchmarks and in some small stress tests actually outperforms the native compiler. We also report comparisons with the open source binary translator Qemu and a commercial tool, Apple's Rosetta. We consistently outperformthe former and are comparable to or faster than the latter on all but one benchmark."
            },
            "slug": "Binary-Translation-Using-Peephole-Superoptimizers-Bansal-Aiken",
            "title": {
                "fragments": [],
                "text": "Binary Translation Using Peephole Superoptimizers"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "A new scheme for performing binary translation that produces code comparable to or better than existing binary translators with much less engineering effort is presented."
            },
            "venue": {
                "fragments": [],
                "text": "OSDI"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445900"
                        ],
                        "name": "Berkeley R. Churchill",
                        "slug": "Berkeley-R.-Churchill",
                        "structuredName": {
                            "firstName": "Berkeley",
                            "lastName": "Churchill",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Berkeley R. Churchill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111333766"
                        ],
                        "name": "Rahul Sharma",
                        "slug": "Rahul-Sharma",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1666358563"
                        ],
                        "name": "J. Bastien",
                        "slug": "J.-Bastien",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Bastien",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bastien"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "Recent work on loop superoptimization for Google Native Client [3] extends DDEC by supporting inequality-based invariants; the evaluation however is limited to a small selection of test cases, and hence does not address several scalability and modeling issues that we tackle in our equivalence checker."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7188487,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics"
            ],
            "id": "fcefce81c8e810711e87170461ad6462d2f731fa",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "Software fault isolation (SFI) is an important technique for the construction of secure operating systems, web browsers, and other extensible software. We demonstrate that superoptimization can dramatically improve the performance of Google Native Client, a SFI system that ships inside the Google Chrome Browser. Key to our results are new techniques for superoptimization of loops: we propose a new architecture for superoptimization tools that incorporates both a fully sound verification technique to ensure correctness and a bounded verification technique to guide the search to optimized code. In our evaluation we optimize 13 libc string functions, formally verify the correctness of the optimizations and report a median and average speedup of 25% over the libraries shipped by Google."
            },
            "slug": "Sound-Loop-Superoptimization-for-Google-Native-Churchill-Sharma",
            "title": {
                "fragments": [],
                "text": "Sound Loop Superoptimization for Google Native Client"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work demonstrates that superoptimization can dramatically improve the performance of Google Native Client, a SFI system that ships inside the Google Chrome Browser and proposes a new architecture for super Optimization tools that incorporates both a fully sound verification technique and a bounded verification technique to guide the search to optimized code."
            },
            "venue": {
                "fragments": [],
                "text": "ASPLOS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143914877"
                        ],
                        "name": "Vu Le",
                        "slug": "Vu-Le",
                        "structuredName": {
                            "firstName": "Vu",
                            "lastName": "Le",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vu Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34751114"
                        ],
                        "name": "Chengnian Sun",
                        "slug": "Chengnian-Sun",
                        "structuredName": {
                            "firstName": "Chengnian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengnian Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38319925"
                        ],
                        "name": "Z. Su",
                        "slug": "Z.-Su",
                        "structuredName": {
                            "firstName": "Zhendong",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Su"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 103
                            }
                        ],
                        "text": "Our work also overlaps with previous work on verified compilation [21, 42, 43], compiler testing tools [17, 18, 40], undefined behaviour detection [39], and domain specific languages for coding and verifying compiler optimizations [19,20]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 15005797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02838cb6982e67992ae54fa616162b16ce5110c6",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Compiler testing is important and challenging. Equivalence Modulo Inputs (EMI) is a recent promising approach for compiler validation. It is based on mutating the unexecuted statements of an existing program under some inputs to produce new equivalent test programs w.r.t. these inputs. Orion is a simple realization of EMI by only randomly deleting unexecuted statements. Despite its success in finding many bugs in production compilers, Orion\u2019s effectiveness is still limited by its simple, blind mutation strategy. To more effectively realize EMI, this paper introduces a guided, advanced mutation strategy based on Bayesian optimization. Our goal is to generate diverse programs to more thoroughly exercise compilers. We achieve this with two techniques: (1) the support of both code deletions and insertions in the unexecuted regions, leading to a much larger test program space; and (2) the use of an objective function that promotes control-flow-diverse programs for guiding Markov Chain Monte Carlo (MCMC) optimization to explore the search space. Our technique helps discover deep bugs that require elaborate mutations. Our realization, Athena, targets C compilers. In 19 months, Athena has found 72 new bugs \u2014 many of which are deep and important bugs \u2014 in GCC and LLVM. Developers have confirmed all 72 bugs and fixed 68 of them."
            },
            "slug": "Finding-deep-compiler-bugs-via-guided-stochastic-Le-Sun",
            "title": {
                "fragments": [],
                "text": "Finding deep compiler bugs via guided stochastic program mutation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "At Athena, a guided, advanced mutation strategy based on Bayesian optimization, is introduced to generate diverse programs to more thoroughly exercise compilers and helps discover deep bugs that require elaborate mutations."
            },
            "venue": {
                "fragments": [],
                "text": "OOPSLA 2015"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30073551"
                        ],
                        "name": "M. Stepp",
                        "slug": "M.-Stepp",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stepp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stepp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6331083"
                        ],
                        "name": "R. Tate",
                        "slug": "R.-Tate",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Tate",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145655689"
                        ],
                        "name": "Sorin Lerner",
                        "slug": "Sorin-Lerner",
                        "structuredName": {
                            "firstName": "Sorin",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorin Lerner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 101
                            }
                        ],
                        "text": "Value-graph translation validation for LLVM has been performed previously in two independent efforts [34, 38]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[34] support all these transformations, and additionally enable partial-redundancy elimination, constant propagation, and basic block placement."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 229,
                                "start": 222
                            }
                        ],
                        "text": "Translation validation for mature compilers on large and complex programs, has been reported in at least two previous works: Translation validation infrastructure (TVI) [26] for GCC, and Value-graph translation validation [34,38] for LLVM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13218010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56202da53b5aa2e770cafd50a44741179039b8db",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "We updated our Peggy tool, previously presented in [6], to perform translation validation for the LLVM compiler using a technique called Equality Saturation. We present the tool, and illustrate its effectiveness at doing translation validation on SPEC 2006 benchmarks."
            },
            "slug": "Equality-Based-Translation-Validator-for-LLVM-Stepp-Tate",
            "title": {
                "fragments": [],
                "text": "Equality-Based Translation Validator for LLVM"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The Peggy tool is updated, to perform translation validation for the LLVM compiler using a technique called Equality Saturation, and its effectiveness at doing translation validation on SPEC 2006 benchmarks is illustrated."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144996001"
                        ],
                        "name": "L. D. Moura",
                        "slug": "L.-D.-Moura",
                        "structuredName": {
                            "firstName": "Leonardo",
                            "lastName": "Moura",
                            "middleNames": [
                                "Mendon\u00e7a",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Moura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3714351"
                        ],
                        "name": "N. Bj\u00f8rner",
                        "slug": "N.-Bj\u00f8rner",
                        "structuredName": {
                            "firstName": "Nikolaj",
                            "lastName": "Bj\u00f8rner",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Bj\u00f8rner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15912959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3960dda299e0f8615a7db675b8e6905b375ecf8a",
            "isKey": false,
            "numCitedBy": 6280,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications."
            },
            "slug": "Z3:-An-Efficient-SMT-Solver-Moura-Bj\u00f8rner",
            "title": {
                "fragments": [],
                "text": "Z3: An Efficient SMT Solver"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Z3 is a new and efficient SMT Solver freely available from Microsoft Research that is used in various software verification and analysis applications."
            },
            "venue": {
                "fragments": [],
                "text": "TACAS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145083365"
                        ],
                        "name": "Sorav Bansal",
                        "slug": "Sorav-Bansal",
                        "structuredName": {
                            "firstName": "Sorav",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorav Bansal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 51
                            }
                        ],
                        "text": "Program synthesis and superoptimization techniques [1, 2, 25, 27, 31, 33, 36] rely on an equivalence checker (verifier) for correctness."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 990671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25a77652204ae3e524a1ca25cca7a44c72d37d6d",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Peephole optimizers are typically constructed using human-written pattern matching rules, an approach that requires expertise and time, as well as being less than systematic at exploiting all opportunities for optimization. We explore fully automatic construction of peephole optimizers using brute force superoptimization. While the optimizations discovered by our automatic system may be less general than human-written counterparts, our approach has the potential to automatically learn a database of thousands to millions of optimizations, in contrast to the hundreds found in current peephole optimizers. We show experimentally that our optimizer is able to exploit performance opportunities not found by existing compilers; in particular, we show speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler."
            },
            "slug": "Automatic-generation-of-peephole-superoptimizers-Bansal-Aiken",
            "title": {
                "fragments": [],
                "text": "Automatic generation of peephole superoptimizers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown experimentally that the fully automatic construction of peephole optimizers using brute force superoptimization is able to exploit performance opportunities not found by existing compilers, and speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler are shown."
            },
            "venue": {
                "fragments": [],
                "text": "ASPLOS XII"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1945094"
                        ],
                        "name": "Eric Schkufza",
                        "slug": "Eric-Schkufza",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Schkufza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Schkufza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111333766"
                        ],
                        "name": "Rahul Sharma",
                        "slug": "Rahul-Sharma",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 51
                            }
                        ],
                        "text": "Program synthesis and superoptimization techniques [1, 2, 25, 27, 31, 33, 36] rely on an equivalence checker (verifier) for correctness."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 683646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "308388616c12158423fbf8bd8c441d11d1f432a2",
            "isKey": false,
            "numCitedBy": 256,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate the loop-free binary superoptimization task as a stochastic search problem. The competing constraints of transformation correctness and performance improvement are encoded as terms in a cost function, and a Markov Chain Monte Carlo sampler is used to rapidly explore the space of all possible programs to find one that is an optimization of a given target program. Although our method sacrifices completeness, the scope of programs we are able to consider, and the resulting quality of the programs that we produce, far exceed those of existing superoptimizers. Beginning from binaries compiled by llvm -O0 for 64-bit x86, our prototype implementation, STOKE, is able to produce programs which either match or outperform the code produced by gcc -O3, icc -O3, and in some cases, expert handwritten assembly."
            },
            "slug": "Stochastic-superoptimization-Schkufza-Sharma",
            "title": {
                "fragments": [],
                "text": "Stochastic superoptimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work forms the loop-free binary superoptimization task as a stochastic search problem, and a Markov Chain Monte Carlo sampler is used to rapidly explore the space of all possible programs to find one that is an optimization of a given target program."
            },
            "venue": {
                "fragments": [],
                "text": "ASPLOS '13"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2773679"
                        ],
                        "name": "B. Dutertre",
                        "slug": "B.-Dutertre",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Dutertre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Dutertre"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 195352208,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c854a5fc3d13549d31006033902948902b058665",
            "isKey": false,
            "numCitedBy": 345,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Yices is an SMT solver developed by SRI International. The first version of Yices was released in 2006 and has been continuously updated since then. In 2007, we started a complete re-implementation of the solver to improve performance and increase modularity and flexibility. We describe the latest release of Yices, namely, Yices 2.2. We present the tool's architecture and discuss the algorithms it implements, and we describe recent developments such as support for the SMT-LIBa2.0 notation and various performance improvements."
            },
            "slug": "Yices-2.2-Dutertre",
            "title": {
                "fragments": [],
                "text": "Yices 2.2"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The latest release of Yices is described, namely, Yices 2.2.0, which presents the tool's architecture and discusses the algorithms it implements, and describes recent developments such as support for the SMT-LIBa 2.0 notation and various performance improvements."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145459273"
                        ],
                        "name": "E. Emerson",
                        "slug": "E.-Emerson",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Emerson",
                            "middleNames": [
                                "Allen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Emerson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794041"
                        ],
                        "name": "A. P. Sistla",
                        "slug": "A.-P.-Sistla",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Sistla",
                            "middleNames": [
                                "Prasad"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. P. Sistla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 24
                            }
                        ],
                        "text": "We use Z3 [5] and Yices [6] SMT solvers running in parallel for discharging proof obligations over our custom-simplified expressions, and use the result of solver which finishes first."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33340875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b620de3893fe3484216fdc5ed1e4d1b39c7669fb",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-Aided-Verification-Emerson-Sistla",
            "title": {
                "fragments": [],
                "text": "Computer Aided Verification"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 2000
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 15,
            "methodology": 20,
            "result": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 39,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Black-Box-Equivalence-Checking-Across-Compiler-Dahiya-Bansal/5da199a2b340da3bec5aced418d7e52ccabda182?sort=total-citations"
}