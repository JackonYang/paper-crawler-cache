{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3104897"
                        ],
                        "name": "Markus H\u00f6hfeld",
                        "slug": "Markus-H\u00f6hfeld",
                        "structuredName": {
                            "firstName": "Markus",
                            "lastName": "H\u00f6hfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Markus H\u00f6hfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13282575,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac904eea95ee29921401cf383fc2e1ae01523b9b",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Probabilistic-rounding-in-neural-network-learning-H\u00f6hfeld-Fahlman",
            "title": {
                "fragments": [],
                "text": "Probabilistic rounding in neural network learning with limited precision"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388466"
                        ],
                        "name": "Matthieu Courbariaux",
                        "slug": "Matthieu-Courbariaux",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Courbariaux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Courbariaux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145719986"
                        ],
                        "name": "J. David",
                        "slug": "J.-David",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. David"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 85
                            }
                        ],
                        "text": "Secondly, modern\n5While preparing this paper, we became aware of a very recent work (Courbariaux et al., 2014) that shares our motivations but adopts an orthogonal approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16349374,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b82d54e9a3b06c603d7987ba3ecf437425f6330",
            "isKey": false,
            "numCitedBy": 469,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Multipliers are the most space and power-hungry arithmetic operators of the digital implementation of deep neural networks. We train a set of state-of-the-art neural networks (Maxout networks) on three benchmark datasets: MNIST, CIFAR-10 and SVHN. They are trained with three distinct formats: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those formats, we assess the impact of the precision of the multiplications on the final error after training. We find that very low precision is sufficient not just for running trained networks but also for training them. For example, it is possible to train Maxout networks with 10 bits multiplications."
            },
            "slug": "Training-deep-neural-networks-with-low-precision-Courbariaux-Bengio",
            "title": {
                "fragments": [],
                "text": "Training deep neural networks with low precision multiplications"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is found that very low precision is sufficient not just for running trained networks but also for training them, and it is possible to train Maxout networks with 10 bits multiplications."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388466"
                        ],
                        "name": "Matthieu Courbariaux",
                        "slug": "Matthieu-Courbariaux",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Courbariaux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Courbariaux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145719986"
                        ],
                        "name": "J. David",
                        "slug": "J.-David",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. David"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 66
                            }
                        ],
                        "text": "While preparing this paper, we became aware of a very recent work (Courbariaux et al., 2014) that shares our motivations but adopts an orthogonal approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 85
                            }
                        ],
                        "text": "Secondly, modern\n5While preparing this paper, we became aware of a very recent work (Courbariaux et al., 2014) that shares our motivations but adopts an orthogonal approach."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26450018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8c35c2c39fdd2ec6af37ddc8c51deb396aefef8",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those arithmetics, we assess the impact of the precision of the computations on the final error of the training. We find that very low precision computation is sufficient not just for running trained networks but also for training them. For example, almost state-of-the-art results were obtained on most datasets with 10 bits for computing activations and gradients, and 12 bits for storing updated parameters."
            },
            "slug": "Low-precision-arithmetic-for-deep-learning-Courbariaux-Bengio",
            "title": {
                "fragments": [],
                "text": "Low precision arithmetic for deep learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that very low precision computation is sufficient not just for running trained networks but also for training them."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715548"
                        ],
                        "name": "Mark Z. Mao",
                        "slug": "Mark-Z.-Mao",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Mao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Z. Mao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 147
                            }
                        ],
                        "text": "Some recent studies that embrace this approach have relied on the processor\u2019s vector instructions to perform multiple 8 bit operations in parallel (Vanhoucke et al., 2011), or employ reconfigurable hardware (FPGAs) for high-throughput, energy-efficient inference (Farabet et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 147
                            }
                        ],
                        "text": "Some recent studies that embrace this approach have relied on the processor\u2019s vector instructions to perform multiple 8 bit operations in parallel (Vanhoucke et al., 2011), or employ reconfigurable hardware (FPGAs) for highthroughput, energy-efficient inference (Farabet et al., 2011; Gokhale et\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15196840,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbeaa499e10e98515f7e1c4ad89165e8c0677427",
            "isKey": false,
            "numCitedBy": 692,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in deep learning have made the use of large, deep neural networks with tens of millions of parameters suitable for a number of applications that require real-time processing. The sheer size of these networks can represent a challenging computational burden, even for modern CPUs. For this reason, GPUs are routinely used instead to train and run such networks. This paper is a tutorial for students and researchers on some of the techniques that can be used to reduce this computational cost considerably on modern x86 CPUs. We emphasize data layout, batching of the computation, the use of SSE2 instructions, and particularly leverage SSSE3 and SSE4 fixed-point instructions which provide a 3\u00d7 improvement over an optimized floating-point baseline. We use speech recognition as an example task, and show that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10\u00d7 speedup over an unoptimized baseline and a 4\u00d7 speedup over an aggressively optimized floating-point baseline at no cost in accuracy. The techniques described extend readily to neural network training and provide an effective alternative to the use of specialized hardware."
            },
            "slug": "Improving-the-speed-of-neural-networks-on-CPUs-Vanhoucke-Senior",
            "title": {
                "fragments": [],
                "text": "Improving the speed of neural networks on CPUs"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper uses speech recognition as an example task, and shows that a real-time hybrid hidden Markov model / neural network (HMM/NN) large vocabulary system can be built with a 10\u00d7 speedup over an unoptimized baseline and a 4\u00d7 speed up over an aggressively optimized floating-point baseline at no cost in accuracy."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2588465"
                        ],
                        "name": "J. L. Holt",
                        "slug": "J.-L.-Holt",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Holt",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. L. Holt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145159381"
                        ],
                        "name": "Jenq-Neng Hwang",
                        "slug": "Jenq-Neng-Hwang",
                        "structuredName": {
                            "firstName": "Jenq-Neng",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jenq-Neng Hwang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1353891,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b119dc8a4e38824d73a9f88179935a96e001aaf",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Through parallel processing, low precision fixed point hardware can be used to build a very high speed neural network computing engine where the low precision results in a drastic reduction in system cost. The reduced silicon area required to implement a single processing unit is taken advantage of by implementing multiple processing units on a single piece of silicon and operating them in parallel. The important question which arises is how much precision is required to implement neural network algorithms on this low precision hardware. A theoretical analysis of error due to finite precision computation was undertaken to determine the necessary precision for successful forward retrieving and back-propagation learning in a multilayer perceptron. This analysis can easily be further extended to provide a general finite precision analysis technique by which most neural network algorithms under any set of hardware constraints may be evaluated. >"
            },
            "slug": "Finite-Precision-Error-Analysis-of-Neural-Network-Holt-Hwang",
            "title": {
                "fragments": [],
                "text": "Finite Precision Error Analysis of Neural Network Hardware Implementations"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A theoretical analysis of error due to finite precision computation was undertaken to determine the necessary precision for successful forward retrieving and back-propagation learning in a multilayer perceptron."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Computers"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116011472"
                        ],
                        "name": "Suyog Gupta",
                        "slug": "Suyog-Gupta",
                        "structuredName": {
                            "firstName": "Suyog",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suyog Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808676"
                        ],
                        "name": "V. Sindhwani",
                        "slug": "V.-Sindhwani",
                        "structuredName": {
                            "firstName": "Vikas",
                            "lastName": "Sindhwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sindhwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33678523"
                        ],
                        "name": "K. Gopalakrishnan",
                        "slug": "K.-Gopalakrishnan",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Gopalakrishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Gopalakrishnan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12472404,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0631a000233bc04da1f69f94b17e959f92d60994",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper highlights new opportunities for designing large-scale machine learning systems as a consequence of blurring traditional boundaries that have allowed algorithm designers and application-level practitioners to stay -- for the most part -- oblivious to the details of the underlying hardware-level implementations. The hardware/software co-design methodology advocated here hinges on the deployment of compute-intensive machine learning kernels onto compute platforms that trade-off determinism in the computation for improvement in speed and/or energy efficiency. To achieve this, we revisit digital stochastic circuits for approximating matrix computations that are ubiquitous in machine learning algorithms. Theoretical and empirical evaluation is undertaken to assess the impact of the hardware-induced computational noise on algorithm performance. As a proof-of-concept, a stochastic hardware simulator is employed for training deep neural networks for image recognition problems."
            },
            "slug": "Learning-Machines-Implemented-on-Non-Deterministic-Gupta-Sindhwani",
            "title": {
                "fragments": [],
                "text": "Learning Machines Implemented on Non-Deterministic Hardware"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The hardware/software co-design methodology advocated here hinges on the deployment of compute-intensive machine learning kernels onto compute platforms that trade-off determinism in the computation for improvement in speed and/or energy efficiency."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144638694"
                        ],
                        "name": "Adam Coates",
                        "slug": "Adam-Coates",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Coates",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Coates"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2570381"
                        ],
                        "name": "Brody Huval",
                        "slug": "Brody-Huval",
                        "structuredName": {
                            "firstName": "Brody",
                            "lastName": "Huval",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brody Huval"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2156632012"
                        ],
                        "name": "Tao Wang",
                        "slug": "Tao-Wang",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25629078"
                        ],
                        "name": "David J. Wu",
                        "slug": "David-J.-Wu",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2301680"
                        ],
                        "name": "Bryan Catanzaro",
                        "slug": "Bryan-Catanzaro",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Catanzaro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan Catanzaro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 62
                            }
                        ],
                        "text": "or a combination of CPUs and GPUs scaled-up to multiple nodes (Coates et al., 2013; Wu et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 279,
                                "start": 260
                            }
                        ],
                        "text": "\u2026direction include distributed computing infrastructure using thousands of CPU cores (Dean et al., 2012; Chilimbi et al., 2014), or high-end graphics processors (GPUs) (Krizhevsky & Hinton, 2009), or a combination of CPUs and GPUs scaled-up to multiple nodes (Coates et al., 2013; Wu et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8604637,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d1208ac421cf8ff67b27d93cd19ae42b8d596f95",
            "isKey": false,
            "numCitedBy": 679,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Scaling up deep learning algorithms has been shown to lead to increased performance in benchmark tasks and to enable discovery of complex high-level features. Recent efforts to train extremely large networks (with over 1 billion parameters) have relied on cloudlike computing infrastructure and thousands of CPU cores. In this paper, we present technical details and results from our own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI. Our system is able to train 1 billion parameter networks on just 3 machines in a couple of days, and we show that it can scale to networks with over 11 billion parameters using just 16 machines. As this infrastructure is much more easily marshaled by others, the approach enables much wider-spread research with extremely large neural networks."
            },
            "slug": "Deep-learning-with-COTS-HPC-systems-Coates-Huval",
            "title": {
                "fragments": [],
                "text": "Deep learning with COTS HPC systems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents technical details and results from their own system based on Commodity Off-The-Shelf High Performance Computing (COTS HPC) technology: a cluster of GPU servers with Infiniband interconnects and MPI, and shows that it can scale to networks with over 11 billion parameters using just 16 machines."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7377735"
                        ],
                        "name": "Yunji Chen",
                        "slug": "Yunji-Chen",
                        "structuredName": {
                            "firstName": "Yunji",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunji Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068286576"
                        ],
                        "name": "Tao Luo",
                        "slug": "Tao-Luo",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Luo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Luo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39419985"
                        ],
                        "name": "Shaoli Liu",
                        "slug": "Shaoli-Liu",
                        "structuredName": {
                            "firstName": "Shaoli",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoli Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145407329"
                        ],
                        "name": "Shijin Zhang",
                        "slug": "Shijin-Zhang",
                        "structuredName": {
                            "firstName": "Shijin",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijin Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37167270"
                        ],
                        "name": "Liqiang He",
                        "slug": "Liqiang-He",
                        "structuredName": {
                            "firstName": "Liqiang",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liqiang He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110368816"
                        ],
                        "name": "Jia Wang",
                        "slug": "Jia-Wang",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3353457"
                        ],
                        "name": "Ling Li",
                        "slug": "Ling-Li",
                        "structuredName": {
                            "firstName": "Ling",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ling Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144049725"
                        ],
                        "name": "Tianshi Chen",
                        "slug": "Tianshi-Chen",
                        "structuredName": {
                            "firstName": "Tianshi",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tianshi Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719934"
                        ],
                        "name": "Zhiwei Xu",
                        "slug": "Zhiwei-Xu",
                        "structuredName": {
                            "firstName": "Zhiwei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiwei Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145550877"
                        ],
                        "name": "Ninghui Sun",
                        "slug": "Ninghui-Sun",
                        "structuredName": {
                            "firstName": "Ninghui",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ninghui Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731764"
                        ],
                        "name": "O. Temam",
                        "slug": "O.-Temam",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Temam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Temam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 15
                            }
                        ],
                        "text": "A recent work (Chen et al., 2014) presents a hardware accelerator for deep neural network training that employs fixed-point computation units, but finds it necessary to use 32-bit fixed-point representation to achieve convergence while training a convolutional neural network on the MNIST dataset."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6838992,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4157ed3db4c656854e69931cb6089b64b08784b9",
            "isKey": false,
            "numCitedBy": 1065,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "Many companies are deploying services, either for consumers or industry, which are largely based on machine-learning algorithms for sophisticated processing of large amounts of data. The state-of-the-art and most popular such machine-learning algorithms are Convolutional and Deep Neural Networks (CNNs and DNNs), which are known to be both computationally and memory intensive. A number of neural network accelerators have been recently proposed which can offer high computational capacity/area ratio, but which remain hampered by memory accesses. However, unlike the memory wall faced by processors on general-purpose workloads, the CNNs and DNNs memory footprint, while large, is not beyond the capability of the on chip storage of a multi-chip system. This property, combined with the CNN/DNN algorithmic characteristics, can lead to high internal bandwidth and low external communications, which can in turn enable high-degree parallelism at a reasonable area cost. In this article, we introduce a custom multi-chip machine-learning architecture along those lines. We show that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system. We implement the node down to the place and route at 28nm, containing a combination of custom storage and computational units, with industry-grade interconnects."
            },
            "slug": "DaDianNao:-A-Machine-Learning-Supercomputer-Chen-Luo",
            "title": {
                "fragments": [],
                "text": "DaDianNao: A Machine-Learning Supercomputer"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article introduces a custom multi-chip machine-learning architecture, showing that, on a subset of the largest known neural network layers, it is possible to achieve a speedup of 450.65x over a GPU, and reduce the energy by 150.31x on average for a 64-chip system."
            },
            "venue": {
                "fragments": [],
                "text": "2014 47th Annual IEEE/ACM International Symposium on Microarchitecture"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089272"
                        ],
                        "name": "R. Monga",
                        "slug": "R.-Monga",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Monga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Monga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118440152"
                        ],
                        "name": "Kai Chen",
                        "slug": "Kai-Chen",
                        "structuredName": {
                            "firstName": "Kai",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kai Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145139947"
                        ],
                        "name": "Matthieu Devin",
                        "slug": "Matthieu-Devin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Devin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Devin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715548"
                        ],
                        "name": "Mark Z. Mao",
                        "slug": "Mark-Z.-Mao",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Mao",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mark Z. Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33666044"
                        ],
                        "name": "A. Senior",
                        "slug": "A.-Senior",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Senior",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Senior"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080690"
                        ],
                        "name": "P. Tucker",
                        "slug": "P.-Tucker",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Tucker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143781496"
                        ],
                        "name": "Ke Yang",
                        "slug": "Ke-Yang",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34699434"
                        ],
                        "name": "A. Ng",
                        "slug": "A.-Ng",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 113
                            }
                        ],
                        "text": "Some notable efforts in this direction include distributed computing infrastructure using thousands of CPU cores (Dean et al., 2012; Chilimbi et al., 2014), or high-end graphics processors (GPUs) (Ciresan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 114
                            }
                        ],
                        "text": "Some notable efforts in this direction include distributed computing infrastructure using thousands of CPU cores (Dean et al., 2012; Chilimbi et al., 2014), or high-end graphics processors (GPUs) (Krizhevsky & Hinton, 2009), or a combination of CPUs and GPUs scaled-up to multiple nodes (Coates et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 372467,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3127190433230b3dc1abd0680bb58dced4bcd90e",
            "isKey": false,
            "numCitedBy": 3057,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work in unsupervised feature learning and deep learning has shown that being able to train large models can dramatically improve performance. In this paper, we consider the problem of training a deep network with billions of parameters using tens of thousands of CPU cores. We have developed a software framework called DistBelief that can utilize computing clusters with thousands of machines to train large models. Within this framework, we have developed two algorithms for large-scale distributed training: (i) Downpour SGD, an asynchronous stochastic gradient descent procedure supporting a large number of model replicas, and (ii) Sandblaster, a framework that supports a variety of distributed batch optimization procedures, including a distributed implementation of L-BFGS. Downpour SGD and Sandblaster L-BFGS both increase the scale and speed of deep network training. We have successfully used our system to train a deep network 30x larger than previously reported in the literature, and achieves state-of-the-art performance on ImageNet, a visual object recognition task with 16 million images and 21k categories. We show that these same techniques dramatically accelerate the training of a more modestly- sized deep network for a commercial speech recognition service. Although we focus on and report performance of these methods as applied to training large neural networks, the underlying algorithms are applicable to any gradient-based machine learning algorithm."
            },
            "slug": "Large-Scale-Distributed-Deep-Networks-Dean-Corrado",
            "title": {
                "fragments": [],
                "text": "Large Scale Distributed Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper considers the problem of training a deep network with billions of parameters using tens of thousands of CPU cores and develops two algorithms for large-scale distributed training, Downpour SGD and Sandblaster L-BFGS, which increase the scale and speed of deep network training."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6647160"
                        ],
                        "name": "Vinayak Gokhale",
                        "slug": "Vinayak-Gokhale",
                        "structuredName": {
                            "firstName": "Vinayak",
                            "lastName": "Gokhale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vinayak Gokhale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2592429"
                        ],
                        "name": "Jonghoon Jin",
                        "slug": "Jonghoon-Jin",
                        "structuredName": {
                            "firstName": "Jonghoon",
                            "lastName": "Jin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonghoon Jin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2130620"
                        ],
                        "name": "A. Dundar",
                        "slug": "A.-Dundar",
                        "structuredName": {
                            "firstName": "Aysegul",
                            "lastName": "Dundar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Dundar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940285"
                        ],
                        "name": "B. Martini",
                        "slug": "B.-Martini",
                        "structuredName": {
                            "firstName": "Berin",
                            "lastName": "Martini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Martini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889774"
                        ],
                        "name": "E. Culurciello",
                        "slug": "E.-Culurciello",
                        "structuredName": {
                            "firstName": "Eugenio",
                            "lastName": "Culurciello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Culurciello"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 74
                            }
                        ],
                        "text": "This translates to a throughput of 260 Gops/s at a power efficiency of 37 G-ops/s/W."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 176
                            }
                        ],
                        "text": "\u2026multiple 8 bit operations in parallel (Vanhoucke et al., 2011), or employ reconfigurable hardware (FPGAs) for highthroughput, energy-efficient inference (Farabet et al., 2011; Gokhale et al., 2014), or take the route of custom hardware implementations (Kim et al., 2014; Merolla et al., 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 99
                            }
                        ],
                        "text": ", 2011), or employ reconfigurable hardware (FPGAs) for high-throughput, energy-efficient inference (Farabet et al., 2011; Gokhale et al., 2014), or take the route of custom hardware implementations (Kim et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 170
                            }
                        ],
                        "text": "This compares very favorably against the Intel i7-3720QM CPU, the NVIDIA GT650m and the GTX780 GPUs, all of which achieve power efficiency in the range of 1-5 G-ops/s/W (Gokhale et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 7311716,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "233b1774f28c9972df2dfcf20dfbb0df45792bd0",
            "isKey": true,
            "numCitedBy": 253,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep networks are state-of-the-art models used for understanding the content of images, videos, audio and raw input data. Current computing systems are not able to run deep network models in real-time with low power consumption. In this paper we present nn-X: a scalable, low-power coprocessor for enabling real-time execution of deep neural networks. nn-X is implemented on programmable logic devices and comprises an array of configurable processing elements called collections. These collections perform the most common operations in deep networks: convolution, subsampling and non-linear functions. The nn-X system includes 4 high-speed direct memory access interfaces to DDR3 memory and two ARM Cortex-A9 processors. Each port is capable of a sustained throughput of 950 MB/s in full duplex. nn-X is able to achieve a peak performance of 227 G-ops/s, a measured performance in deep learning applications of up to 200 G-ops/s while consuming less than 4 watts of power. This translates to a performance per power improvement of 10 to 100 times that of conventional mobile and desktop processors."
            },
            "slug": "A-240-G-ops/s-Mobile-Coprocessor-for-Deep-Neural-Gokhale-Jin",
            "title": {
                "fragments": [],
                "text": "A 240 G-ops/s Mobile Coprocessor for Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The nn-X system is presented, a scalable, low-power coprocessor for enabling real-time execution of deep neural networks, able to achieve a peak performance of 227 G-ops/s, which translates to a performance per power improvement of 10 to 100 times that of conventional mobile and desktop processors."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE Conference on Computer Vision and Pattern Recognition Workshops"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3191220"
                        ],
                        "name": "Trishul M. Chilimbi",
                        "slug": "Trishul-M.-Chilimbi",
                        "structuredName": {
                            "firstName": "Trishul",
                            "lastName": "Chilimbi",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trishul M. Chilimbi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3312079"
                        ],
                        "name": "Yutaka Suzue",
                        "slug": "Yutaka-Suzue",
                        "structuredName": {
                            "firstName": "Yutaka",
                            "lastName": "Suzue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yutaka Suzue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2300600"
                        ],
                        "name": "Johnson Apacible",
                        "slug": "Johnson-Apacible",
                        "structuredName": {
                            "firstName": "Johnson",
                            "lastName": "Apacible",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Johnson Apacible"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40444389"
                        ],
                        "name": "Karthik Kalyanaraman",
                        "slug": "Karthik-Kalyanaraman",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Kalyanaraman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karthik Kalyanaraman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 113
                            }
                        ],
                        "text": "Some notable efforts in this direction include distributed computing infrastructure using thousands of CPU cores (Dean et al., 2012; Chilimbi et al., 2014), or high-end graphics processors (GPUs) (Ciresan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 133
                            }
                        ],
                        "text": "Some notable efforts in this direction include distributed computing infrastructure using thousands of CPU cores (Dean et al., 2012; Chilimbi et al., 2014), or high-end graphics processors (GPUs) (Krizhevsky & Hinton, 2009), or a combination of CPUs and GPUs scaled-up to multiple nodes (Coates et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2185117,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e69c8b5df8a4178b1c8c7f154a761147a6f030be",
            "isKey": false,
            "numCitedBy": 670,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Large deep neural network models have recently demonstrated state-of-the-art accuracy on hard visual recognition tasks. Unfortunately such models are extremely time consuming to train and require large amount of compute cycles. We describe the design and implementation of a distributed system called Adam comprised of commodity server machines to train such models that exhibits world-class performance, scaling and task accuracy on visual recognition tasks. Adam achieves high efficiency and scalability through whole system co-design that optimizes and balances workload computation and communication. We exploit asynchrony throughout the system to improve performance and show that it additionally improves the accuracy of trained models. Adam is significantly more efficient and scalable than was previously thought possible and used 30x fewer machines to train a large 2 billion connection model to 2x higher accuracy in comparable time on the ImageNet 22,000 category image classification task than the system that previously held the record for this benchmark. We also show that task accuracy improves with larger models. Our results provide compelling evidence that a distributed systems-driven approach to deep learning using current training algorithms is worth pursuing."
            },
            "slug": "Project-Adam:-Building-an-Efficient-and-Scalable-Chilimbi-Suzue",
            "title": {
                "fragments": [],
                "text": "Project Adam: Building an Efficient and Scalable Deep Learning Training System"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The design and implementation of a distributed system called Adam comprised of commodity server machines to train large deep neural network models that exhibits world-class performance, scaling and task accuracy on visual recognition tasks and shows that task accuracy improves with larger models."
            },
            "venue": {
                "fragments": [],
                "text": "OSDI"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2518416"
                        ],
                        "name": "Jonghong Kim",
                        "slug": "Jonghong-Kim",
                        "structuredName": {
                            "firstName": "Jonghong",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonghong Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549369"
                        ],
                        "name": "Kyuyeon Hwang",
                        "slug": "Kyuyeon-Hwang",
                        "structuredName": {
                            "firstName": "Kyuyeon",
                            "lastName": "Hwang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyuyeon Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145057126"
                        ],
                        "name": "Wonyong Sung",
                        "slug": "Wonyong-Sung",
                        "structuredName": {
                            "firstName": "Wonyong",
                            "lastName": "Sung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wonyong Sung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 62
                            }
                        ],
                        "text": ", 2014), or take the route of custom hardware implementations (Kim et al., 2014; Merolla et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 253
                            }
                        ],
                        "text": "\u2026multiple 8 bit operations in parallel (Vanhoucke et al., 2011), or employ reconfigurable hardware (FPGAs) for highthroughput, energy-efficient inference (Farabet et al., 2011; Gokhale et al., 2014), or take the route of custom hardware implementations (Kim et al., 2014; Merolla et al., 2014)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 907206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e277762804aa4615b2258fbd367d91326c00b90e",
            "isKey": false,
            "numCitedBy": 50,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep neural networks show very good performance in phoneme and speech recognition applications when compared to previously used GMM (Gaussian Mixture Model)-based ones. However, efficient implementation of deep neural networks is difficult because the network size needs to be very large when high recognition accuracy is demanded. In this work, we develop a digital VLSI for phoneme recognition using deep neural networks and assess the design in terms of throughput, chip size, and power consumption. The developed VLSI employs a fixed-point optimization method that only uses +\u0394, 0, and -\u0394 for representing each of the weight. The design employs 1,024 simple processing units in each layer, which however can be scaled easily according to the needed throughput, and the throughput of the architecture varies from 62.5 to 1,000 times of the real-time processing speed."
            },
            "slug": "X1000-real-time-phoneme-recognition-VLSI-using-deep-Kim-Hwang",
            "title": {
                "fragments": [],
                "text": "X1000 real-time phoneme recognition VLSI using feed-forward deep neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work develops a digital VLSI for phoneme recognition using deep neural networks and assess the design in terms of throughput, chip size, and power consumption."
            },
            "venue": {
                "fragments": [],
                "text": "2014 IEEE International Conference on Acoustics, Speech and Signal Processing (ICASSP)"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116926120"
                        ],
                        "name": "Ren Wu",
                        "slug": "Ren-Wu",
                        "structuredName": {
                            "firstName": "Ren",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ren Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2725514"
                        ],
                        "name": "Shengen Yan",
                        "slug": "Shengen-Yan",
                        "structuredName": {
                            "firstName": "Shengen",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shengen Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143689293"
                        ],
                        "name": "Yi Shan",
                        "slug": "Yi-Shan",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Shan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Shan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066154565"
                        ],
                        "name": "Qingqing Dang",
                        "slug": "Qingqing-Dang",
                        "structuredName": {
                            "firstName": "Qingqing",
                            "lastName": "Dang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qingqing Dang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2087137982"
                        ],
                        "name": "Gang Sun",
                        "slug": "Gang-Sun",
                        "structuredName": {
                            "firstName": "Gang",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gang Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 62
                            }
                        ],
                        "text": "or a combination of CPUs and GPUs scaled-up to multiple nodes (Coates et al., 2013; Wu et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 281
                            }
                        ],
                        "text": "\u2026direction include distributed computing infrastructure using thousands of CPU cores (Dean et al., 2012; Chilimbi et al., 2014), or high-end graphics processors (GPUs) (Krizhevsky & Hinton, 2009), or a combination of CPUs and GPUs scaled-up to multiple nodes (Coates et al., 2013; Wu et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7480530,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5ce3abf942cdd685fb0f290f3e741f7b4749f0a",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a state-of-the-art image recognition system, Deep Image, developed using end-to-end deep learning. The key components are a custom-built supercomputer dedicated to deep learning, a highly optimized parallel algorithm using new strategies for data partitioning and communication, larger deep neural network models, novel data augmentation approaches, and usage of multi-scale high-resolution images. Our method achieves excellent results on multiple challenging computer vision benchmarks."
            },
            "slug": "Deep-Image:-Scaling-up-Image-Recognition-Wu-Yan",
            "title": {
                "fragments": [],
                "text": "Deep Image: Scaling up Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A state-of-the-art image recognition system, Deep Image, developed using end-to-end deep learning, which achieves excellent results on multiple challenging computer vision benchmarks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1794112"
                        ],
                        "name": "M. Baboulin",
                        "slug": "M.-Baboulin",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Baboulin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Baboulin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760511"
                        ],
                        "name": "A. Buttari",
                        "slug": "A.-Buttari",
                        "structuredName": {
                            "firstName": "Alfredo",
                            "lastName": "Buttari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Buttari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708869"
                        ],
                        "name": "J. Dongarra",
                        "slug": "J.-Dongarra",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Dongarra",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dongarra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753279"
                        ],
                        "name": "J. Kurzak",
                        "slug": "J.-Kurzak",
                        "structuredName": {
                            "firstName": "Jakub",
                            "lastName": "Kurzak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kurzak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786954"
                        ],
                        "name": "J. Langou",
                        "slug": "J.-Langou",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Langou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1786954"
                        ],
                        "name": "J. Langou",
                        "slug": "J.-Langou",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Langou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Langou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741269"
                        ],
                        "name": "P. Luszczek",
                        "slug": "P.-Luszczek",
                        "structuredName": {
                            "firstName": "Piotr",
                            "lastName": "Luszczek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Luszczek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706099"
                        ],
                        "name": "S. Tomov",
                        "slug": "S.-Tomov",
                        "structuredName": {
                            "firstName": "Stanimire",
                            "lastName": "Tomov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Tomov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 131
                            }
                        ],
                        "text": "Such a concept of employing mixed-precision computations has been explored previously in the context of floating point arithmetic (Baboulin et al., 2009), motivated largely by the fact that most modern processors achieve a factor 2 to 4 higher computational throughput for single-precision (32-bit)\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2237830,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f222ae64c00a2c3bf1294d9acf89a3cf38b74982",
            "isKey": false,
            "numCitedBy": 169,
            "numCiting": 103,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Accelerating-scientific-computations-with-mixed-Baboulin-Buttari",
            "title": {
                "fragments": [],
                "text": "Accelerating scientific computations with mixed precision algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Phys. Commun."
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 48
                            }
                        ],
                        "text": ", 2014), or high-end graphics processors (GPUs) (Ciresan et al., 2010; Krizhevsky et al., 2012),"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 122
                            }
                        ],
                        "text": "We test the validity of the proposed approach by training deep neural networks for the MNIST (Lecun & Cortes) and CIFAR10 (Krizhevsky et al., 2012) image classification tasks."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 97
                            }
                        ],
                        "text": "We test the validity of the proposed approach by training deep neural networks for the MNIST and CIFAR10 image classification tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 132
                            }
                        ],
                        "text": "To further test the validity of the stochastic rounding approach, we consider another commonly used image classification benchmark: CIFAR10."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 165
                            }
                        ],
                        "text": "We consider both fully connected deep neural networks (DNN) as well as convolutional neural networks (CNN) and present results for the MNIST(Lecun & Cortes) and the CIFAR10(Krizhevsky & Hinton, 2009) datasets."
                    },
                    "intents": []
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": true,
            "numCitedBy": 82046,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144423965"
                        ],
                        "name": "A. Murray",
                        "slug": "A.-Murray",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Murray",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053507626"
                        ],
                        "name": "P. J. Edwards",
                        "slug": "P.-J.-Edwards",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Edwards",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. J. Edwards"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 107
                            }
                        ],
                        "text": "Moreover, the addition of noise during training has been shown to improve the neural network\u2019s performance (Murray & Edwards, 1994; Bishop, 1995; Audhkhasi et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 106
                            }
                        ],
                        "text": "With the exception of employing the asynchronous version of the stochastic gradient descent algorithm (Recht et al., 2011) to reduce network traffic, the state-of-the-art large-scale deep learning systems fail to adequately capitalize on the error-resiliency of their workloads."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8666351,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "60e66781bf17f8103bbc57fc5daeb6fbc5e4b910",
            "isKey": false,
            "numCitedBy": 190,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We analyze the effects of analog noise on the synaptic arithmetic during multilayer perceptron training, by expanding the cost function to include noise-mediated terms. Predictions are made in the light of these calculations that suggest that fault tolerance, training quality and training trajectory should be improved by such noise-injection. Extensive simulation experiments on two distinct classification problems substantiate the claims. The results appear to be perfectly general for all training schemes where weights are adjusted incrementally, and have wide-ranging implications for all applications, particularly those involving \"inaccurate\" analog neural VLSI."
            },
            "slug": "Enhanced-MLP-performance-and-fault-tolerance-from-Murray-Edwards",
            "title": {
                "fragments": [],
                "text": "Enhanced MLP performance and fault tolerance resulting from synaptic weight noise during training"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "The effects of analog noise on the synaptic arithmetic during multilayer perceptron training is analyzed, by expanding the cost function to include noise-mediated terms, and predictions are made that fault tolerance, training quality and training trajectory should be improved by such noise-injection."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 55
                            }
                        ],
                        "text": "This architecture is similar to the one introduced in (Hinton et al., 2012) with the exception that it does not implement local response normalization or dropout layers."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14832074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1366de5bb112746a555e9c0cd00de3ad8628aea8",
            "isKey": false,
            "numCitedBy": 6242,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
            },
            "slug": "Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava",
            "title": {
                "fragments": [],
                "text": "Improving neural networks by preventing co-adaptation of feature detectors"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1895356"
                        ],
                        "name": "D. Ciresan",
                        "slug": "D.-Ciresan",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Ciresan",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ciresan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2514691"
                        ],
                        "name": "U. Meier",
                        "slug": "U.-Meier",
                        "structuredName": {
                            "firstName": "Ueli",
                            "lastName": "Meier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Meier"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6803671"
                        ],
                        "name": "L. Gambardella",
                        "slug": "L.-Gambardella",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Gambardella",
                            "middleNames": [
                                "Maria"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gambardella"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 48
                            }
                        ],
                        "text": ", 2014), or high-end graphics processors (GPUs) (Ciresan et al., 2010; Krizhevsky et al., 2012),"
                    },
                    "intents": []
                }
            ],
            "corpusId": 1918673,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b98cd08b75ebf2bd1d1ec47c51ef75777a7e64bd",
            "isKey": false,
            "numCitedBy": 881,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark. All we need to achieve this best result so far are many hidden layers, many neurons per layer, numerous deformed training images to avoid overfitting, and graphics cards to greatly speed up learning."
            },
            "slug": "Deep,-Big,-Simple-Neural-Nets-for-Handwritten-Digit-Ciresan-Meier",
            "title": {
                "fragments": [],
                "text": "Deep, Big, Simple Neural Nets for Handwritten Digit Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Good old online backpropagation for plain multilayer perceptrons yields a very low 0.35 error rate on the MNIST handwritten digits benchmark."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144409142"
                        ],
                        "name": "D. Hammerstrom",
                        "slug": "D.-Hammerstrom",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Hammerstrom",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hammerstrom"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 13
                            }
                        ],
                        "text": "Hammerstrom (Hammerstrom, 1990) presents a framework for on-chip learning using 8 to 16 bit fixed-point arithmetic."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206915906,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c4c310f427e47dc5b70aaad2d8200c8a034d4468",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The motivation for the X1 architecture described was to develop inexpensive commercial hardware suitable for solving large, real-world problems. Such an architecture must be systems oriented and flexible enough to execute any neural network algorithm and work cooperatively with existing hardware and software. The early application of neural networks must proceed in conjunction with existing technologies, both hardware and software. Using state-of-the-art technology and innovative architectural techniques, the author's architecture approaches the speed and cost of analog systems while retaining much of the flexibility of large, general-purpose parallel machines. The author has aimed at a particular set of applications and has made cost-performance tradeoffs accordingly. The goal is an architecture that could be considered a general-purpose microprocessor for neurocomputing"
            },
            "slug": "A-VLSI-architecture-for-high-performance,-low-cost,-Hammerstrom",
            "title": {
                "fragments": [],
                "text": "A VLSI architecture for high-performance, low-cost, on-chip learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Using state-of-the-art technology and innovative architectural techniques, the author's architecture approaches the speed and cost of analog systems while retaining much of the flexibility of large, general-purpose parallel machines."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063964117"
                        ],
                        "name": "A. Iwata",
                        "slug": "A.-Iwata",
                        "structuredName": {
                            "firstName": "Akira",
                            "lastName": "Iwata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Iwata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113575613"
                        ],
                        "name": "Y. Yoshida",
                        "slug": "Y.-Yoshida",
                        "structuredName": {
                            "firstName": "Yukio",
                            "lastName": "Yoshida",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Yoshida"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057100619"
                        ],
                        "name": "S. Matsuda",
                        "slug": "S.-Matsuda",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Matsuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Matsuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107688031"
                        ],
                        "name": "Y. Sato",
                        "slug": "Y.-Sato",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058407596"
                        ],
                        "name": "Y. Suzumura",
                        "slug": "Y.-Suzumura",
                        "structuredName": {
                            "firstName": "Yoshihiro",
                            "lastName": "Suzumura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Suzumura"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "(Iwata et al., 1989) implements the back-propagation algorithm using 24-bit floating-point processing units."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 14
                            }
                        ],
                        "text": "Iwata et al. (Iwata et al., 1989) implements the backpropagation algorithm using 24-bit floating-point processing units."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16223044,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1bd9c35867b1c98a4a42df84f52cb08d16a9a828",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "An artificial neural network (ANN) accelerator named Neuro Turbo was implemented using four recently developed general-purpose 24-b floating-point digital signal processors (DSP) MB86220. The Neuro Turbo is a MIMD (multiple-instruction, multiple-data) parallel processor having four ring-coupled DSPs and four dual-port memories (DPM). It is designed compactly to plug into the extender slots of the NEC personal computer PC98 series. The performance was evaluated by constructing a neural network to recognize the 26 type fonts of the alphabet set. Processing speeds of 2 MCPS (million connections per second) for the learning procedure and 11 MCPS for the forward pass were achieved.<<ETX>>"
            },
            "slug": "An-artificial-neural-network-accelerator-using-24-Iwata-Yoshida",
            "title": {
                "fragments": [],
                "text": "An artificial neural network accelerator using general purpose 24 bit floating point digital signal processors"
            },
            "tldr": {
                "abstractSimilarityScore": 85,
                "text": "An artificial neural network (ANN) accelerator named Neuro Turbo was implemented using four recently developed general-purpose 24-b floating-point digital signal processors (DSP) MB86220 using four ring-coupled DSPs and four dual-port memories."
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 97
                            }
                        ],
                        "text": "We test the validity of the proposed approach by training deep neural networks for the MNIST and CIFAR10 image classification tasks."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 173
                            }
                        ],
                        "text": "We consider both fully connected deep neural networks (DNN) as well as convolutional neural networks (CNN) and present results for the MNIST(Lecun & Cortes) and the CIFAR10(Krizhevsky & Hinton, 2009) datasets."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 132
                            }
                        ],
                        "text": "To further test the validity of the stochastic rounding approach, we consider another commonly used image classification benchmark: CIFAR10."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 169
                            }
                        ],
                        "text": "\u2026direction include distributed computing infrastructure using thousands of CPU cores (Dean et al., 2012; Chilimbi et al., 2014), or high-end graphics processors (GPUs) (Krizhevsky & Hinton, 2009), or a combination of CPUs and GPUs scaled-up to multiple nodes (Coates et al., 2013; Wu et al., 2015)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18268744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "isKey": true,
            "numCitedBy": 17474,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images."
            },
            "slug": "Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky",
            "title": {
                "fragments": [],
                "text": "Learning Multiple Layers of Features from Tiny Images"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex, using a novel parallelization algorithm to distribute the work among multiple machines connected on a network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9229182"
                        ],
                        "name": "B. Recht",
                        "slug": "B.-Recht",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Recht",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Recht"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1803218"
                        ],
                        "name": "Christopher R\u00e9",
                        "slug": "Christopher-R\u00e9",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "R\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher R\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144731788"
                        ],
                        "name": "Stephen J. Wright",
                        "slug": "Stephen-J.-Wright",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Wright",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen J. Wright"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47657030"
                        ],
                        "name": "Feng Niu",
                        "slug": "Feng-Niu",
                        "structuredName": {
                            "firstName": "Feng",
                            "lastName": "Niu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Feng Niu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "neural network\u2019s performance (Murray &amp; Edwards, 1994; Bishop, 1995; Audhkhasi et al., 2013). With the exception of employing the asynchronous version of the stochastic gradient descent algorithm (Recht et al., 2011) to reduce network trac, the state-of-the-art large-scale deep learning systems fail to adequately capitalize on the error-resiliency of their workloads. These systems are built by assembling general"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6108215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36f49b05d764bf5c10428b082c2d96c13c4203b9",
            "isKey": false,
            "numCitedBy": 2027,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Stochastic Gradient Descent (SGD) is a popular algorithm that can achieve state-of-the-art performance on a variety of machine learning tasks. Several researchers have recently proposed schemes to parallelize SGD, but all require performance-destroying memory locking and synchronization. This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking. We present an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work. We show that when the associated optimization problem is sparse, meaning most gradient updates only modify small parts of the decision variable, then HOGWILD! achieves a nearly optimal rate of convergence. We demonstrate experimentally that HOGWILD! outperforms alternative schemes that use locking by an order of magnitude."
            },
            "slug": "Hogwild:-A-Lock-Free-Approach-to-Parallelizing-Recht-R\u00e9",
            "title": {
                "fragments": [],
                "text": "Hogwild: A Lock-Free Approach to Parallelizing Stochastic Gradient Descent"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work aims to show using novel theoretical analysis, algorithms, and implementation that SGD can be implemented without any locking, and presents an update scheme called HOGWILD! which allows processors access to shared memory with the possibility of overwriting each other's work."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 131
                            }
                        ],
                        "text": "Moreover, the addition of noise during training has been shown to improve the neural network\u2019s performance (Murray & Edwards, 1994; Bishop, 1995; Audhkhasi et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 107
                            }
                        ],
                        "text": "Moreover, the addition of noise during training has been shown to improve the neural network\u2019s performance (Murray & Edwards, 1994; Bishop, 1995; An, 1996; Audhkhasi et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16096318,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c3ecd8e19e016d15670c8953b4b9afaa5186b0f3",
            "isKey": false,
            "numCitedBy": 1001,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well known that the addition of noise to the input data of a neural network during training can, in some circumstances, lead to significant improvements in generalization performance. Previous work has shown that such training with noise is equivalent to a form of regularization in which an extra term is added to the error function. However, the regularization term, which involves second derivatives of the error function, is not bounded below, and so can lead to difficulties if used directly in a learning algorithm based on error minimization. In this paper we show that for the purposes of network training, the regularization term can be reduced to a positive semi-definite form that involves only first derivatives of the network mapping. For a sum-of-squares error function, the regularization term belongs to the class of generalized Tikhonov regularizers. Direct minimization of the regularized error function provides a practical alternative to training with noise."
            },
            "slug": "Training-with-Noise-is-Equivalent-to-Tikhonov-Bishop",
            "title": {
                "fragments": [],
                "text": "Training with Noise is Equivalent to Tikhonov Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper shows that for the purposes of network training, the regularization term can be reduced to a positive semi-definite form that involves only first derivatives of the network mapping."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144153718"
                        ],
                        "name": "H. T. Kung",
                        "slug": "H.-T.-Kung",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Kung",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. T. Kung"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 102
                            }
                        ],
                        "text": "The DSP units within the FPGA are organized as a massively parallel 2-dimensional systolic array (SA) (Kung, 1982) of size n such that n(2) < 840."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 104
                            }
                        ],
                        "text": "The DSP units within the FPGA are organized as a massively parallel 2- dimensional systolic array (SA) (Kung, 1982) of size n such that n2   840."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1858965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "256bbe8e9fa3f5b72c24f1037ab734f9e7dd01c4",
            "isKey": false,
            "numCitedBy": 2228,
            "numCiting": 77,
            "paperAbstract": {
                "fragments": [],
                "text": "f High-performance, special-purpose computer systems are typically used to meet specific application requirements or to off-load computations that are especially taxing to general-purpose computers. As hardware cost and size continue to drop and processing requirements become well-understood in areas such as signal and image processing, more special-purpose systems are being constructed. However, since most of these systems are built on an ad hoc basis for specific tasks, methodological work in this area is rare. Because the knowledge gaited from individual experiences is neither accumulated nor properly organized, the same errors are repeated. I/O and computation imbalance is a notable example-often, the fact that I/O interfaces cannot keep up with device speed is discovered only after constructing a high-speed, special-purpose device. We intend to help correct this ad hoc approach by providing a general guideline-specifically, the concept of systolic architecture, a general methodology for mapping high-level computations into hardware structures. In a systolic system, data flows from the computer memcory in a rhythmic fashion, passing through many processing elements before it returns to memory, much as blood circulates to and from the heart. The system works like an autombbile assembly line where different people work on the same car at different times and many cars are assembled simultaneously. An assembly line is always linear, however, and systolic systems are sometimes two-dimensional. They can be rectangular, triangular, or hexagonal to make use of higher degrees of parallelism. Moreover, to implement a variety of computations, data flow in a systolic system may be at multiple speeds in multiple directions-both inputs and (partial) results flow, whereas only results flow in classical pipelined systems. Generally speaking, a systolic system is easy to implement because of its regularity and easy to reconfigure (to meet various outside constraints) because of its modularity. The systolic architectural concept was developed at Carnegie-Mellon University,'17 and versions of systolic processors are being designed and built by several industrial and governmental organizations.840 This article reviews the basic principle of systolic architectures and explains why they should result in cost-effective, highperformance special-purpose systems for a wide range of problems."
            },
            "slug": "Why-systolic-architectures-Kung",
            "title": {
                "fragments": [],
                "text": "Why systolic architectures?"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The basic principle of systolic architectures is reviewed and it is explained why they should result in cost-effective, highperformance special-purpose systems for a wide range of problems."
            },
            "venue": {
                "fragments": [],
                "text": "Computer"
            },
            "year": 1982
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3104038"
                        ],
                        "name": "Kartik Audhkhasi",
                        "slug": "Kartik-Audhkhasi",
                        "structuredName": {
                            "firstName": "Kartik",
                            "lastName": "Audhkhasi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kartik Audhkhasi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2457761"
                        ],
                        "name": "Osonde A. Osoba",
                        "slug": "Osonde-A.-Osoba",
                        "structuredName": {
                            "firstName": "Osonde",
                            "lastName": "Osoba",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Osonde A. Osoba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 145
                            }
                        ],
                        "text": "Moreover, the addition of noise during training has been shown to improve the neural network\u2019s performance (Murray & Edwards, 1994; Bishop, 1995; Audhkhasi et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 107
                            }
                        ],
                        "text": "Moreover, the addition of noise during training has been shown to improve the neural network\u2019s performance (Murray & Edwards, 1994; Bishop, 1995; An, 1996; Audhkhasi et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7252068,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a52d3122e045d6311bf309aed2c42ca75fb1e07f",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We prove that noise can speed convergence in the backpropagation algorithm. The proof consists of two separate results. The first result proves that the backpropagation algorithm is a special case of the generalized Expectation-Maximization (EM) algorithm for iterative maximum likelihood estimation. The second result uses the recent EM noise benefit to derive a sufficient condition for backpropagation training. The noise adds directly to the training data. A noise benefit also applies to the deep bidirectional pre-training of the neural network as well as to the backpropagation training of the network. The geometry of the noise benefit depends on the probability structure of the neurons at each layer. Logistic sigmoidal neurons produce a forbidden noise region that lies below a hyperplane. Then all noise on or above the hyperplane can only speed convergence of the neural network. The forbidden noise region is a sphere if the neurons have a Gaussian signal or activation function. These noise benefits all follow from the general noise benefit of the EM algorithm. Monte Carlo sample means estimate the population expectations in the EM algorithm. We demonstrate the noise benefits using MNIST digit classification."
            },
            "slug": "Noise-benefits-in-backpropagation-and-deep-Audhkhasi-Osoba",
            "title": {
                "fragments": [],
                "text": "Noise benefits in backpropagation and deep bidirectional pre-training"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "It is proved that noise can speed convergence in the backpropagation algorithm and the recent EM noise benefit is used to derive a sufficient condition for back Propagation training."
            },
            "venue": {
                "fragments": [],
                "text": "The 2013 International Joint Conference on Neural Networks (IJCNN)"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2256269"
                        ],
                        "name": "C. Farabet",
                        "slug": "C.-Farabet",
                        "structuredName": {
                            "firstName": "Cl\u00e9ment",
                            "lastName": "Farabet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Farabet"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940285"
                        ],
                        "name": "B. Martini",
                        "slug": "B.-Martini",
                        "structuredName": {
                            "firstName": "Berin",
                            "lastName": "Martini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Martini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079955555"
                        ],
                        "name": "B. Corda",
                        "slug": "B.-Corda",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Corda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Corda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2447628"
                        ],
                        "name": "Polina Akselrod",
                        "slug": "Polina-Akselrod",
                        "structuredName": {
                            "firstName": "Polina",
                            "lastName": "Akselrod",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Polina Akselrod"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2889774"
                        ],
                        "name": "E. Culurciello",
                        "slug": "E.-Culurciello",
                        "structuredName": {
                            "firstName": "Eugenio",
                            "lastName": "Culurciello",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Culurciello"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 64
                            }
                        ],
                        "text": "We also employ to denote the smallest positive number that may be represented in the given fixed-point format."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 851574,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "204710a6a6d935150b5b16daf74493dea6d1b7a2",
            "isKey": false,
            "numCitedBy": 349,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms \u2014 neuFlow \u2014 and a dataflow compiler \u2014 luaFlow \u2014 that transforms high-level flow-graph representations of these algorithms into machine code for neuFlow. This system was designed with the goal of providing real-time detection, categorization and localization of objects in complex scenes, while consuming 10 Watts when implemented on a Xilinx Virtex 6 FPGA platform, or about ten times less than a laptop computer, and producing speedups of up to 100 times in real-world applications. We present an application of the system on street scene analysis, segmenting 20 categories on 500 \u00d7 375 frames at 12 frames per second on our custom hardware neuFlow."
            },
            "slug": "NeuFlow:-A-runtime-reconfigurable-dataflow-for-Farabet-Martini",
            "title": {
                "fragments": [],
                "text": "NeuFlow: A runtime reconfigurable dataflow processor for vision"
            },
            "tldr": {
                "abstractSimilarityScore": 91,
                "text": "A scalable dataflow hardware architecture optimized for the computation of general-purpose vision algorithms \u2014 neuFlow \u2014 and a dataflow compiler \u2014 luaFlow \u2014 that transforms high-level flow-graph representations of these algorithms into machine code for neu Flow are presented."
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 2011 WORKSHOPS"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2408151"
                        ],
                        "name": "P. Merolla",
                        "slug": "P.-Merolla",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Merolla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Merolla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2100344"
                        ],
                        "name": "J. Arthur",
                        "slug": "J.-Arthur",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Arthur",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Arthur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1393944534"
                        ],
                        "name": "Rodrigo Alvarez-Icaza",
                        "slug": "Rodrigo-Alvarez-Icaza",
                        "structuredName": {
                            "firstName": "Rodrigo",
                            "lastName": "Alvarez-Icaza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rodrigo Alvarez-Icaza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34019307"
                        ],
                        "name": "A. Cassidy",
                        "slug": "A.-Cassidy",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Cassidy",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Cassidy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48775866"
                        ],
                        "name": "J. Sawada",
                        "slug": "J.-Sawada",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Sawada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sawada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3349882"
                        ],
                        "name": "Filipp Akopyan",
                        "slug": "Filipp-Akopyan",
                        "structuredName": {
                            "firstName": "Filipp",
                            "lastName": "Akopyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Filipp Akopyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3234142"
                        ],
                        "name": "Bryan L. Jackson",
                        "slug": "Bryan-L.-Jackson",
                        "structuredName": {
                            "firstName": "Bryan",
                            "lastName": "Jackson",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bryan L. Jackson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39536844"
                        ],
                        "name": "N. Imam",
                        "slug": "N.-Imam",
                        "structuredName": {
                            "firstName": "Nabil",
                            "lastName": "Imam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Imam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110229776"
                        ],
                        "name": "Chen Guo",
                        "slug": "Chen-Guo",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109993277"
                        ],
                        "name": "Yutaka Nakamura",
                        "slug": "Yutaka-Nakamura",
                        "structuredName": {
                            "firstName": "Yutaka",
                            "lastName": "Nakamura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yutaka Nakamura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2938842"
                        ],
                        "name": "Bernard Brezzo",
                        "slug": "Bernard-Brezzo",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Brezzo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bernard Brezzo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "22497177"
                        ],
                        "name": "I. Vo",
                        "slug": "I.-Vo",
                        "structuredName": {
                            "firstName": "Ivan",
                            "lastName": "Vo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Vo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2357931"
                        ],
                        "name": "Steven K. Esser",
                        "slug": "Steven-K.-Esser",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Esser",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steven K. Esser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2730753"
                        ],
                        "name": "R. Appuswamy",
                        "slug": "R.-Appuswamy",
                        "structuredName": {
                            "firstName": "Rathinakumar",
                            "lastName": "Appuswamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Appuswamy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736425"
                        ],
                        "name": "Brian Taba",
                        "slug": "Brian-Taba",
                        "structuredName": {
                            "firstName": "Brian",
                            "lastName": "Taba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brian Taba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767364"
                        ],
                        "name": "A. Amir",
                        "slug": "A.-Amir",
                        "structuredName": {
                            "firstName": "Arnon",
                            "lastName": "Amir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Amir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712991"
                        ],
                        "name": "M. Flickner",
                        "slug": "M.-Flickner",
                        "structuredName": {
                            "firstName": "Myron",
                            "lastName": "Flickner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Flickner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2904790"
                        ],
                        "name": "W. Risk",
                        "slug": "W.-Risk",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Risk",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Risk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144576833"
                        ],
                        "name": "R. Manohar",
                        "slug": "R.-Manohar",
                        "structuredName": {
                            "firstName": "Rajit",
                            "lastName": "Manohar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Manohar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944330"
                        ],
                        "name": "D. Modha",
                        "slug": "D.-Modha",
                        "structuredName": {
                            "firstName": "Dharmendra",
                            "lastName": "Modha",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Modha"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12706847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "680a38e8f025685b192e9e0cf755c6b664963551",
            "isKey": false,
            "numCitedBy": 2491,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Modeling computer chips on real brains Computers are nowhere near as versatile as our own brains. Merolla et al. applied our present knowledge of the structure and function of the brain to design a new computer chip that uses the same wiring rules and architecture. The flexible, scalable chip operated efficiently in real time, while using very little power. Science, this issue p. 668 A large-scale computer chip mimics many features of a real brain. Inspired by the brain\u2019s structure, we have developed an efficient, scalable, and flexible non\u2013von Neumann architecture that leverages contemporary silicon technology. To demonstrate, we built a 5.4-billion-transistor chip with 4096 neurosynaptic cores interconnected via an intrachip network that integrates 1 million programmable spiking neurons and 256 million configurable synapses. Chips can be tiled in two dimensions via an interchip communication interface, seamlessly scaling the architecture to a cortexlike sheet of arbitrary size. The architecture is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification. With 400-pixel-by-240-pixel video input at 30 frames per second, the chip consumes 63 milliwatts."
            },
            "slug": "A-million-spiking-neuron-integrated-circuit-with-a-Merolla-Arthur",
            "title": {
                "fragments": [],
                "text": "A million spiking-neuron integrated circuit with a scalable communication network and interface"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Inspired by the brain\u2019s structure, an efficient, scalable, and flexible non\u2013von Neumann architecture is developed that leverages contemporary silicon technology and is well suited to many applications that use complex neural networks in real time, for example, multiobject detection and classification."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51072723"
                        ],
                        "name": "Guozhong An",
                        "slug": "Guozhong-An",
                        "structuredName": {
                            "firstName": "Guozhong",
                            "lastName": "An",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guozhong An"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 107
                            }
                        ],
                        "text": "Moreover, the addition of noise during training has been shown to improve the neural network\u2019s performance (Murray & Edwards, 1994; Bishop, 1995; An, 1996; Audhkhasi et al., 2013)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 49741739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8d37cf4c4a2f15acd7c6ab2b4b4f25a3f3d7f9c",
            "isKey": false,
            "numCitedBy": 375,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the effects of adding noise to the inputs, outputs, weight connections, and weight changes of multilayer feedforward neural networks during backpropagation training. We rigorously derive and analyze the objective functions that are minimized by the noise-affected training processes. We show that input noise and weight noise encourage the neural-network output to be a smooth function of the input or its weights, respectively. In the weak-noise limit, noise added to the output of the neural networks only changes the objective function by a constant. Hence, it cannot improve generalization. Input noise introduces penalty terms in the objective function that are related to, but distinct from, those found in the regularization approaches. Simulations have been performed on a regression and a classification problem to further substantiate our analysis. Input noise is found to be effective in improving the generalization performance for both problems. However, weight noise is found to be effective in improving the generalization performance only for the classification problem. Other forms of noise have practically no effect on generalization."
            },
            "slug": "The-Effects-of-Adding-Noise-During-Backpropagation-An",
            "title": {
                "fragments": [],
                "text": "The Effects of Adding Noise During Backpropagation Training on a Generalization Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that input noise and weight noise encourage the neural-network output to be a smooth function of the input or its weights, respectively, and in the weak-noise limit, noise added to the output of the neural networks only changes the objective function by a constant, it cannot improve generalization."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "s, even the stochastic rounding scheme is unable to fully prevent the loss of gradient information. 4.1.2. CNN Using the MNIST dataset, we also evaluate a CNN with an architecture similar to LeNet-5 (LeCun et al., 1998). It comprises of 2 convolutional layers with 5x5 lters and ReLU activation function. The rst layer has 8 feature maps while the second convolutional layer produces 16 feature maps. Each convolutional"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 89
                            }
                        ],
                        "text": "Using the MNIST dataset, we also evaluate a CNN with an architecture similar to LeNet-5 (LeCun et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": true,
            "numCitedBy": 35623,
            "numCiting": 246,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 89
                            }
                        ],
                        "text": "Using the MNIST dataset, we also evaluate a CNN with an architecture similar to LeNet-5 (LeCun et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 64294544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f42b865e20e61a954239f421b42007236e671f19",
            "isKey": false,
            "numCitedBy": 3561,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique. Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called Graph Transformer Networks (GTN), allows such multi-module systems to be trained globally using Gradient-Based methods so as to minimize an overall performance measure. Two systems for on-line handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of Graph Transformer Networks. A Graph Transformer Network for reading bank check is also described. It uses Convolutional Neural Network character recognizers combined with global training techniques to provides record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day."
            },
            "slug": "GradientBased-Learning-Applied-to-Document-Haykin-Kosko",
            "title": {
                "fragments": [],
                "text": "GradientBased Learning Applied to Document Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Various methods applied to handwritten character recognition are reviewed and compared and Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698617"
                        ],
                        "name": "O. Bousquet",
                        "slug": "O.-Bousquet",
                        "structuredName": {
                            "firstName": "Olivier",
                            "lastName": "Bousquet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Bousquet"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7431525,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5936754b5762260bf102ac95d7b26cfc9d31956a",
            "isKey": false,
            "numCitedBy": 1493,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms. The analysis shows distinct tradeoffs for the case of small-scale and large-scale learning problems. Small-scale learning problems are subject to the usual approximation-estimation tradeoff. Large-scale learning problems are subject to a qualitatively different tradeoff involving the computational complexity of the underlying optimization algorithms in non-trivial ways."
            },
            "slug": "The-Tradeoffs-of-Large-Scale-Learning-Bottou-Bousquet",
            "title": {
                "fragments": [],
                "text": "The Tradeoffs of Large Scale Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "This contribution develops a theoretical framework that takes into account the effect of approximate optimization on learning algorithms and shows distinct tradeoffs for the case of small-scale and large-scale learning problems."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145115014"
                        ],
                        "name": "Corinna Cortes",
                        "slug": "Corinna-Cortes",
                        "structuredName": {
                            "firstName": "Corinna",
                            "lastName": "Cortes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Corinna Cortes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 60282629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc52d1ede1b90bf9d296bc5b34c9310b7eaa99a2",
            "isKey": false,
            "numCitedBy": 4474,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Disclosed is an improved articulated bar flail having shearing edges for efficiently shredding materials. An improved shredder cylinder is disclosed with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft. Also disclosed is an improved shredder apparatus which has a pair of these shredder cylinders mounted to rotate about spaced parallel axes which cooperates with a conveyer apparatus which has a pair of inclined converging conveyer belts with one of the belts mounted to move with respect to the other belt to allow the transport of articles of various sizes therethrough."
            },
            "slug": "The-mnist-database-of-handwritten-digits-LeCun-Cortes",
            "title": {
                "fragments": [],
                "text": "The mnist database of handwritten digits"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "An improved articulated bar flail having shearing edges for efficiently shredding materials and an improved shredder cylinder with a plurality of these flails circumferentially spaced and pivotally attached to the periphery of a rotatable shaft are disclosed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 74
                            }
                        ],
                        "text": "This translates to a throughput of 260 Gops/s at a power efficiency of 37 G-ops/s/W."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 176
                            }
                        ],
                        "text": "\u2026multiple 8 bit operations in parallel (Vanhoucke et al., 2011), or employ reconfigurable hardware (FPGAs) for highthroughput, energy-efficient inference (Farabet et al., 2011; Gokhale et al., 2014), or take the route of custom hardware implementations (Kim et al., 2014; Merolla et al., 2014)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 170
                            }
                        ],
                        "text": "This compares very favorably against the Intel i7-3720QM CPU, the NVIDIA GT650m and the GTX780 GPUs, all of which achieve power efficiency in the range of 1-5 G-ops/s/W (Gokhale et al., 2014)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 98
                            }
                        ],
                        "text": ", 2011), or employ reconfigurable hardware (FPGAs) for highthroughput, energy-efficient inference (Farabet et al., 2011; Gokhale et al., 2014), or take the route of custom hardware implementations (Kim et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A 240 gops/s mobile coprocessor for deep neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "In Computer Vision and Pattern Recognition Workshops (CVPRW),"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 104
                            }
                        ],
                        "text": "The DSP units within the FPGA are organized as a massively parallel 2- dimensional systolic array (SA) (Kung, 1982) of size n such that n2   840."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Why systolic architectures? Computer"
            },
            "venue": {
                "fragments": [],
                "text": "Why systolic architectures? Computer"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 133
                            }
                        ],
                        "text": "Some notable efforts in this direction include distributed computing infrastructure using thousands of CPU cores (Dean et al., 2012; Chilimbi et al., 2014), or high-end graphics processors (GPUs) (Krizhevsky & Hinton, 2009), or a combination of CPUs and GPUs scaled-up to multiple nodes (Coates et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Project adam: Building an efficient and scalable deep learning training Deep Learning with Limited Numerical Precision system"
            },
            "venue": {
                "fragments": [],
                "text": "11th USENIX Symposium on Operating Systems Design and Implementation (OSDI 14)"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gokhale , Vinayak , Jin , Jonghoon , Dundar , Aysegul , Martini , Berin , and Culurciello , Eugenio . A 240 gops / s mobile coprocessor for deep neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Computer Vision and Pattern Recognition Workshops ( CVPRW ) Neural Networks , 1990 . , 1990 IJCNN International Joint Conference on"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 114
                            }
                        ],
                        "text": "Some notable efforts in this direction include distributed computing infrastructure using thousands of CPU cores (Dean et al., 2012; Chilimbi et al., 2014), or high-end graphics processors (GPUs) (Krizhevsky & Hinton, 2009), or a combination of CPUs and GPUs scaled-up to multiple nodes (Coates et\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Large scale distributed deep networks. In Advances in Neural Information Processing Systems"
            },
            "venue": {
                "fragments": [],
                "text": "Large scale distributed deep networks. In Advances in Neural Information Processing Systems"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 15
                            }
                        ],
                        "text": "A recent work (Chen et al., 2014) presents a hardware accelerator for deep neural network training that employs fixed-point computation units, but finds it necessary to use 32-bit fixed-point representation to achieve convergence while training a convolutional neural network on the MNIST dataset."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Dadiannao: A machinelearning supercomputer"
            },
            "venue": {
                "fragments": [],
                "text": "Microarchitecture (MI- CRO), 2014 47th Annual IEEE/ACM International Symposium on"
            },
            "year": 2014
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 20,
            "methodology": 11,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 38,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Deep-Learning-with-Limited-Numerical-Precision-Gupta-Agrawal/b7cf49e30355633af2db19f35189410c8515e91f?sort=total-citations"
}