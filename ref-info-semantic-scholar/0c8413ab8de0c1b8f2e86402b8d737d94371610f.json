{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688482"
                        ],
                        "name": "S. Gr\u00fcnew\u00e4lder",
                        "slug": "S.-Gr\u00fcnew\u00e4lder",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Gr\u00fcnew\u00e4lder",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gr\u00fcnew\u00e4lder"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015507"
                        ],
                        "name": "Jean-Yves Audibert",
                        "slug": "Jean-Yves-Audibert",
                        "structuredName": {
                            "firstName": "Jean-Yves",
                            "lastName": "Audibert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Yves Audibert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691022"
                        ],
                        "name": "M. Opper",
                        "slug": "M.-Opper",
                        "structuredName": {
                            "firstName": "Manfred",
                            "lastName": "Opper",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Opper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 0
                            }
                        ],
                        "text": "Gru\u0308newa\u0308lder et al. (2010) consider the pure exploration problem for GPs, where the goal is to find the optimal decision over T rounds, rather than maximize cumulative reward (with no exploration/exploitation dilemma)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 215763200,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a99af9810155bfcbec517c9799d65f88851d6594",
            "isKey": false,
            "numCitedBy": 81,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Bandit algorithms are concerned with trading exploration with exploitation where a number of options are available but we can only learn their quality by experimenting with them. We consider the scenario in which the reward distribution for arms is modelled by a Gaussian process and there is no noise in the observed reward. Our main result is to bound the regret experienced by algorithms relative to the a posteriori optimal strategy of playing the best arm throughout based on benign assumptions about the covariance function dening the Gaussian process. We further complement these upper bounds with corresponding lower bounds for particular covariance functions demonstrating that in general there is at most a logarithmic looseness in our upper bounds."
            },
            "slug": "Regret-Bounds-for-Gaussian-Process-Bandit-Problems-Gr\u00fcnew\u00e4lder-Audibert",
            "title": {
                "fragments": [],
                "text": "Regret Bounds for Gaussian Process Bandit Problems"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The main result is to bound the regret experienced by algorithms relative to the a posteriori optimal strategy of playing the best arm throughout based on benign assumptions about the covariance function dening the Gaussian process."
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS 2010"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714811"
                        ],
                        "name": "Varsha Dani",
                        "slug": "Varsha-Dani",
                        "structuredName": {
                            "firstName": "Varsha",
                            "lastName": "Dani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varsha Dani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806117"
                        ],
                        "name": "Thomas P. Hayes",
                        "slug": "Thomas-P.-Hayes",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hayes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas P. Hayes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 81
                            }
                        ],
                        "text": "While our proofs \u2013 all provided in the Appendix \u2013 use techniques similar to those of Dani et al. (2008), we face a number of additional\n3 In general, d is the dimensionality of the input space D, which in the finite-dimensional linear case coincides with the feature space.\nsignificant technical\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 21
                            }
                        ],
                        "text": "For the linear case, Dani et al. (2008) provide regret bounds that explicitly depend on the dimensionality3 d. GPs can be seen as random functions in some infinite-dimensional linear space, so their results do not apply in this case."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 161
                            }
                        ],
                        "text": "In particular, we analyze the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, a simple and intuitive Bayesian method (Auer et al., 2002; Auer, 2002; Dani et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 33
                            }
                        ],
                        "text": "For the standard linear setting, Dani et al. (2008) provide a near-complete characterization\n1\nar X\niv :0\n91 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 117
                            }
                        ],
                        "text": "Our regret bounds are of the form O\u2217( \u221a T\u03b2T \u03b3T ), where \u03b2T is the confidence parameter in Algorithm 1, while the bounds of Dani et al. (2008) are of the form O\u2217( \u221a T\u03b2T d) (d the dimensionality of the linear function space)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9134969,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "551e19e5113cdff60a3c545d684fc4b9eb9a7306",
            "isKey": false,
            "numCitedBy": 682,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In the classical stochastic k-armed bandit problem, in each of a sequence of T rounds, a decision maker chooses one of k arms and incurs a cost chosen from an unknown distribution associated with that arm. The goal is to minimize regret, defined as the difference between the cost incurred by the algorithm and the optimal cost. In the linear optimization version of this problem (first considered by Auer [2002]), we view the arms as vectors in R, and require that the costs be linear functions of the chosen vector. As before, it is assumed that the cost functions are sampled independently from an unknown distribution. In this setting, the goal is to find algorithms whose running time and regret behave well as functions of the number of rounds T and the dimensionality n (rather than the number of arms, k, which may be exponential in n or even infinite). We give a nearly complete characterization of this problem in terms of both upper and lower bounds for the regret. In certain special cases (such as when the decision region is a polytope), the regret is polylog(T ). In general though, the optimal regret is \u0398\u2217( \u221a T ) \u2014 our lower bounds rule out the possibility of obtaining polylog(T ) rates in general. We present two variants of an algorithm based on the idea of \u201cupper confidence bounds.\u201d The first, due to Auer [2002], but not fully analyzed, obtains regret whose dependence on n and T are both essentially optimal, but which may be computationally intractable when the decision set is a polytope. The second version can be efficiently implemented when the decision set is a polytope (given as an intersection of half-spaces), but gives up a factor of \u221a n in the regret bound. Our results also extend to the setting where the set of allowed decisions may change over time. \u2217Department of Computer Science, University of Chicago, varsha@cs.uchicago.edu \u2020Toyota Technological Institute at Chicago, {hayest,sham}@tti-c.org"
            },
            "slug": "Stochastic-Linear-Optimization-under-Bandit-Dani-Hayes",
            "title": {
                "fragments": [],
                "text": "Stochastic Linear Optimization under Bandit Feedback"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A nearly complete characterization of the classical stochastic k-armed bandit problem in terms of both upper and lower bounds for the regret is given, and two variants of an algorithm based on the idea of \u201cupper confidence bounds\u201d are presented."
            },
            "venue": {
                "fragments": [],
                "text": "COLT"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815542"
                        ],
                        "name": "S\u00e9bastien Bubeck",
                        "slug": "S\u00e9bastien-Bubeck",
                        "structuredName": {
                            "firstName": "S\u00e9bastien",
                            "lastName": "Bubeck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S\u00e9bastien Bubeck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708654"
                        ],
                        "name": "R. Munos",
                        "slug": "R.-Munos",
                        "structuredName": {
                            "firstName": "R\u00e9mi",
                            "lastName": "Munos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Munos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806997"
                        ],
                        "name": "Gilles Stoltz",
                        "slug": "Gilles-Stoltz",
                        "structuredName": {
                            "firstName": "Gilles",
                            "lastName": "Stoltz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gilles Stoltz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40868287"
                        ],
                        "name": "Csaba Szepesvari",
                        "slug": "Csaba-Szepesvari",
                        "structuredName": {
                            "firstName": "Csaba",
                            "lastName": "Szepesvari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Csaba Szepesvari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 148
                            }
                        ],
                        "text": "Kleinberg et al. (2008) provide regret bounds under weaker and less configurable assumptions (only Lipschitz-continuity w.r.t. a metric is assumed; Bubeck et al. 2008 consider arbitrary topological spaces), which however degrade rapidly with the dimensionality of the problem (\u2126(T d+1 d+2 ))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 581697,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "90381e0e90ea3cb7fe043fe8f3988bc469b03bb8",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider a generalization of stochastic bandit problems where the set of arms, \u03a7, is allowed to be a generic topological space. We constraint the mean-payoff function with a dissimilarity function over \u03a7 in a way that is more general than Lipschitz. We construct an arm selection policy whose regret improves upon previous result for a large class of problems. In particular, our results imply that if \u03a7 is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally Holder with a known exponent, then the expected regret is bounded up to a logarithmic factor by \u221an, i.e., the rate of the growth of the regret is independent of the dimension of the space. Moreover, we prove the minimax optimality of our algorithm for the class of mean-payoff functions we consider."
            },
            "slug": "Online-Optimization-in-X-Armed-Bandits-Bubeck-Munos",
            "title": {
                "fragments": [],
                "text": "Online Optimization in X-Armed Bandits"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The results imply that if \u03a7 is the unit hypercube in a Euclidean space and the mean-payoff function has a finite number of global maxima around which the behavior of the function is locally Holder with a known exponent, then the expected regret is bounded up to a logarithmic factor by \u221an."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703039"
                        ],
                        "name": "Paat Rusmevichientong",
                        "slug": "Paat-Rusmevichientong",
                        "structuredName": {
                            "firstName": "Paat",
                            "lastName": "Rusmevichientong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paat Rusmevichientong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144224173"
                        ],
                        "name": "J. Tsitsiklis",
                        "slug": "J.-Tsitsiklis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Tsitsiklis",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Tsitsiklis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 77
                            }
                        ],
                        "text": "L\nG ]\n9 J\nun 2\n(also see Auer 2002; Dani et al. 2007; Abernethy et al. 2008; Rusmevichientong & Tsitsiklis 2008), explicitly dependent on the dimensionality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3204347,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "4afa0dfb8c3cff652f9113707109a372d9381ab2",
            "isKey": false,
            "numCitedBy": 468,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider bandit problems involving a large (possibly infinite) collection of arms, in which the expected reward of each arm is a linear function of an r-dimensional random vector Z \u2208 Rr, where r \u2265 2. The objective is to minimize the cumulative regret and Bayes risk. When the set of arms corresponds to the unit sphere, we prove that the regret and Bayes risk is of order \u0398(r \u221aT), by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases. The phase-based policy is also shown to be effective if the set of arms satisfies a strong convexity condition. For the case of a general set of arms, we describe a near-optimal policy whose regret and Bayes risk admit upper bounds of the form O(r \u221aT log3/2T)."
            },
            "slug": "Linearly-Parameterized-Bandits-Rusmevichientong-Tsitsiklis",
            "title": {
                "fragments": [],
                "text": "Linearly Parameterized Bandits"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "It is proved that the regret and Bayes risk is of order \u0398(r \u221aT), by establishing a lower bound for an arbitrary policy, and showing that a matching upper bound is obtained through a policy that alternates between exploration and exploitation phases."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Oper. Res."
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2714811"
                        ],
                        "name": "Varsha Dani",
                        "slug": "Varsha-Dani",
                        "structuredName": {
                            "firstName": "Varsha",
                            "lastName": "Dani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Varsha Dani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806117"
                        ],
                        "name": "Thomas P. Hayes",
                        "slug": "Thomas-P.-Hayes",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hayes",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas P. Hayes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 30
                            }
                        ],
                        "text": "The proof methodology follows Dani et al. (2007) in that we relate the regret to the growth of the log volume of the confidence ellipsoid \u2014 a novelty in our proof is showing how this growth is characterized by the information gain."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 21
                            }
                        ],
                        "text": "For the linear case, Dani et al. (2008) provide regret bounds that explicitly depend on the dimensionality(3) d."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 413,
                                "start": 123
                            }
                        ],
                        "text": "Our regret bounds are of the form O\u2217( \u221a T\u03b2T \u03b3T ), where \u03b2T is the confidence parameter in Algorithm 1, while the bounds of Dani et al. (2008) are of the form O\u2217( \u221a T\u03b2T d) (d the dimensionality of the linear function space). Here and below, the O\u2217 notation is a variant of O, where log factors are suppressed. While our proofs \u2013 all provided in the Appendix \u2013 use techniques similar to those of Dani et al. (2008), we face a number of additional"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 36
                            }
                        ],
                        "text": "L\nG ]\n9 J\nun 2\n(also see Auer 2002; Dani et al. 2007; Abernethy et al. 2008; Rusmevichientong & Tsitsiklis 2008), explicitly dependent on the dimensionality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 33
                            }
                        ],
                        "text": "For the standard linear setting, Dani et al. (2008) provide a near-complete characterization 1 ar X iv :0 91 2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 123
                            }
                        ],
                        "text": "Our regret bounds are of the form O\u2217( \u221a T\u03b2T \u03b3T ), where \u03b2T is the confidence parameter in Algorithm 1, while the bounds of Dani et al. (2008) are of the form O\u2217( \u221a T\u03b2T d) (d the dimensionality of the linear function space)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 0
                            }
                        ],
                        "text": "(also see Auer 2002; Dani et al. 2007; Abernethy et al. 2008; Rusmevichientong & Tsitsiklis 2008), explicitly dependent on the dimensionality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1028408,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df50df30706ba37eacc41b6f6ee9f2aa0e806a5b",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In the online linear optimization problem, a learner must choose, in each round, a decision from a set D \u2282 \u211dn in order to minimize an (unknown and changing) linear cost function. We present sharp rates of convergence (with respect to additive regret) for both the full information setting (where the cost function is revealed at the end of each round) and the bandit setting (where only the scalar cost incurred is revealed). In particular, this paper is concerned with the price of bandit information, by which we mean the ratio of the best achievable regret in the bandit setting to that in the full-information setting. For the full information case, the upper bound on the regret is O*( \u221anT), where n is the ambient dimension and T is the time horizon. For the bandit case, we present an algorithm which achieves O*(n3/2 \u221aT) regret \u2014 all previous (nontrivial) bounds here were O(poly(n)T2/3) or worse. It is striking that the convergence rate for the bandit setting is only a factor of n worse than in the full information case \u2014 in stark contrast to the K-arm bandit setting, where the gap in the dependence on K is exponential (\u221aTK vs. \u221aT log K). We also present lower bounds showing that this gap is at least \u221an, which we conjecture to be the correct order. The bandit algorithm we present can be implemented efficiently in special cases of particular interest, such as path planning and Markov Decision Problems."
            },
            "slug": "The-Price-of-Bandit-Information-for-Online-Dani-Hayes",
            "title": {
                "fragments": [],
                "text": "The Price of Bandit Information for Online Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "This paper presents an algorithm which achieves O*(n3/2 \u221aT) regret and presents lower bounds showing that this gap is at least \u221an, which is conjecture to be the correct order."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145343838"
                        ],
                        "name": "Andreas Krause",
                        "slug": "Andreas-Krause",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Krause",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andreas Krause"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1730156"
                        ],
                        "name": "Carlos Guestrin",
                        "slug": "Carlos-Guestrin",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Guestrin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Guestrin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 18
                            }
                        ],
                        "text": "Proof As shown by Krause & Guestrin (2005), the function F (A) = I(yA;f ) is submodular."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 85
                            }
                        ],
                        "text": "This is because F (A) satisfies a diminishing returns property called submodularity (Krause & Guestrin, 2005), and the greedy approximation guarantee (5) holds for any submodular function (Nemhauser et al., 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15969222,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2b80b93ddfa9fbaf1d9c7d580664e91626066db5",
            "isKey": false,
            "numCitedBy": 415,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "A fundamental issue in real-world systems, such as sensor networks, is the selection of observations which most effectively reduce uncertainty. More specifically, we address the long standing problem of nonmyopically selecting the most informative subset of variables in a graphical model. We present the first efficient randomized algorithm providing a constant factor (1 - 1/e \u2013 e) approximation guarantee for any e > 0 with high confidence. The algorithm leverages the theory of submodular functions, in combination with a polynomial bound on sample complexity. We furthermore prove that no polynomial time algorithm can provide a constant factor approximation better than (1 - 1/e) unless P = NP. Finally, we provide extensive evidence of the effectiveness of our method on two complex real-world datasets."
            },
            "slug": "Near-optimal-Nonmyopic-Value-of-Information-in-Krause-Guestrin",
            "title": {
                "fragments": [],
                "text": "Near-optimal Nonmyopic Value of Information in Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work addresses the long standing problem of nonmyopically selecting the most informative subset of variables in a graphical model and presents the first efficient randomized algorithm providing a constant factor (1 - 1/e \u2013 e) approximation guarantee for any e > 0 with high confidence."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144543541"
                        ],
                        "name": "P. Auer",
                        "slug": "P.-Auer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Auer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Auer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10485293,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "103f6fe35033f9327611ddafde74a2b544072980",
            "isKey": false,
            "numCitedBy": 1420,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "We show how a standard tool from statistics --- namely confidence bounds --- can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off. Our technique for designing and analyzing algorithms for such situations is general and can be applied when an algorithm has to make exploitation-versus-exploration decisions based on uncertain information provided by a random process. We apply our technique to two models with such an exploitation-exploration trade-off. For the adversarial bandit problem with shifting our new algorithm suffers only O((ST)1/2) regret with high probability over T trials with S shifts. Such a regret bound was previously known only in expectation. The second model we consider is associative reinforcement learning with linear value functions. For this model our technique improves the regret from O(T3/4) to O(T1/2)."
            },
            "slug": "Using-Confidence-Bounds-for-Trade-offs-Auer",
            "title": {
                "fragments": [],
                "text": "Using Confidence Bounds for Exploitation-Exploration Trade-offs"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "It is shown how a standard tool from statistics, namely confidence bounds, can be used to elegantly deal with situations which exhibit an exploitation-exploration trade-off, and improves the regret from O(T3/4) to T1/2."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2633757"
                        ],
                        "name": "Robert D. Kleinberg",
                        "slug": "Robert-D.-Kleinberg",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Kleinberg",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert D. Kleinberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158559"
                        ],
                        "name": "Aleksandrs Slivkins",
                        "slug": "Aleksandrs-Slivkins",
                        "structuredName": {
                            "firstName": "Aleksandrs",
                            "lastName": "Slivkins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aleksandrs Slivkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735099"
                        ],
                        "name": "E. Upfal",
                        "slug": "E.-Upfal",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Upfal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Upfal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 0
                            }
                        ],
                        "text": "Kleinberg et al. (2008) provide regret bounds under weaker and less configurable assumptions (only Lipschitz-continuity w.r.t. a metric is assumed; Bubeck et al. 2008 consider arbitrary topological spaces), which however degrade rapidly with the dimensionality of the problem (\u2126(T d+1 d+2 ))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1304870,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "5b3e9552d4e3e216817fda8f43931f217fbfe5b1",
            "isKey": false,
            "numCitedBy": 402,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In a multi-armed bandit problem, an online algorithm chooses from a set of strategies in a sequence of $n$ trials so as to maximize the total payoff of the chosen strategies. While the performance of bandit algorithms with a small finite strategy set is quite well understood, bandit problems with large strategy sets are still a topic of very active investigation, motivated by practical applications such as online auctions and web advertisement. The goal of such research is to identify broad and natural classes of strategy sets and payoff functions which enable the design of efficient solutions. In this work we study a very general setting for the multi-armed bandit problem in which the strategies form a metric space, and the payoff function satisfies a Lipschitz condition with respect to the metric. We refer to this problem as the \"Lipschitz MAB problem\". We present a complete solution for the multi-armed problem in this setting. That is, for every metric space (L,X) we define an isometry invariant Max Min COV(X) which bounds from below the performance of Lipschitz MAB algorithms for $X$, and we present an algorithm which comes arbitrarily close to meeting this bound. Furthermore, our technique gives even better results for benign payoff functions."
            },
            "slug": "Multi-armed-bandits-in-metric-spaces-Kleinberg-Slivkins",
            "title": {
                "fragments": [],
                "text": "Multi-armed bandits in metric spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work defines an isometry invariant Max Min COV(X) which bounds from below the performance of Lipschitz MAB algorithms for X, and presents an algorithm which comes arbitrarily close to meeting this bound."
            },
            "venue": {
                "fragments": [],
                "text": "STOC"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144543541"
                        ],
                        "name": "P. Auer",
                        "slug": "P.-Auer",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Auer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Auer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388387856"
                        ],
                        "name": "N. Cesa-Bianchi",
                        "slug": "N.-Cesa-Bianchi",
                        "structuredName": {
                            "firstName": "Nicol\u00f2",
                            "lastName": "Cesa-Bianchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cesa-Bianchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152702481"
                        ],
                        "name": "P. Fischer",
                        "slug": "P.-Fischer",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Fischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fischer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 82
                            }
                        ],
                        "text": "6 is motivated by the UCB algorithm for the classical multi-armed bandit problem (Auer et al., 2002; Kocsis & Szepesva\u0301ri, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 81
                            }
                        ],
                        "text": "6 is motivated by the UCB algorithm for the classical multi-armed bandit problem (Auer et al., 2002; Kocsis & Szepesv\u00e1ri, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 130
                            }
                        ],
                        "text": "In particular, we analyze the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, a simple and intuitive Bayesian method (Auer et al., 2002; Auer, 2002; Dani et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207609497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1e1d35136b1bf3b13ef6b53f6039f39d9ee820e3",
            "isKey": false,
            "numCitedBy": 5232,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Reinforcement learning policies face the exploration versus exploitation dilemma, i.e. the search for a balance between exploring the environment to find profitable actions while taking the empirically best action as often as possible. A popular measure of a policy's success in addressing this dilemma is the regret, that is the loss due to the fact that the globally optimal policy is not followed all the times. One of the simplest examples of the exploration/exploitation dilemma is the multi-armed bandit problem. Lai and Robbins were the first ones to show that the regret for this problem has to grow at least logarithmically in the number of plays. Since then, policies which asymptotically achieve this regret have been devised by Lai and Robbins and many others. In this work we show that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support."
            },
            "slug": "Finite-time-Analysis-of-the-Multiarmed-Bandit-Auer-Cesa-Bianchi",
            "title": {
                "fragments": [],
                "text": "Finite-time Analysis of the Multiarmed Bandit Problem"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work shows that the optimal logarithmic regret is also achievable uniformly over time, with simple and efficient policies, and for all reward distributions with bounded support."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79904726"
                        ],
                        "name": "C. Ko",
                        "slug": "C.-Ko",
                        "structuredName": {
                            "firstName": "Chun",
                            "lastName": "Ko",
                            "middleNames": [
                                "Wa"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Ko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143413071"
                        ],
                        "name": "Jon Lee",
                        "slug": "Jon-Lee",
                        "structuredName": {
                            "firstName": "Jon",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jon Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744003"
                        ],
                        "name": "M. Queyranne",
                        "slug": "M.-Queyranne",
                        "structuredName": {
                            "firstName": "Maurice",
                            "lastName": "Queyranne",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Queyranne"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 74
                            }
                        ],
                        "text": "While finding the information gain maximizer among A \u2282 D, |A| \u2264 T is NP-hard (Ko et al., 1995), it can be approximated by an efficient greedy algorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 46877159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed8a65d9f23b70f8ff37da5e119819f53b448751",
            "isKey": false,
            "numCitedBy": 318,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the experimental design problem of selecting a most informative subset, having prespecified size, from a set of correlated random variables. The problem arises in many applied domains, such as meteorology, environmental statistics, and statistical geology. In these applications, observations can be collected at different locations, and possibly, at different times. Information is measured by \u201centropy.\u201d In the Gaussian case, the problem is recast as that of maximizing the determinant of the covariance matrix of the chosen subset. We demonstrate that this problem is NP-hard. We establish an upper bound for the entropy, based on the eigenvalue interlacing property, and we incorporate this bound in a branch-and-bound algorithm for the exact solution of the problem. We present computational results for estimated covariance matrices that correspond to sets of environmental monitoring stations in the United States."
            },
            "slug": "An-Exact-Algorithm-for-Maximum-Entropy-Sampling-Ko-Lee",
            "title": {
                "fragments": [],
                "text": "An Exact Algorithm for Maximum Entropy Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "An upper bound for the entropy is established, based on the eigenvalue interlacing property, and incorporated in a branch-and-bound algorithm for the exact solution of the experimental design problem of selecting a most informative subset, having prespecified size, from a set of correlated random variables."
            },
            "venue": {
                "fragments": [],
                "text": "Oper. Res."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680574"
                        ],
                        "name": "M. Seeger",
                        "slug": "M.-Seeger",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Seeger",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Seeger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144695232"
                        ],
                        "name": "S. Kakade",
                        "slug": "S.-Kakade",
                        "structuredName": {
                            "firstName": "Sham",
                            "lastName": "Kakade",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kakade"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145346320"
                        ],
                        "name": "Dean P. Foster",
                        "slug": "Dean-P.-Foster",
                        "structuredName": {
                            "firstName": "Dean",
                            "lastName": "Foster",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dean P. Foster"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 185
                            }
                        ],
                        "text": "Finally, we bound this empirical expression in terms of the kernel operator eigenvalues of k w.r.t. the uniform distribution on D. Asymptotic expressions for the latter are reviewed in Seeger et al. (2008), which we plug in to obtain our results."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 69
                            }
                        ],
                        "text": "This should be compared with E[I(yT ;fT )] = O((log T )d+1) given by Seeger et al. (2008), where the xt are drawn independently from a Gaussian base distribution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 56
                            }
                        ],
                        "text": "For the Squared Exponential kernel k, Bk(T\u2217) is given by Seeger et al. (2008)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 66
                            }
                        ],
                        "text": "For Mate\u0301rn kernels k with roughness parameter \u03bd, Bk(T\u2217) is given by Seeger et al. (2008) for the uniform base distribution \u00b5(x) on D."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 99
                            }
                        ],
                        "text": "For the kernels of interest here, asymptotic expressions for the operator eigenvalues are given in Seeger et al. (2008), who derived bounds on the information gain for fixed and random designs (in contrast to the worst-case information gain considered here, which is substantially more challenging to bound)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 99
                            }
                        ],
                        "text": "For the kernels of interest here, asymptotic expressions for the operator eigenvalues are given in Seeger et al. (2008), who derived bounds on the information gain for fixed and random designs (in contrast to the worst-case information gain considered here, which is substantially more challenging\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 67
                            }
                        ],
                        "text": "How does this bound compare to the bound on E[I(yT ;fT )] given by Seeger et al. (2008)?"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2197379,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f8348741ca22058f9955f932bd9d63ccd9e81319",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Bayesian nonparametric models are widely and successfully used for statistical prediction. While posterior consistency properties are well studied in quite general settings, results have been proved using abstract concepts such as metric entropy, and they come with subtle conditions which are hard to validate and not intuitive when applied to concrete models. Furthermore, convergence rates are difficult to obtain. By focussing on the concept of information consistency for Bayesian Gaussian process (GP)models, consistency results and convergence rates are obtained via a regret bound on cumulative log loss. These results depend strongly on the covariance function of the prior process, thereby giving a novel interpretation to penalization with reproducing kernel Hilbert space norms and to commonly used covariance function classes and their parameters. The proof of the main result employs elementary convexity arguments only. A theorem of Widom is used in order to obtain precise convergence rates for several covariance functions widely used in practice."
            },
            "slug": "Information-Consistency-of-Nonparametric-Gaussian-Seeger-Kakade",
            "title": {
                "fragments": [],
                "text": "Information Consistency of Nonparametric Gaussian Process Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "By focussing on the concept of information consistency for Bayesian Gaussian process (GP)models, consistency results and convergence rates are obtained via a regret bound on cumulative log loss."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2291789"
                        ],
                        "name": "S. Ghosal",
                        "slug": "S.-Ghosal",
                        "structuredName": {
                            "firstName": "Subhashis",
                            "lastName": "Ghosal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ghosal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3155262"
                        ],
                        "name": "A. Roy",
                        "slug": "A.-Roy",
                        "structuredName": {
                            "firstName": "Anindya",
                            "lastName": "Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Roy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 133
                            }
                        ],
                        "text": "For a certain event in this sample space, all \u2202f/(\u2202xj) exist, are continuous, and the complement of (10) holds for all j. Theorem 5 of Ghosal & Roy (2006), together with the union bound, implies that this event has probability \u2265 1\u2212 \u03b4/2."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 98
                            }
                        ],
                        "text": "It holds for stationary kernels k(x,x\u2032) = k(x \u2212 x\u2032) which are four times differentiable (Theorem 5 of Ghosal & Roy (2006)), such as the Squared Exponential and Mate\u0301rn kernels with \u03bd > 2 (see Section 5.2), while it is violated for the OrnsteinUhlenbeck kernel (Mate\u0301rn with \u03bd = 1/2; a stationary\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 116
                            }
                        ],
                        "text": "For samples f of the GP, consider partial derivatives \u2202f/(\u2202xj) of this sample path for j = 1, . . . , d. Theorem 5 of Ghosal & Roy (2006)\nstates that if derivatives up to fourth order exists for (x,x\u2032) 7\u2192 k(x,x\u2032), then f is almost surely continuously differentiable, with \u2202f/(\u2202xj) distributed as\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10616419,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "3b4dc23a0559d7edc99fbd05e7c22057529a7169",
            "isKey": true,
            "numCitedBy": 156,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Consider binary observations whose response probability is an unknown smooth function of a set of covariates. Suppose that a prior on the response probability function is induced by a Gaussian process mapped to the unit interval through a link function. In this paper we study consistency of the resulting posterior distribution. If the covariance kernel has derivatives up to a desired order and the bandwidth parameter of the kernel is allowed to take arbitrarily small values, we show that the posterior distribution is consistent in the L 1 -distance. As an auxiliary result to our proofs, we show that, under certain conditions, a Gaussian process assigns positive probabilities to the uniform neighborhoods of a continuous function. This result may be of independent interest in the literature for small ball probabilities of Gaussian processes."
            },
            "slug": "Posterior-consistency-of-Gaussian-process-prior-for-Ghosal-Roy",
            "title": {
                "fragments": [],
                "text": "Posterior consistency of Gaussian process prior for nonparametric binary regression"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "If the covariance kernel has derivatives up to a desired order and the bandwidth parameter of the kernel is allowed to take arbitrarily small values, it is shown that the posterior distribution is consistent in the L 1 -distance."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705564"
                        ],
                        "name": "G. Nemhauser",
                        "slug": "G.-Nemhauser",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Nemhauser",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Nemhauser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736128"
                        ],
                        "name": "L. Wolsey",
                        "slug": "L.-Wolsey",
                        "structuredName": {
                            "firstName": "Laurence",
                            "lastName": "Wolsey",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Wolsey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145456487"
                        ],
                        "name": "M. Fisher",
                        "slug": "M.-Fisher",
                        "structuredName": {
                            "firstName": "Marshall",
                            "lastName": "Fisher",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 98
                            }
                        ],
                        "text": "Note that \u222b k(x,x)\u00b5(x) dx = 1 by our assumption k(x,x) = 1, so that k is Hilbert-\n5While the result of Nemhauser et al. (1978) is stated in terms of finite sets, it extends to infinite sets as long as the greedy selection can be implemented efficiently."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 248,
                                "start": 226
                            }
                        ],
                        "text": "\u2026contributions are twofold: first, we show how to analyze the nonlinear setting by focusing on the concept of information gain, and second, we explicitly bound this information gain measure using the concept of submodularity (Nemhauser et al., 1978) and knowledge about kernel operator spectra."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 211,
                                "start": 189
                            }
                        ],
                        "text": "This is because F (A) satisfies a diminishing returns property called submodularity (Krause & Guestrin, 2005), and the greedy approximation guarantee (5) holds for any submodular function (Nemhauser et al., 1978)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 33
                            }
                        ],
                        "text": "Thus, we can apply the result of Nemhauser et al. (1978)5 which guarantees that \u03b3\u0303T is upper-bounded by 1/(1 \u2212 1/e) times the value the greedy maximization algorithm attains."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206800425,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b9e43395663f74c581982e9ca97a0d7057a0008c",
            "isKey": true,
            "numCitedBy": 4028,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "LetN be a finite set andz be a real-valued function defined on the set of subsets ofN that satisfies z(S)+z(T)\u2265z(S\u22c3T)+z(S\u22c2T) for allS, T inN. Such a function is called submodular. We consider the problem maxS\u2282N{a(S):|S|\u2264K,z(S) submodular}.Several hard combinatorial optimization problems can be posed in this framework. For example, the problem of finding a maximum weight independent set in a matroid, when the elements of the matroid are colored and the elements of the independent set can have no more thanK colors, is in this class. The uncapacitated location problem is a special case of this matroid optimization problem.We analyze greedy and local improvement heuristics and a linear programming relaxation for this problem. Our results are worst case bounds on the quality of the approximations. For example, whenz(S) is nondecreasing andz(0) = 0, we show that a \u201cgreedy\u201d heuristic always produces a solution whose value is at least 1 \u2212[(K \u2212 1)/K]K times the optimal value. This bound can be achieved for eachK and has a limiting value of (e \u2212 1)/e, where e is the base of the natural logarithm."
            },
            "slug": "An-analysis-of-approximations-for-maximizing-set-Nemhauser-Wolsey",
            "title": {
                "fragments": [],
                "text": "An analysis of approximations for maximizing submodular set functions\u2014I"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "It is shown that a \u201cgreedy\u201d heuristic always produces a solution whose value is at least 1 \u2212[(K \u2212 1/K]K times the optimal value, which can be achieved for eachK and has a limiting value of (e \u2212 1)/e, where e is the base of the natural logarithm."
            },
            "venue": {
                "fragments": [],
                "text": "Math. Program."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144272396"
                        ],
                        "name": "E. V\u00e1zquez",
                        "slug": "E.-V\u00e1zquez",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "V\u00e1zquez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. V\u00e1zquez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691181"
                        ],
                        "name": "J. Bect",
                        "slug": "J.-Bect",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Bect",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bect"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 43
                            }
                        ],
                        "text": "While convergence of EGO is established by Vazquez & Bect (2007), convergence rates have remained elusive."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52065066,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "3c5ddc50b656c923b8173c8fc40b6c36f5cd3dac",
            "isKey": false,
            "numCitedBy": 185,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Convergence-properties-of-the-expected-improvement-V\u00e1zquez-Bect",
            "title": {
                "fragments": [],
                "text": "Convergence properties of the expected improvement algorithm with fixed mean and covariance functions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144272396"
                        ],
                        "name": "E. V\u00e1zquez",
                        "slug": "E.-V\u00e1zquez",
                        "structuredName": {
                            "firstName": "Emmanuel",
                            "lastName": "V\u00e1zquez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. V\u00e1zquez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691181"
                        ],
                        "name": "J. Bect",
                        "slug": "J.-Bect",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Bect",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bect"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 43
                            }
                        ],
                        "text": "While convergence of EGO is established by Vazquez & Bect (2007), convergence rates have remained elusive."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 88511936,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8c469d247c2845670a098c03f1a4edf66e27f96b",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper has been withdrawn from the arXiv. It is now published by Elsevier in the Journal of Statistical Planning and Inference, under the modified title \"Convergence properties of the expected improvement algorithm with fixed mean and covariance functions\". See this http URL \nAn author-generated post-print version is available from the HAL repository of SUPELEC at this http URL \nAbstract : \"This paper deals with the convergence of the expected improvement algorithm, a popular global optimization algorithm based on a Gaussian process model of the function to be optimized. The first result is that under some mild hypotheses on the covariance function k of the Gaussian process, the expected improvement algorithm produces a dense sequence of evaluation points in the search domain, when the function to be optimized is in the reproducing kernel Hilbert space generated by k. The second result states that the density property also holds for P-almost all continuous functions, where P is the (prior) probability distribution induced by the Gaussian process.\""
            },
            "slug": "Convergence-properties-of-the-expected-improvement-V\u00e1zquez-Bect",
            "title": {
                "fragments": [],
                "text": "Convergence properties of the expected improvement algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that under some mild hypotheses on the covariance function k of the Gaussian process, the expected improvement algorithm produces a dense sequence of evaluation points in the search domain, when the function to be optimized is in the reproducing kernel Hilbert space generated by k."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118096690"
                        ],
                        "name": "Donald R. Jones",
                        "slug": "Donald-R.-Jones",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Jones",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald R. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815604"
                        ],
                        "name": "M. Schonlau",
                        "slug": "M.-Schonlau",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Schonlau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schonlau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145228938"
                        ],
                        "name": "W. Welch",
                        "slug": "W.-Welch",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Welch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Welch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 110
                            }
                        ],
                        "text": "The Efficient Global Optimization (EGO) algorithm for optimizing expensive black-box functions is proposed by Jones et al. (1998) and extended to GPs by Huang et al. (2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13068209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "daa63f57c3fbe994c4356f8d986a22e696e776d2",
            "isKey": false,
            "numCitedBy": 5764,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome."
            },
            "slug": "Efficient-Global-Optimization-of-Expensive-Jones-Schonlau",
            "title": {
                "fragments": [],
                "text": "Efficient Global Optimization of Expensive Black-Box Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering and shows how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule."
            },
            "venue": {
                "fragments": [],
                "text": "J. Glob. Optim."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144180357"
                        ],
                        "name": "E. Brochu",
                        "slug": "E.-Brochu",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Brochu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brochu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2221163"
                        ],
                        "name": "Vlad M. Cora",
                        "slug": "Vlad-M.-Cora",
                        "structuredName": {
                            "firstName": "Vlad",
                            "lastName": "Cora",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vlad M. Cora"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 412,
                                "start": 0
                            }
                        ],
                        "text": "Brochu et al. (2009) provide a comprehensive review of and motivation for Bayesian optimization using GPs. The Efficient Global Optimization (EGO) algorithm for optimizing expensive black-box functions is proposed by Jones et al. (1998) and extended to GPs by Huang et al. (2006). Little is known about theoretical performance of GP optimization. While convergence of EGO is established by Vazquez & Bect (2007), convergence rates have remained elusive."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 66
                            }
                        ],
                        "text": "However, global search heuristics are very effective in practice (Brochu et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 237,
                                "start": 0
                            }
                        ],
                        "text": "Brochu et al. (2009) provide a comprehensive review of and motivation for Bayesian optimization using GPs. The Efficient Global Optimization (EGO) algorithm for optimizing expensive black-box functions is proposed by Jones et al. (1998) and extended to GPs by Huang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 280,
                                "start": 0
                            }
                        ],
                        "text": "Brochu et al. (2009) provide a comprehensive review of and motivation for Bayesian optimization using GPs. The Efficient Global Optimization (EGO) algorithm for optimizing expensive black-box functions is proposed by Jones et al. (1998) and extended to GPs by Huang et al. (2006). Little is known about theoretical performance of GP optimization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 480,
                                "start": 0
                            }
                        ],
                        "text": "Brochu et al. (2009) provide a comprehensive review of and motivation for Bayesian optimization using GPs. The Efficient Global Optimization (EGO) algorithm for optimizing expensive black-box functions is proposed by Jones et al. (1998) and extended to GPs by Huang et al. (2006). Little is known about theoretical performance of GP optimization. While convergence of EGO is established by Vazquez & Bect (2007), convergence rates have remained elusive. Gr\u00fcnew\u00e4lder et al. (2010) consider the pure exploration problem for GPs, where the goal is to find the optimal decision over T rounds, rather than maximize cumulative reward (with no exploration/exploitation dilemma)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 0
                            }
                        ],
                        "text": "Brochu et al. (2009) provide a comprehensive review of and motivation for Bayesian optimization using GPs."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1640103,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd5a26b89f0799db1cbc1dff5607cb6815739fe7",
            "isKey": true,
            "numCitedBy": 1765,
            "numCiting": 129,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions. Bayesian optimization employs the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function. This permits a utility-based selection of the next observation to make on the objective function, which must take into account both exploration (sampling from areas of high uncertainty) and exploitation (sampling areas likely to offer improvement over the current best observation). We also present two detailed extensions of Bayesian optimization, with experiments---active user modelling with preferences, and hierarchical reinforcement learning---and a discussion of the pros and cons of Bayesian optimization based on our experiences."
            },
            "slug": "A-Tutorial-on-Bayesian-Optimization-of-Expensive-to-Brochu-Cora",
            "title": {
                "fragments": [],
                "text": "A Tutorial on Bayesian Optimization of Expensive Cost Functions, with Application to Active User Modeling and Hierarchical Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "A tutorial on Bayesian optimization, a method of finding the maximum of expensive cost functions using the Bayesian technique of setting a prior over the objective function and combining it with evidence to get a posterior function."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49071449"
                        ],
                        "name": "Sandeep Pandey",
                        "slug": "Sandeep-Pandey",
                        "structuredName": {
                            "firstName": "Sandeep",
                            "lastName": "Pandey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeep Pandey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072569"
                        ],
                        "name": "Christopher Olston",
                        "slug": "Christopher-Olston",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Olston",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher Olston"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 106
                            }
                        ],
                        "text": "Examples include choosing advertisements in sponsored search to maximize profit in a click-through model (Pandey & Olston, 2007) or learning optimal control strategies for robots (Lizotte et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 147
                            }
                        ],
                        "text": "UCB algorithms (and GP optimization techniques in general) have been applied to a large number of problems in practice (Kocsis & Szepesva\u0301ri, 2006; Pandey & Olston, 2007; Lizotte et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9036944,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9451a1e23254a2caa631fd7ce3a5c7b0e3de3248",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider how a search engine should select advertisements to display with search results, in order to maximize its revenue. Under the standard \"pay-per-click\" arrangement, revenue depends on how well the displayed advertisements appeal to users. The main difficulty stems from new advertisements whose degree of appeal has yet to be determined. Often the only reliable way of determining appeal is exploration via display to users, which detracts from exploitation of other advertisements known to have high appeal. Budget constraints and finite advertisement lifetimes make it necessary to explore as well as exploit. \n \nIn this paper we study the tradeoff between exploration and exploitation, modeling advertisement placement as a multi-armed bandit problem. We extend traditional bandit formulations to account for budget constraints that occur in search engine advertising markets, and derive theoretical bounds on the performance of a family of algorithms. We measure empirical performance via extensive experiments over real-world data."
            },
            "slug": "Handling-Advertisements-of-Unknown-Quality-in-Pandey-Olston",
            "title": {
                "fragments": [],
                "text": "Handling Advertisements of Unknown Quality in Search Advertising"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Traditional bandit formulations are extended to account for budget constraints that occur in search engine advertising markets, and theoretical bounds on the performance of a family of algorithms are derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 103
                            }
                        ],
                        "text": "The RKHS Hk(D) is a complete subspace of L2(D) of nicely behaved functions, with an\n2This is w.l.o.g. (Rasmussen & Williams, 2006).\ninner product \u3008\u00b7, \u00b7\u3009k obeying the reproducing property: \u3008f, k(x, \u00b7)\u3009k = f(x) for all f \u2208 Hk(D)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 64
                            }
                        ],
                        "text": "Sample functions are differentiable to any order almost surely (Rasmussen & Williams, 2006)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 121
                            }
                        ],
                        "text": "For the former, much progress has been made in machine learning through kernel methods and Gaussian process (GP) models (Rasmussen & Williams, 2006), where smoothness assumptions about f are encoded through the choice of kernel in a flexible nonparametric fashion."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 270
                            }
                        ],
                        "text": "\u2026smoothness without relying on any parametric assumptions, modeling f as a sample from a Gaussian process (GP): a collection of dependent random variables, one for each x \u2208 D, every finite subset of which is multivariate Gaussian distributed in an overall consistent way (Rasmussen & Williams, 2006)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1430472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82266f6103bade9005ec555ed06ba20b5210ff22",
            "isKey": true,
            "numCitedBy": 18076,
            "numCiting": 231,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received growing attention in the machine learning community over the past decade. The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises. Code and datasets can be obtained on the web. Appendices provide mathematical background and a discussion of Gaussian Markov processes."
            },
            "slug": "Gaussian-Processes-for-Machine-Learning-Rasmussen-Williams",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics, and deals with the supervised learning problem for both regression and classification."
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive computation and machine learning"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690162"
                        ],
                        "name": "D. Lizotte",
                        "slug": "D.-Lizotte",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Lizotte",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lizotte"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155450432"
                        ],
                        "name": "Tao Wang",
                        "slug": "Tao-Wang",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687780"
                        ],
                        "name": "Michael Bowling",
                        "slug": "Michael-Bowling",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Bowling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Bowling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714772"
                        ],
                        "name": "Dale Schuurmans",
                        "slug": "Dale-Schuurmans",
                        "structuredName": {
                            "firstName": "Dale",
                            "lastName": "Schuurmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dale Schuurmans"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 180
                            }
                        ],
                        "text": "Examples include choosing advertisements in sponsored search to maximize profit in a click-through model (Pandey & Olston, 2007) or learning optimal control strategies for robots (Lizotte et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 241
                            }
                        ],
                        "text": "Several heuristics for trading off exploration and exploitation in GP optimization have been proposed (such as Expected Improvement, Mockus et al. 1978, and Most Probable Improvement, Mockus 1989) and successfully applied in practice (c.f., Lizotte et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 170
                            }
                        ],
                        "text": "UCB algorithms (and GP optimization techniques in general) have been applied to a large number of problems in practice (Kocsis & Szepesva\u0301ri, 2006; Pandey & Olston, 2007; Lizotte et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 119
                            }
                        ],
                        "text": "UCB algorithms (and GP optimization techniques in general) have been applied to a large number of problems in practice (Kocsis & Szepesv\u00e1ri, 2006; Pandey & Olston, 2007; Lizotte et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10441616,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d89e23d3267c75db75e2055951facc2d74f2908b",
            "isKey": true,
            "numCitedBy": 274,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Gait optimization is a basic yet challenging problem for both quadrupedal and bipedal robots. Although techniques for automating the process exist, most involve local function optimization procedures that suffer from three key drawbacks. Local optimization techniques are naturally plagued by local optima, make no use of the expensive gait evaluations once a local step is taken, and do not explicitly model noise in gait evaluation. These drawbacks increase the need for a large number of gait evaluations, making optimization slow, data inefficient, and manually intensive. We present a Bayesian approach based on Gaussian process regression that addresses all three drawbacks. It uses a global search strategy based on a posterior model inferred from all of the individual noisy evaluations. We demonstrate the technique on a quadruped robot, using it to optimize two different criteria: speed and smoothness. We show in both cases our technique requires dramatically fewer gait evaluations than state-of-the-art local gradient approaches."
            },
            "slug": "Automatic-Gait-Optimization-with-Gaussian-Process-Lizotte-Wang",
            "title": {
                "fragments": [],
                "text": "Automatic Gait Optimization with Gaussian Process Regression"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A Bayesian approach based on Gaussian process regression that addresses all three drawbacks of local optimization procedures, using a global search strategy based on a posterior model inferred from all of the individual noisy evaluations."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107327555"
                        ],
                        "name": "D. Huang",
                        "slug": "D.-Huang",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "115429409"
                        ],
                        "name": "T. Allen",
                        "slug": "T.-Allen",
                        "structuredName": {
                            "firstName": "Theodore",
                            "lastName": "Allen",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Allen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50238440"
                        ],
                        "name": "W. Notz",
                        "slug": "W.-Notz",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Notz",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Notz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2065868465"
                        ],
                        "name": "N. Zheng",
                        "slug": "N.-Zheng",
                        "structuredName": {
                            "firstName": "Ning",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Zheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 172,
                                "start": 153
                            }
                        ],
                        "text": "The Efficient Global Optimization (EGO) algorithm for optimizing expensive black-box functions is proposed by Jones et al. (1998) and extended to GPs by Huang et al. (2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14688276,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0c51104c58c3e05f487d87ee94ce2e3b2d11dce6",
            "isKey": false,
            "numCitedBy": 584,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new method that extends the efficient global optimization to address stochastic black-box systems. The method is based on a kriging meta-model that provides a global prediction of the objective values and a measure of prediction uncertainty at every point. The criterion for the infill sample selection is an augmented expected improvement function with desirable properties for stochastic responses. The method is empirically compared with the revised simplex search, the simultaneous perturbation stochastic approximation, and the DIRECT methods using six test problems from the literature. An application case study on an inventory system is also documented. The results suggest that the proposed method has excellent consistency and efficiency in finding global optimal solutions, and is particularly useful for expensive systems."
            },
            "slug": "Global-Optimization-of-Stochastic-Black-Box-Systems-Huang-Allen",
            "title": {
                "fragments": [],
                "text": "Global Optimization of Stochastic Black-Box Systems via Sequential Kriging Meta-Models"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The proposed method is based on a kriging meta-model that provides a global prediction of the objective values and a measure of prediction uncertainty at every point and has excellent consistency and efficiency in finding global optimal solutions."
            },
            "venue": {
                "fragments": [],
                "text": "J. Glob. Optim."
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34908205"
                        ],
                        "name": "Levente Kocsis",
                        "slug": "Levente-Kocsis",
                        "structuredName": {
                            "firstName": "Levente",
                            "lastName": "Kocsis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Levente Kocsis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40868287"
                        ],
                        "name": "Csaba Szepesvari",
                        "slug": "Csaba-Szepesvari",
                        "structuredName": {
                            "firstName": "Csaba",
                            "lastName": "Szepesvari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Csaba Szepesvari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 101
                            }
                        ],
                        "text": "6 is motivated by the UCB algorithm for the classical multi-armed bandit problem (Auer et al., 2002; Kocsis & Szepesva\u0301ri, 2006)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 120
                            }
                        ],
                        "text": "UCB algorithms (and GP optimization techniques in general) have been applied to a large number of problems in practice (Kocsis & Szepesva\u0301ri, 2006; Pandey & Olston, 2007; Lizotte et al., 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15184765,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e635d81a617d1239232a9c9a11a196c53dab8240",
            "isKey": false,
            "numCitedBy": 2651,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "For large state-space Markovian Decision Problems Monte-Carlo planning is one of the few viable approaches to find near-optimal solutions. In this paper we introduce a new algorithm, UCT, that applies bandit ideas to guide Monte-Carlo planning. In finite-horizon or discounted MDPs the algorithm is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling. Experimental results show that in several domains, UCT is significantly more efficient than its alternatives."
            },
            "slug": "Bandit-Based-Monte-Carlo-Planning-Kocsis-Szepesvari",
            "title": {
                "fragments": [],
                "text": "Bandit Based Monte-Carlo Planning"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new algorithm is introduced, UCT, that applies bandit ideas to guide Monte-Carlo planning and is shown to be consistent and finite sample bounds are derived on the estimation error due to sampling."
            },
            "venue": {
                "fragments": [],
                "text": "ECML"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39886803"
                        ],
                        "name": "R. Woodard",
                        "slug": "R.-Woodard",
                        "structuredName": {
                            "firstName": "Roger",
                            "lastName": "Woodard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Woodard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 29921342,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "c1a6bc3e20f8d0a7244211e96fdeeb59dfc7b9b0",
            "isKey": false,
            "numCitedBy": 2866,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "1 Linear Prediction.- 1.1 Introduction.- 1.2 Best linear prediction.- Exercises.- 1.3 Hilbert spaces and prediction.- Exercises.- 1.4 An example of a poor BLP.- Exercises.- 1.5 Best linear unbiased prediction.- Exercises.- 1.6 Some recurring themes.- The Matern model.- BLPs and BLUPs.- Inference for differentiable random fields.- Nested models are not tenable.- 1.7 Summary of practical suggestions.- 2 Properties of Random Fields.- 2.1 Preliminaries.- Stationarity.- Isotropy.- Exercise.- 2.2 The turning bands method.- Exercise.- 2.3 Elementary properties of autocovariance functions.- Exercise.- 2.4 Mean square continuity and differentiability.- Exercises.- 2.5 Spectral methods.- Spectral representation of a random field.- Bochner's Theorem.- Exercises.- 2.6 Two corresponding Hilbert spaces.- An application to mean square differentiability.- Exercises.- 2.7 Examples of spectral densities on 112.- Rational spectral densities.- Principal irregular term.- Gaussian model.- Triangular autocovariance functions.- Matern class.- Exercises.- 2.8 Abelian and Tauberian theorems.- Exercises.- 2.9 Random fields with nonintegrable spectral densities.- Intrinsic random functions.- Semivariograms.- Generalized random fields.- Exercises.- 2.10 Isotropic autocovariance functions.- Characterization.- Lower bound on isotropic autocorrelation functions.- Inversion formula.- Smoothness properties.- Matern class.- Spherical model.- Exercises.- 2.11 Tensor product autocovariances.- Exercises.- 3 Asymptotic Properties of Linear Predictors.- 3.1 Introduction.- 3.2 Finite sample results.- Exercise.- 3.3 The role of asymptotics.- 3.4 Behavior of prediction errors in the frequency domain.- Some examples.- Relationship to filtering theory.- Exercises.- 3.5 Prediction with the wrong spectral density.- Examples of interpolation.- An example with a triangular autocovariance function.- More criticism of Gaussian autocovariance functions.- Examples of extrapolation.- Pseudo-BLPs with spectral densities misspecified at high frequencies.- Exercises.- 3.6 Theoretical comparison of extrapolation and ointerpolation.- An interpolation problem.- An extrapolation problem.- Asymptotics for BLPs.- Inefficiency of pseudo-BLPs with misspecified high frequency behavior.- Presumed mses for pseudo-BLPs with misspecified high frequency behavior.- Pseudo-BLPs with correctly specified high frequency behavior.- Exercises.- 3.7 Measurement errors.- Some asymptotic theory.- Exercises.- 3.8 Observations on an infinite lattice.- Characterizing the BLP.- Bound on fraction of mse of BLP attributable to a set of frequencies.- Asymptotic optimality of pseudo-BLPs.- Rates of convergence to optimality.- Pseudo-BLPs with a misspecified mean function.- Exercises.- 4 Equivalence of Gaussian Measures and Prediction.- 4.1 Introduction.- 4.2 Equivalence and orthogonality of Gaussian measures.- Conditions for orthogonality.- Gaussian measures are equivalent or orthogonal.- Determining equivalence or orthogonality for periodic random fields.- Determining equivalence or orthogonality for nonperiodic random fields.- Measurement errors and equivalence and orthogonality.- Proof of Theorem 1.- Exercises.- 4.3 Applications of equivalence of Gaussian measures to linear prediction.- Asymptotically optimal pseudo-BLPs.- Observations not part of a sequence.- A theorem of Blackwell and Dubins.- Weaker conditions for asymptotic optimality of pseudo-BLPs.- Rates of convergence to asymptotic optimality.- Asymptotic optimality of BLUPs.- Exercises.- 4.4 Jeffreys's law.- A Bayesian version.- Exercises.- 5 Integration of Random Fields.- 5.1 Introduction.- 5.2 Asymptotic properties of simple average.- Results for sufficiently smooth random fields.- Results for sufficiently rough random fields.- Exercises.- 5.3 Observations on an infinite lattice.- Asymptotic mse of BLP.- Asymptotic optimality of simple average.- Exercises.- 5.4 Improving on the sample mean.- Approximating $$\\int_0^1 {\\exp } (ivt)dt$$.- Approximating $$\\int_{{{[0,1]}^d}} {\\exp (i{\\omega ^T}x)} dx$$ in more than one dimension.- Asymptotic properties of modified predictors.- Are centered systematic samples good designs?.- Exercises.- 5.5 Numerical results.- Exercises.- 6 Predicting With Estimated Parameters.- 6.1 Introduction.- 6.2 Microergodicity and equivalence and orthogonality of Gaussian measures.- Observations with measurement error.- Exercises.- 6.3 Is statistical inference for differentiable processes possible?.- An example where it is possible.- Exercises.- 6.4 Likelihood Methods.- Restricted maximum likelihood estimation.- Gaussian assumption.- Computational issues.- Some asymptotic theory.- Exercises.- 6.5 Matern model.- Exercise.- 6.6 A numerical study of the Fisher information matrix under the Matern model.- No measurement error and?unknown.- No measurement error and?known.- Observations with measurement error.- Conclusions.- Exercises.- 6.7 Maximum likelihood estimation for a periodic version of the Matern model.- Discrete Fourier transforms.- Periodic case.- Asymptotic results.- Exercises.- 6.8 Predicting with estimated parameters.- Jeffreys's law revisited.- Numerical results.- Some issues regarding asymptotic optimality.- Exercises.- 6.9 An instructive example of plug-in prediction.- Behavior of plug-in predictions.- Cross-validation.- Application of Matern model.- Conclusions.- Exercises.- 6.10 Bayesian approach.- Application to simulated data.- Exercises.- A Multivariate Normal Distributions.- B Symbols.- References."
            },
            "slug": "Interpolation-of-Spatial-Data:-Some-Theory-for-Woodard",
            "title": {
                "fragments": [],
                "text": "Interpolation of Spatial Data: Some Theory for Kriging"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This chapter discusses the role of asymptotics for BLPs, and applications of equivalence and orthogonality of Gaussian measures to linear prediction, and the importance of Observations not part of a sequence."
            },
            "venue": {
                "fragments": [],
                "text": "Technometrics"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685083"
                        ],
                        "name": "N. Cristianini",
                        "slug": "N.-Cristianini",
                        "structuredName": {
                            "firstName": "Nello",
                            "lastName": "Cristianini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Cristianini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145536952"
                        ],
                        "name": "J. Kandola",
                        "slug": "J.-Kandola",
                        "structuredName": {
                            "firstName": "Jaz",
                            "lastName": "Kandola",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kandola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 0
                            }
                        ],
                        "text": "Shawe-Taylor et al.\n(2005) show that E[S] \u2265 \u2211T\u2217 t=1 \u03bbt."
                    },
                    "intents": []
                }
            ],
            "corpusId": 522771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "24b2ee598765b1369ce425664f5ba8d412ba748b",
            "isKey": false,
            "numCitedBy": 155,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, the relationships between the eigenvalues of the m/spl times/m Gram matrix K for a kernel /spl kappa/(/spl middot/,/spl middot/) corresponding to a sample x/sub 1/,...,x/sub m/ drawn from a density p(x) and the eigenvalues of the corresponding continuous eigenproblem is analyzed. The differences between the two spectra are bounded and a performance bound on kernel principal component analysis (PCA) is provided showing that good performance can be expected even in very-high-dimensional feature spaces provided the sample eigenvalues fall sufficiently quickly."
            },
            "slug": "On-the-eigenspectrum-of-the-gram-matrix-and-the-of-Shawe-Taylor-Williams",
            "title": {
                "fragments": [],
                "text": "On the eigenspectrum of the gram matrix and the generalization error of kernel-PCA"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The differences between the two spectra are bounded and a performance bound on kernel principal component analysis (PCA) is provided showing that good performance can be expected even in very-high-dimensional feature spaces provided the sample eigenvalues fall sufficiently quickly."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Information Theory"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145648751"
                        ],
                        "name": "H. Robbins",
                        "slug": "H.-Robbins",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Robbins",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Robbins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 79
                            }
                        ],
                        "text": "Predominant approaches to this problem include the multi-armed bandit paradigm (Robbins, 1952), where the goal is to maximize cumulative reward by optimally balancing exploration and exploitation, and experimental design (Chaloner & Verdinelli, 1995), where the function is to be explored globally with as few evaluations as possible, for example by maximizing information"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 80
                            }
                        ],
                        "text": "Predominant approaches to this problem include the multi-armed bandit paradigm (Robbins, 1952), where the goal is to maximize cumulative reward by optimally balancing exploration and exploitation, and experimental design (Chaloner & Verdinelli, 1995), where the function is to be explored globally\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15556973,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b77aa53b45f6aa1873780d0fa7aad50efb422458",
            "isKey": false,
            "numCitedBy": 1859,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Until recently, statistical theory has been restricted to the design and analysis of sampling experiments in which the size and composition of the samples are completely determined before the experimentation begins. The reasons for this are partly historical, dating back to the time when the statistician was consulted, if at all, only after the experiment was over, and partly intrinsic in the mathematical difficulty of working with anything but a fixed number of independent random variables. A major advance now appears to be in the making with the creation of a theory of the sequential design of experiments, in which the size and composition of the samples are not fixed in advance but are functions of the observations themselves."
            },
            "slug": "Some-aspects-of-the-sequential-design-of-Robbins",
            "title": {
                "fragments": [],
                "text": "Some aspects of the sequential design of experiments"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1952
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752732"
                        ],
                        "name": "T. Cover",
                        "slug": "T.-Cover",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Cover",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Cover"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115282352"
                        ],
                        "name": "Joy A. Thomas",
                        "slug": "Joy-A.-Thomas",
                        "structuredName": {
                            "firstName": "Joy",
                            "lastName": "Thomas",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joy A. Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 86
                            }
                        ],
                        "text": "Furthermore, the information gain is monotonic in A (i.e., F (A) \u2264 F (B) whenever A \u2286 B) (Cover & Thomas, 1991)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 147
                            }
                        ],
                        "text": "\u2026\u201cED\u201d; see Chaloner & Verdinelli 1995), where the informativeness of a set of sampling points A \u2282 D about f is measured by the information gain (c.f., Cover & Thomas 1991), which is the mutual information between f and observations yA = fA+ A at these points:\nI(yA; f) = H(yA)\u2212H(yA|f),\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 190432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dbdb4209626fd92d2436a058663206216036e68",
            "isKey": false,
            "numCitedBy": 42946,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface to the Second Edition. Preface to the First Edition. Acknowledgments for the Second Edition. Acknowledgments for the First Edition. 1. Introduction and Preview. 1.1 Preview of the Book. 2. Entropy, Relative Entropy, and Mutual Information. 2.1 Entropy. 2.2 Joint Entropy and Conditional Entropy. 2.3 Relative Entropy and Mutual Information. 2.4 Relationship Between Entropy and Mutual Information. 2.5 Chain Rules for Entropy, Relative Entropy, and Mutual Information. 2.6 Jensen's Inequality and Its Consequences. 2.7 Log Sum Inequality and Its Applications. 2.8 Data-Processing Inequality. 2.9 Sufficient Statistics. 2.10 Fano's Inequality. Summary. Problems. Historical Notes. 3. Asymptotic Equipartition Property. 3.1 Asymptotic Equipartition Property Theorem. 3.2 Consequences of the AEP: Data Compression. 3.3 High-Probability Sets and the Typical Set. Summary. Problems. Historical Notes. 4. Entropy Rates of a Stochastic Process. 4.1 Markov Chains. 4.2 Entropy Rate. 4.3 Example: Entropy Rate of a Random Walk on a Weighted Graph. 4.4 Second Law of Thermodynamics. 4.5 Functions of Markov Chains. Summary. Problems. Historical Notes. 5. Data Compression. 5.1 Examples of Codes. 5.2 Kraft Inequality. 5.3 Optimal Codes. 5.4 Bounds on the Optimal Code Length. 5.5 Kraft Inequality for Uniquely Decodable Codes. 5.6 Huffman Codes. 5.7 Some Comments on Huffman Codes. 5.8 Optimality of Huffman Codes. 5.9 Shannon-Fano-Elias Coding. 5.10 Competitive Optimality of the Shannon Code. 5.11 Generation of Discrete Distributions from Fair Coins. Summary. Problems. Historical Notes. 6. Gambling and Data Compression. 6.1 The Horse Race. 6.2 Gambling and Side Information. 6.3 Dependent Horse Races and Entropy Rate. 6.4 The Entropy of English. 6.5 Data Compression and Gambling. 6.6 Gambling Estimate of the Entropy of English. Summary. Problems. Historical Notes. 7. Channel Capacity. 7.1 Examples of Channel Capacity. 7.2 Symmetric Channels. 7.3 Properties of Channel Capacity. 7.4 Preview of the Channel Coding Theorem. 7.5 Definitions. 7.6 Jointly Typical Sequences. 7.7 Channel Coding Theorem. 7.8 Zero-Error Codes. 7.9 Fano's Inequality and the Converse to the Coding Theorem. 7.10 Equality in the Converse to the Channel Coding Theorem. 7.11 Hamming Codes. 7.12 Feedback Capacity. 7.13 Source-Channel Separation Theorem. Summary. Problems. Historical Notes. 8. Differential Entropy. 8.1 Definitions. 8.2 AEP for Continuous Random Variables. 8.3 Relation of Differential Entropy to Discrete Entropy. 8.4 Joint and Conditional Differential Entropy. 8.5 Relative Entropy and Mutual Information. 8.6 Properties of Differential Entropy, Relative Entropy, and Mutual Information. Summary. Problems. Historical Notes. 9. Gaussian Channel. 9.1 Gaussian Channel: Definitions. 9.2 Converse to the Coding Theorem for Gaussian Channels. 9.3 Bandlimited Channels. 9.4 Parallel Gaussian Channels. 9.5 Channels with Colored Gaussian Noise. 9.6 Gaussian Channels with Feedback. Summary. Problems. Historical Notes. 10. Rate Distortion Theory. 10.1 Quantization. 10.2 Definitions. 10.3 Calculation of the Rate Distortion Function. 10.4 Converse to the Rate Distortion Theorem. 10.5 Achievability of the Rate Distortion Function. 10.6 Strongly Typical Sequences and Rate Distortion. 10.7 Characterization of the Rate Distortion Function. 10.8 Computation of Channel Capacity and the Rate Distortion Function. Summary. Problems. Historical Notes. 11. Information Theory and Statistics. 11.1 Method of Types. 11.2 Law of Large Numbers. 11.3 Universal Source Coding. 11.4 Large Deviation Theory. 11.5 Examples of Sanov's Theorem. 11.6 Conditional Limit Theorem. 11.7 Hypothesis Testing. 11.8 Chernoff-Stein Lemma. 11.9 Chernoff Information. 11.10 Fisher Information and the Cram-er-Rao Inequality. Summary. Problems. Historical Notes. 12. Maximum Entropy. 12.1 Maximum Entropy Distributions. 12.2 Examples. 12.3 Anomalous Maximum Entropy Problem. 12.4 Spectrum Estimation. 12.5 Entropy Rates of a Gaussian Process. 12.6 Burg's Maximum Entropy Theorem. Summary. Problems. Historical Notes. 13. Universal Source Coding. 13.1 Universal Codes and Channel Capacity. 13.2 Universal Coding for Binary Sequences. 13.3 Arithmetic Coding. 13.4 Lempel-Ziv Coding. 13.5 Optimality of Lempel-Ziv Algorithms. Compression. Summary. Problems. Historical Notes. 14. Kolmogorov Complexity. 14.1 Models of Computation. 14.2 Kolmogorov Complexity: Definitions and Examples. 14.3 Kolmogorov Complexity and Entropy. 14.4 Kolmogorov Complexity of Integers. 14.5 Algorithmically Random and Incompressible Sequences. 14.6 Universal Probability. 14.7 Kolmogorov complexity. 14.9 Universal Gambling. 14.10 Occam's Razor. 14.11 Kolmogorov Complexity and Universal Probability. 14.12 Kolmogorov Sufficient Statistic. 14.13 Minimum Description Length Principle. Summary. Problems. Historical Notes. 15. Network Information Theory. 15.1 Gaussian Multiple-User Channels. 15.2 Jointly Typical Sequences. 15.3 Multiple-Access Channel. 15.4 Encoding of Correlated Sources. 15.5 Duality Between Slepian-Wolf Encoding and Multiple-Access Channels. 15.6 Broadcast Channel. 15.7 Relay Channel. 15.8 Source Coding with Side Information. 15.9 Rate Distortion with Side Information. 15.10 General Multiterminal Networks. Summary. Problems. Historical Notes. 16. Information Theory and Portfolio Theory. 16.1 The Stock Market: Some Definitions. 16.2 Kuhn-Tucker Characterization of the Log-Optimal Portfolio. 16.3 Asymptotic Optimality of the Log-Optimal Portfolio. 16.4 Side Information and the Growth Rate. 16.5 Investment in Stationary Markets. 16.6 Competitive Optimality of the Log-Optimal Portfolio. 16.7 Universal Portfolios. 16.8 Shannon-McMillan-Breiman Theorem (General AEP). Summary. Problems. Historical Notes. 17. Inequalities in Information Theory. 17.1 Basic Inequalities of Information Theory. 17.2 Differential Entropy. 17.3 Bounds on Entropy and Relative Entropy. 17.4 Inequalities for Types. 17.5 Combinatorial Bounds on Entropy. 17.6 Entropy Rates of Subsets. 17.7 Entropy and Fisher Information. 17.8 Entropy Power Inequality and Brunn-Minkowski Inequality. 17.9 Inequalities for Determinants. 17.10 Inequalities for Ratios of Determinants. Summary. Problems. Historical Notes. Bibliography. List of Symbols. Index."
            },
            "slug": "Elements-of-Information-Theory-Cover-Thomas",
            "title": {
                "fragments": [],
                "text": "Elements of Information Theory"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The author examines the role of entropy, inequality, and randomness in the design of codes and the construction of codes in the rapidly changing environment."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5054589"
                        ],
                        "name": "K. Chaloner",
                        "slug": "K.-Chaloner",
                        "structuredName": {
                            "firstName": "Kathryn",
                            "lastName": "Chaloner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chaloner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1816502"
                        ],
                        "name": "I. Verdinelli",
                        "slug": "I.-Verdinelli",
                        "structuredName": {
                            "firstName": "Isabella",
                            "lastName": "Verdinelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Verdinelli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 144
                            }
                        ],
                        "text": "\u2026(Robbins, 1952), where the goal is to maximize cumulative reward by optimally balancing exploration and exploitation, and experimental design (Chaloner & Verdinelli, 1995), where the function is to be explored globally with as few evaluations as possible, for example by maximizing\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 77
                            }
                        ],
                        "text": "This question comes down to Bayesian Experimental Design (henceforth \u201cED\u201d; see Chaloner & Verdinelli 1995), where the informativeness of a set of sampling points A \u2282 D about f is measured by the information gain (c.f., Cover & Thomas 1991), which is the mutual information between f and observations\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 13676847,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f79332b1361e9eaa9da3327f83f57dcac5cd11d",
            "isKey": false,
            "numCitedBy": 1718,
            "numCiting": 319,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reviews the literature on Bayesian experimental design, both for linear and nonlinear models. A uniied view of the topic is presented by putting experimental design in a decision theoretic framework. This framework justiies many optimality criteria, and opens new possibilities. Various design criteria become part of a single, coherent approach."
            },
            "slug": "Bayesian-Experimental-Design:-A-Review-Chaloner-Verdinelli",
            "title": {
                "fragments": [],
                "text": "Bayesian Experimental Design: A Review"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This paper reviews the literature on Bayesian experimental design, both for linear and nonlinear models, and presents a uniied view of the topic by putting experimental design in a decision theoretic framework."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145733439"
                        ],
                        "name": "G. Wahba",
                        "slug": "G.-Wahba",
                        "structuredName": {
                            "firstName": "Grace",
                            "lastName": "Wahba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wahba"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 23
                            }
                        ],
                        "text": "Then, Mercer\u2019s theorem (Wahba, 1990) states that the corresponding kernel operator has a discrete eigenspectrum {(\u03bbs, \u03c6s(\u00b7))}, and\nk(x,x\u2032) = \u2211\ns\u22651 \u03bbs\u03c6s(x)\u03c6s(x\n\u2032),\nwhere \u03bb1 \u2265 \u03bb2 \u2265 \u00b7 \u00b7 \u00b7 \u2265 0, and E\u00b5[\u03c6s(x)\u03c6t(x)] = \u03b4s,t."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 58
                            }
                        ],
                        "text": "In contrast, if f \u223c GP(0, k(x,x\u2032)), then \u2016f\u2016k = \u221e almost surely (Wahba, 1990): sample paths are rougher than RKHS functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 55
                            }
                        ],
                        "text": "The notion of reproducing kernel Hilbert spaces (RKHS, Wahba 1990) is intimately related to GPs and their covariance functions k(x,x\u2032)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 121858740,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "e786caa59202d923ccaae00ae6a4682eec92699b",
            "isKey": true,
            "numCitedBy": 5217,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Foreword 1. Background 2. More splines 3. Equivalence and perpendicularity, or, what's so special about splines? 4. Estimating the smoothing parameter 5. 'Confidence intervals' 6. Partial spline models 7. Finite dimensional approximating subspaces 8. Fredholm integral equations of the first kind 9. Further nonlinear generalizations 10. Additive and interaction splines 11. Numerical methods 12. Special topics Bibliography Author index."
            },
            "slug": "Spline-Models-for-Observational-Data-Wahba",
            "title": {
                "fragments": [],
                "text": "Spline Models for Observational Data"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32854168"
                        ],
                        "name": "D. Freedman",
                        "slug": "D.-Freedman",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Freedman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Freedman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 113
                            }
                        ],
                        "text": "In our analysis, we use the following Bernstein-type concentration inequality for martingale differences, due to Freedman (1975) (see also Theorem 3.15 of McDiarmid 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123246626,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "531aab3000f5373d2c99c2c171b7185f83f3a167",
            "isKey": true,
            "numCitedBy": 588,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "On-Tail-Probabilities-for-Martingales-Freedman",
            "title": {
                "fragments": [],
                "text": "On Tail Probabilities for Martingales"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 155
                            }
                        ],
                        "text": "In our analysis, we use the following Bernstein-type concentration inequality for martingale differences, due to Freedman (1975) (see also Theorem 3.15 of McDiarmid 1998)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Concentration. In Probabilistiic Methods for Algorithmic Discrete Mathematics"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2210196"
                        ],
                        "name": "J. Mockus",
                        "slug": "J.-Mockus",
                        "structuredName": {
                            "firstName": "Jonas",
                            "lastName": "Mockus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mockus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 184
                            }
                        ],
                        "text": "Several heuristics for trading off exploration and exploitation in GP optimization have been proposed (such as Expected Improvement, Mockus et al. 1978, and Most Probable Improvement, Mockus 1989) and successfully applied in practice (c.f., Lizotte et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 119855746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "701d221508f65926edf214d07b43f7fc38131665",
            "isKey": false,
            "numCitedBy": 286,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Bayesian-approach-to-global-optimization-Mockus",
            "title": {
                "fragments": [],
                "text": "The Bayesian approach to global optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152731"
                        ],
                        "name": "Louis Dorard",
                        "slug": "Louis-Dorard",
                        "structuredName": {
                            "firstName": "Louis",
                            "lastName": "Dorard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Louis Dorard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090521"
                        ],
                        "name": "D. Glowacka",
                        "slug": "D.-Glowacka",
                        "structuredName": {
                            "firstName": "Dorota",
                            "lastName": "Glowacka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Glowacka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1404459229"
                        ],
                        "name": "J. Shawe-Taylor",
                        "slug": "J.-Shawe-Taylor",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Shawe-Taylor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Shawe-Taylor"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 151
                            }
                        ],
                        "text": "Among competing criteria for GP optimization (see Section 1), a variant of the GP-UCB rule has been demonstrated to be effective for this application (Dorard et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 59830296,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c2c407906c5d0560a5cc68229e94f35b70a45bb8",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "GAUSSIAN-PROCESS-MODELLING-OF-DEPENDENCIES-IN-Dorard-Glowacka",
            "title": {
                "fragments": [],
                "text": "GAUSSIAN PROCESS MODELLING OF DEPENDENCIES IN MULTI-ARMED BANDIT PROBLEMS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 54
                            }
                        ],
                        "text": "L\nG ]\n9 J\nun 2\n(also see Auer 2002; Dani et al. 2007; Abernethy et al. 2008; Rusmevichientong & Tsitsiklis 2008), explicitly dependent on the dimensionality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An efficient algorithm for linear bandit optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2008
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 25
                            }
                        ],
                        "text": "L\nG ]\n9 J\nun 2\n(also see Auer 2002; Dani et al. 2007; Abernethy et al. 2008; Rusmevichientong & Tsitsiklis 2008), explicitly dependent on the dimensionality."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 149
                            }
                        ],
                        "text": "In particular, we analyze the Gaussian Process Upper Confidence Bound (GP-UCB) algorithm, a simple and intuitive Bayesian method (Auer et al., 2002; Auer, 2002; Dani et al., 2008)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using confidence bounds for exploitationexploration"
            },
            "venue": {
                "fragments": [],
                "text": "trade-offs. JMLR,"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 133
                            }
                        ],
                        "text": "Several heuristics for trading off exploration and exploitation in GP optimization have been proposed (such as Expected Improvement, Mockus et al. 1978, and Most Probable Improvement, Mockus 1989) and successfully applied in practice (c.f., Lizotte et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Toward Global Optimization"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 288,
                                "start": 266
                            }
                        ],
                        "text": "\u2026exploration and exploitation, and experimental design (Chaloner & Verdinelli, 1995), where the function is to be explored globally with as few evaluations as possible, for example by maximizing information\n1This is the longer version of our paper in ICML 2010; see Srinivas et al. (2010)\ngain."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 58
                            }
                        ],
                        "text": "This is the longer version of our paper in ICML 2010; see Srinivas et al. (2010) gain."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Gaussian process optimization in the bandit setting: No regret and experimental design"
            },
            "venue": {
                "fragments": [],
                "text": "In ICML,"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 133
                            }
                        ],
                        "text": "Several heuristics for trading off exploration and exploitation in GP optimization have been proposed (such as Expected Improvement, Mockus et al. 1978, and Most Probable Improvement, Mockus 1989) and successfully applied in practice (c.f., Lizotte et al. 2007)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Toward Global Optimization, volume 2, chapter Bayesian Methods for Seeking the Extremum"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1978
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 40
                            }
                        ],
                        "text": "It holds for stationary kernels k(x,x\u2032) = k(x \u2212 x\u2032) which are four times differentiable (Theorem 5 of Ghosal & Roy (2006)), such as the Squared Exponential and Mat\u00e9rn kernels with \u03bd > 2 (see Section 5."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "is the spectrum of the kernel matrix KDT . Here, if T > nT , then mt = 0 for t > nT"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "submodular. Furthermore, the information gain is monotonic in A (i.e., F (A) \u2264 F (B) whenever A \u2286 B"
            },
            "venue": {
                "fragments": [],
                "text": "(Cover & Thomas,"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 24,
            "methodology": 18
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 39,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Gaussian-Process-Optimization-in-the-Bandit-No-and-Srinivas-Krause/0c8413ab8de0c1b8f2e86402b8d737d94371610f?sort=total-citations"
}