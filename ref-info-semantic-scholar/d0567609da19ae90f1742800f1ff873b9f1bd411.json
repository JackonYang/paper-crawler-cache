{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21876257"
                        ],
                        "name": "Congjie Mi",
                        "slug": "Congjie-Mi",
                        "structuredName": {
                            "firstName": "Congjie",
                            "lastName": "Mi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Congjie Mi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110355636"
                        ],
                        "name": "Yuan Xu",
                        "slug": "Yuan-Xu",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143663454"
                        ],
                        "name": "Hong Lu",
                        "slug": "Hong-Lu",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905953"
                        ],
                        "name": "X. Xue",
                        "slug": "X.-Xue",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Xue"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[13] present a text extraction approach based"
                    },
                    "intents": []
                }
            ],
            "corpusId": 16899024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "784d308afa4ba71076cd33deff43d99edc4efc47",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a novel approach to detect and segment static superimposed texts by utilizing multiple video frame information. For text detection, multiple frames are used to verify the appearance of the text regions which have been detected on a single frame. In order to refine the text regions, text detection is performed again on a synthesized image, which is produced by minimum/maximum pixel search on consecutive frames. In text segmentation, we exploit edge feature to further remove complex background in addition to traditional gray-value integration. Experimental results demonstrate the effectiveness of the proposed method"
            },
            "slug": "A-Novel-Video-Text-Extraction-Approach-Based-on-Mi-Xu",
            "title": {
                "fragments": [],
                "text": "A Novel Video Text Extraction Approach Based on Multiple Frames"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "A novel approach to detect and segment static superimposed texts by utilizing multiple video frame information and exploiting edge feature to further remove complex background in addition to traditional gray-value integration is described."
            },
            "venue": {
                "fragments": [],
                "text": "2005 5th International Conference on Information Communications & Signal Processing"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47538088"
                        ],
                        "name": "Duarte Palma",
                        "slug": "Duarte-Palma",
                        "structuredName": {
                            "firstName": "Duarte",
                            "lastName": "Palma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Duarte Palma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145601383"
                        ],
                        "name": "J. Ascenso",
                        "slug": "J.-Ascenso",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Ascenso",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Ascenso"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144367287"
                        ],
                        "name": "F. Pereira",
                        "slug": "F.-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pereira"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[19] extracts the text in videos based on motion analysis and temporal redundancy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 773276,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5ea1f70e8f02a09700e6c19c2ff6ba736c74bc6",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "It is well known that the text that appears in a video scene or is graphically added to it is an important source of semantic information for indexing and retrieval, notably in the context of video databases. This paper proposes an improved algorithm for the automatic extraction of text in digital video; its major strengths are its robustness in terms of text skew and its improved performance in dealing with scene text. The system is based on a segmentation approach, using geometrical and spatial analyses for text detection. After, temporal redundancy is exploited to improve the detection performance by means of motion analysis. The output of the text detection step is then directly passed to a standard OCR software package in order to obtain the detected text as ASCII characters."
            },
            "slug": "Automatic-Text-Extraction-in-Digital-Video-Based-on-Palma-Ascenso",
            "title": {
                "fragments": [],
                "text": "Automatic Text Extraction in Digital Video Based on Motion Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "An improved algorithm for the automatic extraction of text in digital video is proposed, based on a segmentation approach, using geometrical and spatial analyses for text detection and its major strengths are its robustness in terms of text skew and its improved performance in dealing with scene text."
            },
            "venue": {
                "fragments": [],
                "text": "ICIAR"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145295484"
                        ],
                        "name": "Anil K. Jain",
                        "slug": "Anil-K.-Jain",
                        "structuredName": {
                            "firstName": "Anil",
                            "lastName": "Jain",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anil K. Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[3] presented comprehensive surveys of the text extraction approaches for images and videos proposed before 2000 and 2004 respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 132
                            }
                        ],
                        "text": "to 2003, only a few text extraction approaches considered the temporal nature of video [1], very little work was done on scene text [3], and objective performance evaluation metrics were scarce."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5999466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf72be1fe814ef2ee9d65633dc3226f80f0785",
            "isKey": false,
            "numCitedBy": 937,
            "numCiting": 100,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-information-extraction-in-images-and-video:-a-Jung-Kim",
            "title": {
                "fragments": [],
                "text": "Text information extraction in images and video: a survey"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2263912"
                        ],
                        "name": "Jingchao Zhou",
                        "slug": "Jingchao-Zhou",
                        "structuredName": {
                            "firstName": "Jingchao",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jingchao Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112280400"
                        ],
                        "name": "Lei Xu",
                        "slug": "Lei-Xu",
                        "structuredName": {
                            "firstName": "Lei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lei Xu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2658590"
                        ],
                        "name": "Baihua Xiao",
                        "slug": "Baihua-Xiao",
                        "structuredName": {
                            "firstName": "Baihua",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baihua Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145841729"
                        ],
                        "name": "Ruwei Dai",
                        "slug": "Ruwei-Dai",
                        "structuredName": {
                            "firstName": "Ruwei",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruwei Dai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053740005"
                        ],
                        "name": "Si si",
                        "slug": "Si-si",
                        "structuredName": {
                            "firstName": "Si",
                            "lastName": "si",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Si si"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Zhou et al. [ 9 ] utilize edge information and geometrical constraints to form a coarse-to-fine approach to determine text regions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11104278,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "f1407d98cf7d73542f743a08755eaeb28832bdae",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel system to extract caption text in video. Firstly, text regions are detected primarily with emphasis on the recall rate. Then a multiple stage verification scheme is adopted to discard false alarms and boost the precision rate. Secondly, a text polarity estimation algorithm is provided. Based on it, multiple frame enhancement is conducted to strengthen the contrast between text and its background. Finally, a connected component filtering method is proposed to generate clear segmentation results and improve recognition performance. Experimental results confirm that the proposed system is robust and efficient."
            },
            "slug": "A-robust-system-for-text-extraction-in-video-Zhou-Xu",
            "title": {
                "fragments": [],
                "text": "A robust system for text extraction in video"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A multiple stage verification scheme is adopted to discard false alarms and boost the precision rate, and a text polarity estimation algorithm is provided to strengthen the contrast between text and its background."
            },
            "venue": {
                "fragments": [],
                "text": "2007 International Conference on Machine Vision"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48873890"
                        ],
                        "name": "Qifeng Liu",
                        "slug": "Qifeng-Liu",
                        "structuredName": {
                            "firstName": "Qifeng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qifeng Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "95055206"
                        ],
                        "name": "Cheolkon Jung",
                        "slug": "Cheolkon-Jung",
                        "structuredName": {
                            "firstName": "Cheolkon",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheolkon Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2198635"
                        ],
                        "name": "Youngsu Moon",
                        "slug": "Youngsu-Moon",
                        "structuredName": {
                            "firstName": "Youngsu",
                            "lastName": "Moon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Youngsu Moon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[14] use stroke filter to segment text by"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18522972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23e9a4a9ebdff7fdbf8119241bd62d144a426f31",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Most existing methods of text segmentation in video images are not robust because they do not consider the intrinsic characteristics of text. In this paper, we propose a novel method of text segmentation based on stroke filter (SF). First, we give the definition of text, which is realized in the form of stroke filter based on local region analysis. Based on stroke filter response, text polarity determination and local region growing modules are performed successively. The effectiveness of our method is validated by experiments on a challenging database."
            },
            "slug": "Text-segmentation-based-on-stroke-filter-Liu-Jung",
            "title": {
                "fragments": [],
                "text": "Text segmentation based on stroke filter"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper gives the definition of text, which is realized in the form of stroke filter based on local region analysis, and proposes a novel method of text segmentation based on stroke filter (SF)."
            },
            "venue": {
                "fragments": [],
                "text": "MM '06"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2821130"
                        ],
                        "name": "David J. Crandall",
                        "slug": "David-J.-Crandall",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Crandall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David J. Crandall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721328"
                        ],
                        "name": "Sameer Kiran Antani",
                        "slug": "Sameer-Kiran-Antani",
                        "structuredName": {
                            "firstName": "Sameer",
                            "lastName": "Antani",
                            "middleNames": [
                                "Kiran"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sameer Kiran Antani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 87
                            }
                        ],
                        "text": "to 2003, only a few text extraction approaches considered the temporal nature of video [1], very little work was done on scene text [3], and objective performance evaluation metrics were scarce."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 14
                            }
                        ],
                        "text": "Compared with [1], only the motion vectors in text marcoblocks are extracted, and instead of using clustering method, the"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "terms, we use the definitions given by our previous work [1] in this paper."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] perform 8\u00d78 block-wise DCT on a video frame."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[1] present two tracking algorithms to track rigid text and changing text in videos respectively."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18084231,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37903a00047e1cf377408ca4119b48f2bfab89c4",
            "isKey": false,
            "numCitedBy": 94,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract. The popularity of digital video is increasing rapidly. To help users navigate libraries of video, algorithms that automatically index video based on content are needed. One approach is to extract text appearing in video, which often reflects a scene's semantic content. This is a difficult problem due to the unconstrained nature of general-purpose video. Text can have arbitrary color, size, and orientation. Backgrounds may be complex and changing. Most work so far has made restrictive assumptions about the nature of text occurring in video. Such work is therefore not directly applicable to unconstrained, general-purpose video. In addition, most work so far has focused only on detecting the spatial extent of text in individual video frames. However, text occurring in video usually persists for several seconds. This constitutes a text event that should be entered only once in the video index. Therefore it is also necessary to determine the temporal extent of text events. This is a non-trivial problem because text may move, rotate, grow, shrink, or otherwise change over time. Such text effects are common in television programs and commercials but so far have received little attention in the literature. This paper discusses detecting, binarizing, and tracking caption text in general-purpose MPEG-1 video. Solutions are proposed for each of these problems and compared with existing work found in the literature."
            },
            "slug": "Extraction-of-special-effects-caption-text-events-Crandall-Antani",
            "title": {
                "fragments": [],
                "text": "Extraction of special effects caption text events from digital video"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper discusses detecting, binarizing, and tracking caption text in general-purpose MPEG-1 video, and solutions are proposed for each of these problems and compared with existing work found in the literature."
            },
            "venue": {
                "fragments": [],
                "text": "International Journal on Document Analysis and Recognition"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6468417"
                        ],
                        "name": "Xueming Qian",
                        "slug": "Xueming-Qian",
                        "structuredName": {
                            "firstName": "Xueming",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xueming Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47062194"
                        ],
                        "name": "Guizhong Liu",
                        "slug": "Guizhong-Liu",
                        "structuredName": {
                            "firstName": "Guizhong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guizhong Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In a similar way, Qian et al. [ 39 ] also use DCT transform to extract text in MPEG format, however, only seven DCT coefficients (three horizontal, three vertical and one diagonal) are selected to compute \u0093text energy\u0094."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16453629,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66fe32c2c6c953d5bbba0e20198180fd7f1705fb",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Video text information plays an important role in semantic-based video analysis, indexing and retrieval. Video texts are closely related to the content of a video. Text-based video analysis, browsing and retrieval are usually carried out in the following for steps: video text detection, localization, segmentation and recognition. Videos are commonly stored in compressed formats where MPEG coding techniques are adopted. In this paper, a DCT coefficient based multilingual video text detection and localization scheme for compressed videos is proposed. Candidate text blocks are detected in terms of block texture constraint. An adaptive method for the horizontal and vertical aligned text lines determination is then designed according to the run length of the horizontal and vertical block numbers. The remaining block regions are further verified by local block texture constraints. And the text block region can be localized by virtue of the horizontal and vertical block texture projections. Finally, a foreground and background integrated (FBI) video text segmentation approach is adopted in this paper to eliminate the complex background in text regions. The final experimental results show the effectiveness of our methods"
            },
            "slug": "Text-Detection,-Localization-and-Segmentation-in-Qian-Liu",
            "title": {
                "fragments": [],
                "text": "Text Detection, Localization and Segmentation in Compressed Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A foreground and background integrated (FBI) video text segmentation approach is adopted in this paper to eliminate the complex background in text regions in compressed videos."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719610"
                        ],
                        "name": "J. Odobez",
                        "slug": "J.-Odobez",
                        "structuredName": {
                            "firstName": "Jean-Marc",
                            "lastName": "Odobez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Odobez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[24] present a two-step approach for text extraction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11796155,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42a8ff86566538103c6116f9047a4c3128e1542c",
            "isKey": false,
            "numCitedBy": 303,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-detection,-recognition-in-images-and-video-Chen-Odobez",
            "title": {
                "fragments": [],
                "text": "Text detection, recognition in images and video frames"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognit."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152803133"
                        ],
                        "name": "Yang Liu",
                        "slug": "Yang-Liu",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143663454"
                        ],
                        "name": "Hong Lu",
                        "slug": "Hong-Lu",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hong Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145905953"
                        ],
                        "name": "X. Xue",
                        "slug": "X.-Xue",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Xue",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Xue"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689805"
                        ],
                        "name": "Yap-Peng Tan",
                        "slug": "Yap-Peng-Tan",
                        "structuredName": {
                            "firstName": "Yap-Peng",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yap-Peng Tan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 32
                            }
                        ],
                        "text": "First, the approach proposed in [7] is adopted to detect and localize text objects in a single frame."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[7] uses line features to detect text objects in videos."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10219325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "300a13b816a303ee0f498d702a4b997cb377d44a",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Text superimposed on video frames provides synoptic or supplemental information on video semantics. In this paper, we propose a novel method to detect superimposed text effectively. First, we detect edges by an improved Canny edge detector. Then, a line-feature vector graph is generated based on the edge map and the stroke information is extracted. Finally text regions are generated and filtered according to line features. Experimental results show that, without much increasing the computational cost, our proposed method could suppress the false alarms notably. Furthermore, our method can be easily customized to applications with different tradeoffs in recall and precision."
            },
            "slug": "Effective-video-text-detection-using-line-features-Liu-Lu",
            "title": {
                "fragments": [],
                "text": "Effective video text detection using line features"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Experimental results show that, without much increasing the computational cost, the proposed method could suppress the false alarms notably and can be easily customized to applications with different tradeoffs in recall and precision."
            },
            "venue": {
                "fragments": [],
                "text": "ICARCV 2004 8th Control, Automation, Robotics and Vision Conference, 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6468417"
                        ],
                        "name": "Xueming Qian",
                        "slug": "Xueming-Qian",
                        "structuredName": {
                            "firstName": "Xueming",
                            "lastName": "Qian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xueming Qian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47062194"
                        ],
                        "name": "Guizhong Liu",
                        "slug": "Guizhong-Liu",
                        "structuredName": {
                            "firstName": "Guizhong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guizhong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113269345"
                        ],
                        "name": "Huan Wang",
                        "slug": "Huan-Wang",
                        "structuredName": {
                            "firstName": "Huan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144045444"
                        ],
                        "name": "Rui Su",
                        "slug": "Rui-Su",
                        "structuredName": {
                            "firstName": "Rui",
                            "lastName": "Su",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rui Su"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Qian et al. [ 46 ] present a text tracking approach in compressed video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18856025,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d52fc4e045798c1122d0cb4b4035095edbd8546",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Text-detection,-localization,-and-tracking-in-video-Qian-Liu",
            "title": {
                "fragments": [],
                "text": "Text detection, localization, and tracking in compressed video"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process. Image Commun."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785083"
                        ],
                        "name": "Michael R. Lyu",
                        "slug": "Michael-R.-Lyu",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lyu",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael R. Lyu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3332642"
                        ],
                        "name": "Jiqiang Song",
                        "slug": "Jiqiang-Song",
                        "structuredName": {
                            "firstName": "Jiqiang",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiqiang Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052052370"
                        ],
                        "name": "Min Cai",
                        "slug": "Min-Cai",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Cai"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In order to remove this redundancy, [ 6 ] proposes a new sequential multi-resolution paradigm (Fig. 1), in which once the text edges are detected and saved for final merging at a resolution level, they are erased immediately from the current edge map, and then the modified edge map is utilized as input to the next level, so that no text edges can appear several times at different resolution levels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the same paper [ 6 ], the authors propose a text extraction approach that emphasizes the multilingual ability."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "As noted by Lyu et al. [ 6 ], however, this strategy often spends unnecessary time on detecting and merging the same text objects appearing in many adjacent resolution levels."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, the sequential multi-resolution paradigm [ 6 ] can remove the redundancy of parallel multi-resolution paradigm [4] [5]; Fuzzy C-means based individual frame clustering [22] is replaced by the fuzzy clustering ensemble (FCE) based multi-frame clustering [23] to utilize temporal redundancy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Fig. 2. Stepwise results of text detection (From Lyu et al. [ 6 ])"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Fig. 1 Sequential multi-resolution paradigm (From Lyu et al. [ 6 ])"
                    },
                    "intents": []
                }
            ],
            "corpusId": 18648576,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e14cf92ecb1589d21324d934b2009451e602d1be",
            "isKey": true,
            "numCitedBy": 372,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Text in video is a very compact and accurate clue for video indexing and summarization. Most video text detection and extraction methods hold assumptions on text color, background contrast, and font style. Moreover, few methods can handle multilingual text well since different languages may have quite different appearances. This paper performs a detailed analysis of multilingual text characteristics, including English and Chinese. Based on the analysis, we propose a comprehensive, efficient video text detection, localization, and extraction method, which emphasizes the multilingual capability over the whole processing. The proposed method is also robust to various background complexities and text appearances. The text detection is carried out by edge detection, local thresholding, and hysteresis edge recovery. The coarse-to-fine localization scheme is then performed to identify text regions accurately. The text extraction consists of adaptive thresholding, dam point labeling, and inward filling. Experimental results on a large number of video images and comparisons with other methods are reported in detail."
            },
            "slug": "A-comprehensive-method-for-multilingual-video-text-Lyu-Song",
            "title": {
                "fragments": [],
                "text": "A comprehensive method for multilingual video text detection, localization, and extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A comprehensive, efficient video text detection, localization, and extraction method, which emphasizes the multilingual capability over the whole processing, and is also robust to various background complexities and text appearances."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146276219"
                        ],
                        "name": "Yichao Ma",
                        "slug": "Yichao-Ma",
                        "structuredName": {
                            "firstName": "Yichao",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichao Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1683416"
                        ],
                        "name": "Chunheng Wang",
                        "slug": "Chunheng-Wang",
                        "structuredName": {
                            "firstName": "Chunheng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunheng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2658590"
                        ],
                        "name": "Baihua Xiao",
                        "slug": "Baihua-Xiao",
                        "structuredName": {
                            "firstName": "Baihua",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Baihua Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145841729"
                        ],
                        "name": "Ruwei Dai",
                        "slug": "Ruwei-Dai",
                        "structuredName": {
                            "firstName": "Ruwei",
                            "lastName": "Dai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruwei Dai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[49] propose a usage-oriented performance evaluation method for text localization algorithms."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "However, as noted in [49], these measures fail to consider the areas of false alarms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13110085,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2dbe3a585ebe4cab4c780c6db636f73fbf5c53da",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "The localization of texts in image/video is the first step in a text processing system. Its effect will do great impact on the following processing steps. Although many studies have been done on text localization algorithms, there is not a universally accepted performance evaluation method. In this paper we propose two sets of metrics to evaluate the performance of text localization algorithms in different usage conditions. The metrics also consider the text distribution characteristics, and the difficulties of the underlying task. Some experiments on the proposed metrics are also given."
            },
            "slug": "Usage-Oriented-Performance-Evaluation-for-Text-Ma-Wang",
            "title": {
                "fragments": [],
                "text": "Usage-Oriented Performance Evaluation for Text Localization Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper proposes two sets of metrics to evaluate the performance of text localization algorithms in different usage conditions, which consider the text distribution characteristics, and the difficulties of the underlying task."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804700"
                        ],
                        "name": "Krishna Subramanian",
                        "slug": "Krishna-Subramanian",
                        "structuredName": {
                            "firstName": "Krishna",
                            "lastName": "Subramanian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Krishna Subramanian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145603129"
                        ],
                        "name": "P. Natarajan",
                        "slug": "P.-Natarajan",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Natarajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Natarajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2128428"
                        ],
                        "name": "M. Decerbo",
                        "slug": "M.-Decerbo",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Decerbo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Decerbo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752462"
                        ],
                        "name": "D. Casta\u00f1\u00f3n",
                        "slug": "D.-Casta\u00f1\u00f3n",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Casta\u00f1\u00f3n",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Casta\u00f1\u00f3n"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Character-stroke is also employed by Subramanian et al. [ 15 ] to extract text objects."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Fig. 6 (a) Original image, (b) Intensity plots along the blue line l, l-2, and l+2, \u0394 is the stroke width, (c) threshold Ig \u2264 0.35, (d) The thresholded image after morphological operations and connected component analysis (From Subramanian et al. [ 15 ])."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 18378610,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "477831f8f3b0ac37042fdf6ea4b3e7842739e797",
            "isKey": true,
            "numCitedBy": 28,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a new approach for analysis of images for text-localization and extraction. Our approach puts very few constraints on the font, size and color of text and is capable of handling both scene text and artificial text well. In this paper, we exploit two well-known features of text: approximately constant stroke width and local contrast, and develop a fast, simple, and effective algorithm to detect character strokes. We also show how these can be used for accurate extraction and motivate some advantages of using this approach for text localization over other color-space segmentation based approaches. We analyze the performance of our stroke detection algorithm on images collected for the robust-reading competitions at ICDAR 2003."
            },
            "slug": "Character-Stroke-Detection-for-Text-Localization-Subramanian-Natarajan",
            "title": {
                "fragments": [],
                "text": "Character-Stroke Detection for Text-Localization and Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper develops a fast, simple, and effective algorithm to detect character strokes and analyzes the performance of the stroke detection algorithm on images collected for the robust-reading competitions at ICDAR 2003."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739319"
                        ],
                        "name": "R. Lienhart",
                        "slug": "R.-Lienhart",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Lienhart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lienhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32544716"
                        ],
                        "name": "Axel Wernicke",
                        "slug": "Axel-Wernicke",
                        "structuredName": {
                            "firstName": "Axel",
                            "lastName": "Wernicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Axel Wernicke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 122
                            }
                        ],
                        "text": "For example, the sequential multi-resolution paradigm [6] can remove the redundancy of parallel multi-resolution paradigm [4] [5]; Fuzzy C-means based individual frame clustering [22] is replaced by the fuzzy clustering ensemble (FCE) based multi-frame clustering [23] to utilize temporal redundancy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 89
                            }
                        ],
                        "text": "Most reported approaches employ parallel multi-resolution paradigm to solve this problem [4] [5]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 143774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8387d4998f810cd2b60bd81545cb993087bc8788",
            "isKey": false,
            "numCitedBy": 468,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Many images, especially those used for page design on Web pages, as well as videos contain visible text. If these text occurrences could be detected, segmented, and recognized automatically, they would be a valuable source of high-level semantics for indexing and retrieval. We propose a novel method for localizing and segmenting text in complex images and videos. Text lines are identified by using a complex-valued multilayer feed-forward network trained to detect text at a fixed scale and position. The network's output at all scales and positions is integrated into a single text-saliency map, serving as a starting point for candidate text lines. In the case of video, these candidate text lines are refined by exploiting the temporal redundancy of text in video. Localized text lines are then scaled to a fixed height of 100 pixels and segmented into a binary image with black characters on white background. For videos, temporal redundancy is exploited to improve segmentation performance. Input images and videos can be of any size due to a true multiresolution approach. Moreover, the system is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video, so that one text bitmap is created for all instances of that text line. Therefore, our text segmentation results can also be used for object-based video encoding such as that enabled by MPEG-4."
            },
            "slug": "Localizing-and-segmenting-text-in-images-and-videos-Lienhart-Wernicke",
            "title": {
                "fragments": [],
                "text": "Localizing and segmenting text in images and videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes a novel method for localizing and segmenting text in complex images and videos that is not only able to locate and segment text occurrences into large binary images, but is also able to track each text line with sub-pixel accuracy over the entire occurrence in a video."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Circuits Syst. Video Technol."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146016491"
                        ],
                        "name": "Yuan-Kai Wang",
                        "slug": "Yuan-Kai-Wang",
                        "structuredName": {
                            "firstName": "Yuan-Kai",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan-Kai Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2120300537"
                        ],
                        "name": "Jian-Ming Chen",
                        "slug": "Jian-Ming-Chen",
                        "structuredName": {
                            "firstName": "Jian-Ming",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian-Ming Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1740861,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "58a10d107ddad8ff7bf4f46bd8a7376a73552e34",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a novel approach to detect texts in video frames. The approach proposes a spatio-temporal wavelet transform to integrate information of multiple frames rather than a single one. Static and dynamic texts are detected separately due to their characteristics in temporal domain. Sub-bands decomposed from the original image sequence are combined to form a salience map, which features are extracted from. The approach is verified by experiments with various types of videos. High average recall and precision rates confirm the effectiveness of the proposed method"
            },
            "slug": "Detecting-Video-Texts-Using-Spatial-Temporal-Wang-Chen",
            "title": {
                "fragments": [],
                "text": "Detecting Video Texts Using Spatial-Temporal Wavelet Transform"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This paper proposes a spatio-temporal wavelet transform to integrate information of multiple frames rather than a single one, which is verified by experiments with various types of videos."
            },
            "venue": {
                "fragments": [],
                "text": "18th International Conference on Pattern Recognition (ICPR'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113279916"
                        ],
                        "name": "Zhiguo Cheng",
                        "slug": "Zhiguo-Cheng",
                        "structuredName": {
                            "firstName": "Zhiguo",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhiguo Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117417037"
                        ],
                        "name": "Yun-cai Liu",
                        "slug": "Yun-cai-Liu",
                        "structuredName": {
                            "firstName": "Yun-cai",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yun-cai Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similar utilization of SVM for text extraction is also reported by Cheng et al. [ 28 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 23588388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32ca6b5e957a3ff89da3841f963d1b1286e321e4",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Text that appears in a scene or graphically added to video can provide an important supplemental source of index information as well as clues for decoding the video's structure and for classification, and we call them closed caption. In this work, a novel algorithm is presented for detecting and locating caption in digital video. The first module of the system divides an image into small blocks featured by pixel value that is fed to SVM (support vector machine) to classify whether they are text blocks or not. The other module is to do post-processing on the classified text blocks to identify the rectangle region of them and OCR can be used further and easily. Experiments conducted with a variety of video sources show that our method could detect and locate caption region successfully by SVM with comparatively less samples."
            },
            "slug": "Caption-location-and-extraction-in-digital-video-on-Cheng-Liu",
            "title": {
                "fragments": [],
                "text": "Caption location and extraction in digital video based on SVM"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel algorithm is presented for detecting and locating caption in digital video that could detect and locate caption region successfully by SVM with comparatively less samples."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 2004 International Conference on Machine Learning and Cybernetics (IEEE Cat. No.04EX826)"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522004"
                        ],
                        "name": "J. Gllavata",
                        "slug": "J.-Gllavata",
                        "structuredName": {
                            "firstName": "Julinda",
                            "lastName": "Gllavata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gllavata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738703"
                        ],
                        "name": "R. Ewerth",
                        "slug": "R.-Ewerth",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Ewerth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ewerth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685922"
                        ],
                        "name": "B. Freisleben",
                        "slug": "B.-Freisleben",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Freisleben",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Freisleben"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "Based on their previous work [22], Gllavata et al."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 183,
                                "start": 179
                            }
                        ],
                        "text": "For example, the sequential multi-resolution paradigm [6] can remove the redundancy of parallel multi-resolution paradigm [4] [5]; Fuzzy C-means based individual frame clustering [22] is replaced by the fuzzy clustering ensemble (FCE) based multi-frame clustering [23] to utilize temporal redundancy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31975917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "593e78f18ba5f5577b34ed81663db3e5d7d569cd",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Text localization and recognition in images is important for searching information in digital photo archives, video databases and Web sites. However, since text is often printed against a complex background, it is often difficult to detect. In this paper, a robust text localization approach is presented, which can automatically detect horizontally aligned text with different sizes, fonts, colors and languages. First, a wavelet transform is applied to the image and the distribution of high-frequency wavelet coefficients is considered to statistically characterize text and non-text areas. Then, the k-means algorithm is used to classify text areas in the image. The detected text areas undergo a projection analysis in order to refine their localization. Finally, a binary segmented text image is generated, to be used as input to an OCR engine. The detection performance of our approach is demonstrated by presenting experimental results for a set of video frames taken from the MPEG-7 video test set."
            },
            "slug": "Text-detection-in-images-based-on-unsupervised-of-Gllavata-Ewerth",
            "title": {
                "fragments": [],
                "text": "Text detection in images based on unsupervised classification of high-frequency wavelet coefficients"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A robust text localization approach is presented, which can automatically detect horizontally aligned text with different sizes, fonts, colors and languages and is demonstrated by presenting experimental results for a set of video frames taken from the MPEG-7 video test set."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522004"
                        ],
                        "name": "J. Gllavata",
                        "slug": "J.-Gllavata",
                        "structuredName": {
                            "firstName": "Julinda",
                            "lastName": "Gllavata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gllavata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2446781"
                        ],
                        "name": "E. Qeli",
                        "slug": "E.-Qeli",
                        "structuredName": {
                            "firstName": "Ermir",
                            "lastName": "Qeli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Qeli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685922"
                        ],
                        "name": "B. Freisleben",
                        "slug": "B.-Freisleben",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Freisleben",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Freisleben"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[23] present a temporal-redundancy-based method for detecting text object in videos."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 268,
                                "start": 264
                            }
                        ],
                        "text": "For example, the sequential multi-resolution paradigm [6] can remove the redundancy of parallel multi-resolution paradigm [4] [5]; Fuzzy C-means based individual frame clustering [22] is replaced by the fuzzy clustering ensemble (FCE) based multi-frame clustering [23] to utilize temporal redundancy."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21125380,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27b3e66b853f824b03e43a2f6b7e4418ec14e61b",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Detection and localization of text in videos is an important task towards enabling automatic content-based retrieval of digital video databases. However, since text is often displayed against a complex background, its detection is a challenging problem. In this paper, a novel approach based on fuzzy cluster ensemble techniques to solve this problem is presented. The advantage of this approach is that the fuzzy clustering ensemble allows the incremental inclusion of temporal information regarding the appearance of static text in videos. Comparative experimental results for a test set of 10.92 minutes of video sequences have shown the very good performance of the proposed approach with an overall recall of 92.04% and a precision of 96.71%"
            },
            "slug": "Detecting-Text-in-Videos-Using-Fuzzy-Clustering-Gllavata-Qeli",
            "title": {
                "fragments": [],
                "text": "Detecting Text in Videos Using Fuzzy Clustering Ensembles"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel approach based on fuzzy cluster ensemble techniques to solve the problem of detection and localization of text in videos by allowing the incremental inclusion of temporal information regarding the appearance of staticText in videos."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth IEEE International Symposium on Multimedia (ISM'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144897940"
                        ],
                        "name": "L. Tang",
                        "slug": "L.-Tang",
                        "structuredName": {
                            "firstName": "Lijun",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719062"
                        ],
                        "name": "J. Kender",
                        "slug": "J.-Kender",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Kender",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kender"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[34] present a unified approach to detect"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 94
                            }
                        ],
                        "text": "For example, [43] uses an improved drift compensation approach for scrolling text extraction; [34] use comparatively static periods of instructional video to locate text frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16061554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "076a76f317766fb4aa52d7b2ef3e958d28c2255e",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Videotext can be an efficient semantic index and summary for instructional videos. However, videotext usually appears in different visual formats: handwritten slides, electronic slides, book pages, web pages, handwriting on chalkboard, etc. We propose a unified approach to handle all these kinds of videotext in three steps. First, we detect still video segments by analyzing motion energy patterns in instructional videos, and construct a quality-enhanced candidate text frame for each still video segment. Then, we use a trained SVM classifier to verify the candidate text frames, as well as to segment the text region and individual text blocks from the verified frames. Finally, we filter redundant text frames with similar text content by a Hausdorff distance-based image comparison algorithm. The resulting text frames are automatically organized into HTML and PDF documents to serve as an imagery summarization of the instructional videos. We show the application of our method to 75 instructional videos of five different courses, and discuss its applications."
            },
            "slug": "A-unified-text-extraction-method-for-instructional-Tang-Kender",
            "title": {
                "fragments": [],
                "text": "A unified text extraction method for instructional videos"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A unified approach to handle all kinds of videotext in three steps, using a trained SVM classifier to verify the candidate text frames, as well as to segment the text region and individual text blocks from the verified frames."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE International Conference on Image Processing 2005"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717886"
                        ],
                        "name": "Wen Wu",
                        "slug": "Wen-Wu",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[32] [33] present an approach to detect"
                    },
                    "intents": []
                }
            ],
            "corpusId": 10007203,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f5ff7a415810fa61c1894cb10fd0b9ca0c8a44d",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "A fast and robust framework for incrementally detecting text on road signs from video is presented in this paper. This new framework makes two main contributions. 1) The framework applies a divide-and-conquer strategy to decompose the original task into two subtasks, that is, the localization of road signs and the detection of text on the signs. The algorithms for the two subtasks are naturally incorporated into a unified framework through a feature-based tracking algorithm. 2) The framework provides a novel way to detect text from video by integrating two-dimensional (2-D) image features in each video frame (e.g., color, edges, texture) with the three-dimensional (3-D) geometric structure information of objects extracted from video sequence (such as the vertical plane property of road signs). The feasibility of the proposed framework has been evaluated using 22 video sequences captured from a moving vehicle. This new framework gives an overall text detection rate of 88.9% and a false hit rate of 9.2%. It can easily be applied to other tasks of text detection from video and potentially be embedded in a driver assistance system."
            },
            "slug": "Detection-of-text-on-road-signs-from-video-Wu-Chen",
            "title": {
                "fragments": [],
                "text": "Detection of text on road signs from video"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A fast and robust framework for incrementally detecting text on road signs from video by integrating two-dimensional image features in each video frame with the three-dimensional geometric structure information of objects extracted from video sequence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Intell. Transp. Syst."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34713082"
                        ],
                        "name": "P. Dubey",
                        "slug": "P.-Dubey",
                        "structuredName": {
                            "firstName": "Premnath",
                            "lastName": "Dubey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Dubey"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 66
                            }
                        ],
                        "text": "Unlike many other edge-based approaches using all detected edges, Dubey [8] use only the vertical edge features to find text regions based on the observation that vertical edges can enhance the characteristic of text and eliminate most irrelevant information."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 67
                            }
                        ],
                        "text": "Edge feature based algorithm output image at each step (From Dubey [8])"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "[8] P. Dubey, Edge Based Text Detection for Multi-purpose Application, Proceedings of International Conference Signal Processing, Vol. 4, 2006."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[8] use only the vertical edge features to find text regions based on the observation that vertical edges can enhance the characteristic of text and eliminate"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14062660,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d2ba6a11b3b257136c63443f41c39e4d7d833cd7",
            "isKey": true,
            "numCitedBy": 26,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Text detection plays a crucial role in various applications. In this paper we present an edge based text detection technique in the complex images for multi purpose application. The technique applied vertical Sobel edge detection and a newly proposed morphological technique that used to connect the edges to form the candidate regions. The technique has special advantage, by providing a distinguishable texture on the text area over the others. The connected components are then extracted using a purposed segmentation algorithm. Later all the candidate regions are verified to specify the text region. The propose techniques has been tested with different types of image acquired from different input sources and environment. The experimental result shows highly successful rate"
            },
            "slug": "Edge-Based-Text-Detection-for-Multi-purpose-Dubey",
            "title": {
                "fragments": [],
                "text": "Edge Based Text Detection for Multi-purpose Application"
            },
            "tldr": {
                "abstractSimilarityScore": 58,
                "text": "An edge based text detection technique in the complex images for multi purpose application using vertical Sobel edge detection and a newly proposed morphological technique that used to connect the edges to form the candidate regions is presented."
            },
            "venue": {
                "fragments": [],
                "text": "2006 8th international Conference on Signal Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821502"
                        ],
                        "name": "Chien-Cheng Lee",
                        "slug": "Chien-Cheng-Lee",
                        "structuredName": {
                            "firstName": "Chien-Cheng",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chien-Cheng Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1825918"
                        ],
                        "name": "Yu-Chun Chiang",
                        "slug": "Yu-Chun-Chiang",
                        "structuredName": {
                            "firstName": "Yu-Chun",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-Chun Chiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136761871"
                        ],
                        "name": "Hau-Ming Huang",
                        "slug": "Hau-Ming-Huang",
                        "structuredName": {
                            "firstName": "Hau-Ming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hau-Ming Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49970683"
                        ],
                        "name": "Chun-Li Tsai",
                        "slug": "Chun-Li-Tsai",
                        "structuredName": {
                            "firstName": "Chun-Li",
                            "lastName": "Tsai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chun-Li Tsai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "[36] [37] define two features: block pixel variance, the deviation of pixel values in a block, and average pixel difference, the average difference of pixel values between two consecutive frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37710287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "70d678fbcf3a76f93f800e4af1c582cd43b0069a",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an algorithm to detect captions from news videos. The propose method only detects captions excluding other miscellaneous types of text. The algorithm makes use of the fact that the text remains in many consecutive frames to reduce the number of the processing frames. The caption beginning frame is detected firs, then a caption candidate region in the caption beginning frame is defined. Twelve wavelet features are extracted from the region and considered as the input of the classifier to detect the text blocks. Experimental results show that the proposed approach can fast and robustly detect captions from news video."
            },
            "slug": "A-Fast-Caption-Localization-and-Detection-for-News-Lee-Chiang",
            "title": {
                "fragments": [],
                "text": "A Fast Caption Localization and Detection for News Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Experimental results show that the proposed approach can fast and robustly detect captions from news video."
            },
            "venue": {
                "fragments": [],
                "text": "Second International Conference on Innovative Computing, Informatio and Control (ICICIC 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678373"
                        ],
                        "name": "J. Luettin",
                        "slug": "J.-Luettin",
                        "structuredName": {
                            "firstName": "Juergen",
                            "lastName": "Luettin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Luettin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61028857,
            "fieldsOfStudy": [
                "Computer Science",
                "Physics",
                "Art"
            ],
            "id": "53757cc24a70c812a97b5869df45a78d3ab97f74",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A Survey of Text Detection and Recognition in Images and Videos, including the state-of-the-art methods and systems."
            },
            "slug": "A-Survey-of-Text-Detection-and-Recognition-in-and-Chen-Luettin",
            "title": {
                "fragments": [],
                "text": "A Survey of Text Detection and Recognition in Images and Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A Survey of Text Detection and Recognition in Images and Videos, including the state-of-the-art methods and systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863244"
                        ],
                        "name": "Xiansheng Hua",
                        "slug": "Xiansheng-Hua",
                        "structuredName": {
                            "firstName": "Xiansheng",
                            "lastName": "Hua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiansheng Hua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120640002"
                        ],
                        "name": "Wenyin Liu",
                        "slug": "Wenyin-Liu",
                        "structuredName": {
                            "firstName": "Wenyin",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wenyin Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108698841"
                        ],
                        "name": "HongJiang Zhang",
                        "slug": "HongJiang-Zhang",
                        "structuredName": {
                            "firstName": "HongJiang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "HongJiang Zhang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[50] propose a modified performance evaluation approach which is more suitable for video"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15798803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c44fc2f6d748f54badcaf86feef8eb347d0b1c2",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Text presented in videos provides important supplemental information for video indexing and retrieval. Many efforts have been made for text detection in videos. However, there is still a lack of performance evaluation protocols for video text detection. In this paper, we propose an objective and comprehensive performance evaluation protocol for video text detection algorithms. The protocol includes a positive set and a negative set of indices at the textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes. In the protocol, we assign a detection difficulty (DD) level to each ground truth textbox. The performance indices can then be normalized with respect to the textbox DD level and are therefore tolerant to different ground-truth difficulties to a certain degree. We also assign a detectability index (DI) value to each ground-truth textbox. The overall detection rate is the DI-weighted average of the detection qualities of all ground-truth textboxes, which makes the detection rate more accurate to reveal the real performance. The automatic performance evaluation scheme has been applied to performance evaluation of a text detection approach to determine the best thresholds that can yield the best detection results. The protocol has also been employed to compare the performances of several text detection systems. Hence, we believe that the proposed protocol can be used to compare the performance of different video/image text detection algorithms/systems and can even help improve, select, and design new text detection methods."
            },
            "slug": "An-automatic-performance-evaluation-protocol-for-Hua-Liu",
            "title": {
                "fragments": [],
                "text": "An automatic performance evaluation protocol for video text detection algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "An objective and comprehensive performance evaluation protocol for video text detection algorithms, which includes a positive set and a negative set of indices at the textbox level, which evaluate the detection quality in terms of both location accuracy and fragmentation of the detected textboxes."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1918919"
                        ],
                        "name": "V. Papavassiliou",
                        "slug": "V.-Papavassiliou",
                        "structuredName": {
                            "firstName": "Vassilis",
                            "lastName": "Papavassiliou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Papavassiliou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799540"
                        ],
                        "name": "Themos Stafylakis",
                        "slug": "Themos-Stafylakis",
                        "structuredName": {
                            "firstName": "Themos",
                            "lastName": "Stafylakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Themos Stafylakis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684430"
                        ],
                        "name": "V. Katsouros",
                        "slug": "V.-Katsouros",
                        "structuredName": {
                            "firstName": "Vassilis",
                            "lastName": "Katsouros",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Katsouros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143978280"
                        ],
                        "name": "G. Carayannis",
                        "slug": "G.-Carayannis",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Carayannis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Carayannis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Papavassiliou et al. [ 21 ] propose a parametric spectral-based method for text verification in videos."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17296220,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d8661f3d161d1eb4ea7d39b1032db741b0c3577",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "A new method for verifying text areas detected in video streams is proposed. The algorithm explores the spectral properties of the horizontal projection of candidate text regions in order to reduce the high amount of false alarms that most text detection algorithms suffer from. The full algorithm (text localization followed by verification and temporal redundancy module) has been tested on newscast video sequences (MPEG-1-720 x 576 resolution-184 minutes). The detection module produced 94.82% recall rate but only 51.84% precision rate. The addition of the verification module increased the precision rate to 78.93% keeping the recall rate almost unaffected."
            },
            "slug": "A-Parametric-Spectral-Based-Method-for-Verification-Papavassiliou-Stafylakis",
            "title": {
                "fragments": [],
                "text": "A Parametric Spectral-Based Method for Verification of Text in Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A new method for verifying text areas detected in video streams by exploring the spectral properties of the horizontal projection of candidate text regions in order to reduce the high amount of false alarms that most text detection algorithms suffer from."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403970934"
                        ],
                        "name": "C. Mancas-Thillou",
                        "slug": "C.-Mancas-Thillou",
                        "structuredName": {
                            "firstName": "C\u00e9line",
                            "lastName": "Mancas-Thillou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mancas-Thillou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50276543"
                        ],
                        "name": "B. Gosselin",
                        "slug": "B.-Gosselin",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Gosselin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gosselin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Celine et al. [ 30 ] [31] propose a selective metricbased clustering method to extract text from natural scene."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 759760,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d881266612513db360b794f2c7cbb6aa8b638e6",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural scene images brought new challenges for a few years and one of them is text understanding over images or videos. Text extraction which consists to segment textual foreground from the background succeeds using color information. Faced to the large diversity of text information in daily life and artistic ways of display, we are convinced that this only information is no more enough and we present a color segmentation algorithm using spatial information. Moreover, a new method is proposed in this paper to handle uneven lighting, blur and complex backgrounds which are inherent degradations to natural scene images. To merge text pixels together, complementary clustering distances are used to support simultaneously clear and well-contrasted images with complex and degraded images. Tests on a public database show finally efficiency of the whole proposed method."
            },
            "slug": "Spatial-and-Color-Spaces-Combination-for-Natural-Mancas-Thillou-Gosselin",
            "title": {
                "fragments": [],
                "text": "Spatial and Color Spaces Combination for Natural Scene Text Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new method is proposed in this paper to handle uneven lighting, blur and complex backgrounds which are inherent degradations to natural scene images and to merge text pixels together, complementary clustering distances are used."
            },
            "venue": {
                "fragments": [],
                "text": "2006 International Conference on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3061097"
                        ],
                        "name": "N. Ezaki",
                        "slug": "N.-Ezaki",
                        "structuredName": {
                            "firstName": "Nobuo",
                            "lastName": "Ezaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ezaki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1806816"
                        ],
                        "name": "M. Bulacu",
                        "slug": "M.-Bulacu",
                        "structuredName": {
                            "firstName": "Marius",
                            "lastName": "Bulacu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bulacu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1799278"
                        ],
                        "name": "Lambert Schomaker",
                        "slug": "Lambert-Schomaker",
                        "structuredName": {
                            "firstName": "Lambert",
                            "lastName": "Schomaker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lambert Schomaker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[54] describe a system equipped with a PDA, a CCD-camera and a voice synthesizer, to assist visually impaired persons by detecting text objects from natural scenes and transforming them into voice signals."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2561294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "95e6599c7ac506446c4feefbf5a22841d24b08b0",
            "isKey": false,
            "numCitedBy": 219,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a system that reads the text encountered in natural scenes with the aim to provide assistance to the visually impaired persons. This paper describes the system design and evaluates several character extraction methods. Automatic text recognition from natural images receives a growing attention because of potential applications in image retrieval, robotics and intelligent transport system. Camera-based document analysis becomes a real possibility with the increasing resolution and availability of digital cameras. However, in the case of a blind person, finding the text region is the first important problem that must be addressed, because it cannot be assumed that the acquired image contains only characters. At first, our system tries to find in the image areas with small characters. Then it zooms into the found areas to retake higher resolution images necessary for character recognition. In the present paper, we propose four character-extraction methods based on connected components. We tested the effectiveness of our methods on the ICDAR 2003 Robust Reading Competition data. The performance of the different methods depends on character size. In the data, bigger characters are more prevalent and the most effective extraction method proves to be the sequence: Sobel edge detection, Otsu binarization, connected component extraction and rule-based connected component filtering."
            },
            "slug": "Text-detection-from-natural-scene-images:-towards-a-Ezaki-Bulacu",
            "title": {
                "fragments": [],
                "text": "Text detection from natural scene images: towards a system for visually impaired persons"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A system that reads the text encountered in natural scenes with the aim to provide assistance to the visually impaired persons and evaluates several character extraction methods based on connected components."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 17th International Conference on Pattern Recognition, 2004. ICPR 2004."
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308212"
                        ],
                        "name": "Libo Fu",
                        "slug": "Libo-Fu",
                        "structuredName": {
                            "firstName": "Libo",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Libo Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109080279"
                        ],
                        "name": "Weiqiang Wang",
                        "slug": "Weiqiang-Wang",
                        "structuredName": {
                            "firstName": "Weiqiang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weiqiang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2930851"
                        ],
                        "name": "Yaowen Zhan",
                        "slug": "Yaowen-Zhan",
                        "structuredName": {
                            "firstName": "Yaowen",
                            "lastName": "Zhan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaowen Zhan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12] propose a text detection method in complex background based on multiple constraints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17219614,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f21c375121fbd0007fabc8f8903ad3bed0776303",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a robust text segmentation method in complex background. The proposed method first utilizes the K-means algorithm to decompose a detected text block into different binary image layers. Then an effective post-processing is followed to eliminate background residues in each layer. In this step we develop a group of robust constraints to characterize general text regions based on color, edge and stroke thickness. We also propose the components relation constraint (CRC) designed specifically for Chinese characters. Finally the text image layer is identified based on the periodical and symmetrical layout of text lines. The experimental results show that our method can effectively eliminate a wide range of background residues, and has a better performance than the K-means method, as well as a high speed."
            },
            "slug": "A-Robust-Text-Segmentation-Approach-in-Complex-on-Fu-Wang",
            "title": {
                "fragments": [],
                "text": "A Robust Text Segmentation Approach in Complex Background Based on Multiple Constraints"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The experimental results show that the proposed robust text segmentation method can effectively eliminate a wide range of background residues, and has a better performance than the K-means method, as well as a high speed."
            },
            "venue": {
                "fragments": [],
                "text": "PCM"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38911842"
                        ],
                        "name": "H. Tran",
                        "slug": "H.-Tran",
                        "structuredName": {
                            "firstName": "Hong",
                            "lastName": "Tran",
                            "middleNames": [
                                "Tai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Tran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2599357"
                        ],
                        "name": "A. Lux",
                        "slug": "A.-Lux",
                        "structuredName": {
                            "firstName": "Augustin",
                            "lastName": "Lux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403980960"
                        ],
                        "name": "T. H.L.Nguyen",
                        "slug": "T.-H.L.Nguyen",
                        "structuredName": {
                            "firstName": "T",
                            "lastName": "H.L.Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. H.L.Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143858619"
                        ],
                        "name": "A. Boucher",
                        "slug": "A.-Boucher",
                        "structuredName": {
                            "firstName": "Alain",
                            "lastName": "Boucher",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Boucher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, in ridge-based text extraction approach [ 10 ], a text string is modeled as its center line and the skeletons of characters by ridges at different hierarchical scales; The GMM based algorithm [16] treats the text features of three neighboring characters as three mixed Gaussian models to extract text objects; The VACE and CLEAR evaluation measures [51] support direct measurement of detection and tracking technologies, facilitates ..."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Based on a structural model of text, Tran et al. [ 10 ] use ridges detected at several scales to extract text objects."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "This figure shows the independence on orientation of the ridge-based text representation (From Tran et al. [ 10 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 269134,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d56f45d16e9950de19e745acefc888094433cb57",
            "isKey": true,
            "numCitedBy": 33,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel approach for finding text in images by using ridges at several scales. A text string is modelled by a ridge at a coarse scale representing its center line and numerous short ridges at a smaller scale representing the skeletons of characters. Skeleton ridges have to satisfy geometrical and spatial constraints such as the perpendicularity or non-parallelism to the central ridge. In this way, we obtain a hierarchical description of text strings, which can provide direct input to an OCR or a text analysis system. The proposed method does not depend on a particular alphabet, it works with a wide variety in size of characters and does not depend on orientation of text string. The experimental results show a good detection."
            },
            "slug": "A-Novel-Approach-for-Text-Detection-in-Images-Using-Tran-Lux",
            "title": {
                "fragments": [],
                "text": "A Novel Approach for Text Detection in Images Using Structural Features"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A novel approach for finding text in images by using ridges at several scales to obtain a hierarchical description of text strings, which can provide direct input to an OCR or a text analysis system."
            },
            "venue": {
                "fragments": [],
                "text": "ICAPR"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113504611"
                        ],
                        "name": "Hui Fu",
                        "slug": "Hui-Fu",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752239"
                        ],
                        "name": "Xiabi Liu",
                        "slug": "Xiabi-Liu",
                        "structuredName": {
                            "firstName": "Xiabi",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiabi Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7415267"
                        ],
                        "name": "Yunde Jia",
                        "slug": "Yunde-Jia",
                        "structuredName": {
                            "firstName": "Yunde",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunde Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39945712"
                        ],
                        "name": "Hongbin Deng",
                        "slug": "Hongbin-Deng",
                        "structuredName": {
                            "firstName": "Hongbin",
                            "lastName": "Deng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hongbin Deng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, in ridge-based text extraction approach [10], a text string is modeled as its center line and the skeletons of characters by ridges at different hierarchical scales; The GMM based algorithm [ 16 ] treats the text features of three neighboring characters as three mixed Gaussian models to extract text objects; The VACE and CLEAR evaluation measures [51] support direct measurement of detection and tracking technologies, facilitates ..."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Gaussian Mixture Model (GMM) is used by Fu et al. [ 16 ] to discriminate characters from background."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7967390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f3c0ce663c44248d8478815fa3bc0920b5aafe04",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new method to extract multilingual text in images through discriminating characters from non-characters based on the Gaussian mixture modeling of neighbor characters. The image is binarized and the morphological closing operation is performed on the binary image, in order that each character in it can be treated as a connected component; the neighborhood of connected components are computed based on the Voronoi partition of the image, and each connected component is labeled as character or non-character according to its neighbors. We applied the proposed text extraction method to Chinese and English text extraction, the effectiveness of which is confirmed by the experimental results."
            },
            "slug": "Gaussian-Mixture-Modeling-of-Neighbor-Characters-in-Fu-Liu",
            "title": {
                "fragments": [],
                "text": "Gaussian Mixture Modeling of Neighbor Characters for Multilingual Text Extraction in Images"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed text extraction method was applied to Chinese and English text extraction, and the effectiveness of the method was confirmed by the experimental results."
            },
            "venue": {
                "fragments": [],
                "text": "2006 International Conference on Image Processing"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145717886"
                        ],
                        "name": "Wen Wu",
                        "slug": "Wen-Wu",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46772547"
                        ],
                        "name": "Xilin Chen",
                        "slug": "Xilin-Chen",
                        "structuredName": {
                            "firstName": "Xilin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xilin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118579343"
                        ],
                        "name": "Jie Yang",
                        "slug": "Jie-Yang",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "[32] [33] present an approach to detect"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2326270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f362174f3a453f67af7d83bac4030a52878906af",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a fast and robust framework for incrementally detecting text on road signs from natural scene video. The new framework makes two main contributions. First, the framework applies a Divide-and-Conquer strategy to decompose the original task into two sub-tasks, that is, localization of road signs and detection of text. The algorithms for the two sub-tasks are smoothly incorporated into a unified framework through a real time tracking algorithm. Second, the framework provides a novel way for text detection from video by integrating 2D features in each video frame (e.g., color, edges, texture) with 3D information available in a video sequence (e.g., object structure). The feasibility of the proposed framework has been evaluated on the video sequences captured from a moving vehicle. The new framework can be applied to a driving assistant system and other tasks of text detection from video."
            },
            "slug": "Incremental-detection-of-text-on-road-signs-from-to-Wu-Chen",
            "title": {
                "fragments": [],
                "text": "Incremental detection of text on road signs from video with application to a driving assistant system"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "A fast and robust framework for incrementally detecting text on road signs from natural scene video by applying a Divide-and-Conquer strategy to decompose the original task into two sub-tasks, that is, localization of road signs and detection of text."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3312922"
                        ],
                        "name": "H. Aradhye",
                        "slug": "H.-Aradhye",
                        "structuredName": {
                            "firstName": "Hrishikesh",
                            "lastName": "Aradhye",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aradhye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31693932"
                        ],
                        "name": "G. Myers",
                        "slug": "G.-Myers",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Myers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Myers"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "By noticing that the appearance and disappearance of text events can cause significant changes of the magnitude of text density, Aradhye et al. [ 29 ] applies multi-scale statistical process control (MSSPC) to detect changes in text density in video documents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 19353275,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "66f4f2d6fce7879f40412689d62bb908fadbec6e",
            "isKey": false,
            "numCitedBy": 2,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Text in video, whether overlay or in-scene, contains a wealth of information vital to automated content analysis systems. However, low resolution of the imagery, coupled with richness of the background and compression artifacts limit the detection accuracy that can be achieved in practice using existing text detection algorithms. This paper presents a novel, non-causal temporal aggregation method that acts as a second pass over the output of an existing text detector over the entire video clip. A multiresolution change detection algorithm is used along the time axis to detect the appearance and disappearance of multiple, concurrent lines of text followed by recursive time-averaged projections on Y and X axes. This algorithm detects and rectifies instances of missed text and enhances spatial boundaries of detected text lines using consensus estimates. Experimental results, which demonstrate significant performance gain on publicly collected and annotated data, are presented."
            },
            "slug": "Exploiting-Videotext-_Events_-for-Improved-Aradhye-Myers",
            "title": {
                "fragments": [],
                "text": "Exploiting Videotext _Events_ for Improved Videotext Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel, non-causal temporal aggregation method that acts as a second pass over the output of an existing text detector over the entire video clip, which detects and rectifies instances of missed text and enhances spatial boundaries of detected text lines using consensus estimates."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2144602022"
                        ],
                        "name": "K. Kim",
                        "slug": "K.-Kim",
                        "structuredName": {
                            "firstName": "Kwang",
                            "lastName": "Kim",
                            "middleNames": [
                                "In"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121267347"
                        ],
                        "name": "K. Jung",
                        "slug": "K.-Jung",
                        "structuredName": {
                            "firstName": "Keechul",
                            "lastName": "Jung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152675651"
                        ],
                        "name": "J. Kim",
                        "slug": "J.-Kim",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Kim",
                            "middleNames": [
                                "Hyung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kim"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17901853,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14bbcdc1744cc5982ffb64ea4755a72921d98d08",
            "isKey": false,
            "numCitedBy": 504,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "The current paper presents a novel texture-based method for detecting texts in images. A support vector machine (SVM) is used to analyze the textural properties of texts. No external texture feature extraction module is used, but rather the intensities of the raw pixels that make up the textural pattern are fed directly to the SVM, which works well even in high-dimensional spaces. Next, text regions are identified by applying a continuously adaptive mean shift algorithm (CAMSHIFT) to the results of the texture analysis. The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "slug": "Texture-Based-Approach-for-Text-Detection-in-Images-Kim-Jung",
            "title": {
                "fragments": [],
                "text": "Texture-Based Approach for Text Detection in Images Using Support Vector Machines and Continuously Adaptive Mean Shift Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The combination of CAMSHIFT and SVMs produces both robust and efficient text detection, as time-consuming texture analyses for less relevant pixels are restricted, leaving only a small part of the input image to be texture-analyzed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31693932"
                        ],
                        "name": "G. Myers",
                        "slug": "G.-Myers",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Myers",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Myers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[47] track scene text undergoing scale changes and 3D motion."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 73657550,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "15ca362db10e43cad9914433485d1b868e1e4e9e",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Text on planar surfaces in 3-D scenes in video imagery can undergo complex apparent motion and distortion as the surfaces move relative to the camera. Tracking such text and its motion through a contiguous sequence of video frames in which it is visible is desirable primarily for two reasons. First, reliable tracking of text enables the images of text persisting across multiple frames to be grouped, processed, and understood as a single unit. Second, text tracking aids the mapping of corresponding text and background pixels across multiple frames to enhance image quality and resolution before character recognition. Existing text tracking approaches, however, are limited to approximate pixel-based correspondences of adjacent frames without any explicit, rigorous modeling of 3-D scene geometry. To this end, we describe an approach that tracks planar regions of scene text that can undergo arbitrary 3-D rigid motion and scale changes. Our approach computes homographies on blocks of contiguous frames simultaneously using a combination of factorization and robust statistical methods. In spite of low resolution and noisy imagery, this approach produces a more accurate and stable motion estimate than existing methods using only two adjacent frames. In addition, our method is robust enough to tolerate imperfections in the spatial localization of text. Our results demonstrate that the mean offset pixel error of our tracker is as small as 1.1 pixels."
            },
            "slug": "A-Robust-Method-for-Tracking-Scene-Text-in-Video-Myers",
            "title": {
                "fragments": [],
                "text": "A Robust Method for Tracking Scene Text in Video Imagery"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681758"
                        ],
                        "name": "Wumo Pan",
                        "slug": "Wumo-Pan",
                        "structuredName": {
                            "firstName": "Wumo",
                            "lastName": "Pan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wumo Pan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144957197"
                        ],
                        "name": "T. Bui",
                        "slug": "T.-Bui",
                        "structuredName": {
                            "firstName": "Tien",
                            "lastName": "Bui",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713795"
                        ],
                        "name": "C. Suen",
                        "slug": "C.-Suen",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Suen",
                            "middleNames": [
                                "Yee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Suen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[42] to segment text objects from complex background."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 27251601,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00d5c60e5a72c5295981069f1acbac1aea1be19f",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "A novel text segmentation method from complex background is presented in this paper. The idea is inspired by the recent development in searching for the sparse signal representation among a family of over-complete atoms, which is called a dictionary. We assume that the image under investigation is composed of two components: the foreground text and the complex background. We further assume that the latter can be modeled as a piece-wise smooth function. Then we choose two dictionaries, where the first one gives sparse representation to one component and non-sparse representation to another while the second one does the opposite. By looking for the sparse representations in each dictionary, we can decompose the image into the two composing components. After that, text segmentation can be easily achieved by applying simple thresholding to the text component. Preliminary experiments show some promising results."
            },
            "slug": "Text-Segmentation-from-Complex-Background-Using-Pan-Bui",
            "title": {
                "fragments": [],
                "text": "Text Segmentation from Complex Background Using Sparse Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "A novel text segmentation method from complex background inspired by the recent development in searching for the sparse signal representation among a family of over-complete atoms, which is called a dictionary is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403970934"
                        ],
                        "name": "C. Mancas-Thillou",
                        "slug": "C.-Mancas-Thillou",
                        "structuredName": {
                            "firstName": "C\u00e9line",
                            "lastName": "Mancas-Thillou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Mancas-Thillou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50276543"
                        ],
                        "name": "B. Gosselin",
                        "slug": "B.-Gosselin",
                        "structuredName": {
                            "firstName": "Bernard",
                            "lastName": "Gosselin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Gosselin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "For example, selective metric clustering algorithm [31] uses two complementary clustering methods,"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 5
                            }
                        ],
                        "text": "[30] [31] propose a selective metricbased clustering method to extract text from natural scene."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42539643,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0aaca7527d703a6945ba73ce15e7e7353258fc8a",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Color-text-extraction-with-selective-metric-based-Mancas-Thillou-Gosselin",
            "title": {
                "fragments": [],
                "text": "Color text extraction with selective metric-based clustering"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Vis. Image Underst."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1821502"
                        ],
                        "name": "Chien-Cheng Lee",
                        "slug": "Chien-Cheng-Lee",
                        "structuredName": {
                            "firstName": "Chien-Cheng",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chien-Cheng Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1825918"
                        ],
                        "name": "Yu-Chun Chiang",
                        "slug": "Yu-Chun-Chiang",
                        "structuredName": {
                            "firstName": "Yu-Chun",
                            "lastName": "Chiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-Chun Chiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144217758"
                        ],
                        "name": "Cheng-Yuan Shih",
                        "slug": "Cheng-Yuan-Shih",
                        "structuredName": {
                            "firstName": "Cheng-Yuan",
                            "lastName": "Shih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Yuan Shih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2136761871"
                        ],
                        "name": "Hau-Ming Huang",
                        "slug": "Hau-Ming-Huang",
                        "structuredName": {
                            "firstName": "Hau-Ming",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hau-Ming Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[36] [37] define two features: block pixel variance, the deviation of pixel values in a block, and average pixel difference, the average difference of pixel values between two consecutive frames."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5790938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db944c1adc1724b43413acba3af158da01540a5e",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose an algorithm to detect captions from news videos. The propose method only detects captions excluding other miscellaneous types of text. The algorithm makes use of the fact that the text remains in many consecutive frames to reduce the number of the processing frames. The caption beginning frame is detected first, then a caption candidate strip in the caption beginning frame is defined. Moreover, the difference of the caption candidate strip between consecutive frames is computed, and then the difference information is transformed to frequency domain by discrete cosine transform. Frequency analysis is used to define the caption candidate region, and twelve wavelet features are extracted from the region and considered as the input of the classifier to detect the text blocks. Experimental results show that the proposed approach can fast and robustly detect captions from news video."
            },
            "slug": "Caption-Localization-and-Detection-for-News-Videos-Lee-Chiang",
            "title": {
                "fragments": [],
                "text": "Caption Localization and Detection for News Videos Using Frequency Analysis and Wavelet Features"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The proposed algorithm only detects captions excluding other miscellaneous types of text, and can fast and robustly detect captions from news video."
            },
            "venue": {
                "fragments": [],
                "text": "19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113504611"
                        ],
                        "name": "Hui Fu",
                        "slug": "Hui-Fu",
                        "structuredName": {
                            "firstName": "Hui",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hui Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752239"
                        ],
                        "name": "Xiabi Liu",
                        "slug": "Xiabi-Liu",
                        "structuredName": {
                            "firstName": "Xiabi",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiabi Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7415267"
                        ],
                        "name": "Yunde Jia",
                        "slug": "Yunde-Jia",
                        "structuredName": {
                            "firstName": "Yunde",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunde Jia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "After that, in order to estimate better parameters of GMM, the authors develop an updated approach [18] by using Maximum-Minimum Similarity (MMS) training, which can maximize the similarities between observations and models from the same classes, and minimize those for different classes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16703294,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aed1e666882bbd163e558185c1c0c1f31de8d088",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, the discriminative training criterion of maximum-minimum similarity (MMS) is used to improve the performance of text extraction based on Gaussian mixture modeling of neighbor characters. A recognizer is optimized in the MMS training through maximizing the similarities between observations and models from the same classes, and minimizing those for different classes. Based on this idea, we define the corresponding objective function for text extraction. Through minimizing the objective function by using the gradient descent method, the optimum parameters of our text extraction method are obtained. Compared with the maximum likelihood estimation (MLE) of parameters, the result trained with the MMS method makes the overall performance of text extraction improved greatly. The precision rate decreased little from 94.59% to 93.56%, but the recall rate increased a lot from 80.39% to 98.55%."
            },
            "slug": "Maximum-Minimum-Similarity-Training-for-Text-Fu-Liu",
            "title": {
                "fragments": [],
                "text": "Maximum-Minimum Similarity Training for Text Extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "This paper uses the discriminative training criterion of maximum-minimum similarity (MMS) to improve the performance of text extraction based on Gaussian mixture modeling of neighbor characters and defines the corresponding objective function for text extraction."
            },
            "venue": {
                "fragments": [],
                "text": "ICONIP"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115518537"
                        ],
                        "name": "David Liu",
                        "slug": "David-Liu",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40894914"
                        ],
                        "name": "Tsuhan Chen",
                        "slug": "Tsuhan-Chen",
                        "structuredName": {
                            "firstName": "Tsuhan",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tsuhan Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[40] extend Discriminative Random Field (DRF) from 2D to 3D by integrating both intra-frame neighbors and inter-frame neighbors, and build two graphical models which combine the DRF with Hidden Markov Model (HMM) to detect text objects in videos."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2548955,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c87335c18ece5ae93b5aceacc86a745dca34d8f4",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a general object detection framework which combines the hidden Markov model with the discriminative random fields. Recent object detection algorithms have achieved impressive results by using graphical models, such as Markov random field. These models, however, have only been applied to two dimensional images. In many scenarios, video is the directly available source rather than images, hence an important information for detecting objects has been omitted - the temporal information. To demonstrate the importance of temporal information, we apply graphical models to the task of text detection in video and compare the result of with and without temporal information. We also show the superiority of the proposed models over simple heuristics such as median filter over time"
            },
            "slug": "Object-Detection-in-Video-with-Graphical-Models-Liu-Chen",
            "title": {
                "fragments": [],
                "text": "Object Detection in Video with Graphical Models"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A general object detection framework which combines the hidden Markov model with the discriminative random fields is proposed and the superiority of the proposed models over simple heuristics such as median filter over time is shown."
            },
            "venue": {
                "fragments": [],
                "text": "2006 IEEE International Conference on Acoustics Speech and Signal Processing Proceedings"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144116250"
                        ],
                        "name": "D. Cortez",
                        "slug": "D.-Cortez",
                        "structuredName": {
                            "firstName": "Diogo",
                            "lastName": "Cortez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cortez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120366350"
                        ],
                        "name": "P. Nunes",
                        "slug": "P.-Nunes",
                        "structuredName": {
                            "firstName": "Paulo",
                            "lastName": "Nunes",
                            "middleNames": [
                                "J.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Nunes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37760096"
                        ],
                        "name": "M. Sequeira",
                        "slug": "M.-Sequeira",
                        "structuredName": {
                            "firstName": "Manuel",
                            "lastName": "Sequeira",
                            "middleNames": [
                                "Menezes",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sequeira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144367287"
                        ],
                        "name": "F. Pereira",
                        "slug": "F.-Pereira",
                        "structuredName": {
                            "firstName": "Fernando",
                            "lastName": "Pereira",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Pereira"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 77
                            }
                        ],
                        "text": "After splitting the image into homogeneous regions based on texture features [20], the character detection and word formation are performed by using geometrical constraints, including height, width, aspect ratio, proximity, alignment, luminance and dimension."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 37896502,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbf520504091e7a3929def080577cb012d0915a2",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Image-segmentation-towards-new-image-representation-Cortez-Nunes",
            "title": {
                "fragments": [],
                "text": "Image segmentation towards new image representation methods"
            },
            "venue": {
                "fragments": [],
                "text": "Signal Process. Image Commun."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2396754"
                        ],
                        "name": "Chekuri Choudary",
                        "slug": "Chekuri-Choudary",
                        "structuredName": {
                            "firstName": "Chekuri",
                            "lastName": "Choudary",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chekuri Choudary"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "93645996"
                        ],
                        "name": "Tiecheng Liu",
                        "slug": "Tiecheng-Liu",
                        "structuredName": {
                            "firstName": "Tiecheng",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tiecheng Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Similarly, focusing on the instructional videos of chalk board presentations, Choudary et al. [ 35 ] use the content fluctuation curve based on the number of chalk pixels to measure the content in each frame of an instructional video."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16972177,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5fbc1fb7dee25e4bd54bd51549604f2509846df1",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "In instructional videos of chalk board presentations, the visual content refers to the text and figures written on the boards. Existing methods on video summarization are not effective for this video domain because they are mainly based on low-level image features such as color and edges. In this work, we present a novel approach to summarizing the visual content in instructional videos using middle-level features. We first develop a robust algorithm to extract content text and figures from instructional videos by statistical modelling and clustering. This algorithm addresses the image noise, nonuniformity of the board regions, camera movements, occlusions, and other challenges in the instructional videos that are recorded in real classrooms. Using the extracted text and figures as the middle level features, we retrieve a set of key frames that contain most of the visual content. We further reduce content redundancy and build a mosaicked summary image by matching extracted content based on K-th Hausdorff distance and connected component decomposition. Performance evaluation on four full-length instructional videos shows that our algorithm is highly effective in summarizing instructional video content."
            },
            "slug": "Summarization-of-Visual-Content-in-Instructional-Choudary-Liu",
            "title": {
                "fragments": [],
                "text": "Summarization of Visual Content in Instructional Videos"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work develops a robust algorithm to extract content text and figures from instructional videos by statistical modelling and clustering and retrieves a set of key frames that contain most of the visual content."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1787634"
                        ],
                        "name": "David Bargeron",
                        "slug": "David-Bargeron",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Bargeron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Bargeron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731948"
                        ],
                        "name": "Paul A. Viola",
                        "slug": "Paul-A.-Viola",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Viola",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul A. Viola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2812486"
                        ],
                        "name": "P. Simard",
                        "slug": "P.-Simard",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Simard",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Simard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Bargeron et al. [ 44 ] combines boosting with transductive learning to train an automatic text detector."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1325319,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45650c3eafce9fec37ebb8f3d6b35be85b54be7a",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a promising new framework for improving boosting performance with transductive inference when training an automatic text detector. The resulting detector is fast and efficient, and it exhibits high accuracy on a large test set."
            },
            "slug": "Boosting-based-transductive-learning-for-text-Bargeron-Viola",
            "title": {
                "fragments": [],
                "text": "Boosting-based transductive learning for text detection"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A promising new framework for improving boosting performance with transductive inference when training an automatic text detector is presented, which is fast and efficient, and it exhibits high accuracy on a large test set."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3110392"
                        ],
                        "name": "R. Kasturi",
                        "slug": "R.-Kasturi",
                        "structuredName": {
                            "firstName": "Rangachar",
                            "lastName": "Kasturi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Kasturi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698267"
                        ],
                        "name": "D. Goldgof",
                        "slug": "D.-Goldgof",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Goldgof",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldgof"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47007842"
                        ],
                        "name": "P. Soundararajan",
                        "slug": "P.-Soundararajan",
                        "structuredName": {
                            "firstName": "Padmanabhan",
                            "lastName": "Soundararajan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Soundararajan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144249397"
                        ],
                        "name": "V. Manohar",
                        "slug": "V.-Manohar",
                        "structuredName": {
                            "firstName": "Vasant",
                            "lastName": "Manohar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Manohar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2467151"
                        ],
                        "name": "John S. Garofolo",
                        "slug": "John-S.-Garofolo",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Garofolo",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John S. Garofolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33875838"
                        ],
                        "name": "Rachel Bowers",
                        "slug": "Rachel-Bowers",
                        "structuredName": {
                            "firstName": "Rachel",
                            "lastName": "Bowers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rachel Bowers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47371075"
                        ],
                        "name": "Matthew Boonstra",
                        "slug": "Matthew-Boonstra",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Boonstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Boonstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1906374"
                        ],
                        "name": "V. Korzhova",
                        "slug": "V.-Korzhova",
                        "structuredName": {
                            "firstName": "Valentina",
                            "lastName": "Korzhova",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Korzhova"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2155699044"
                        ],
                        "name": "Jing Zhang",
                        "slug": "Jing-Zhang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Zhang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 152
                            }
                        ],
                        "text": "treats the text features of three neighboring characters as three mixed Gaussian models to extract text objects; The VACE and CLEAR evaluation measures [51]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14662457,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b0246b2edca38d7def294cb55faccc70c55c8a69",
            "isKey": false,
            "numCitedBy": 494,
            "numCiting": 63,
            "paperAbstract": {
                "fragments": [],
                "text": "Common benchmark data sets, standardized performance metrics, and baseline algorithms have demonstrated considerable impact on research and development in a variety of application domains. These resources provide both consumers and developers of technology with a common framework to objectively compare the performance of different algorithms and algorithmic improvements. In this paper, we present such a framework for evaluating object detection and tracking in video: specifically for face, text, and vehicle objects. This framework includes the source video data, ground-truth annotations (along with guidelines for annotation), performance metrics, evaluation protocols, and tools including scoring software and baseline algorithms. For each detection and tracking task and supported domain, we developed a 50-clip training set and a 50-clip test set. Each data clip is approximately 2.5 minutes long and has been completely spatially/temporally annotated at the I-frame level. Each task/domain, therefore, has an associated annotated corpus of approximately 450,000 frames. The scope of such annotation is unprecedented and was designed to begin to support the necessary quantities of data for robust machine learning approaches, as well as a statistically significant comparison of the performance of algorithms. The goal of this work was to systematically address the challenges of object detection and tracking through a common evaluation framework that permits a meaningful objective comparison of techniques, provides the research community with sufficient data for the exploration of automatic modeling techniques, encourages the incorporation of objective evaluation into the development process, and contributes useful lasting resources of a scale and magnitude that will prove to be extremely useful to the computer vision research community for years to come."
            },
            "slug": "Framework-for-Performance-Evaluation-of-Face,-Text,-Kasturi-Goldgof",
            "title": {
                "fragments": [],
                "text": "Framework for Performance Evaluation of Face, Text, and Vehicle Detection and Tracking in Video: Data, Metrics, and Protocol"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The goal of this work was to systematically address the challenges of object detection and tracking through a common evaluation framework that permits a meaningful objective comparison of techniques, provides the research community with sufficient data for the exploration of automatic modeling techniques, encourages the incorporation of objective evaluation into the development process, and contributes useful lasting resources."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Pattern Analysis and Machine Intelligence"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "87531536"
                        ],
                        "name": "A. Panaretos",
                        "slug": "A.-Panaretos",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Panaretos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Panaretos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073684197"
                        ],
                        "name": "Luis Sosa",
                        "slug": "Luis-Sosa",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Sosa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luis Sosa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052189571"
                        ],
                        "name": "Anthony Tang",
                        "slug": "Anthony-Tang",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anthony Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108862960"
                        ],
                        "name": "Shirley Wong",
                        "slug": "Shirley-Wong",
                        "structuredName": {
                            "firstName": "Shirley",
                            "lastName": "Wong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shirley Wong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114080648"
                        ],
                        "name": "Robert Young",
                        "slug": "Robert-Young",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert Young"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[48] propose a new forgiving version of precision and recall."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6379469,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ce39eb5cc1049a1060a499d6b6e94c8b2ec11da1",
            "isKey": false,
            "numCitedBy": 593,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes the robust reading competitions forICDAR 2003. With the rapid growth in research over thelast few years on recognizing text in natural scenes, thereis an urgent need to establish some common benchmarkdatasets, and gain a clear understanding of the current stateof the art. We use the term robust reading to refer to text imagesthat are beyond the capabilities of current commercialOCR packages. We chose to break down the robust readingproblem into three sub-problems, and run competitionsfor each stage, and also a competition for the best overallsystem. The sub-problems we chose were text locating,character recognition and word recognition.By breaking down the problem in this way, we hope togain a better understanding of the state of the art in eachof the sub-problems. Furthermore, our methodology involvesstoring detailed results of applying each algorithm toeach image in the data sets, allowing researchers to study indepth the strengths and weaknesses of each algorithm. Thetext locating contest was the only one to have any entries.We report the results of this contest, and show cases wherethe leading algorithms succeed and fail."
            },
            "slug": "ICDAR-2003-robust-reading-competitions-Lucas-Panaretos",
            "title": {
                "fragments": [],
                "text": "ICDAR 2003 robust reading competitions"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The robust reading problem was broken down into three sub-problems, and competitions for each stage, and also a competition for the best overall system, which was the only one to have any entries."
            },
            "venue": {
                "fragments": [],
                "text": "Seventh International Conference on Document Analysis and Recognition, 2003. Proceedings."
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109552014"
                        ],
                        "name": "Dong-Qing Zhang",
                        "slug": "Dong-Qing-Zhang",
                        "structuredName": {
                            "firstName": "Dong-Qing",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong-Qing Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[41] propose a parts-based approach for scene text detection using a high-order Markov Random Field (MRF) model with belief propagation, which overcomes the limitation of the pairwise MRF that spatial relationship of three characters cannot be captured."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14341205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df6e78f220d598fb961aab165a68697f67be7d30",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting text in natural 3D scenes is a challenging problem due to background clutter and photometric/gemetric variations of scene text. Most prior systems adopt approaches based on deterministic rules, lacking a systematic and scalable framework. In this paper, we present a parts-based approach for 3D scene text detection using a higher-order MRF model. The higher-order structure is used to capture the spatial-feature relations among multiple parts in scene text. The use of higher-order structure and the feature-dependent potential function represents significant departure from the conventional pairwise MRF, which has been successfully applied in several low-level applications. We further develop a variational approximation method, in the form of belief propagation, for inference in the higher-order model. Our experiments using the ICDAR'03 benchmark showed promising results in detecting scene text with significant geometric variations, background clutter on planar surfaces or non-planar surfaces with limited angles."
            },
            "slug": "Learning-to-Detect-Scene-Text-Using-a-Higher-Order-Zhang-Chang",
            "title": {
                "fragments": [],
                "text": "Learning to Detect Scene Text Using a Higher-Order MRF with Belief Propagation"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A parts-based approach for 3D scene text detection using a higher-order MRF model that captures the spatial-feature relations among multiple parts in scene text and develops a variational approximation method, in the form of belief propagation, for inference in the higher- order model."
            },
            "venue": {
                "fragments": [],
                "text": "2004 Conference on Computer Vision and Pattern Recognition Workshop"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47744684"
                        ],
                        "name": "S. Lef\u00e8vre",
                        "slug": "S.-Lef\u00e8vre",
                        "structuredName": {
                            "firstName": "S\u00e9bastien",
                            "lastName": "Lef\u00e8vre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lef\u00e8vre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145645182"
                        ],
                        "name": "N. Vincent",
                        "slug": "N.-Vincent",
                        "structuredName": {
                            "firstName": "Nicole",
                            "lastName": "Vincent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Vincent"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17999153,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "16cccb659174bb9ae66485bee67e6968902ad679",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this article, we focus on the problem of caption detection in video sequences. Contrary to most of existing approaches based on a single detector followed by an ad hoc and costly post-processing, we have decided to consider several detectors and to merge their results in order to combine advantages of each one. First we made a study of captions in video sequences to determine how they are represented in images and to identify their main features (color constancy and background contrast, edge density and regularity, temporal persistence). Based on these features, we then select or define the appropriate detectors and we compare several fusion strategies which can be involved. The logical process we have followed and the satisfying results we have obtained let us validate our contribution."
            },
            "slug": "Caption-localisation-in-video-sequences-by-fusion-Lef\u00e8vre-Vincent",
            "title": {
                "fragments": [],
                "text": "Caption localisation in video sequences by fusion of multiple detectors"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This article considers several detectors of caption detection in video sequences to merge their results in order to combine advantages of each one and compares several fusion strategies which can be involved."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720184"
                        ],
                        "name": "G. Bradski",
                        "slug": "G.-Bradski",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "Bradski",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Bradski"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 138,
                                "start": 134
                            }
                        ],
                        "text": "For example, the continuously adaptive mean shift algorithm (CAMSHIFT) was initially used to detect and track faces in a video stream [55]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 12002813,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fe88db144825e3fa72a8fae1e307cc7d056db5c",
            "isKey": false,
            "numCitedBy": 666,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "As a step towards a perceptual user interface, an object tracking algorithm is developed and demonstrated tracking human faces. Computer vision algorithms that are intended to form part of a perceptual user interface must be fast and efficient. They must be able to track in real time and yet not absorb a major share of computational resources. An efficient, new algorithm is described here based on the mean shift algorithm. The mean shift algorithm robustly finds the mode (peak) of probability distributions. We first describe histogram based methods of producing object probability distributions. In our case, we want to track the mode of an object's probability distribution within a video scene. Since the probability distribution of the object can change and move dynamically in time, the mean shift algorithm is modified to deal with dynamically changing probability distributions. The modified algorithm is called the Continuously Adaptive Mean Shift (CAMSHIFT) algorithm. CAMSHIFT is then used as an interface for games and graphics."
            },
            "slug": "Real-time-face-and-object-tracking-as-a-component-a-Bradski",
            "title": {
                "fragments": [],
                "text": "Real time face and object tracking as a component of a perceptual user interface"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An efficient, new algorithm is described here based on the mean shift algorithm, which robustly finds the mode (peak) of probability distributions within a video scene and is used as an interface for games and graphics."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Fourth IEEE Workshop on Applications of Computer Vision. WACV'98 (Cat. No.98EX201)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38817267"
                        ],
                        "name": "K. Sung",
                        "slug": "K.-Sung",
                        "structuredName": {
                            "firstName": "Kah",
                            "lastName": "Sung",
                            "middleNames": [
                                "Kay"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685292"
                        ],
                        "name": "T. Poggio",
                        "slug": "T.-Poggio",
                        "structuredName": {
                            "firstName": "Tomaso",
                            "lastName": "Poggio",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Poggio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 32
                            }
                        ],
                        "text": "In addition, a bootstrap method [27] is adopted in order to attain a large enough training set."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7164794,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "088eb2d102c6bb486f5270d0b2adff76961994cf",
            "isKey": false,
            "numCitedBy": 2062,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an example-based learning approach for locating vertical frontal views of human faces in complex scenes. The technique models the distribution of human face patterns by means of a few view-based \"face\" and \"nonface\" model clusters. At each image location, a difference feature vector is computed between the local image pattern and the distribution-based model. A trained classifier determines, based on the difference feature vector measurements, whether or not a human face exists at the current image location. We show empirically that the distance metric we adopt for computing difference feature vectors, and the \"nonface\" clusters we include in our distribution-based model, are both critical for the success of our system."
            },
            "slug": "Example-Based-Learning-for-View-Based-Human-Face-Sung-Poggio",
            "title": {
                "fragments": [],
                "text": "Example-Based Learning for View-Based Human Face Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "An example-based learning approach for locating vertical frontal views of human faces in complex scenes and shows empirically that the distance metric adopted for computing difference feature vectors, and the \"nonface\" clusters included in the distribution-based model, are both critical for the success of the system."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Pattern Anal. Mach. Intell."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110180514"
                        ],
                        "name": "Xi Shi",
                        "slug": "Xi-Shi",
                        "structuredName": {
                            "firstName": "Xi",
                            "lastName": "Shi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xi Shi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47103905"
                        ],
                        "name": "Yangsheng Xu",
                        "slug": "Yangsheng-Xu",
                        "structuredName": {
                            "firstName": "Yangsheng",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangsheng Xu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[53] develop a wearable translation robot (Fig."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15249430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f54ef2335cec0e5b5d91061a146f9b1ea42f95bb",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we introduce an intelligent glasses, which can automatically translate multiple languages in real-time, called wearable translation robot. This paper proposes the concept of the system and demonstrates the advantages of the device over other existed systems. The paper presents the system architecture and the functions of components in the system. We then focus on the most crucial technical component, text detection. The paper proposes a novel algorithm based on the fundamental characteristics of all the characters in common use called CIC-based text detection algorithm. We show the effectiveness of the proposed methods, and define some future works."
            },
            "slug": "A-Wearable-Translation-Robot-Shi-Xu",
            "title": {
                "fragments": [],
                "text": "A Wearable Translation Robot"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An intelligent glasses, which can automatically translate multiple languages in real-time, called wearable translation robot is introduced, and a novel algorithm based on the fundamental characteristics of all the characters in common use called CIC-based text detection algorithm is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2005 IEEE International Conference on Robotics and Automation"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145349582"
                        ],
                        "name": "Sanjiv Kumar",
                        "slug": "Sanjiv-Kumar",
                        "structuredName": {
                            "firstName": "Sanjiv",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sanjiv Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145670946"
                        ],
                        "name": "M. Hebert",
                        "slug": "M.-Hebert",
                        "structuredName": {
                            "firstName": "Martial",
                            "lastName": "Hebert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hebert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "Discriminative Random Fields (DRF) was initially applied to detect man-made building in 2D images [57]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10689850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e54f01884e1fba4a0bbd2f0989ad21a16ebb13e3",
            "isKey": false,
            "numCitedBy": 532,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we present discriminative random fields (DRFs), a discriminative framework for the classification of image regions by incorporating neighborhood interactions in the labels as well as the observed data. The discriminative random fields offer several advantages over the conventional Markov random field (MRF) framework. First, the DRFs allow to relax the strong assumption of conditional independence of the observed data generally used in the MRF framework for tractability. This assumption is too restrictive for a large number of applications in vision. Second, the DRFs derive their classification power by exploiting the probabilistic discriminative models instead of the generative models used in the MRF framework. Finally, all the parameters in the DRF model are estimated simultaneously from the training data unlike the MRF framework where likelihood parameters are usually learned separately from the field parameters. We illustrate the advantages of the DRFs over the MRF framework in an application of man-made structure detection in natural images taken from the Corel database."
            },
            "slug": "Discriminative-random-fields:-a-discriminative-for-Kumar-Hebert",
            "title": {
                "fragments": [],
                "text": "Discriminative random fields: a discriminative framework for contextual interaction in classification"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "This work presents discriminative random fields (DRFs), a discrim inative framework for the classification of image regions by incorporating neighborhood interactions in the labels as well as the observed data that offers several advantages over the conventional Markov random field framework."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Computer Vision"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2555244"
                        ],
                        "name": "E. Parvizi",
                        "slug": "E.-Parvizi",
                        "structuredName": {
                            "firstName": "Ehsan",
                            "lastName": "Parvizi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Parvizi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107693222"
                        ],
                        "name": "Q. M. Wu",
                        "slug": "Q.-M.-Wu",
                        "structuredName": {
                            "firstName": "Q.",
                            "lastName": "Wu",
                            "middleNames": [
                                "M.",
                                "Jonathan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. M. Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5099748,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "524941e1eed9965712e9b8dbc9e1d45ea5df052d",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel head detection algorithm based on contour analysis on depth images. A sequence of depth-valued images is used as the system input using a 3D time-of-flight depth sensor. The background and foreground layers of the image are segmented using a straightforward depth thresholding technique. Moving regions are further processed in each frame, and contour analysis is performed on the depth maps to extract the curves of moving regions. Finally, ellipse fitting is performed to determine the objective head targets in the image. This information will be passed to the tracker in order to accomplish tracking the targets in the scene. Experimental results demonstrate the efficiency of the proposed method."
            },
            "slug": "Real-Time-3D-Head-Tracking-Based-on-Time-of-Flight-Parvizi-Wu",
            "title": {
                "fragments": [],
                "text": "Real-Time 3D Head Tracking Based on Time-of-Flight Depth Sensor"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "A novel head detection algorithm based on contour analysis on depth images used as the system input using a 3D time-of-flight depth sensor to determine the objective head targets in the image."
            },
            "venue": {
                "fragments": [],
                "text": "19th IEEE International Conference on Tools with Artificial Intelligence(ICTAI 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2379542"
                        ],
                        "name": "D. Eberly",
                        "slug": "D.-Eberly",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Eberly",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Eberly"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "matrix [11] to capture local and global shape information."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20437637,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "b8344fbd40bb27cc3ce99ad973ffe655b8dc21e3",
            "isKey": false,
            "numCitedBy": 287,
            "numCiting": 92,
            "paperAbstract": {
                "fragments": [],
                "text": "Preface. 1. Introduction. 2. Mathematical Preliminaries. 3. Ridges in Euclidean Geometry. 4. Ridges in Riemannian Geometry. 5. Ridges of Functions Defined on Manifolds. 6. Applications to Image and Data Analysis. 7. Implementation Issues. Bibliography. Index."
            },
            "slug": "Ridges-in-Image-and-Data-Analysis-Eberly",
            "title": {
                "fragments": [],
                "text": "Ridges in Image and Data Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This book discussesidges in Euclidean Geometry, a model of geometry based on the model of Riemannian geometry, and applications to Image and Data Analysis."
            },
            "venue": {
                "fragments": [],
                "text": "Computational Imaging and Vision"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1708655"
                        ],
                        "name": "B. Olshausen",
                        "slug": "B.-Olshausen",
                        "structuredName": {
                            "firstName": "Bruno",
                            "lastName": "Olshausen",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Olshausen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49649079"
                        ],
                        "name": "D. Field",
                        "slug": "D.-Field",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Field",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Field"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Sparse representation was initially used for research on the receptive fields of simple cells [ 58 ]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4358477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8012c4a1e2ca663f1a04e80cbb19631a00cbab27",
            "isKey": false,
            "numCitedBy": 5635,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "THE receptive fields of simple cells in mammalian primary visual cortex can be characterized as being spatially localized, oriented1\u20134 and bandpass (selective to structure at different spatial scales), comparable to the basis functions of wavelet transforms5,6. One approach to understanding such response properties of visual neurons has been to consider their relationship to the statistical structure of natural images in terms of efficient coding7\u201312. Along these lines, a number of studies have attempted to train unsupervised learning algorithms on natural images in the hope of developing receptive fields with similar properties13\u201318, but none has succeeded in producing a full set that spans the image space and contains all three of the above properties. Here we investigate the proposal8,12 that a coding strategy that maximizes sparseness is sufficient to account for these properties. We show that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex. The resulting sparse image code provides a more efficient representation for later stages of processing because it possesses a higher degree of statistical independence among its outputs."
            },
            "slug": "Emergence-of-simple-cell-receptive-field-properties-Olshausen-Field",
            "title": {
                "fragments": [],
                "text": "Emergence of simple-cell receptive field properties by learning a sparse code for natural images"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "It is shown that a learning algorithm that attempts to find sparse linear codes for natural scenes will develop a complete family of localized, oriented, bandpass receptive fields, similar to those found in the primary visual cortex."
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3312922"
                        ],
                        "name": "H. Aradhye",
                        "slug": "H.-Aradhye",
                        "structuredName": {
                            "firstName": "Hrishikesh",
                            "lastName": "Aradhye",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aradhye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1954664"
                        ],
                        "name": "B. Bakshi",
                        "slug": "B.-Bakshi",
                        "structuredName": {
                            "firstName": "Bhavik",
                            "lastName": "Bakshi",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bakshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50038502"
                        ],
                        "name": "R. Strauss",
                        "slug": "R.-Strauss",
                        "structuredName": {
                            "firstName": "Ramon",
                            "lastName": "Strauss",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Strauss"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48058858"
                        ],
                        "name": "James F. Davis",
                        "slug": "James-F.-Davis",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Davis",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James F. Davis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18498427,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "4ac1c793c7dced11732f1c737cfccf93d192f11c",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Most practical process data contain contributions at multiple scales in time and frequency, but most existing SPC (statistical process control) methods are best for detecting events at only one scale. For example, Shewhart charts are best for detecting large, localized changes, while EWMA and CUSUM charts are best for detecting small changes at coarse scales. A multiscale approach for SPC, adaptable to the scale of relevant signal features and developed based on wavelet analysis, detects abnormal events at multiple scales as relatively large wavelet coefficients. Univariate and multivariate multiscale SPC (MSSPC) for detecting abnormal operation are theoretically analyzed, and their properties are compared with existing SPC methods based on their average run lengths. SPC methods are best for detecting features over a narrow range of scales. Their performance can deteriorate rapidly if abnormal features lie outside this limited range. Since in most industrial processes, the nature of abnormal features is not known a priori, MSSPC performs better on average due to its adaptability to the scale of the features and for monitoring autocorrelated measurements since dyadic wavelets decorrelate most stochastic processes. MSSPC with dyadic discretization is appropriate for SPC of highly autocorrelated or nonstationary stochastic processes. If normal measurements are uncorrelated or contain only mild autocorrelation, it is better to use MSSPC with integer or unformly discretized wavelets. Many existing methods such as MA, EWMA, CUSUM, Shewhart, batch means charts, and their multivariate extensions are special cases of MSSPC."
            },
            "slug": "Multiscale-SPC-using-wavelets:-Theoretical-analysis-Aradhye-Bakshi",
            "title": {
                "fragments": [],
                "text": "Multiscale SPC using wavelets: Theoretical analysis and properties"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110306087"
                        ],
                        "name": "Makoto Tanaka",
                        "slug": "Makoto-Tanaka",
                        "structuredName": {
                            "firstName": "Makoto",
                            "lastName": "Tanaka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Makoto Tanaka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2257624"
                        ],
                        "name": "Hideaki Goto",
                        "slug": "Hideaki-Goto",
                        "structuredName": {
                            "firstName": "Hideaki",
                            "lastName": "Goto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hideaki Goto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Fig. 11. (a) Autonomous text capturing robot (From Tanaka et al. [ 52 ])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Tanaka et al. [ 52 ] describe an autonomous text capturing robot (Fig. 11(a)) equipped with SONY EVI-D100 camera, NISC-DV converter, and two laptops."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31217889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "416d6be1f83ae578a61eba2ac975ebd4b32114b4",
            "isKey": false,
            "numCitedBy": 26,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "When an autonomous robot tries to find text in the surrounding scene using an onboard video camera, some duplicate text images appear in the video frames. To avoid recognizing the same text many times, it is necessary to decrease the number of text candidate regions for recognition. This paper presents a text capturing robot that can look around the environment using an active camera. The text candidate regions are extracted from the images using an improved DCT feature. The text regions are tracked in the video sequence so that the number of text images to be recognized is reduced. In the experiment, we tested 460 images of a corridor with fifteen signboards including text. The number of text candidate regions is reduced by 90.1% using our text tracking method."
            },
            "slug": "Autonomous-Text-Capturing-Robot-Using-Improved-DCT-Tanaka-Goto",
            "title": {
                "fragments": [],
                "text": "Autonomous Text Capturing Robot Using Improved DCT Feature and Text Tracking"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper presents a text capturing robot that can look around the environment using an active camera and the number of text candidate regions is reduced by 90.1% using the text tracking method."
            },
            "venue": {
                "fragments": [],
                "text": "Ninth International Conference on Document Analysis and Recognition (ICDAR 2007)"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2767216"
                        ],
                        "name": "Chan Umai",
                        "slug": "Chan-Umai",
                        "structuredName": {
                            "firstName": "Chan",
                            "lastName": "Umai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chan Umai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731233"
                        ],
                        "name": "A. Kassim",
                        "slug": "A.-Kassim",
                        "structuredName": {
                            "firstName": "Ashraf",
                            "lastName": "Kassim",
                            "middleNames": [
                                "Ali"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kassim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38700274"
                        ],
                        "name": "L.-Y. Chew",
                        "slug": "L.-Y.-Chew",
                        "structuredName": {
                            "firstName": "L.-Y.",
                            "lastName": "Chew",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L.-Y. Chew"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, [ 43 ] uses an improved drift compensation approach for scrolling text extraction; [34] use comparatively static periods of instructional video to locate text frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "Umai et al. [ 43 ] present a modified drift compensation approach for scrolling text extraction."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17179985,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2c5b0fd9bd533c54782fff208d2ab883045ad24",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Text superimposed on the video frames provides supplemental but important information for video indexing and retrieval. The detection and recognition of text from video is thus an important issue in automated content-based indexing of visual information in video archives. Text of interest is not limited to static text. They could be scrolling in a linear motion where only part of the text information is available during different frames of the video. The problem is further complicated if the video is corrupted with noise. An algorithm is proposed to detect, classify and segment both static and simple linear moving text in complex noisy background. The extracted texts are further processed using averaging to attain a quality suitable for text recognition by commercial optical character recognition (OCR) software"
            },
            "slug": "Detection-and-Interpretation-of-Text-Information-in-Umai-Kassim",
            "title": {
                "fragments": [],
                "text": "Detection and Interpretation of Text Information in Noisy Video Sequences"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An algorithm is proposed to detect, classify and segment both static and simple linear moving text in complex noisy background and attain a quality suitable for text recognition by commercial optical character recognition software."
            },
            "venue": {
                "fragments": [],
                "text": "2006 9th International Conference on Control, Automation, Robotics and Vision"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2522004"
                        ],
                        "name": "J. Gllavata",
                        "slug": "J.-Gllavata",
                        "structuredName": {
                            "firstName": "Julinda",
                            "lastName": "Gllavata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gllavata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738703"
                        ],
                        "name": "R. Ewerth",
                        "slug": "R.-Ewerth",
                        "structuredName": {
                            "firstName": "Ralph",
                            "lastName": "Ewerth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Ewerth"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1685922"
                        ],
                        "name": "B. Freisleben",
                        "slug": "B.-Freisleben",
                        "structuredName": {
                            "firstName": "Bernd",
                            "lastName": "Freisleben",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Freisleben"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Gallavata et al. [ 45 ] propose a similar motion vector based text tracking approach for MEPG videos."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6267516,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bd76cb9405970b6c60be542c1085cc99e9a2dd7f",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Tracking superimposed text moving across several frames of a video is relevant for exploiting its temporal occurrence for effective video content indexing and retrieval. In this paper, an approach is presented that automatically detects, localizes and tracks text appearing in videos. The proposed approach consists of two steps: (1) unsupervised text detection and localization in each Nth frame to monitor new text events, i.e. text appearing in a video for the first time; (2) text tracking within a group of pictures (GOP) using MPEG motion vector information extracted directly from the compressed video stream. Comparative experimental results for a set of videos are presented to show the benefits of our approach."
            },
            "slug": "Tracking-text-in-MPEG-videos-Gllavata-Ewerth",
            "title": {
                "fragments": [],
                "text": "Tracking text in MPEG videos"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An approach is presented that automatically detects, localizes and tracks text appearing in videos using unsupervised text detection and localization in each Nth frame to monitor new text events."
            },
            "venue": {
                "fragments": [],
                "text": "MULTIMEDIA '04"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784196"
                        ],
                        "name": "Datong Chen",
                        "slug": "Datong-Chen",
                        "structuredName": {
                            "firstName": "Datong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Datong Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733733"
                        ],
                        "name": "H. Bourlard",
                        "slug": "H.-Bourlard",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "Bourlard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bourlard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710257"
                        ],
                        "name": "J. Thiran",
                        "slug": "J.-Thiran",
                        "structuredName": {
                            "firstName": "Jean-Philippe",
                            "lastName": "Thiran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Thiran"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "Bounding boxes of candidate regions are generated by a baseline algorithm [25] and empirical constrains."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5170600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8cb23672f5d94a75a7ed9cc7c870be398bc0259",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents a fast and robust algorithm to identify text in image or video frames with complex backgrounds and compression effects. The algorithm first extracts the candidate text line on the basis of edge analysis, baseline location and heuristic constraints. Support Vector Machine (SVM) is then used to identify text line from the candidates in edge-based distance map feature space. Experiments based on a large amount of images and video frames from different sources showed the advantages of this algorithm compared to conventional methods in both identification quality and computation time."
            },
            "slug": "Text-identification-in-complex-background-using-SVM-Chen-Bourlard",
            "title": {
                "fragments": [],
                "text": "Text identification in complex background using SVM"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A fast and robust algorithm to identify text in image or video frames with complex backgrounds and compression effects with advantages compared to conventional methods in both identification quality and computation time is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 2001 IEEE Computer Society Conference on Computer Vision and Pattern Recognition. CVPR 2001"
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 136
                            }
                        ],
                        "text": "The multiscale statistical process control (MSSPC) was originally proposed for detecting changes in univariate and multivariate signals [56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Multiscale Statistical Process Control Using Wavelets\u0096 Theoretical Analysis and Properties"
            },
            "venue": {
                "fragments": [],
                "text": "AIChE Journal, 49(4), pp. 939-958"
            },
            "year": 2003
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "For example, in ridge-based text extraction approach [10], a text string is modeled as its center line and the skeletons of characters by ridges at different hierarchical scales; The GMM based algorithm [16]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A lux"
            },
            "venue": {
                "fragments": [],
                "text": "H.L. Nguyen T. and A. Boucher, A novel approach for text detection in images using structural features, The 3rd International Conference on Advances in Pattern Recognition, pp. 627-635"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Texture - based approach for text detection in image using support vector machine and continuously adaptive mean shift algorithm , IEEE Trans"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Analysis and Machine Intelligence"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "For example, [43] uses an improved drift compensation approach for scrolling text extraction; [34] use comparatively static periods of instructional video to locate text frames."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kassim A"
            },
            "venue": {
                "fragments": [],
                "text": "C.L. Yue, Detection and interpretation of text information in noisy video sequences, Proceedings of International Conference on Control, Automation, Robotics and Vision, pp. 1-4"
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "First, the approach extracts candidate text regions by edge pixels clustering [17]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text Area extraction Method Based on Edge-pixels Clustering"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 8th International Computer Scientists, Convergence of Computing Technologies"
            },
            "year": 2005
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[39] also use DCT transform to extract text in MPEG format, however, only seven DCT coefficients (three"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Text Detection"
            },
            "venue": {
                "fragments": [],
                "text": "Localization, and Segmentation in Compressed Videos, Proceedings of IEEE international conference on acoustics, speech and signal processing, Vol. 2, pp. 385-388"
            },
            "year": 2006
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 27,
            "methodology": 34,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 64,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/Extraction-of-Text-Objects-in-Video-Documents:-Zhang-Kasturi/d0567609da19ae90f1742800f1ff873b9f1bd411?sort=total-citations"
}