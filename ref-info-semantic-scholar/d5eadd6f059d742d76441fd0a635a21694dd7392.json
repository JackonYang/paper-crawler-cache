{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39713408"
                        ],
                        "name": "Mikael Henaff",
                        "slug": "Mikael-Henaff",
                        "structuredName": {
                            "firstName": "Mikael",
                            "lastName": "Henaff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikael Henaff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 63
                            }
                        ],
                        "text": "FFT based convnet implementations have exploited this property [11, 15]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18233038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a7621b4ec18719b08f3a2a444b6d37a2e20227b7",
            "isKey": false,
            "numCitedBy": 460,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional networks are one of the most widely employed architectures in computer vision and machine learning. In order to leverage their ability to learn complex functions, large amounts of data are required for training. Training a large convolutional network to produce state-of-the-art results can take weeks, even when using modern GPUs. Producing labels using a trained network can also be costly when dealing with web-scale datasets. In this work, we present a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations. This is done by computing convolutions as pointwise products in the Fourier domain while reusing the same transformed feature map many times. The algorithm is implemented on a GPU architecture and addresses a number of related challenges."
            },
            "slug": "Fast-Training-of-Convolutional-Networks-through-Mathieu-Henaff",
            "title": {
                "fragments": [],
                "text": "Fast Training of Convolutional Networks through FFTs"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work presents a simple algorithm which accelerates training and inference by a significant factor, and can yield improvements of over an order of magnitude compared to existing state-of-the-art implementations."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800919"
                        ],
                        "name": "Nicolas Vasilache",
                        "slug": "Nicolas-Vasilache",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Vasilache",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Vasilache"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115354049"
                        ],
                        "name": "Jeff Johnson",
                        "slug": "Jeff-Johnson",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143949035"
                        ],
                        "name": "Micha\u00ebl Mathieu",
                        "slug": "Micha\u00ebl-Mathieu",
                        "structuredName": {
                            "firstName": "Micha\u00ebl",
                            "lastName": "Mathieu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Micha\u00ebl Mathieu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2127604"
                        ],
                        "name": "Soumith Chintala",
                        "slug": "Soumith-Chintala",
                        "structuredName": {
                            "firstName": "Soumith",
                            "lastName": "Chintala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Soumith Chintala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3058304"
                        ],
                        "name": "Serkan Piantino",
                        "slug": "Serkan-Piantino",
                        "structuredName": {
                            "firstName": "Serkan",
                            "lastName": "Piantino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Serkan Piantino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "Low performance at moderate values of N suggests that the FFT convolution implementation either uses large tiles, or possibly just a single tile per image, as in [12], which leads to inefficient multiplication stages unlessN is large."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[12], and then implemented in the NVIDIA cuDNN library [ 1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15193948,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "326d65827307862ddc3d39b84ebc662e83ff95b3",
            "isKey": false,
            "numCitedBy": 293,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "We examine the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units. We introduce two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFT (over 1.5x) for whole CNNs. Both of these convolution implementations are available in open source, and are faster than NVIDIA's cuDNN implementation for many common convolutional layers (up to 23.5x for some synthetic kernel configurations). We discuss different performance regimes of convolutions, comparing areas where straightforward time domain convolutions outperform Fourier frequency domain convolutions. Details on algorithmic applications of NVIDIA GPU hardware specifics in the implementation of fbfft are also provided."
            },
            "slug": "Fast-Convolutional-Nets-With-fbfft:-A-GPU-Vasilache-Johnson",
            "title": {
                "fragments": [],
                "text": "Fast Convolutional Nets With fbfft: A GPU Performance Evaluation"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "This work examines the performance profile of Convolutional Neural Network training on the current generation of NVIDIA Graphics Processing Units, and introduces two new Fast Fourier Transform convolution implementations: one based on NVIDIA's cuFFT library, and another based on a Facebook authored FFT implementation, fbfft, that provides significant speedups over cuFFt."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2259796"
                        ],
                        "name": "J. Cong",
                        "slug": "J.-Cong",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Cong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082437233"
                        ],
                        "name": "Bing-Yu Xiao",
                        "slug": "Bing-Yu-Xiao",
                        "structuredName": {
                            "firstName": "Bing-Yu",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bing-Yu Xiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 92
                            }
                        ],
                        "text": "Strassen recursions can be performed before the fast convolution algorithms, as was done in [3] with direct convolution."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 58
                            }
                        ],
                        "text": "Then we define element multiplication to be 2D correlation[3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "Cambridge University Press, 2010.3, 4\n[3] Jason Cong and Bingjun Xiao."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 13
                            }
                        ],
                        "text": "Cong and Xiao[3] used the Strassen algorithm for fast matrix multiplication to reduce the number of correlations, thereby reducing the total arithmetic complexity of the convnet layer."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 66
                            }
                        ],
                        "text": "The Strassen algorithm for fast matrix multiplication was used by Cong and Xiao [3] to reduce the number of convolutions in a convnet layer, thereby reducing its total arithmetic complexity."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16946782,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5f1beada9e269b2a7faed8dfe936919ac0c2397",
            "isKey": true,
            "numCitedBy": 277,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Convolutional Neural Networks (CNNs) have been successfully used for many computer vision applications. It would be beneficial to these applications if the computational workload of CNNs could be reduced. In this work we analyze the linear algebraic properties of CNNs and propose an algorithmic modification to reduce their computational workload. An up to a 47% reduction can be achieved without any change in the image recognition results or the addition of any hardware accelerators."
            },
            "slug": "Minimizing-Computation-in-Convolutional-Neural-Cong-Xiao",
            "title": {
                "fragments": [],
                "text": "Minimizing Computation in Convolutional Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work analyzes the linear algebraic properties of CNNs and proposes an algorithmic modification to reduce their computational workload, achieving up to a 47% reduction."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Deep convolutional neural networks (convnets) achieve state of the art results on image recognition problems [11][7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "We ran accuracy and speed experiments with VGG Network E [11]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14124313,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb42cf88027de515750f230b23b1a057dc782108",
            "isKey": false,
            "numCitedBy": 63195,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we investigate the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting. Our main contribution is a thorough evaluation of networks of increasing depth using an architecture with very small (3x3) convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers. These findings were the basis of our ImageNet Challenge 2014 submission, where our team secured the first and the second places in the localisation and classification tracks respectively. We also show that our representations generalise well to other datasets, where they achieve state-of-the-art results. We have made our two best-performing ConvNet models publicly available to facilitate further research on the use of deep visual representations in computer vision."
            },
            "slug": "Very-Deep-Convolutional-Networks-for-Large-Scale-Simonyan-Zisserman",
            "title": {
                "fragments": [],
                "text": "Very Deep Convolutional Networks for Large-Scale Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "This work investigates the effect of the convolutional network depth on its accuracy in the large-scale image recognition setting using an architecture with very small convolution filters, which shows that a significant improvement on the prior-art configurations can be achieved by pushing the depth to 16-19 weight layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116011472"
                        ],
                        "name": "Suyog Gupta",
                        "slug": "Suyog-Gupta",
                        "structuredName": {
                            "firstName": "Suyog",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suyog Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31651864"
                        ],
                        "name": "A. Agrawal",
                        "slug": "A.-Agrawal",
                        "structuredName": {
                            "firstName": "Ankur",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33678523"
                        ],
                        "name": "K. Gopalakrishnan",
                        "slug": "K.-Gopalakrishnan",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Gopalakrishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Gopalakrishnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32967358"
                        ],
                        "name": "P. Narayanan",
                        "slug": "P.-Narayanan",
                        "structuredName": {
                            "firstName": "Pritish",
                            "lastName": "Narayanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Narayanan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 149,
                                "start": 146
                            }
                        ],
                        "text": "\u20261)2/m2NHWCK\n= \u03b1\u2032NHWCK (20)\nwhere\u03b1 = (m+R\u2212 1)2 and\u03b1\u2032 = \u03b1/m2\nThe total arithmetic complexities of the data, filter, and inverse transforms can be written as:\nT (D) = \u03b2/m2NHWC\nT (F ) = \u03b3CK\nT (I) = \u03b4/m2NHWK\n(21)\nwhere\u03b2, \u03b3, and\u03b4 are the number of floating point instructions used by the\u2026"
                    },
                    "intents": []
                }
            ],
            "corpusId": 2547043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b7cf49e30355633af2db19f35189410c8515e91f",
            "isKey": false,
            "numCitedBy": 1510,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Training of large-scale deep neural networks is often constrained by the available computational resources. We study the effect of limited precision data representation and computation on neural network training. Within the context of low-precision fixed-point computations, we observe the rounding scheme to play a crucial role in determining the network's behavior during training. Our results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy. We also demonstrate an energy-efficient hardware accelerator that implements low-precision fixed-point arithmetic with stochastic rounding."
            },
            "slug": "Deep-Learning-with-Limited-Numerical-Precision-Gupta-Agrawal",
            "title": {
                "fragments": [],
                "text": "Deep Learning with Limited Numerical Precision"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results show that deep networks can be trained using only 16-bit wide fixed-point number representation when using stochastic rounding, and incur little to no degradation in the classification accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73175007"
                        ],
                        "name": "Andrew Lavin",
                        "slug": "Andrew-Lavin",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Lavin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Lavin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 149
                            }
                        ],
                        "text": "Recent work has shown that the direct convolution algorithm for multi-channel filtering can be computed at near full utilization on modern computers.[8] Therefore further speedup requires fast algorithms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14595069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4fe1c707a48869cbbdf3eb0384e526d1d294f7e2",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes maxDNN, a computationally efficient convolution kernel for deep learning with the NVIDIA Maxwell GPU. maxDNN reaches 96.3% computational efficiency on typical deep learning network architectures. The design combines ideas from cuda-convnet2 with the Maxas SGEMM assembly code. We only address forward propagation (FPROP) operation of the network, but we believe that the same techniques used here will be effective for backward propagation (BPROP) as well."
            },
            "slug": "maxDNN:-An-Efficient-Convolution-Kernel-for-Deep-Lavin",
            "title": {
                "fragments": [],
                "text": "maxDNN: An Efficient Convolution Kernel for Deep Learning with Maxwell GPUs"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper describes maxDNN, a computationally efficient convolution kernel for deep learning with the NVIDIA Maxwell GPU, which reaches 96.3% computational efficiency on typical deep learning network architectures."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388466"
                        ],
                        "name": "Matthieu Courbariaux",
                        "slug": "Matthieu-Courbariaux",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Courbariaux",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Courbariaux"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145719986"
                        ],
                        "name": "J. David",
                        "slug": "J.-David",
                        "structuredName": {
                            "firstName": "Jean-Pierre",
                            "lastName": "David",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. David"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 97
                            }
                        ],
                        "text": "Because direct convolution is accurate enough for training and inference with low precision data [5, 6], we conclude that F (4\u00d7 4, 3\u00d7 3) is too."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 55
                            }
                        ],
                        "text": "Convnets require surprisingly little numeric precision [5, 6]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 26450018,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8c35c2c39fdd2ec6af37ddc8c51deb396aefef8",
            "isKey": false,
            "numCitedBy": 166,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We simulate the training of a set of state of the art neural networks, the Maxout networks (Goodfellow et al., 2013a), on three benchmark datasets: the MNIST, CIFAR10 and SVHN, with three distinct arithmetics: floating point, fixed point and dynamic fixed point. For each of those datasets and for each of those arithmetics, we assess the impact of the precision of the computations on the final error of the training. We find that very low precision computation is sufficient not just for running trained networks but also for training them. For example, almost state-of-the-art results were obtained on most datasets with 10 bits for computing activations and gradients, and 12 bits for storing updated parameters."
            },
            "slug": "Low-precision-arithmetic-for-deep-learning-Courbariaux-Bengio",
            "title": {
                "fragments": [],
                "text": "Low precision arithmetic for deep learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is found that very low precision computation is sufficient not just for running trained networks but also for training them."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054165706"
                        ],
                        "name": "S. Ioffe",
                        "slug": "S.-Ioffe",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Ioffe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ioffe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2574060"
                        ],
                        "name": "Christian Szegedy",
                        "slug": "Christian-Szegedy",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Szegedy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christian Szegedy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5808102,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d376d6978dad0374edfa6709c9556b42d3594d3",
            "isKey": false,
            "numCitedBy": 29648,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Training Deep Neural Networks is complicated by the fact that the distribution of each layer's inputs changes during training, as the parameters of the previous layers change. This slows down the training by requiring lower learning rates and careful parameter initialization, and makes it notoriously hard to train models with saturating nonlinearities. We refer to this phenomenon as internal covariate shift, and address the problem by normalizing layer inputs. Our method draws its strength from making normalization a part of the model architecture and performing the normalization for each training mini-batch. Batch Normalization allows us to use much higher learning rates and be less careful about initialization, and in some cases eliminates the need for Dropout. Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin. Using an ensemble of batch-normalized networks, we improve upon the best published result on ImageNet classification: reaching 4.82% top-5 test error, exceeding the accuracy of human raters."
            },
            "slug": "Batch-Normalization:-Accelerating-Deep-Network-by-Ioffe-Szegedy",
            "title": {
                "fragments": [],
                "text": "Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Applied to a state-of-the-art image classification model, Batch Normalization achieves the same accuracy with 14 times fewer training steps, and beats the original model by a significant margin."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805342"
                        ],
                        "name": "R. Blahut",
                        "slug": "R.-Blahut",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Blahut",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Blahut"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "uivalent to polynomial multiplication, then transposes the linear convolution algorithm to yield a minimal \ufb01ltering algorithm. The reader is referred to Winograd\u2019s seminal book [13], or Blahut\u2019s book [2] for a modern treatment of the subject. We provide derivations of the speci\ufb01c algorithms used in this paper in the supplementary material. Algorithm 1 Compute Convnet Layer with Winograd Minimal Filte"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 220905055,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c750ac12cbf5afa045e04cbbaab4f447af8a0672",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 119,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Introduction 2. Introduction to abstract algebra 3. Fast algorithms for the discrete Fourier transform 4. Fast algorithms based on doubling strategies 5. Fast algorithms for short convolutions 6. Architecture of filters and transforms 7. Fast algorithms for solving Toeplitz systems 8. Fast algorithms for trellis search 9. Numbers and fields 10. Computation in finite fields and rings 11. Fast algorithms and multidimensional convolutions 12. Fast algorithms and multidimensional transforms Appendices: A. A collection of cyclic convolution algorithms B. A collection of Winograd small FFT algorithms."
            },
            "slug": "Fast-Algorithms-for-Signal-Processing-Blahut",
            "title": {
                "fragments": [],
                "text": "Fast Algorithms for Signal Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A collection of cyclic convolution algorithms and a collection of Winograd small FFT algorithms for solving Toeplitz systems are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5556470,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80d800dfadbe2e6c7b2367d9229cc82912d55889",
            "isKey": false,
            "numCitedBy": 889,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "I present a new way to parallelize the training of convolutional neural networks across multiple GPUs. The method scales significantly better than all alternatives when applied to modern convolutional neural networks."
            },
            "slug": "One-weird-trick-for-parallelizing-convolutional-Krizhevsky",
            "title": {
                "fragments": [],
                "text": "One weird trick for parallelizing convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A new way to parallelize the training of convolutional neural networks across multiple GPUs is presented, which scales significantly better than all alternatives when applied to modern convolutionAL neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116011472"
                        ],
                        "name": "Suyog Gupta",
                        "slug": "Suyog-Gupta",
                        "structuredName": {
                            "firstName": "Suyog",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suyog Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31765881"
                        ],
                        "name": "Wei Zhang",
                        "slug": "Wei-Zhang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148956216"
                        ],
                        "name": "Fei Wang",
                        "slug": "Fei-Wang",
                        "structuredName": {
                            "firstName": "Fei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fei Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7063004,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "45dd7be0cf09bfc8b97522890e3ed703defd9995",
            "isKey": false,
            "numCitedBy": 101,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep learning with a large number of parametersrequires distributed training, where model accuracy and runtimeare two important factors to be considered. However, there hasbeen no systematic study of the tradeoff between these two factorsduring the model training process. This paper presents Rudra, aparameter server based distributed computing framework tunedfor training large-scale deep neural networks. Using variants ofthe asynchronous stochastic gradient descent algorithm we studythe impact of synchronization protocol, stale gradient updates, minibatch size, learning rates, and number of learners on runtimeperformance and model accuracy. We introduce a new learningrate modulation strategy to counter the effect of stale gradientsand propose a new synchronization protocol that can effectivelybound the staleness in gradients, improve runtime performanceand achieve good model accuracy. Our empirical investigationreveals a principled approach for distributed training of neuralnetworks: the mini-batch size per learner should be reducedas more learners are added to the system to preserve the modelaccuracy. We validate this approach using commonly-used imageclassification benchmarks: CIFAR10 and ImageNet."
            },
            "slug": "Model-Accuracy-and-Runtime-Tradeoff-in-Distributed-Gupta-Zhang",
            "title": {
                "fragments": [],
                "text": "Model Accuracy and Runtime Tradeoff in Distributed Deep Learning: A Systematic Study"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents Rudra, a parameter server based distributed computing framework tuned for training large-scale deep neural networks, and introduces a new learningrate modulation strategy to counter the effect of stale gradients and proposes a new synchronization protocol that can effectively bound the staleness in gradients, improve runtime performance and achieve good model accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE 16th International Conference on Data Mining (ICDM)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2284787"
                        ],
                        "name": "V. Strassen",
                        "slug": "V.-Strassen",
                        "structuredName": {
                            "firstName": "Volker",
                            "lastName": "Strassen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Strassen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The Strassen algorithm for fast matrix multiplication [13] was used by Cong and Xiao [3] to reduce the number of convolutions in a convnet layer, thereby reducing its total arithmetic complexity."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 121656251,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "cb80b424db4c94cbaf4c3ae0e570ac3eb6f3bcf3",
            "isKey": false,
            "numCitedBy": 2347,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "t. Below we will give an algorithm which computes the coefficients of the product of two square matrices A and B of order n from the coefficients of A and B with tess than 4 . 7 n l\u00b0g7 arithmetical operations (all logarithms in this paper are for base 2, thus tog 7 ~ 2.8; the usual method requires approximately 2n 3 arithmetical operations). The algorithm induces algorithms for invert ing a matr ix of order n, solving a system of n linear equations in n unknowns, comput ing a determinant of order n etc. all requiring less than const n l\u00b0g 7 arithmetical operations. This fact should be compared with the result of KLYUYEV and KOKOVKINSHCHERBAK [1 ] tha t Gaussian elimination for solving a system of l inearequations is optimal if one restricts oneself to operations upon rows and columns as a whole. We also note tha t WlNOGRAD [21 modifies the usual algorithms for matr ix multiplication and inversion and for solving systems of linear equations, trading roughly half of the multiplications for additions and subtractions. I t is a pleasure to thank D. BRILLINGER for inspiring discussions about the present subject and ST. COOK and B. PARLETT for encouraging me to write this paper. 2. We define algorithms e~, ~ which mult iply matrices of order m2 ~, by induction on k: ~ , 0 is the usual algorithm, for matr ix multiplication (requiring m a multiplications and m 2 ( m t) additions), e~,k already being known, define ~ , ~ +t as follows: If A, B are matrices of order m 2 k ~ to be multiplied, write"
            },
            "slug": "Gaussian-elimination-is-not-optimal-Strassen",
            "title": {
                "fragments": [],
                "text": "Gaussian elimination is not optimal"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1930930"
                        ],
                        "name": "F. Luk",
                        "slug": "F.-Luk",
                        "structuredName": {
                            "firstName": "Franklin",
                            "lastName": "Luk",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Luk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60110741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ca0f2270d17cf238e27897772c82c86fc45de8a",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : Research in the area of matrix-based signal processing included matric theory, numerical and parallel computing, signal processing and a Very Large Scale Integration implementation. Results of the research are summarized in the final report with details in the publications and proceedings issued during the course of the research."
            },
            "slug": "Fast-Algorithms-for-Signal-Processing-Luk",
            "title": {
                "fragments": [],
                "text": "Fast Algorithms for Signal Processing"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "Research in the area of matrix-based signal processing included matric theory, numerical and parallel computing, signal processing and a Very Large Scale Integration implementation."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690630"
                        ],
                        "name": "S. Winograd",
                        "slug": "S.-Winograd",
                        "structuredName": {
                            "firstName": "Shmuel",
                            "lastName": "Winograd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Winograd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207051239,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ad377ed3e552fe9bb686cda9464e08663c6dcc92",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The multiplicative complexity of the direct product of algebras $A_p $ of polynomials modulo a polynomial P is studied. In particular, we show that if P and Q are irreducible polynomials then the multiplicative complexity of $A_{\\text{P}} \\times A_{\\text{Q}} $ is $2\\deg ({\\text{P}})\\deg ({\\text{Q}}) - {\\text{k}}$, where k is the number of factors of P in the field extended by a root of ${\\text{Q}}$."
            },
            "slug": "On-Multiplication-of-Polynomials-Modulo-a-Winograd",
            "title": {
                "fragments": [],
                "text": "On Multiplication of Polynomials Modulo a Polynomial"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "If P and Q are irreducible polynomials then the multiplicative complexity of A-P times A-Q is 2deg (P) - k (Q) where k is the number of factors of P in the field extended by a root of Q."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Comput."
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14027261"
                        ],
                        "name": "F. Gall",
                        "slug": "F.-Gall",
                        "structuredName": {
                            "firstName": "Fran\u00e7ois",
                            "lastName": "Gall",
                            "middleNames": [
                                "Le"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gall"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "m for fast matrix multiplication to reduce the number of correlations, thereby reducing the total arithmetic complexity of the convnet layer. Using the best known upperbound for matrix multiplication [5], this implies the algorithmic complexity of a convnet layer is less than max(N,C,K)2.3728639 correlations. We still have not addressed the algorithmic complexity of the correlations themselves. The s"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2597483,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26e02fc5572fcf1e55496a2846aaa77b9b45b14d",
            "isKey": true,
            "numCitedBy": 1010,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a method to analyze the powers of a given trilinear form (a special kind of algebraic construction also called a tensor) and obtain upper bounds on the asymptotic complexity of matrix multiplication. Compared with existing approaches, this method is based on convex optimization, and thus has polynomial-time complexity. As an application, we use this method to study powers of the construction given by Coppersmith and Winograd [Journal of Symbolic Computation, 1990] and obtain the upper bound \u03c9"
            },
            "slug": "Powers-of-tensors-and-fast-matrix-multiplication-Gall",
            "title": {
                "fragments": [],
                "text": "Powers of tensors and fast matrix multiplication"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "A method to analyze the powers of a given trilinear form (a special kind of algebraic construction also called a tensor) and obtain upper bounds on the asymptotic complexity of matrix multiplication and obtain the upper bound \u03c9."
            },
            "venue": {
                "fragments": [],
                "text": "ISSAC"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690630"
                        ],
                        "name": "S. Winograd",
                        "slug": "S.-Winograd",
                        "structuredName": {
                            "firstName": "Shmuel",
                            "lastName": "Winograd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Winograd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117762612,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "1ff2a79066f2501453c750f40b17613324ec12b9",
            "isKey": false,
            "numCitedBy": 429,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Three examples General background Product of polynomials FIR filters Product of polynomials modulo a polynomial Cyclic convolution and discrete Fourier transform."
            },
            "slug": "Arithmetic-complexity-of-computations-Winograd",
            "title": {
                "fragments": [],
                "text": "Arithmetic complexity of computations"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "Three examples of polynomials modulo a polynomial Cyclic convolution and discrete Fourier transform are shown."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1980
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145664902"
                        ],
                        "name": "I. Young",
                        "slug": "I.-Young",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Young",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3322989"
                        ],
                        "name": "J. J. Gerbrands",
                        "slug": "J.-J.-Gerbrands",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Gerbrands",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. Gerbrands"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729533"
                        ],
                        "name": "L. V. Vliet",
                        "slug": "L.-V.-Vliet",
                        "structuredName": {
                            "firstName": "Lucas",
                            "lastName": "Vliet",
                            "middleNames": [
                                "J.",
                                "van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. V. Vliet"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53909233,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "5f7f41b893746ec552240c652f5ca06feff222b8",
            "isKey": false,
            "numCitedBy": 205,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A starter blade assembly for a thread roller starter mechanism which comprises a reciprocable starter slide mounted for precision rectilinear movement throughout its full range of travel, a starter blade supported on the slide for gating and feeding a work blank between starting ends of a pair of thread rolling dies, and an adjustable biasing means acting directly on the slide for providing a preselected starting force responsive to a return movement of a one-way drive lever, permitting following movement of the slide and starter blade in a blank feeding direction under the force of the biasing means."
            },
            "slug": "The-digital-signal-processing-handbook-Young-Gerbrands",
            "title": {
                "fragments": [],
                "text": "The digital signal processing handbook"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A starter blade assembly for a thread roller starter mechanism which comprises a reciprocable starter slide mounted for precision rectilinear movement throughout its full range of travel."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "The networks take several days of GPU time to train and require significant compute resources during classification as well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 144
                            }
                        ],
                        "text": "The transforms forF (3 \u00d7 3, 2\u00d7 2) are given by:\nBT =  \n1 0 \u22121 0 0 1 1 0 0 \u22121 1 0 0 \u22121 0 1\n \nAT =\n  1 1 1 0 0 1 \u22121 0 0 1 1 1  \n, G =   1 0 1 2 1 2\n1 2 \u2212 1 2 0 1\n  (14)\nWith (3 + 2\u2212 1)2 = 16 multiplies versus direct convolution\u2019s 3 \u00d7 3 \u00d7 2 \u00d7 2 = 36 multiplies, it achieves the same 36/16 = 2.25\u2026"
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Serkan Piantino, and Yann LeCun. Fast convolutional nets with fbfft: A GPU performance evaluation. CoRR, abs/1412"
            },
            "venue": {
                "fragments": [],
                "text": "Serkan Piantino, and Yann LeCun. Fast convolutional nets with fbfft: A GPU performance evaluation. CoRR, abs/1412"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "[1] The transform complexity is Nlog2N \u2212 3N + 4 multiplies and 3Nlog2N\u22123N+4 additions.[1] We can also exploit hermitian symmetry to reduce the arithmetic complexity for real data by half."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 171
                            }
                        ],
                        "text": "To evaluate the transform complexity of the FFT, we consider the split-radix FFT algorithm, which is considered the minimal practical FFT algorithm when N is a power of 2.[1] The transform complexity is Nlog2N \u2212 3N + 4 multiplies and 3Nlog2N\u22123N+4 additions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 108
                            }
                        ],
                        "text": "Combining these techniques we arrive at a 2D FFT algorithm with the arithmetic complexity listed in Table 1.[1]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Paterson-Stephens. The DSP handbook"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 165,
                                "start": 162
                            }
                        ],
                        "text": "This paper introduces a new class of fast algorithms for convolutional neural networks based on the minimal filtering algorithms discovered by Toom [14] and Cook [4] and generalized by Winograd [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the minimum computation time for multiplication"
            },
            "venue": {
                "fragments": [],
                "text": "Doctoral diss., Harvard U.,"
            },
            "year": 1966
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 162
                            }
                        ],
                        "text": "A large batch size adversely affects convergence of the network, so the minimum batch size that can be computed efficiently places an upper limit on cluster size [9, 7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Milthrope. Model accuracy and runtime tradeoff in distributed deep learning"
            },
            "venue": {
                "fragments": [],
                "text": "arXiv preprint arXiv:1509.04210,"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Soumith Chintala, Serkan Piantino, and Yann LeCun. Fast convolutional nets with fbfft: A GPU performance evaluation. CoRR, abs/1412"
            },
            "venue": {
                "fragments": [],
                "text": "Soumith Chintala, Serkan Piantino, and Yann LeCun. Fast convolutional nets with fbfft: A GPU performance evaluation. CoRR, abs/1412"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Blahut.Fast algorithms for signal processing"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The complexity of a scheme of functional elements realizing the multiplication of integers"
            },
            "venue": {
                "fragments": [],
                "text": "Soviet Mathematics Doklady"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 148
                            }
                        ],
                        "text": "This paper introduces a new class of fast algorithms for convolutional neural networks based on the minimal filtering algorithms discovered by Toom [14] and Cook [4] and generalized by Winograd [16]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The complexity of a scheme of functional elements realizing the multiplication of integers"
            },
            "venue": {
                "fragments": [],
                "text": "In Soviet Mathematics Doklady,"
            },
            "year": 1963
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "We used random data and filters from the uniform distribution [\u22121, 1] and measured absolute element error."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arithmetic complexity of computations, volume 33"
            },
            "venue": {
                "fragments": [],
                "text": "Siam, 1980"
            },
            "year": 1980
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "https://developer.nvidia.com/ cudnn"
            },
            "venue": {
                "fragments": [],
                "text": "https://developer.nvidia.com/ cudnn"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The DSP handbook"
            },
            "venue": {
                "fragments": [],
                "text": "The DSP handbook"
            },
            "year": 2002
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Digital Signal Processing Handbook. Number v. 2 in Electrical engineering handbook series"
            },
            "venue": {
                "fragments": [],
                "text": "CRC"
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "We benchmark a GPU implementation of our algorithm with the VGG network and show state of the art throughput at batch sizes from 1 to 64."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "On the minimum computation time for multiplication . Doctoral diss"
            },
            "venue": {
                "fragments": [],
                "text": "On the minimum computation time for multiplication . Doctoral diss"
            },
            "year": 1966
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Milthrope. Model accuracy and runtime tradeoff in distributed deep learning.arXiv preprint arXiv:1509.04210"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 13
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 31,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Fast-Algorithms-for-Convolutional-Neural-Networks-Lavin-Gray/d5eadd6f059d742d76441fd0a635a21694dd7392?sort=total-citations"
}