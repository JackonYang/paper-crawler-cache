{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "The author is grateful to D. B. Fogel and an anonymous referee for their constructive comment on earlier versions of this paper."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 46
                            }
                        ],
                        "text": "The design of training algorithms, more fundamentally the learning rules used to adjust connection weights, depends on the type of architectures under investigation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 61519731,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ae89454a8a41015ebe0eeab9f51fe4eee459a25",
            "isKey": false,
            "numCitedBy": 2102,
            "numCiting": 276,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning and evolution are two fundamental forms of adaptation. There has been a great interest in combining learning and evolution with artificial neural networks (ANNs) in recent years. This paper: 1) reviews different combinations between ANNs and evolutionary algorithms (EAs), including using EAs to evolve ANN connection weights, architectures, learning rules, and input features; 2) discusses different search operators which have been used in various EAs; and 3) points out possible future research directions. It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone."
            },
            "slug": "Evolving-artificial-neural-networks-Yao",
            "title": {
                "fragments": [],
                "text": "Evolving artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown, through a considerably large literature review, that combinations between ANNs and EAs can lead to significantly better intelligent systems than relying on ANNs or EAs alone."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152891866"
                        ],
                        "name": "Yong Liu",
                        "slug": "Yong-Liu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 106
                            }
                        ],
                        "text": "There have been some very successful experiments which show that EA\u2019s can be used to evolve ANN ensembles [192], [193], [302]\u2013[305]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2748390,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb73de3fcee36f8c535af6f3d2383fd13d0d8dd6",
            "isKey": false,
            "numCitedBy": 61,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary artificial neural networks (EANNs) refer to a special class of artificial neural networks (ANNs) in which evolution is another fundamental form of adaptation in addition to learning. Evolution can be introduced at various levels of ANNs. It can be used to evolve weights, architectures, and learning parameters and rules. The paper is concerned with the evolution of ANN architectures, where an evolutionary algorithm is used to evolve a population of ANNs. The current practice in evolving ANNs is to choose the best ANN in the last population as the final result. The paper proposes a novel approach to form the final result by combining all the individuals in the last generation in order to make best use of all the information contained in the whole population. This approach regards a population of ANNs as an ensemble of ANNs and use a method to combine them. We have used four simple methods in our computational studies. The first is the majority voting method. The second and third are linear combination methods over the whole population. The fourth is a linear combination method over a subset of the whole population. The near optimal subset is obtained by a genetic algorithm search. Our experiments have shown that all four methods have produced better results than those produced by the single best individual."
            },
            "slug": "Ensemble-structure-of-evolutionary-artificial-Yao-Liu",
            "title": {
                "fragments": [],
                "text": "Ensemble structure of evolutionary artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel approach to form the final result by combining all the individuals in the last generation in order to make best use of all the information contained in the whole population of ANNs is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Evolutionary Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 205966451,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "add58c2acbe76ae430cc207356acb02ee6171c4e",
            "isKey": false,
            "numCitedBy": 491,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "Research on potential interactions between connectionist learning systems, i.e., artificial neural networks (ANNs), and evolutionary search procedures, like genetic algorithms (GAs), has attracted a lot of attention recently. Evolutionary ANNs (EANNs) can be considered as the combination of ANNs and evolutionary search procedures. This article first distinguishes among three kinds of evolution in EANNs, i.e., the evolution of connection weights, of architectures, and of learning rules. Then it reviews each kind of evolution in detail and analyzes critical issues related to different evolutions. the review shows that although a lot of work has been done on the evolution of connection weights and architectures, few attempts have been made to understand the evolution of learning rules. Interactions among different evolutions are seldom mentioned in current research. However, the evolution of learning rules and its interactions with other kinds of evolution, play a vital role in EANNs. Finally, this article briefly describes a general framework for EANNs, which not only includes the aforementioned three kinds of evolution, but also considers interactions among them. \u00a9 1993 John Wiley & Sons, Inc."
            },
            "slug": "A-review-of-evolutionary-artificial-neural-networks-Yao",
            "title": {
                "fragments": [],
                "text": "A review of evolutionary artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This article distinguishes among three kinds of evolution in EANNs, i.e., the evolution of connection weights, of architectures, and of learning rules, and describes a general framework for E ANNs, which not only includes the aforementioned three types of evolution, but also considers interactions among them."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Intell. Syst."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152891866"
                        ],
                        "name": "Yong Liu",
                        "slug": "Yong-Liu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12430187,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9d7c04de906823a60d3ccb5f510fd0029af5c8b0",
            "isKey": false,
            "numCitedBy": 927,
            "numCiting": 112,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new evolutionary system, i.e., EPNet, for evolving artificial neural networks (ANNs). The evolutionary algorithm used in EPNet is based on Fogel's evolutionary programming (EP). Unlike most previous studies on evolving ANN's, this paper puts its emphasis on evolving ANN's behaviors. Five mutation operators proposed in EPNet reflect such an emphasis on evolving behaviors. Close behavioral links between parents and their offspring are maintained by various mutations, such as partial training and node splitting. EPNet evolves ANN's architectures and connection weights (including biases) simultaneously in order to reduce the noise in fitness evaluation. The parsimony of evolved ANN's is encouraged by preferring node/connection deletion to addition. EPNet has been tested on a number of benchmark problems in machine learning and ANNs, such as the parity problem, the medical diagnosis problems, the Australian credit card assessment problem, and the Mackey-Glass time series prediction problem. The experimental results show that EPNet can produce very compact ANNs with good generalization ability in comparison with other algorithms."
            },
            "slug": "A-new-evolutionary-system-for-evolving-artificial-Yao-Liu",
            "title": {
                "fragments": [],
                "text": "A new evolutionary system for evolving artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The experimental results show that EPNet can produce very compact ANNs with good generalization ability in comparison with other algorithms, and has been tested on a number of benchmark problems in machine learning and ANNs."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152891866"
                        ],
                        "name": "Yong Liu",
                        "slug": "Yong-Liu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10859249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4b7289deeb1de9c74f005631945d95fe119cfe2",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper is concerned with the simultaneous evolution of artificial neural network (ANN) architectures and weights. The current practice in evolving ANN's is to choose the best ANN in the last generation as the final result. This paper proposes a different approach to form the final result by combining all the individuals in the last generation in order to make best use of all the information contained in the whole population. This approach regards a population of ANN's as an ensemble and uses a combination method to integrate them. Although there has been some work on integrating ANN modules, little has been done in evolutionary learning to make best use of its population information. Four linear combination methods have been investigated in this paper to illustrate our ideas. Three real-world data sets have been used in our experimental studies, which show that the recursive least-square (RLS) algorithm always produces an integrated system that outperforms the best individual. The results confirm that a population contains more information than a single individual. Evolutionary learning should exploit such information to improve generalization of learned systems."
            },
            "slug": "Making-use-of-population-information-in-artificial-Yao-Liu",
            "title": {
                "fragments": [],
                "text": "Making use of population information in evolutionary artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper proposes a different approach to form the final result by combining all the individuals in the last generation in order to make best use of all the information contained in the whole population."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern. Part B"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679704"
                        ],
                        "name": "Y. Liu",
                        "slug": "Y.-Liu",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 150
                            }
                        ],
                        "text": "EPNet uses rank-based selection [125] and five mutations: hybrid training; node deletion; connection deletion; connection addition; and node addition [188], [194], [254]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14361287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1b74914c12a4e677b7ae0cd79cacfc716bedf08b",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary artiicial neural networks (EANNs) refer to a special class of artiicial neural networks (ANNs) in which evolution is another fundamental form of adaptation in addition to learning. The evolution in EANNs is often simulated by genetic algorithms (GAs), evolutionary programming (EP), or other evolutionary algorithms. This paper describes an EP-based EANNs which learn both their weights and architectures through the combination of a hybrid learning algorithm and the EP algorithm. A nonlinear ranking scheme and ve mutation operators are used in our EP. These ve mutation operators are applied sequentially and selectively to each individual in a population. Such sequential application encourages the evolution of smaller ANNs with fewer hidden nodes and connections. We have tested our EP-based EANNs on the parity problem of various sizes. Very good results have been achieved. For example, a three hidden node feed-forward ANN can be evolved for the 9-parity problem. In order to improve the generalisation capability of our EP-based EANNs, we have introduced two validation sets in the combined evolution and learning process. Such an approach can improve the generalisation ability of our EP-based EANNs signiicantly. Our experimental study with a credit card problem has connrmed that EP-based EANNs can generalise well, at least for the problem concerned. In fact, it produces one of the best results we are aware of."
            },
            "slug": "Evolutionary-Artificial-Neural-Networks-That-Learn-Yao-Liu",
            "title": {
                "fragments": [],
                "text": "Evolutionary Artificial Neural Networks That Learn and Generalise Well"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An EP-based EANNs which learn both their weights and architectures through the combination of a hybrid learning algorithm and the EP algorithm is described, and two validation sets are introduced in the combined evolution and learning process to improve the generalisation ability."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679704"
                        ],
                        "name": "Y. Liu",
                        "slug": "Y.-Liu",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15460875,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "255a0b5b14f82b24107668e0a3806c5eafa6d888",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Towards-designing-artificial-neural-networks-by-Yao-Liu",
            "title": {
                "fragments": [],
                "text": "Towards designing artificial neural networks by evolution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 197
                            }
                        ],
                        "text": "Evolutionary artificial neural networks (EANN\u2019s) refer to a special class of artificial neural networks (ANN\u2019s) in which evolution is another fundamental form of adaptation in addition to learning [1]\u2013[5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60107095,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "0bcf2c473d9ef6c5750c626475df77194c6bf0cf",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning and evolution are two fundamental processes of adaptation. Various models have been proposed to explain their behaviour. Rather than discussing these models in detail, this paper concentrates on the interaction between learning and evolution as well as the interaction between different levels of evolution. We will argue that the evolution of learning rules and its interaction with other evolutionary developments (in either artificial or biological systems) plays a key role in accounting for the creativity of those systems. We will concentrate on two models of learning and evolution: connectionistlearning (artificial neural networks, or ANNs) and genetic algorithms (GAs)."
            },
            "slug": "The-Evolution-of-Connectionist-Networks-Yao",
            "title": {
                "fragments": [],
                "text": "The Evolution of Connectionist Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is argued that the evolution of learning rules and its interaction with other evolutionary developments (in either artificial or biological systems) plays a key role in accounting for the creativity of those systems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152891866"
                        ],
                        "name": "Yong Liu",
                        "slug": "Yong-Liu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 12
                            }
                        ],
                        "text": "Liu and Yao [191] used EP to evolve ANN\u2019s with both sigmoidal and Gaussian nodes."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 63,
                                "start": 58
                            }
                        ],
                        "text": "Good performance was reported for some benchmark problems [191]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Yao and Liu [193], [194] developed an automatic system, EPNet, based on EP for simultaneous evolution of ANN architectures and connection weights."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1739,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dde73feddc236881c967d8d3556275c9cdb41b0f",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary design of artificial neural networks (ANNs) offers a very promising and automatic alternative to designing ANNs manually. The advantage of evolutionary design over the manual design is their adaptability to a dynamic environment. Most research in evolving ANNs only deals with the topological structure of ANNs and little has been done on the evolution of both topological structures and node transfer functions. The paper presents a new automatic method to design general neural networks (GNNs) with different nodes. GNNs combine generalisation capabilities of distributed neural networks (DNNs) and computational efficiency of local neural networks (LNNs). We use an evolutionary programming (EP) algorithm with new mutation operators which are very effective for evolving GNN architectures and weights simultaneously. Our EP algorithm allows GNNs to grow as well as shrink during the evolutionary process. Our experiment results show the effectiveness and accuracy of evolved GNNs."
            },
            "slug": "Evolutionary-design-of-artificial-neural-networks-Liu-Yao",
            "title": {
                "fragments": [],
                "text": "Evolutionary design of artificial neural networks with different nodes"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A new automatic method to design general neural networks (GNNs) with different nodes using an evolutionary programming (EP) algorithm with new mutation operators which are very effective for evolving GNN architectures and weights simultaneously."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Evolutionary Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723666"
                        ],
                        "name": "Sung-Bae Cho",
                        "slug": "Sung-Bae-Cho",
                        "structuredName": {
                            "firstName": "Sung-Bae",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sung-Bae Cho"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60610206,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "624198ac2901d66d7b1ca373e109823eb2ffe789",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The evolutionary approach to artificial neural networks has been developing rapidly in recent years and shows great possibility as a powerful tool. However, most evolutionary neural networks use the simple node as a building block to evolve and select the one network producing the best result after evolution. In this paper, we present concepts and methodologies for evolutionary modular neural networks, which boost the overall performance by combining several potential networks which have emerged during the course of the evolution. Experimental results with the problem of the recognition of handwritten numerals shows the possibility of combining a number of characteristic networks from a gene pool."
            },
            "slug": "Combining-modular-neural-networks-developed-by-Cho",
            "title": {
                "fragments": [],
                "text": "Combining modular neural networks developed by evolutionary algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1997 IEEE International Conference on Evolutionary Computation (ICEC '97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2102751189"
                        ],
                        "name": "Xin YaoComputational",
                        "slug": "Xin-YaoComputational",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "YaoComputational",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin YaoComputational"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 164
                            }
                        ],
                        "text": "EPNet uses rank-based selection [125] and five mutations: hybrid training; node deletion; connection deletion; connection addition; and node addition [188], [194], [254]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14274324,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f57440bd8b404620868ad80d86e58acfbec6df5",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "One of the major issues in the eld of artiicial neural networks (ANNs) is the design of their architectures. There are strong biological and engineering evidences to support that the information processing capability of an ANN is determined by its architecture. This paper proposes a new population-based learning algorithm (PBLA) which learns both ANN's architecture and weights. The evolutionary approach is used to evolve a population of ANNs. Unlike other evolutionary approaches to ANN learning, each ANN (i.e., individual) in the population is evaluated by partial training rather than complete training. Substantial savings in computational cost can be achieved by such progressive partial training. This training process can change both ANN's architecture and weights. Our preliminary experiments have demonstrated the eeectiveness of our algorithm."
            },
            "slug": "A-Population-Based-Learning-Algorithm-Which-Learns-YaoComputational",
            "title": {
                "fragments": [],
                "text": "A Population-Based Learning Algorithm Which Learns BothArchitectures and Weights of Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new population-based learning algorithm (PBLA) which learns both ANN's architecture and weights and the evolutionary approach is used to evolve a population of ANNs."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47206715"
                        ],
                        "name": "D. J. Janson",
                        "slug": "D.-J.-Janson",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Janson",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. J. Janson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144482114"
                        ],
                        "name": "J. Frenzel",
                        "slug": "J.-Frenzel",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Frenzel",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Frenzel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "[117], [126]\u2013[128], higher order ANN\u2019s [52], [53], and"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "[24], [26], [28], [37], [38], [41], [52], [53]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4354307,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a3f425e3e5a8484cc61046f6544df4c1b21ad7dd",
            "isKey": false,
            "numCitedBy": 157,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Product unit neural networks are useful because they can handle higher order combinations of inputs. When trained using traditional backpropagation, however, they are often susceptible to local minima. The use of genetic algorithm exploratory procedures that can often locate near-optimal solutions to complex problems to overcome this, is discussed. The genetic algorithm maintains a set of trial solutions and forces them to evolve toward an acceptable solution. A representation for possible solutions must first be developed. Then, with an initial random population, the algorithm uses survival of the fittest techniques as well as old knowledge in the gene pool to improve each generation's ability to solve the problem. This improvement is achieved through a four-step process of evaluation, reproduction, breeding, and mutation. An example application is described.<<ETX>>"
            },
            "slug": "Training-product-unit-neural-networks-with-genetic-Janson-Frenzel",
            "title": {
                "fragments": [],
                "text": "Training product unit neural networks with genetic algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The use of genetic algorithm exploratory procedures that can often locate near-optimal solutions to complex problems to overcome this, is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Expert"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760426"
                        ],
                        "name": "V. Maniezzo",
                        "slug": "V.-Maniezzo",
                        "structuredName": {
                            "firstName": "Vittorio",
                            "lastName": "Maniezzo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Maniezzo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 174,
                                "start": 169
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 101
                            }
                        ],
                        "text": "The second approach evolves architectures and connection weights simultaneously [149], [179], [180], [182], [185]\u2013[200]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20561554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e589508e3095f96c034072b859561f31bcd6378b",
            "isKey": false,
            "numCitedBy": 483,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a system based on a parallel genetic algorithm with enhanced encoding and operational abilities. The system, used to evolve feedforward artificial neural networks, has been applied to two widely different problem areas: Boolean function learning and robot control. It is shown that the good results obtained in both cases are due to two factors: first, the enhanced exploration abilities provided by the search-space reducing evolution of both coding granularity and network topology, and, second, the enhanced exploitational abilities due to a recently proposed cooperative local optimizing genetic operator."
            },
            "slug": "Genetic-evolution-of-the-topology-and-weight-of-Maniezzo",
            "title": {
                "fragments": [],
                "text": "Genetic evolution of the topology and weight distribution of neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This paper proposes a system based on a parallel genetic algorithm with enhanced encoding and operational abilities that has been applied to two widely different problem areas: Boolean function learning and robot control."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723475"
                        ],
                        "name": "D. Montana",
                        "slug": "D.-Montana",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Montana",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Montana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144286518"
                        ],
                        "name": "L. Davis",
                        "slug": "L.-Davis",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Davis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 40
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6336712,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "09920800fa7841c84a551d70c6101d9510e6fcc8",
            "isKey": false,
            "numCitedBy": 1119,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayered feedforward neural networks possess a number of properties which make them particularly suited to complex pattern classification problems. However, their application to some realworld problems has been hampered by the lack of a training algonthm which reliably finds a nearly globally optimal set of weights in a relatively short time. Genetic algorithms are a class of optimization procedures which are good at exploring a large and complex space in an intelligent way to find values close to the global optimum. Hence, they are well suited to the problem of training feedforward networks. In this paper, we describe a set of experiments performed on data from a sonar image classification problem. These experiments both 1) illustrate the improvements gained by using a genetic algorithm rather than backpropagation and 2) chronicle the evolution of the performance of the genetic algorithm as we added more and more domain-specific knowledge into it."
            },
            "slug": "Training-Feedforward-Neural-Networks-Using-Genetic-Montana-Davis",
            "title": {
                "fragments": [],
                "text": "Training Feedforward Neural Networks Using Genetic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A set of experiments performed on data from a sonar image classification problem are described to illustrate the improvements gained by using a genetic algorithm rather than backpropagation and chronicle the evolution of the performance of the genetic algorithm as it added more and more domain-specific knowledge into it."
            },
            "venue": {
                "fragments": [],
                "text": "IJCAI"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47392513"
                        ],
                        "name": "Jonathan Baxter",
                        "slug": "Jonathan-Baxter",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Baxter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Baxter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "Baxter [269] took one step further than just the evolution"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 177,
                                "start": 172
                            }
                        ],
                        "text": "Although Baxter\u2019s experiments were rather simple, they confirmed that complex behaviors could be learned and the ANN\u2019s learning ability could be improved through evolution [269]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 108
                            }
                        ],
                        "text": "Similar experiments on the evolution of learning rules were also carried out by others [265], [266], [267], [269], [270]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 15237898,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b332d94a2ea0c9ff6507f074b83c58e9da4d2439",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we investigate a neural network model in which weights between computational nodes are modiied according to a local learning rule. To determine whether local learning rules are suucient for learning, we encode the network architectures and learning dynamics genetically and then apply selection pressure to evolve networks capable of learning the four boolean functions of one variable. The successful networks are analysed and we show how learning behaviour emerges as a distributed property of the entire network. Finally the utility of genetic algorithms as a tool of discovery is discussed."
            },
            "slug": "The-evolution-of-learning-algorithms-for-artificial-Baxter",
            "title": {
                "fragments": [],
                "text": "The evolution of learning algorithms for artificial neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A neural network model in which weights between computational nodes are modiied according to a local learning rule is investigated and it is shown how learning behaviour emerges as a distributed property of the entire network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2435447"
                        ],
                        "name": "Byungjoo Yoon",
                        "slug": "Byungjoo-Yoon",
                        "structuredName": {
                            "firstName": "Byungjoo",
                            "lastName": "Yoon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Byungjoo Yoon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1405985027"
                        ],
                        "name": "Dawn J. Holmes",
                        "slug": "Dawn-J.-Holmes",
                        "structuredName": {
                            "firstName": "Dawn J.",
                            "lastName": "Holmes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dawn J. Holmes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2185683"
                        ],
                        "name": "G. Langholz",
                        "slug": "G.-Langholz",
                        "structuredName": {
                            "firstName": "Gideon",
                            "lastName": "Langholz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Langholz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145608940"
                        ],
                        "name": "A. Kandel",
                        "slug": "A.-Kandel",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Kandel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kandel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5220656,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a05121aa978dd3355c81544760a1bb7e954c6c5",
            "isKey": false,
            "numCitedBy": 25,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-Genetic-Algorithms-for-Training-Layered-Yoon-Holmes",
            "title": {
                "fragments": [],
                "text": "Efficient Genetic Algorithms for Training Layered Feedback Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "Inf. Sci."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46861885"
                        ],
                        "name": "M. Koppen",
                        "slug": "M.-Koppen",
                        "structuredName": {
                            "firstName": "Mario",
                            "lastName": "Koppen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Koppen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39944424"
                        ],
                        "name": "M. Teunis",
                        "slug": "M.-Teunis",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Teunis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Teunis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714891"
                        ],
                        "name": "B. Nickolay",
                        "slug": "B.-Nickolay",
                        "structuredName": {
                            "firstName": "Bertram",
                            "lastName": "Nickolay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Nickolay"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17693332,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "63ab04cc048320e1d71507ee7d463f72d2b0d556",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new neural architecture (Nessy) which uses evolutionary optimization for learning. The architecture, the outline of its evolutionary algorithm and the learning laws are given. Nessy is based on several modifications of the multilayer backpropagation neural network. The neurons represent genes of evolutionary optimization, referred to as solutions. Weights represent probabilities and are used for selection. The training value of the output layer is set to zero, the theoretical limit of every cost-oriented optimization, and the crossover operator is replaced by a transduction operator. Mutation is used as usual. Nessy algorithm can be characterized as an individual evolutionary algorithm, but as a neural network too. It was designed for image processing applications. A short example is presented, where the discriminative feature of two images is successfully detected by the proposed evolutionary neural network."
            },
            "slug": "A-neural-network-that-uses-evolutionary-learning-Koppen-Teunis",
            "title": {
                "fragments": [],
                "text": "A neural network that uses evolutionary learning"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "A new neural architecture (Nessy) which uses evolutionary optimization for learning, based on several modifications of the multilayer backpropagation neural network, designed for image processing applications."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1997 IEEE International Conference on Evolutionary Computation (ICEC '97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693885"
                        ],
                        "name": "Qiangfu Zhao",
                        "slug": "Qiangfu-Zhao",
                        "structuredName": {
                            "firstName": "Qiangfu",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiangfu Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2752122"
                        ],
                        "name": "T. Higuchi",
                        "slug": "T.-Higuchi",
                        "structuredName": {
                            "firstName": "Tatsuo",
                            "lastName": "Higuchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Higuchi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26737270,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e03510d6ec0c5e8347c480f5e4db02fb752e97cb",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Efficient-learning-of-NN-MLP-based-on-individual-Zhao-Higuchi",
            "title": {
                "fragments": [],
                "text": "Efficient learning of NN-MLP based on individual evolutionary algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682655"
                        ],
                        "name": "R. Belew",
                        "slug": "R.-Belew",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Belew",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Belew"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49268728"
                        ],
                        "name": "J. McInerney",
                        "slug": "J.-McInerney",
                        "structuredName": {
                            "firstName": "John G.",
                            "lastName": "McInerney",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McInerney"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60923273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5378dbf09e75d133cafdcb388bb5546f91b2a02",
            "isKey": false,
            "numCitedBy": 387,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "It is appealing to consider hybrids of neural-network learning algorithms with evolutionary search procedures, simply because Nature has so successfully done so. In fact, computational models of learning and evolution ooer theoretical biology new tools for addressing questions about Nature that have dogged that eld since Darwin Belew, 1990]. The concern of this paper, however, is strictly artiicial: Can hybrids of connectionist learning algorithms and genetic algorithms produce more eecient and eeective algorithms than either technique applied in isolation? The paper begins with a survey of recent work (by us and others) that combines Holland's Genetic Algorithm (GA) with con-nectionist techniques and delineates some of the basic design problems these hybrids share. This analysis suggests the dangers of overly literal representations of the network on the genome (e.g., encoding each weight explicitly). A preliminary set of experiments that use the GA to nd unusual but successful values for BP parameters (learning rate, momentum) are also reported. The focus of the report is a series of experiments that use the GA to explore the space of initial weight values , from which two diierent gradient techniques (conjugate gradient and back propagation) are then allowed to optimize. We nd that use of the GA provides much greater conndence in the face of the stochas-tic variation that can plague gradient techniques, and can also allow training times to be reduced by as much as two orders of magnitude. Computational trade-oos between BP and the GA are considered, including discussion of a software facility that exploits the parallelism inherent in GA/BP hybrids. This evidence leads us to conclude that the GA's global sampling characteristics compliment connectionist local search techniques well, leading to eecient and reliable hybrids."
            },
            "slug": "Evolving-networks:-using-the-genetic-algorithm-with-Belew-McInerney",
            "title": {
                "fragments": [],
                "text": "Evolving networks: using the genetic algorithm with connectionist learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A survey of recent work that combines Holland's Genetic Algorithm with con-nectionist techniques and delineates some of the basic design problems these hybrids share concludes that the GA's global sampling characteristics compliment connectionist local search techniques well, leading to eecient and reliable hybrids."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072252"
                        ],
                        "name": "D. Chalmers",
                        "slug": "D.-Chalmers",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Chalmers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Chalmers"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 9
                            }
                        ],
                        "text": "Chalmers [264] defined a learning rule as a linear combination of four local variables and their six pairwise products."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 10538501,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "74fd7e2750614b8b405a9e45d639331c3ed9d811",
            "isKey": false,
            "numCitedBy": 239,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Evolution-of-Learning:-An-Experiment-in-Genetic-Chalmers",
            "title": {
                "fragments": [],
                "text": "The Evolution of Learning: An Experiment in Genetic Connectionism"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109646882"
                        ],
                        "name": "Jinn-Moon Yang",
                        "slug": "Jinn-Moon-Yang",
                        "structuredName": {
                            "firstName": "Jinn-Moon",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jinn-Moon Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1697926"
                        ],
                        "name": "Cheng-Yan Kao",
                        "slug": "Cheng-Yan-Kao",
                        "structuredName": {
                            "firstName": "Cheng-Yan",
                            "lastName": "Kao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Cheng-Yan Kao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143898207"
                        ],
                        "name": "Jorng-Tzong Horng",
                        "slug": "Jorng-Tzong-Horng",
                        "structuredName": {
                            "firstName": "Jorng-Tzong",
                            "lastName": "Horng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jorng-Tzong Horng"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 107
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 161
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 56989325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "30159135a589a3d98745f98b95d8085c15fad598",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a new algorithm called combined evolutionary algorithm (CEA) to train a neural network, and demonstrates its use in inducing thefinite state automata task. This algorithm evolves neural networks by incorporating the ideas of evolutionary programming(EP) and real coded genetic algorithms (RCGA) into evolution strategies (ESs). Simultaneously, we add the local competition into the CEA in order to reduce the complexity and maintain the diversity. This algorithm is able to balance the exploration and exploitation dynamically. We implement CEA and experiment on seven benchmark problems of regular language. The results indicate that the CF-4 is a powerful technique to construct neural networks."
            },
            "slug": "Evolving-neural-induction-regular-language-using-Yang-Kao",
            "title": {
                "fragments": [],
                "text": "Evolving neural induction regular language using combined evolutionary algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 78,
                "text": "A new algorithm called combined evolutionary algorithm (CEA) to train a neural network, and its use in inducing the infinite state automata task is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Mexico-USA Collaboration in Intelligent Systems Technologies."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713769"
                        ],
                        "name": "S. Likothanassis",
                        "slug": "S.-Likothanassis",
                        "structuredName": {
                            "firstName": "Spiridon",
                            "lastName": "Likothanassis",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Likothanassis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694475"
                        ],
                        "name": "E. Georgopoulos",
                        "slug": "E.-Georgopoulos",
                        "structuredName": {
                            "firstName": "Efstratios",
                            "lastName": "Georgopoulos",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Georgopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153564929"
                        ],
                        "name": "D. Fotakis",
                        "slug": "D.-Fotakis",
                        "structuredName": {
                            "firstName": "Dimitris",
                            "lastName": "Fotakis",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fotakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 298
                            }
                        ],
                        "text": "Some researchers thus avoided crossover and adopted only mutations in the evolution of architectures [45], [128], [149], [179], [185]\u2013[197], [217], [223], although it has been shown that crossover may be useful and important in increasing the efficiency of evolution for some problems [48], [113], [212], [229]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56586938,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "80143789480f2dc4b26140151ccd0550c5f1c10b",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary methods have been widely used to determine the structure of artificial neural networks. In this work, we propose a more efficient implementation which face the above problem, by using a neural network model as general as possible. So, we used a fully connected network, consisting of three parts, the input layer, the output layer and a number of hidden layers. This implementation has been proved, via simulations, that it optimazes the size of the network and gives better results compared with other existing methods, while the use of crossover results to more efficient networks. Furthermore, the random initialization has been proved more efficient, since it reduces significantly the number of the generations needed for the convergence of the algorithm. Finally, some aspects for the convergence of the parallel implementations of the algorithm are discussed."
            },
            "slug": "Optimizing-The-Structure-Of-Neural-NetworksUsing-Likothanassis-Georgopoulos",
            "title": {
                "fragments": [],
                "text": "Optimizing The Structure Of Neural NetworksUsing Evolution Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work uses a fully connected network, consisting of three parts, the input layer, the output layer and a number of hidden layers, and proves that the random initialization has been proved more efficient, since it reduces significantly the number of the generations needed for the convergence of the algorithm."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1679704"
                        ],
                        "name": "Y. Liu",
                        "slug": "Y.-Liu",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6524138,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d06d44d9a90ad292ad71915b573f6190ef6f926f",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Artiicial neural network (ANN) architecture design has been one of the most tedious and diicult tasks in ANN applications due to the lack of satisfactory and systematic methods of designing a near optimal architecture. Evolutionary algorithms have been shown to be very eeective in evolving novel ANN architectures for various problems. This paper proposes a new method for evolving ANN architectures and weights at the same time. The new method has been applied to four real-world data sets in the medical domain and achieved very good results. The traditional trial-and-error approach to designing ANNs has been replaced by an automatic evolutionary system which can nd a near optimal architecture and connection weights for a problem."
            },
            "slug": "Evolving-Artificial-Neural-Networks-for-Medical-Yao-Liu",
            "title": {
                "fragments": [],
                "text": "Evolving Artificial Neural Networks for Medical Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A new method for evolving ANN architectures and weights at the same time is proposed and has been applied to four real-world data sets in the medical domain and achieved very good results."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152891866"
                        ],
                        "name": "Yong Liu",
                        "slug": "Yong-Liu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 18179447,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0a21d49bf49255700abc25027764bac497b48125",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a co-evolutionary learning system, i.e., CELS, to design neural network (NN) ensembles. CELS addresses the issue of automatic determination of the number of individual NNs in an ensemble and the exploitation of the interaction between individual NN design and combination. The idea of CELS is to encourage different individual NNs in the ensemble to learn different parts or aspects of the training data so that the ensemble can learn the whole training data better. The cooperation and specialisation among different individual NNs are considered during the individual NN design. This provides an opportunity for different NNs to interact with each other and to specialise. Experiments on two real-world problems demonstrate that CELS can produce NN ensembles with good generalisation ability."
            },
            "slug": "Towards-Designing-Neural-Network-Ensembles-by-Liu-Yao",
            "title": {
                "fragments": [],
                "text": "Towards Designing Neural Network Ensembles by Evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Experiments on two real-world problems demonstrate that CELS can produce NN ensembles with good generalisation ability."
            },
            "venue": {
                "fragments": [],
                "text": "PPSN"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37919510"
                        ],
                        "name": "M. Sarkar",
                        "slug": "M.-Sarkar",
                        "structuredName": {
                            "firstName": "Manish",
                            "lastName": "Sarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sarkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741065"
                        ],
                        "name": "B. Yegnanarayana",
                        "slug": "B.-Yegnanarayana",
                        "structuredName": {
                            "firstName": "Bayya",
                            "lastName": "Yegnanarayana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yegnanarayana"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 89
                            }
                        ],
                        "text": "There are some other different EP-based systems for designing ANN\u2019s [128], [129], [217], [223], but none has been tested on as many different benchmark problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 148
                            }
                        ],
                        "text": "Some researchers thus avoided crossover and adopted only mutations in the evolution of architectures [45], [128], [149], [179], [185]\u2013[197], [217], [223], although it has been shown that crossover may be useful and important in increasing the efficiency of evolution for some problems [48], [113], [212], [229]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61411450,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9dd5522edba772d3bb6d6be6dbb10d3da2d3dc56",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes an evolutionary programming based neural network construction algorithm, that efficiently configures feedforward neural networks in terms of optimum structure and optimum parameter set. The proposed method determines the appropriate structure, i.e. an appropriate number of hidden nodes, in such a way that locally optimal solutions are avoided. While choosing the number of hidden nodes, this method performs a trade-off between generalization and memorization. In this method, the network is evolved so that it learns an optimum parameter set, i.e. weights and bias, without being trapped into a locally optimal solution. Efficiency of this method is further enhanced by incorporating the concepts of adaptive structural mutation. Finally, efficacy of the proposed scheme is demonstrated on a Contract Bridge game opening bid problem."
            },
            "slug": "Feedforward-neural-networks-configuration-using-Sarkar-Yegnanarayana",
            "title": {
                "fragments": [],
                "text": "Feedforward neural networks configuration using evolutionary programming"
            },
            "tldr": {
                "abstractSimilarityScore": 81,
                "text": "An evolutionary programming based neural network construction algorithm, that efficiently configures feedforward neural networks in terms of optimum structure and optimum parameter set and efficacy of this method is demonstrated on a Contract Bridge game opening bid problem."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2550546"
                        ],
                        "name": "J. J. M. Guerv\u00f3s",
                        "slug": "J.-J.-M.-Guerv\u00f3s",
                        "structuredName": {
                            "firstName": "Juan",
                            "lastName": "Guerv\u00f3s",
                            "middleNames": [
                                "Juli\u00e1n",
                                "Merelo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. J. M. Guerv\u00f3s"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076621510"
                        ],
                        "name": "M. Pat\u00f3n",
                        "slug": "M.-Pat\u00f3n",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Pat\u00f3n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pat\u00f3n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144620251"
                        ],
                        "name": "A. Ca\u00f1as",
                        "slug": "A.-Ca\u00f1as",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Ca\u00f1as",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ca\u00f1as"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145179597"
                        ],
                        "name": "A. Prieto",
                        "slug": "A.-Prieto",
                        "structuredName": {
                            "firstName": "Alberto",
                            "lastName": "Prieto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Prieto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805307"
                        ],
                        "name": "F. Mor\u00e1n",
                        "slug": "F.-Mor\u00e1n",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Mor\u00e1n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Mor\u00e1n"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 24
                            }
                        ],
                        "text": "Other researchers [32], [139], [213], [272] also used an evolutionary process to find parameters for BP but ANN\u2019s"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 108
                            }
                        ],
                        "text": "Similar work on the evolution of initial weights has also been done on competitive learning neural networks [139] and Kohonen networks [140]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 45133630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6e73b842987fe22f505ce6f4981d7bcc374766d4",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present the use of a genetic algorithm (GA) for the optimization, in clustering tasks, of a new kind of fast-learning neural network. The network uses a combination of supervised and un-supervised learning that makes it suitable for automatic tuning -by means of the GA-of the learning parameters and initial weights in order to obtain the highest recognition score. Simulation results are presented showing as, for relatively simple clustering tasks, the GA finds in a few generations the parameters of the network that lead to a classification accuracy close to 100%."
            },
            "slug": "Optimization-of-a-Competitive-Learning-Neural-by-Guerv\u00f3s-Pat\u00f3n",
            "title": {
                "fragments": [],
                "text": "Optimization of a Competitive Learning Neural Network by Genetic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This paper presents a new kind of fast-learning neural network that uses a combination of supervised and un-supervised learning that makes it suitable for automatic tuning -by means of the GA- of the learning parameters and initial weights in order to obtain the highest recognition score."
            },
            "venue": {
                "fragments": [],
                "text": "IWANN"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1797302"
                        ],
                        "name": "H. D. Garis",
                        "slug": "H.-D.-Garis",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Garis",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. D. Garis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "[24], [26], [28], [37], [38], [41], [52], [53]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62177721,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3166f149696ce4c7faf5a7937ffcd425bebcf868",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "The author shows that the generic algorithm (GA) can be applied successfully to training nonconvergent networks, and presents some examples of their extraordinary behavioral versatility. He first gives a brief summary of the GA and the genetic programming of neural networks. He shows how GP techniques were used to evolve GenNets with specified operating conditions, and demonstrates some of the extraordinary capacities of time-dependent GenNets. He also makes a plea to the neural network research community to 'shift its sights upwards' by devoting more effort to thinking about 'dynamic' neural networks in general, and the theory of GenNet dynamics and 'evolvability' in particular. >"
            },
            "slug": "GenNets:-genetically-programmed-neural-nets-using-Garis",
            "title": {
                "fragments": [],
                "text": "GenNets: genetically programmed neural nets-using the genetic algorithm to train neural nets whose inputs and/or outputs vary in time"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "It is shown that the generic algorithm (GA) can be applied successfully to training nonconvergent networks, and some examples of their extraordinary behavioral versatility are presented."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 199,
                                "start": 194
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 29
                            }
                        ],
                        "text": "in chromosomes [151], [168], [184], [205], [230], [232]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[184] (the so-called Michigan approach)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 15392806,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f62478f2d594e66b0dbb7943fe158d49ff0a29f2",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The design of optimal artiicial neural networks (ANNs) is a key issue in the study of ANNs from the point of view of both theory and applications. There are strong biological and engineering evidences to support that the information processing capability of an ANN is determined by its architecture. However, no systematic method for designing ANNs exists although there are many attempts in attacking this problem. This paper adopts an evolutionary approach to ANN design. The indirect encoding scheme of ANN ar-chitectures is used. That is, a genetic algorithm is used to evolve a set of grammar rules which generate an ANN architecture. A novel method of co-evolving a set of rules is proposed in this paper. In our co-evolutionary system, each individual in a population represents a rule. The whole population is the complete set of grammar rules which are used to generate an architecture. Preliminary experiments have been carried out to evolve ANN architectures for the parity problem with various sizes."
            },
            "slug": "A-Preliminary-Study-on-Designing-Artiicial-Neural-Yao",
            "title": {
                "fragments": [],
                "text": "A Preliminary Study on Designing Artiicial Neural Networks Using Co-evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel method of co-evolving a set of rules is proposed in this paper, which adopts an evolutionary approach to ANN design and uses the indirect encoding scheme of ANN ar-chitectures."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718584"
                        ],
                        "name": "P. Angeline",
                        "slug": "P.-Angeline",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Angeline",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Angeline"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33047146"
                        ],
                        "name": "G. M. Saunders",
                        "slug": "G.-M.-Saunders",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Saunders",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. M. Saunders"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145539951"
                        ],
                        "name": "J. Pollack",
                        "slug": "J.-Pollack",
                        "structuredName": {
                            "firstName": "Jordan",
                            "lastName": "Pollack",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Pollack"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 50
                            }
                        ],
                        "text": "than the complete class of network architectures\u201d [149]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 35
                            }
                        ],
                        "text": "[118], [127], [128], [130], [138], [149]\u2013[225]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 121
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 80
                            }
                        ],
                        "text": "The second approach evolves architectures and connection weights simultaneously [149], [179], [180], [182], [185]\u2013[200]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[149] and Fogel [12], [243] have provided a more general discussion on the mapping between genotypes"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 114
                            }
                        ],
                        "text": "Some researchers thus avoided crossover and adopted only mutations in the evolution of architectures [45], [128], [149], [179], [185]\u2013[197], [217], [223], although it has been shown that crossover may be useful and important in increasing the efficiency of evolution for some problems [48], [113], [212], [229]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44767,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fba6007d482db5fbe8cd6c3af90fe0922453e1d2",
            "isKey": true,
            "numCitedBy": 1072,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "Standard methods for simultaneously inducing the structure and weights of recurrent neural networks limit every task to an assumed class of architectures. Such a simplification is necessary since the interactions between network structure and function are not well understood. Evolutionary computations, which include genetic algorithms and evolutionary programming, are population-based search methods that have shown promise in many similarly complex tasks. This paper argues that genetic algorithms are inappropriate for network acquisition and describes an evolutionary program, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks. GNARL's empirical acquisition method allows for the emergence of complex behaviors and topologies that are potentially excluded by the artificial architectural constraints imposed in standard network induction methods."
            },
            "slug": "An-evolutionary-algorithm-that-constructs-recurrent-Angeline-Saunders",
            "title": {
                "fragments": [],
                "text": "An evolutionary algorithm that constructs recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is argued that genetic algorithms are inappropriate for network acquisition and an evolutionary program is described, called GNARL, that simultaneously acquires both the structure and weights for recurrent networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723666"
                        ],
                        "name": "Sung-Bae Cho",
                        "slug": "Sung-Bae-Cho",
                        "structuredName": {
                            "firstName": "Sung-Bae",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sung-Bae Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143643764"
                        ],
                        "name": "K. Shimohara",
                        "slug": "K.-Shimohara",
                        "structuredName": {
                            "firstName": "Katsunori",
                            "lastName": "Shimohara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shimohara"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10122388,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25746f77ddeeaafe82935823a9b63fb6113ae65e",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an evolvable model of modular neural networks which are rich in autonomy and creativity. In order to build an artificial neural network which is rich in autonomy and creativity, we have adopted the ideas and methodologies of artificial life. The paper describes the concepts and methodologies for the evolvable model of modular neural networks, which will be able not only to develop new functionality spontaneously but also to grow and evolve its own structure autonomously. Although the ultimate goal of this model is to design the control system for such behavior based robots as Khepera, we have attempted to apply the mechanism to a visual categorization task with handwritten digits. The evolutionary mechanism has shown a strong possibility to generate useful network architectures from an initial set of randomly connected networks."
            },
            "slug": "Modular-neural-networks-evolved-by-genetic-Cho-Shimohara",
            "title": {
                "fragments": [],
                "text": "Modular neural networks evolved by genetic programming"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The paper describes the concepts and methodologies for the evolvable model of modular neural networks, which will be able not only to develop new functionality spontaneously but also to grow and evolve its own structure autonomously."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Evolutionary Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143615998"
                        ],
                        "name": "Yuji Sato",
                        "slug": "Yuji-Sato",
                        "structuredName": {
                            "firstName": "Yuji",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuji Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3339122"
                        ],
                        "name": "T. Furuya",
                        "slug": "T.-Furuya",
                        "structuredName": {
                            "firstName": "Tatsumi",
                            "lastName": "Furuya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Furuya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11508383,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2895d941b4bf02d42018bd93546aa196f178f0ba",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an investigation into the effectiveness of a lookahead model based on recurrent neural networks. An action network and an internal model of the environment are incorporated into the recurrent neural network; lookahead planning is performed while configuring the action network through learning in the internal model. \n \n \n \nA genetic algorithm is applied to the design of the neural networks. The effectiveness of this model is evaluated by applying it to the game of \u201ctic-tac-toe,\u201d and the following result is obtained. It is possibly more effective to perform learning in the internal model by learning algorithms than by memorizing input-output correspondences."
            },
            "slug": "Coevolution-in-recurrent-neural-networks-using-Sato-Furuya",
            "title": {
                "fragments": [],
                "text": "Coevolution in recurrent neural networks using genetic algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "The effectiveness of a lookahead model based on recurrent neural networks based on an action network and an internal model of the environment based on a genetic algorithm is evaluated by applying it to the game of \u201ctic-tac-toe,\u201d and the following result is obtained."
            },
            "venue": {
                "fragments": [],
                "text": "Systems and Computers in Japan"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152891866"
                        ],
                        "name": "Yong Liu",
                        "slug": "Yong-Liu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 128
                            }
                        ],
                        "text": "Some researchers thus avoided crossover and adopted only mutations in the evolution of architectures [45], [128], [149], [179], [185]\u2013[197], [217], [223], although it has been shown that crossover may be useful and important in increasing the efficiency of evolution for some problems [48], [113], [212], [229]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 176
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 108
                            }
                        ],
                        "text": "The second approach evolves architectures and connection weights simultaneously [149], [179], [180], [182], [185]\u2013[200]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 70
                            }
                        ],
                        "text": "Very compact ANN\u2019s with good generalization ability have been evolved [185]\u2013[195]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9804135,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db464185f978b1a2b1c2c900f6205b5e2687ca7e",
            "isKey": false,
            "numCitedBy": 32,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "EPNet is an evolutionary system for automatic design of artificial neural networks (ANNs) [1, 2, 3]. Unlike most previous methods on evolving ANNs, EPNet puts its emphasis on evolving ANN'S behaviours rather than circuitry. The parsimony of evolved ANNs is encouraged by the sequential application of architectural mutations. In this paper, EP Net is applied to a couple of chaotic time-series prediction problems (i.e., the Mackey-Glass differential equation and the logistic map). The experimental results show that EPNet can produce very compact ANNs with good prediction ability in comparison with other algorithms."
            },
            "slug": "EPNet-for-Chaotic-Time-Series-Prediction-Yao-Liu",
            "title": {
                "fragments": [],
                "text": "EPNet for Chaotic Time-Series Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "EP Net is applied to a couple of chaotic time-series prediction problems and the experimental results show that EPNet can produce very compact ANNs with good prediction ability in comparison with other algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "SEAL"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987723"
                        ],
                        "name": "T. Ichimura",
                        "slug": "T.-Ichimura",
                        "structuredName": {
                            "firstName": "Takumi",
                            "lastName": "Ichimura",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ichimura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061518849"
                        ],
                        "name": "T. Takano",
                        "slug": "T.-Takano",
                        "structuredName": {
                            "firstName": "Takeshi",
                            "lastName": "Takano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Takano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144487819"
                        ],
                        "name": "E. Tazaki",
                        "slug": "E.-Tazaki",
                        "structuredName": {
                            "firstName": "Eiichiro",
                            "lastName": "Tazaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Tazaki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 21
                            }
                        ],
                        "text": "[118], [127], [128], [130], [138], [149]\u2013[225]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61249338,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef81770764f0973e4d61800d68ba7afa7b982679",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we present a reasoning and learning method for fuzzy rules using neural networks with adaptive structured genetic algorithm. This adaptive structured genetic algorithm can determine the network structure and their weights solely by an evolutionary process. With this approach, no a priori assumptions about topology are needed and the only information required is the input and output characteristics of the task. The adaptive structured genetic algorithm can generate or annihilate the specified units respectively in hidden layer to achieve an overall good system, without using back propagation or any other learning algorithm."
            },
            "slug": "Reasoning-and-learning-method-for-fuzzy-rules-using-Ichimura-Takano",
            "title": {
                "fragments": [],
                "text": "Reasoning and learning method for fuzzy rules using neural networks with adaptive structured genetic algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "A reasoning and learning method for fuzzy rules using neural networks with adaptive structured genetic algorithm that can determine the network structure and their weights solely by an evolutionary process."
            },
            "venue": {
                "fragments": [],
                "text": "1995 IEEE International Conference on Systems, Man and Cybernetics. Intelligent Systems for the 21st Century"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37919510"
                        ],
                        "name": "M. Sarkar",
                        "slug": "M.-Sarkar",
                        "structuredName": {
                            "firstName": "Manish",
                            "lastName": "Sarkar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sarkar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741065"
                        ],
                        "name": "B. Yegnanarayana",
                        "slug": "B.-Yegnanarayana",
                        "structuredName": {
                            "firstName": "Bayya",
                            "lastName": "Yegnanarayana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Yegnanarayana"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 71
                            }
                        ],
                        "text": "There has been some work in this area where good results were reported [119], [120], [245]\u2013[253]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 175
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62368564,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f81fc1009f392c922cc0a1c43cf9d4112b3dfcc5",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Quick learning ability of a probabilistic neural network (PNN) has made it a popular alternative to feedforward neural networks trained by backpropagation algorithm. However in real life applications, where training set size is quite large, the PNN model suffers from a huge memory overhead and a long testing time. Moreover, its generalization capability critically depends on the choice of certain architectural parameters, which are presently chosen in an ad hoc basis. Since the PNN can be viewed as a Gaussian mixture model, the above drawbacks can be avoided if the PNN is configured based on a Gaussian mixture model with an optimum parameter set. In this paper, an evolutionary programming based clustering technique is employed to determine the optimum parameter set of the Gaussian mixture model. The efficacy of the proposed scheme is demonstrated on a Contract Bridge game opening bid problem."
            },
            "slug": "An-evolutionary-programming-based-probabilistic-Sarkar-Yegnanarayana",
            "title": {
                "fragments": [],
                "text": "An evolutionary programming-based probabilistic neural networks construction technique"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An evolutionary programming based clustering technique is employed to determine the optimum parameter set of the Gaussian mixture model and the efficacy of the proposed scheme is demonstrated on a Contract Bridge game opening bid problem."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8474150"
                        ],
                        "name": "P. Adamidis",
                        "slug": "P.-Adamidis",
                        "structuredName": {
                            "firstName": "Panagiotis",
                            "lastName": "Adamidis",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Adamidis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3284471"
                        ],
                        "name": "V. Petridis",
                        "slug": "V.-Petridis",
                        "structuredName": {
                            "firstName": "Vassilios",
                            "lastName": "Petridis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Petridis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 34
                            }
                        ],
                        "text": "recurrent ANN\u2019s [41], [60], [65], [100], [102], [103], [106],"
                    },
                    "intents": []
                }
            ],
            "corpusId": 15197567,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8f7d941867de10ff755494182b48353eccf2be96",
            "isKey": false,
            "numCitedBy": 41,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Parallel genetic algorithms (PGA) offer a natural and productive way to solve a problem better than a single population. Up to now the different PGA paradigms use the same evolution behaviour on each population. This paper proposes a method, called Co-operating Populations with Different Evolution Behaviours (CoPDEB) where the populations are allowed to exhibit different evolution behaviours. This is achieved by using a variety of selection mechanisms, operators, communication methods and parameters as it is explained in the sequel. This method has been tested on the problem of training a recurrent artificial neural network (RANN)."
            },
            "slug": "Co-operating-Populations-with-Different-Evolution-Adamidis-Petridis",
            "title": {
                "fragments": [],
                "text": "Co-operating Populations with Different Evolution Behaviours"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper proposes a method, called Co-operating Populations with Different Evolution Behaviours (CoPDEB), where the populations are allowed to exhibit different evolution behaviours."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Evolutionary Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401946815"
                        ],
                        "name": "A. Esparcia-Alc\u00e1zar",
                        "slug": "A.-Esparcia-Alc\u00e1zar",
                        "structuredName": {
                            "firstName": "Anna",
                            "lastName": "Esparcia-Alc\u00e1zar",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Esparcia-Alc\u00e1zar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145491770"
                        ],
                        "name": "K. Sharman",
                        "slug": "K.-Sharman",
                        "structuredName": {
                            "firstName": "Ken",
                            "lastName": "Sharman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sharman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61090497,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14b7e9ba55eb4c487f69622324ec04262b3744ca",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel design paradigm for recurrent neural networks. This employs a two-stage genetic programming/simulated annealing hybrid algorithm to produce a neural network which satisfies a set of design constraints. The genetic programming part of the algorithm is used to evolve the general topology of the network, along with specifications for the neuronal transfer functions, while the simulated annealing component of the algorithm adapts the network's connection weights in response to a set of training data. Our approach offers important advantages over existing methods for automated network design. Firstly, it allows us to develop recurrent network connections; secondly, we are able to employ neurones with arbitrary transfer functions, and thirdly, the approach yields an efficient and easy to implement on-line training algorithm. The procedures involved in using the GP/SA hybrid algorithm are illustrated by using it to design a neural network for adaptive filtering in a signal processing application."
            },
            "slug": "Genetic-programming-techniques-that-evolve-neural-Esparcia-Alc\u00e1zar-Sharman",
            "title": {
                "fragments": [],
                "text": "Genetic programming techniques that evolve recurrent neural network architectures for signal processing"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "This work proposes a novel design paradigm for recurrent neural networks which employs a two-stage genetic programming/simulated annealing hybrid algorithm to produce a neural network which satisfies a set of design constraints."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks for Signal Processing VI. Proceedings of the 1996 IEEE Signal Processing Society Workshop"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12409658"
                        ],
                        "name": "M. Hwang",
                        "slug": "M.-Hwang",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Hwang",
                            "middleNames": [
                                "Woong"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hwang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118887994"
                        ],
                        "name": "J. Choi",
                        "slug": "J.-Choi",
                        "structuredName": {
                            "firstName": "Jin",
                            "lastName": "Choi",
                            "middleNames": [
                                "Young"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Choi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48490775"
                        ],
                        "name": "Jaehong Park",
                        "slug": "Jaehong-Park",
                        "structuredName": {
                            "firstName": "Jaehong",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jaehong Park"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 41
                            }
                        ],
                        "text": "[118], [127], [128], [130], [138], [149]\u2013[225]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61047766,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86863208b81695b10fe470e9d3ff45127fee0c68",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents an evolutionary projection neural network (PNN) trained by an evolutionary computation technique. The PNN can activate radial basis functions as well as sigmoid functions with a special type of hidden nodes. The evolutionary learning algorithm for the PNN not only trains the parameters and the connection weights but also optimizes the network structure. The structure optimization strategy not only determines the number of hidden nodes necessary to represent a given target function, but also decides whether the role of each hidden node is a radial basis function node or a sigmoid function node. In order to apply the algorithm, a PNN is realized by a self-organizing genotype representation with a linked-list data structure. Simulations show that the algorithm can build a PNN with less hidden nodes than are required by the existing learning algorithm, which uses error backpropagation (EBP) and the network growing strategy."
            },
            "slug": "Evolutionary-projection-neural-networks-Hwang-Choi",
            "title": {
                "fragments": [],
                "text": "Evolutionary projection neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 36,
                "text": "Simulations show that the algorithm can build a PNN with less hidden nodes than are required by the existing learning algorithm, which uses error backpropagation (EBP) and the network growing strategy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1997 IEEE International Conference on Evolutionary Computation (ICEC '97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715854"
                        ],
                        "name": "L. Medsker",
                        "slug": "L.-Medsker",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Medsker",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Medsker"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "As described by Whitley [71], prior to 1995, several solutions had been proposed for this problem, but none were convincing enough to make EANNs a more efficient approach than backpropagation for the general case of supervised learning in standard feedforward ANNs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "As described in [71] and originally recognized by Radcliffe [53], the symmetry of neural network solutions allows the same functional unit to appear in different locations of different neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58237124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ac00bb6735c0597239370c80bdc3572d93556f5",
            "isKey": false,
            "numCitedBy": 119,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "The integration of genetic algorithms with neural networks is a rapidly expanding area building on the explosion of interest in the two technologies individually. In the early 1990\u2019s, the revolution in the research and application of neural networks was followed by a surge in activity for genetic algorithms. By 1994, several conferences and publications had been dedicated to genetic algorithms where they had formerly been an adjunct of neural network or expert systems forums."
            },
            "slug": "Genetic-Algorithms-and-Neural-Networks-Medsker",
            "title": {
                "fragments": [],
                "text": "Genetic Algorithms and Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The integration of genetic algorithms with neural networks is a rapidly expanding area building on the explosion of interest in the two technologies individually."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "23699861"
                        ],
                        "name": "M. Srinivas",
                        "slug": "M.-Srinivas",
                        "structuredName": {
                            "firstName": "Mekapati",
                            "lastName": "Srinivas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Srinivas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145238397"
                        ],
                        "name": "L. Patnaik",
                        "slug": "L.-Patnaik",
                        "structuredName": {
                            "firstName": "Lalit",
                            "lastName": "Patnaik",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Patnaik"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "In fact, can represent real-valued connection weights from node to node so that the architecture and connection weights can be evolved simultaneously [37], [42], [45], [165], [166], [169]\u2013[171]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 103
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 18
                            }
                        ],
                        "text": "[24], [26], [28], [37], [38], [41], [52], [53]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 58775323,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4a36fd499c4c2d8918fdd46f124227527092e08",
            "isKey": false,
            "numCitedBy": 45,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors present a technique for reducing the search-space of the genetic algorithm (GA) to improve its performance in searching for the globally optimal set of connection-weights. They use the notion of equivalent solutions in the search space, and include in the reduced search-space only one solution, called the base solution, from each set of equivalent solutions. The iteration of the GA consists of an additional step where the solutions are mapped to the respective base solutions. Experiments were conducted to compare the performance of the GAs with and without search-space reduction. The experimental results are presented and discussed.<<ETX>>"
            },
            "slug": "Learning-neural-network-weights-using-genetic-by-Srinivas-Patnaik",
            "title": {
                "fragments": [],
                "text": "Learning neural network weights using genetic algorithms-improving performance by search-space reduction"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "The authors present a technique for reducing the search-space of the genetic algorithm (GA) to improve its performance in searching for the globally optimal set of connection-weights by using the notion of equivalent solutions in the search space."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] 1991 IEEE International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113475446"
                        ],
                        "name": "D. White",
                        "slug": "D.-White",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "White",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. White"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2504354"
                        ],
                        "name": "P. Ligomenides",
                        "slug": "P.-Ligomenides",
                        "structuredName": {
                            "firstName": "Panos",
                            "lastName": "Ligomenides",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Ligomenides"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "White and Ligomenides [171] adopted a simpler approach to the evolution of both topological structures and node transfer functions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 188
                            }
                        ],
                        "text": "In fact, can represent real-valued connection weights from node to node so that the architecture and connection weights can be evolved simultaneously [37], [42], [45], [165], [166], [169]\u2013[171]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20078749,
            "fieldsOfStudy": [
                "Computer Science",
                "Business"
            ],
            "id": "30c7a5bcbef0db074c9761821f35051079268e37",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces a new algorithm which uses a genetic algorithm (GA) to determine the topology and link weights of a neural network. If the genetic algorithm fails to find a satisfactory solution network, the best network developed by the GA is used to try to find a solution via back-propagation. In this way, each algorithm is used to its greatest advantage: the GA (with its global search) determines a (sub-optimal) topology and weights to solve the problem, and back-propagation (with its local search) seeks the best solution in the area of the weight and topology spaces found by the GA."
            },
            "slug": "GANNet:-A-Genetic-Algorithm-for-Optimizing-Topology-White-Ligomenides",
            "title": {
                "fragments": [],
                "text": "GANNet: A Genetic Algorithm for Optimizing Topology and Weights in Neural Network Design"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A new algorithm which uses a genetic algorithm (GA) to determine the topology and link weights of a neural network and the best network developed by the GA is used to try to find a solution via back-propagation."
            },
            "venue": {
                "fragments": [],
                "text": "IWANN"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3310039"
                        ],
                        "name": "Stephen Dominic",
                        "slug": "Stephen-Dominic",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Dominic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Dominic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143863023"
                        ],
                        "name": "R. Das",
                        "slug": "R.-Das",
                        "structuredName": {
                            "firstName": "Rajarshi",
                            "lastName": "Das",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Das"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145285040"
                        ],
                        "name": "L. D. Whitley",
                        "slug": "L.-D.-Whitley",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Whitley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Whitley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144826602"
                        ],
                        "name": "C. Anderson",
                        "slug": "C.-Anderson",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Anderson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Anderson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62724982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "095704af28492e69064fa898719fb2ebd972df6d",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "It is pointed out that the genetic algorithms which have been shown to yield good performance for neural network weight optimization are really genetic hill-climbers, with a strong reliance on mutation rather than hyperplane sampling. Neural control problems are more appropriate for these genetic hill-climbers than supervised learning applications because in reinforcement learning applications gradient information is not directly available. Genetic reinforcement learning produces competitive results with the adaptive heuristic critic method, another reinforcement learning paradigm for neural networks that employs temporal difference methods. The genetic hill-climbing algorithm appears to be robust over a wide range of learning conditions.<<ETX>>"
            },
            "slug": "Genetic-reinforcement-learning-for-neural-networks-Dominic-Das",
            "title": {
                "fragments": [],
                "text": "Genetic reinforcement learning for neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "It is pointed out that the genetic algorithms which have been shown to yield good performance for neural network weight optimization are really genetic hill-climbers, with a strong reliance on mutation rather than hyperplane sampling."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732302"
                        ],
                        "name": "J. Koza",
                        "slug": "J.-Koza",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Koza",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30705144"
                        ],
                        "name": "J. P. Rice",
                        "slug": "J.-P.-Rice",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Rice",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. P. Rice"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "In fact, can represent real-valued connection weights from node to node so that the architecture and connection weights can be evolved simultaneously [37], [42], [45], [165], [166], [169]\u2013[171]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "has been carried out in recent years [33], [42], [45],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 783933,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ac4fedb5efe488addead4ea30c8856d689b156b5",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Shows how to find both the weights and architecture for a neural network, including the number of layers, the number of processing elements per layer, and the connectivity between processing elements. This is accomplished by using a recently developed extension to the genetic algorithm which genetically breeds a population of LISP symbolic expressions of varying size and shape until the desired performance by the network is successfully evolved. The novel 'genetic programming' paradigm is applied to the problem of generating a neural network for a one-bit adder.<<ETX>>"
            },
            "slug": "Genetic-generation-of-both-the-weights-and-for-a-Koza-Rice",
            "title": {
                "fragments": [],
                "text": "Genetic generation of both the weights and architecture for a neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The novel 'genetic programming' paradigm is applied to the problem of generating a neural network for a one-bit adder by using a recently developed extension to the genetic algorithm which genetically breeds a population of LISP symbolic expressions of varying size and shape."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145738991"
                        ],
                        "name": "J. D. Schaffer",
                        "slug": "J.-D.-Schaffer",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Schaffer",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. D. Schaffer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145727186"
                        ],
                        "name": "R. Caruana",
                        "slug": "R.-Caruana",
                        "structuredName": {
                            "firstName": "Rich",
                            "lastName": "Caruana",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Caruana"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3035326"
                        ],
                        "name": "L. Eshelman",
                        "slug": "L.-Eshelman",
                        "structuredName": {
                            "firstName": "Larry",
                            "lastName": "Eshelman",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Eshelman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[153] have presented an experiment which showed"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 61
                            }
                        ],
                        "text": "The length of chromosome can be reduced greatly in this case [153], [154]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 96
                            }
                        ],
                        "text": "The first separates the evolution of architectures from that of connection weights [24], [150], [153], [154], [165], [167], [169], [170]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 122
                            }
                        ],
                        "text": "In the first approach, each connection of an architecture is directly specified by its binary representation [24], [150], [153], [154], [165], [167], [169], [170], [202]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61774671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a74b131850519b03ef248a2acfa886f5b3f9386d",
            "isKey": true,
            "numCitedBy": 114,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Using-genetic-search-to-exploit-the-emergent-of-Schaffer-Caruana",
            "title": {
                "fragments": [],
                "text": "Using genetic search to exploit the emergent behavior of neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144924401"
                        ],
                        "name": "C. R. Chow",
                        "slug": "C.-R.-Chow",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Chow",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. R. Chow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38200353"
                        ],
                        "name": "C. Chu",
                        "slug": "C.-Chu",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Chu",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62570714,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "181d6a0a7d2d585bf4a82e73f7fd81fd496d7a24",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "A learning algorithm based on genetic algorithms to configure multilayered feedforward networks in supervised learning mode is described. The method described lets a population of hidden units compete among themselves and \"sell\" themselves to be connected to members of another pool of output units. An output unit in this sense therefore represents a team comprising the output unit itself and its connected hidden units. This \"team\", of course, defines the architecture of the network. If each output unit can choose for itself how many hidden units it needs to accomplish the classification task, different architectures can be seen to be competing against each other. Experiment results are presented and discussed."
            },
            "slug": "On-the-configuration-of-multilayered-feedforward-by-Chow-Chu",
            "title": {
                "fragments": [],
                "text": "On the configuration of multilayered feedforward networks by an evolutionary process"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A learning algorithm based on genetic algorithms to configure multilayered feedforward networks in supervised learning mode is described to let a population of hidden units compete among themselves and \"sell\" themselves to be connected to members of another pool of output units."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 37th Midwest Symposium on Circuits and Systems"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153691043"
                        ],
                        "name": "R. Smith",
                        "slug": "R.-Smith",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Smith",
                            "middleNames": [
                                "Elliott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195235"
                        ],
                        "name": "H. Cribbs",
                        "slug": "H.-Cribbs",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Cribbs",
                            "middleNames": [
                                "Brown"
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Cribbs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 28250112,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98d368cb0d20f50dcb018d2e757304e9d228edce",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper suggests a simple analogy between learning classifier systems (LCSs) and neural networks (NNs). By clarifying the relationship between LCSs and NNs, the paper indicates how techniques from one can be utilized in the other. The paper points out that the primary distinguishing characteristic of the LCS is its use of a co-adaptive genetic algorithm (GA), where the end product of evolution is a diverse population of individuals that cooperate to perform useful computation. This stands in contrast to typical GA/NN schemes, where a population of networks is employed to evolve a single, optimized network. To fully illustrate the LCS/NN analogy used in this paper, an LCS-like NN is implemented and tested. The test is constructed to run parallel to a similar GA/NN study that did not employ a co-adaptive GA. The test illustrates the LCS/NN analogy and suggests an interesting new method for applying GAs in NNs. Final comments discuss extensions of this work and suggest how LCS and NN studies can further benefit each other."
            },
            "slug": "Is-a-Learning-Classifier-System-a-Type-of-Neural-Smith-Cribbs",
            "title": {
                "fragments": [],
                "text": "Is a Learning Classifier System a Type of Neural Network?"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The paper points out that the primary distinguishing characteristic of the LCS is its use of a co-adaptive genetic algorithm, where the end product of evolution is a diverse population of individuals that cooperate to perform useful computation."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145028228"
                        ],
                        "name": "P. Weller",
                        "slug": "P.-Weller",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Weller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Weller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46920063"
                        ],
                        "name": "R. Summers",
                        "slug": "R.-Summers",
                        "structuredName": {
                            "firstName": "Ron",
                            "lastName": "Summers",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Summers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117429780"
                        ],
                        "name": "A. Thompson",
                        "slug": "A.-Thompson",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Thompson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Thompson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62613569,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3c727e198c42f8592bddb5a786a08d5ede8f5382",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an investigation into using a genetic algorithm to evolve the optimum set of inputs for a neural network. The network is to be used in a novel way for the prediction of nuclear reactor parameters under fault conditions. The development of transients is calculated in a recursive manner. The previous work and the next stage of research are described. The procedure and genetic algorithm options, including fitness, are discussed along with explanations. Finally an outline of the remaining work is introduced."
            },
            "slug": "Using-a-genetic-algorithm-to-evolve-an-optimum-set-Weller-Summers",
            "title": {
                "fragments": [],
                "text": "Using a genetic algorithm to evolve an optimum input set for a predictive neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "An investigation into using a genetic algorithm to evolve the optimum set of inputs for a neural network to be used in a novel way for the prediction of nuclear reactor parameters under fault conditions."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153691043"
                        ],
                        "name": "R. Smith",
                        "slug": "R.-Smith",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Smith",
                            "middleNames": [
                                "Elliott"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2195235"
                        ],
                        "name": "H. Cribbs",
                        "slug": "H.-Cribbs",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Cribbs",
                            "middleNames": [
                                "Brown"
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Cribbs"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 17
                            }
                        ],
                        "text": "Smith and Cribbs [181], [237] also used an individual to represent a hidden node rather than the whole ANN."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43615556,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dff9d3d3f32d375e122ccd499e940ae992813788",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combined-biological-paradigms:-A-neural,-autonomous-Smith-Cribbs",
            "title": {
                "fragments": [],
                "text": "Combined biological paradigms: A neural, genetics-based autonomous systems strategy"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3247745"
                        ],
                        "name": "Thomas B\u00e4ck",
                        "slug": "Thomas-B\u00e4ck",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "B\u00e4ck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas B\u00e4ck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743344"
                        ],
                        "name": "H. Schwefel",
                        "slug": "H.-Schwefel",
                        "structuredName": {
                            "firstName": "Hans-Paul",
                            "lastName": "Schwefel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schwefel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5166635,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb002f44dff9bb33c9d312299bffa1df6a099052",
            "isKey": false,
            "numCitedBy": 2056,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Three main streams of evolutionary algorithms (EAs), probabilistic optimization algorithms based on the model of natural evolution, are compared in this article: evolution strategies (ESs), evolutionary programming (EP), and genetic algorithms (GAs). The comparison is performed with respect to certain characteristic components of EAs: the representation scheme of object variables, mutation, recombination, and the selection operator. Furthermore, each algorithm is formulated in a high-level notation as an instance of the general, unifying basic algorithm, and the fundamental theoretical results on the algorithms are presented. Finally, after presenting experimental results for three test functions representing a unimodal and a multimodal case as well as a step function with discontinuities, similarities and differences of the algorithms are elaborated, and some hints to open research questions are sketched."
            },
            "slug": "An-Overview-of-Evolutionary-Algorithms-for-B\u00e4ck-Schwefel",
            "title": {
                "fragments": [],
                "text": "An Overview of Evolutionary Algorithms for Parameter Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Three main streams of evolutionary algorithms (EAs), probabilistic optimization algorithms based on the model of natural evolution, are compared in this article: evolution strategies (ESs), evolutionary programming (EP), and genetic algorithms (GAs)."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2300746"
                        ],
                        "name": "P. Korning",
                        "slug": "P.-Korning",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Korning",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Korning"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7945307,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7d5e2091936be9c048521afe0824a93d5318eed2",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "In the neural network/genetic algorithm community, rather limited success in the training of neural networks by genetic algorithms has been reported. In a paper by Whitley et al. (1991), he claims that, due to \"the multiple representations problem\", genetic algorithms will not effectively be able to train multilayer perceptrons, whose chromosomal representation of its weights exceeds 300 bits. In the following paper, by use of a \"real-life problem\", known to be non-trivial, and by a comparison with \"classic\" neural net training methods, I will try to show, that the modest success of applying genetic algorithms to the training of perceptrons, is caused not so much by the \"multiple representations problems\" as by the fact that problem-specific knowledge available is often ignored, thus making the problem unnecessarily tough for the genetic algorithm to solve. Special success is obtained by the use of a new fitness function, which takes into account the fact that the search performed by a genetic algorithm is holistic, and not local as is usually the case when perceptrons are trained by traditional methods."
            },
            "slug": "Training-neural-networks-by-means-of-genetic-on-Korning",
            "title": {
                "fragments": [],
                "text": "Training neural networks by means of genetic algorithms working on very long chromosomes"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is shown, that the modest success of applying genetic algorithms to the training of perceptrons, is caused not so much by the \"multiple representations problems\" as by the fact that problem-specific knowledge available is often ignored, thus making the problem unnecessarily tough for the genetic algorithm to solve."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2025680"
                        ],
                        "name": "S. G. Romaniuk",
                        "slug": "S.-G.-Romaniuk",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Romaniuk",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. G. Romaniuk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46575475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4aa90507f7c9fb7ed4311781fa8e4e3b26f87a44",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to automatically construct neural networks is of importance, since it supports reduction in development time and can lead to simpler designs than traditionally handcrafted networks. Automation is further required to take the step towards a more autonomous learning system. In this paper, we report further results involving the automatic network construction algorithm EGP (Evolutionary Growth Perceptron), which utilizes simple evolutionary processes to locally train network features using the perceptron rule. Emphasis is placed on determining the effectiveness of several types of crossover operators in conjunction with varying the population size and the number of epochs during which individual perceptrons are trained. The crossover operators considered and introduced are: simple random, weighted and blocked.<<ETX>>"
            },
            "slug": "Applying-crossover-operators-to-automatic-neural-Romaniuk",
            "title": {
                "fragments": [],
                "text": "Applying crossover operators to automatic neural network construction"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This paper reports further results involving the automatic network construction algorithm EGP (Evolutionary Growth Perceptron), which utilizes simple evolutionary processes to locally train network features using the perceptron rule."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719377"
                        ],
                        "name": "S. Billings",
                        "slug": "S.-Billings",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Billings",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Billings"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32073785"
                        ],
                        "name": "G. L. Zheng",
                        "slug": "G.-L.-Zheng",
                        "structuredName": {
                            "firstName": "Guang",
                            "lastName": "Zheng",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. L. Zheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5286051,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc6d5c6cf5638056ce125d3e8675509b0fe4f227",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Radial-basis-function-network-configuration-using-Billings-Zheng",
            "title": {
                "fragments": [],
                "text": "Radial basis function network configuration using genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108893507"
                        ],
                        "name": "Heung Bum Kim",
                        "slug": "Heung-Bum-Kim",
                        "structuredName": {
                            "firstName": "Heung",
                            "lastName": "Kim",
                            "middleNames": [
                                "Bum"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heung Bum Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47165253"
                        ],
                        "name": "S. Jung",
                        "slug": "S.-Jung",
                        "structuredName": {
                            "firstName": "Sung",
                            "lastName": "Jung",
                            "middleNames": [
                                "Hoon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Jung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761763"
                        ],
                        "name": "T. Kim",
                        "slug": "T.-Kim",
                        "structuredName": {
                            "firstName": "Tag",
                            "lastName": "Kim",
                            "middleNames": [
                                "Gon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400218066"
                        ],
                        "name": "Kyu Ho Park",
                        "slug": "Kyu-Ho-Park",
                        "structuredName": {
                            "firstName": "Kyu Ho",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyu Ho Park"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 189
                            }
                        ],
                        "text": "The adaptive adjustment of BP parameters (such as the learning rate and momentum) through evolution could be considered as the first attempt of the evolution of learning rules [32], [152], [272]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 38
                            }
                        ],
                        "text": "Other researchers [32], [139], [213], [272] also used an evolutionary process to find parameters for BP but ANN\u2019s"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17621223,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "128ca5684011ed316507192d38abcb883e978e25",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-learning-method-for-back-propagation-neural-by-Kim-Jung",
            "title": {
                "fragments": [],
                "text": "Fast learning method for back-propagation neural network by evolutionary adaptation of learning rates"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69506397"
                        ],
                        "name": "M. Zaus",
                        "slug": "M.-Zaus",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Zaus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Zaus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13619106"
                        ],
                        "name": "R. Megnet",
                        "slug": "R.-Megnet",
                        "structuredName": {
                            "firstName": "Roland",
                            "lastName": "Megnet",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Megnet"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59920365,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ff349f1ff4ca4f78605b981b4b24ae932ce6dc3",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "FUSION-TECHNOLOGY-AND-THE-DESIGN-OF-EVOLUTIONARY-Zaus-Megnet",
            "title": {
                "fragments": [],
                "text": "FUSION-TECHNOLOGY AND THE DESIGN OF EVOLUTIONARY MACHINES FOR NEURAL NETWORKS"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2463090"
                        ],
                        "name": "J. G. Elias",
                        "slug": "J.-G.-Elias",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Elias",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. G. Elias"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16324836,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "570dcbcb896f98672de0ff686b68ad029123cc20",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Work-in-progress on the use of a specialized genetic algorithm for training a new type of dynamic artificial neural network is described. The network architecture is completely specified by a list of addresses that are used to connect signal sources to specific artificial synapses, which have both a temporal and spatial significance. The number of different connection patterns is a combinational problem which grows factorially as the number of artificial synapses in the network and the number of sensor elements increases. The network is implemented primarily in analog electronic hardware and constructed from artificial dendritic trees which exhibit a spatiotemporal processing capability that is modeled after morphologically complex biological neurons. The author describes work-in-progress on using the specialized genetic algorithm, which has an embedded optimizer in place of the standard mutation operator, for training a dynamic neural network to follow the position of a target moving across an image sensor array.<<ETX>>"
            },
            "slug": "Genetic-generation-of-connection-patterns-for-a-Elias",
            "title": {
                "fragments": [],
                "text": "Genetic generation of connection patterns for a dynamic artificial neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The author describes work-in-progress on using the specialized genetic algorithm, which has an embedded optimizer in place of the standard mutation operator, for training a dynamic neural network to follow the position of a target moving across an image sensor array."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47955401"
                        ],
                        "name": "Sungzoon Cho",
                        "slug": "Sungzoon-Cho",
                        "structuredName": {
                            "firstName": "Sungzoon",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sungzoon Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25703612"
                        ],
                        "name": "Keonhoe Cha",
                        "slug": "Keonhoe-Cha",
                        "structuredName": {
                            "firstName": "Keonhoe",
                            "lastName": "Cha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Keonhoe Cha"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 0
                            }
                        ],
                        "text": "Cho and Cha [289] proposed another algorithm for evolving training sets by adding virtual samples."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 30
                            }
                        ],
                        "text": "Fontanari and Meir [267] used Chalmers\u2019 approach to evolve learning rules for binary perceptrons."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 12
                            }
                        ],
                        "text": "Cho and Cha [289] proposed another algorithm for evolving training sets by"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 63
                            }
                        ],
                        "text": "Bengioet al.\u2019s approach [265], [266] is slightly different from Chalmers\u2019 in the sense that gradient descent algorithms and simulated annealing, rather than EA\u2019s, were used to find near-optimal\u2019s."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "Chalmers [264] defined a learning rule as a linear combination of four local variables and their six pairwise products."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6794908,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be3687c7e2fb3e8cceac94c4f34043384fd8e60d",
            "isKey": true,
            "numCitedBy": 29,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Using an oversized neural network or too small a training sample set results in overfitting. In order to improve generalization capability, either the network should be reduced or additional training samples have to be collected. Obtaining additional training samples, however, can be often very expensive or impossible. Here we propose an evolutionary approach where new virtual samples are added to the training sample set as a population of MLPs evolve over generations. At each generation, these newly added virtual samples are used to retrain the MLPs. This approach is in contrast to previous evolutionary neural network approaches where connection weights, network architectures, learning rules, or their mixtures evolve. A preliminary result obtained from a robot arm kinematics problem is promising. The generalization error was reduced more than 50%. The approach can be applied in various practical situations where additional training samples are expensive or impossible."
            },
            "slug": "Evolution-of-neural-network-training-set-through-of-Cho-Cha",
            "title": {
                "fragments": [],
                "text": "Evolution of neural network training set through addition of virtual samples"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An evolutionary approach where new virtual samples are added to the training sample set as a population of MLPs evolve over generations, in contrast to previous evolutionary neural network approaches where connection weights, network architectures, learning rules, or their mixtures evolve."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Evolutionary Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12463366"
                        ],
                        "name": "S. Oliker",
                        "slug": "S.-Oliker",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Oliker",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Oliker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061165712"
                        ],
                        "name": "M. Furst",
                        "slug": "M.-Furst",
                        "structuredName": {
                            "firstName": "Merrick",
                            "lastName": "Furst",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Furst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1770487"
                        ],
                        "name": "O. Maimon",
                        "slug": "O.-Maimon",
                        "structuredName": {
                            "firstName": "Oded",
                            "lastName": "Maimon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Maimon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 168
                            }
                        ],
                        "text": "In fact, can represent real-valued connection weights from node to node so that the architecture and connection weights can be evolved simultaneously [37], [42], [45], [165], [166], [169]\u2013[171]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 128
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 110
                            }
                        ],
                        "text": "The first separates the evolution of architectures from that of connection weights [24], [150], [153], [154], [165], [167], [169], [170]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 136
                            }
                        ],
                        "text": "In the first approach, each connection of an architecture is directly specified by its binary representation [24], [150], [153], [154], [165], [167], [169], [170], [202]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18825475,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "114d35baf8bcfe4c3511662292b1197aa2a0416a",
            "isKey": true,
            "numCitedBy": 36,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "A bstract. A new approach for designing and training neural networks is developed using a distr ibuted genet ic algorithm. A search for the optimal architecture and weights of a neural network comprising binary, linear threshold units is performed. For each individual unit , we look for th e opt imal set of connections and associated weights under the restriction of a feedforward network str ucture. This is accomplished with th e modified genet ic algorithm, using an objective function-fitness-that considers, primarily, the overall network error; and, secondarily, using the unit 's possible connections and weights that are preferable for cont inuity of the convergence process. Examples are given showing the pot ential of th e proposed approach."
            },
            "slug": "A-Distributed-Genetic-Algorithm-for-Neural-Network-Oliker-Furst",
            "title": {
                "fragments": [],
                "text": "A Distributed Genetic Algorithm for Neural Network Design and Training"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "A new approach for designing and training neural networks is developed using a modified genet ic algorithm that considers the overall network error, and uses the unit 's possible connections and weights that are preferable for cont inuity of the convergence process."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693885"
                        ],
                        "name": "Qiangfu Zhao",
                        "slug": "Qiangfu-Zhao",
                        "structuredName": {
                            "firstName": "Qiangfu",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiangfu Zhao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7380441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e1995534db28c90b5721e88ffff935f316d52573",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "To design the nearest-neighbor-based multilayer perceptron (NN-MLP) efficiently, the author has proposed a nongenetic-based evolutionary algorithm called the R(4)-rule. For off-line learning, the R(4)-rule can produce the smallest or nearly smallest networks with high generalization ability by iteratively performing four basic operations: recognition, remembrance, reduction, add review. This algorithm, however, cannot be applied directly to online learning because its inherent instability, which is caused by over-reduction and over-review. To stabilize the R(4)-rule, this paper proposes some improvements for reduction and review. The improved reduction is more robust for online learning because the fitness of each hidden neuron is defined by its overall behavior in many learning cycles. The new review is more efficient because hidden neurons are adjusted in a more careful way. The performance of the improved R (4)-rule for online learning is shown by experimental results."
            },
            "slug": "Stable-online-evolutionary-learning-of-NN-MLP-Zhao",
            "title": {
                "fragments": [],
                "text": "Stable online evolutionary learning of NN-MLP"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The improved reduction is more robust for online learning because the fitness of each hidden neuron is defined by its overall behavior in many learning cycles, and the new review is more efficient because hidden neurons are adjusted in a more careful way."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46221743"
                        ],
                        "name": "L. Mart\u00ed",
                        "slug": "L.-Mart\u00ed",
                        "structuredName": {
                            "firstName": "Luis",
                            "lastName": "Mart\u00ed",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Mart\u00ed"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60932141,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d1400786bbd5fb2c03b3670d2aee01393d42f2e",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The author studied several applications of genetic algorithms (GAs) within the neural networks field. After generating a robust GA engine, the system was used to generate neural network circuit architectures. This was accomplished by using the GA to determine the weights in a fully interconnected network. The importance of the internal genetic representation was shown by testing different approaches. The effects on speed of optimization by varying the constraints imposed upon the desired network were also studied. It was observed that relatively loose constraints provided results comparable to a fully constrained system. The type of neural network circuit generated was the recurrent competitive field as described by S. Grossberg (1982).<<ETX>>"
            },
            "slug": "Genetically-generated-neural-networks-I:-effects-Mart\u00ed",
            "title": {
                "fragments": [],
                "text": "Genetically generated neural networks-I: representational effects"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The author studied several applications of genetic algorithms within the neural networks field and observed that relatively loose constraints provided results comparable to a fully constrained system."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings 1992] IJCNN International Joint Conference on Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145285040"
                        ],
                        "name": "L. D. Whitley",
                        "slug": "L.-D.-Whitley",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Whitley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Whitley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2209955"
                        ],
                        "name": "T. Starkweather",
                        "slug": "T.-Starkweather",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Starkweather",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Starkweather"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143882377"
                        ],
                        "name": "C. Bogart",
                        "slug": "C.-Bogart",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Bogart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Bogart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "various areas [20]\u2013[22], but BP has drawbacks due to its use of gradient descent [23], [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "In the first approach, each connection of an architecture is directly specified by its binary representation [24], [150], [153], [154], [165], [167], [169], [170], [202]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 83
                            }
                        ],
                        "text": "The first separates the evolution of architectures from that of connection weights [24], [150], [153], [154], [165], [167], [169], [170]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[24], [26], [28], [37], [38], [41], [52], [53]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 223
                            }
                        ],
                        "text": "Because EA\u2019s can treat large, complex, nondifferentiable, and multimodal spaces, which are the typical case in the real world, considerable research and application has been conducted on the evolution of connection weights [24], [26]\u2013[112]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6273216,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0d52b087713f79b7b1d3fe2112e9cfa3bad221bb",
            "isKey": true,
            "numCitedBy": 742,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Genetic-algorithms-and-neural-networks:-optimizing-Whitley-Starkweather",
            "title": {
                "fragments": [],
                "text": "Genetic algorithms and neural networks: optimizing connections and connectivity"
            },
            "venue": {
                "fragments": [],
                "text": "Parallel Comput."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49387170"
                        ],
                        "name": "Y. Ichikawa",
                        "slug": "Y.-Ichikawa",
                        "structuredName": {
                            "firstName": "Yoshiaki",
                            "lastName": "Ichikawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ichikawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1924526"
                        ],
                        "name": "T. Sawa",
                        "slug": "T.-Sawa",
                        "structuredName": {
                            "firstName": "Toshiyuki",
                            "lastName": "Sawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Sawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11903597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab938b83a19367d1b769053d9cbf5a63f027d009",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "The author presents a learning algorithm and capabilities of perceptron-like neural networks whose outputs and inputs are directly connected to plants just like ordinary feedback controllers. This simple configuration includes the difficulty of teaching the network. In addition, it is preferable to let the network learn so that a global and arbitrary evaluation of the total responses of the plant will be optimized eventually. In order to satisfy these needs, genetic algorithms are modified to accommodate the network learning procedure. This procedure is a kind of simulated evolution process in which a group of networks gradually improves as a whole, by crossing over connection weights among them, or by mutational changes of the weights, according to fitness values assigned to each network by a global evaluation. Simulations demonstrate that these networks can be optimized in terms of various evaluations, and they can discover schemes by themselves, such as state estimation and nonlinear control."
            },
            "slug": "Neural-network-application-for-direct-feedback-Ichikawa-Sawa",
            "title": {
                "fragments": [],
                "text": "Neural network application for direct feedback controllers"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The author presents a learning algorithm and capabilities of perceptron-like neural networks whose outputs and inputs are directly connected to plants just like ordinary feedback controllers, andSimulations demonstrate that these networks can be optimized in terms of various evaluations, and they can discover schemes by themselves, such as state estimation and nonlinear control."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2860703"
                        ],
                        "name": "M. Taha",
                        "slug": "M.-Taha",
                        "structuredName": {
                            "firstName": "Mahmoud",
                            "lastName": "Taha",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Taha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50570163"
                        ],
                        "name": "A. Hanna",
                        "slug": "A.-Hanna",
                        "structuredName": {
                            "firstName": "Awad",
                            "lastName": "Hanna",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hanna"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "which report excellent results using hybrid evolutionary and gradient descent algorithms [32], [67], [70], [71], [74], [80],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Hybrid training has been used successfully in many application areas [32], [67], [70], [71], [74], [80], [81], [86],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43801822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee1861e18c3d715031f0ba319fde52252871b5f8",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks are attracting an enormous amount of attention in many civil engineering disciplines, including transportation, because they represent a class of robust, nonlinear models capable of learning relationships from data. However, in the development of such models for a particular application, various parameter settings are left to the judgment of the network developer. The net result of poor parameter settings will be slow convergence and/or bad performance on unseen cases. Recently, genetic algorithms have emerged as a potential searching technique to design a neural network model that performs best on a specified task according to explicit performance criteria. Genetic algorithms are search algorithms based on the mechanics of natural selection and natural genetics. In this paper the authors present a genetic algorithm method that evolves a neural network model for the selection of the optimum maintenance strategy for flexible pavements. A hybrid evolutionary-learning system using gradient descent learning as well as a genetic algorithm to determine the network connections weights is described. The developed neural network model has an input vector of seven components and an output vector of seven components. The input vector represents the factors affecting the maintenance strategy selection, whereas the output vector represents the different pavement maintenance strategies available. Brainmaker Professional, a commercially available neural network simulator, was used in the development of the neural network model. The performance of the developed neural network model was validated by testing it using 100 unseen cases. The validation results showed that the system misclassified only six cases with an average error rate of 0.024."
            },
            "slug": "Evolutionary-neural-network-model-for-the-selection-Taha-Hanna",
            "title": {
                "fragments": [],
                "text": "Evolutionary neural network model for the selection of pavement maintenance strategy"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A genetic algorithm method that evolves a neural network model for the selection of the optimum maintenance strategy for flexible pavements is presented and a hybrid evolutionary-learning system using gradient descent learning as well as a genetic algorithm to determine the network connections weights is described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145206430"
                        ],
                        "name": "F. A. Dill",
                        "slug": "F.-A.-Dill",
                        "structuredName": {
                            "firstName": "Franz",
                            "lastName": "Dill",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. A. Dill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103689675"
                        ],
                        "name": "B. Deer",
                        "slug": "B.-Deer",
                        "structuredName": {
                            "firstName": "B.",
                            "lastName": "Deer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Deer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61934010,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0296a9e3c633d26dd39af9dc60ce8db1615e9d5",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Genetic algorithms are used to search for network weights which cause the dynamical network to produce long attractors. Several variations of the genetic algorithm are described, and the search performance is compared to that of the base-line method of randomly selected weights. It is pointed out that dynamical networks support self-sustaining patterns of oscillation which can be initiated by a one-time input strobe. These self-sustaining patterns, or attractor cycles, evolve into a repeating pattern for most combinations of network weights and input strobes. Attractor cycles vary in length and are a function of the particular network weights and the particular strobe. An interesting property of these networks is that a particular set of network weights can produce, or recall, a variety of repeating patterns, where the one that is evoked depends on the triggering strobe. This effectively is the storage of sequential patterns in the form of attractors.<<ETX>>"
            },
            "slug": "An-exploration-of-genetic-algorithms-for-the-of-in-Dill-Deer",
            "title": {
                "fragments": [],
                "text": "An exploration of genetic algorithms for the selection of connection weights in dynamical neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "Genetic algorithms are used to search for network weights which cause the dynamical network to produce long attractors, and the search performance is compared to that of the base-line method of randomly selected weights."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE 1991 National Aerospace and Electronics Conference NAECON 1991"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143159732"
                        ],
                        "name": "K. S. Tang",
                        "slug": "K.-S.-Tang",
                        "structuredName": {
                            "firstName": "Katrina",
                            "lastName": "Tang",
                            "middleNames": [
                                "Schack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. S. Tang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110336099"
                        ],
                        "name": "C. Y. Chan",
                        "slug": "C.-Y.-Chan",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Chan",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Y. Chan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2425545"
                        ],
                        "name": "K. Man",
                        "slug": "K.-Man",
                        "structuredName": {
                            "firstName": "Kim",
                            "lastName": "Man",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Man"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687386"
                        ],
                        "name": "S. Kwong",
                        "slug": "S.-Kwong",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "Kwong",
                            "middleNames": [
                                "Tak",
                                "Wu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kwong"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 121
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[118], [127], [128], [130], [138], [149]\u2013[225]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 62702609,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fd00131005bf5d8bbd6ccc16afa77dbae7db21f5",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A structural genetic algorithm is proposed to optimize the neural network topology and connection weightings. This approach is to partition the genes of chromosome into control genes and connection genes in a hierarchical fashion. The control genes represented in bits are used to govern the layers and neurons activation and considered to be the higher level genes. Whereas the connection genes in the form of real values are the weightings and bias representations, regarded as the lower level genes. This inherent genetic variations enable multiple changes in lower level genes by a single change at the higher level genes. Such formulation of chromosome is found to be a phenomenal improvement over the traditional GA approach that without genes classification. As a result, the learning technique of the neural network is greatly improved. Simulation results have indicated that the proposed learning scheme requires the least iteration steps to reach a optimum network as compared to the uses of backpropagation and traditional non-structural genetic algorithms."
            },
            "slug": "Genetic-structure-for-NN-topology-and-weights-Tang-Chan",
            "title": {
                "fragments": [],
                "text": "Genetic structure for NN topology and weights optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Simulation results have indicated that the proposed learning scheme requires the least iteration steps to reach a optimum network as compared to the uses of backpropagation and traditional non-structural genetic algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52427096"
                        ],
                        "name": "D. Prados",
                        "slug": "D.-Prados",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Prados",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Prados"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "For some problems, evolutionary training can be significantly faster and more reliable than BP [30], [34], [40], [63], [83], [89]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123347668,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0459dfc36ccb478f2ab408e45efdc042ae9a514",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The Letter discusses a new method for training multilayered neural networks that uses genetic-algorithm techniques. Tests show that this method, called GenLearn, is significantly faster than methods that use the generalised delta rule. Gen-Learn also has the advantage that, unlike the generalised delta rule, it can escape local minima in its search of weight space."
            },
            "slug": "New-learning-algorithm-for-training-multilayered-Prados",
            "title": {
                "fragments": [],
                "text": "New learning algorithm for training multilayered neural networks that uses genetic-algorithm techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A new method for training multilayered neural networks that uses genetic-algorithm techniques that is significantly faster than methods that use the generalised delta rule and has the advantage that it can escape local minima in its search of weight space."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2253160"
                        ],
                        "name": "G. Greenwood",
                        "slug": "G.-Greenwood",
                        "structuredName": {
                            "firstName": "Garrison",
                            "lastName": "Greenwood",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Greenwood"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "recurrent ANN\u2019s [41], [60], [65], [100], [102], [103], [106],"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7759506,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4d6fd35bc1818bca3e7baf7048fa321f341aaf89",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "This correspondence presents the latest results of using evolutionary strategies (ESs) to design partially recurrent neural networks for viseme recognition. ESs are stochastic optimization algorithms based upon the principles of natural selection found in the biological world. Our results indicate that ESs can be effectively used to determine the synaptic weights in neural networks and can outperform backpropagation techniques."
            },
            "slug": "Training-partially-recurrent-neural-networks-using-Greenwood",
            "title": {
                "fragments": [],
                "text": "Training partially recurrent neural networks using evolutionary strategies"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "This correspondence presents the latest results of using evolutionary strategies to design partially recurrent neural networks for viseme recognition and indicates that ESs can be effectively used to determine the synaptic weights in neural networks and can outperform backpropagation techniques."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Speech Audio Process."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3275927"
                        ],
                        "name": "A. Likartsis",
                        "slug": "A.-Likartsis",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Likartsis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Likartsis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69851167"
                        ],
                        "name": "I. Vlachavas",
                        "slug": "I.-Vlachavas",
                        "structuredName": {
                            "firstName": "I.",
                            "lastName": "Vlachavas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Vlachavas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145695478"
                        ],
                        "name": "L. Tsoukalas",
                        "slug": "L.-Tsoukalas",
                        "structuredName": {
                            "firstName": "Lefteri",
                            "lastName": "Tsoukalas",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Tsoukalas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41493715,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6b48938d13bd112732bf70bca9026cbde6a73643",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "A new hybrid neural-generic methodology is presented that exploits the optimization advantages of genetic algorithms for the purpose of accelerating neural network training. The choice of fitness function is addressed and experimental findings are shown where neural network training is improved through the proposed approach. The results suggest that genetic algorithms can be a powerful tool for improving learning in neural networks."
            },
            "slug": "A-new-hybrid-neural-genetic-methodology-for-Likartsis-Vlachavas",
            "title": {
                "fragments": [],
                "text": "A new hybrid neural-genetic methodology for improving learning"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The proposed hybrid neural-generic methodology exploits the optimization advantages of genetic algorithms for the purpose of accelerating neural network training and suggests that genetic algorithms can be a powerful tool for improving learning in neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings Ninth IEEE International Conference on Tools with Artificial Intelligence"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121794245"
                        ],
                        "name": "J. Astor",
                        "slug": "J.-Astor",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Astor",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Astor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145882965"
                        ],
                        "name": "C. Adami",
                        "slug": "C.-Adami",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Adami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Adami"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 11
                            }
                        ],
                        "text": "Similarly, [1] simulates a complex environment, complete with artificial chemicals that diffuse between cells and trigger various activities, including neuronal duplication and axonal/dendritic growth."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 106
                            }
                        ],
                        "text": "Thus, a lot of the work into growing axonal connections or gradually duplicating neurons in ALife systems [1, 33, 52] may occupy a researchers no-man\u2019s land: too abstract to answer detailed questions in neurodevelopment (perhaps best modeled by work such as [70]), while rather superfluous and computationally expensive for AI."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14960799,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "bfd868edc5fb1578f1d3bb8895c5ae7235738dca",
            "isKey": false,
            "numCitedBy": 106,
            "numCiting": 89,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model of decentralized growth and development for artificial neural networks (ANNs), inspired by developmental biology and the physiology of nervous systems. In this model, each individual artificial neuron is an autonomous unit whose behavior is determined only by the genetic information it harbors and local concentrations of substrates. The chemicals and substrates, in turn, are modeled by a simple artificial chemistry. While the system is designed to allow for the evolution of complex networks, we demonstrate the power of the artificial chemistry by analyzing engineered (handwritten) genomes that lead to the growth of simple networks with behaviors known from physiology. To evolve more complex structures, a Java-based, platform-independent, asynchronous, distributed genetic algorithm (GA) has been implemented that allows users to participate in evolutionary experiments via the World Wide Web."
            },
            "slug": "A-Developmental-Model-for-the-Evolution-of-Neural-Astor-Adami",
            "title": {
                "fragments": [],
                "text": "A Developmental Model for the Evolution of Artificial Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "This work presents a model of decentralized growth and development for artificial neural networks (ANNs), inspired by developmental biology and the physiology of nervous systems, and demonstrates the power of the artificial chemistry by analyzing engineered genomes that lead to the growth of simple networks with behaviors known from physiology."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Life"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2025680"
                        ],
                        "name": "S. G. Romaniuk",
                        "slug": "S.-G.-Romaniuk",
                        "structuredName": {
                            "firstName": "Steve",
                            "lastName": "Romaniuk",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. G. Romaniuk"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60784287,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e3c2a6747835846c20c2fcb86c14d821ec70b198",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper points out how simple learning rules such as perceptron and delta can be re-introduced as local learning techniques to yield an effective automatic network construction algorithm. This feat is accomplished by choosing the right training set during network construction. The choice of partitions can have profound affects on the quality of the created networks, in terms of number of hidden units and connections. Selection of partitions during various network construction phases is achieved by means of evolutionary processes. Empirical evidence underlining the effectiveness of this approach is provided for several well known benchmark problems such as parity, encoder and adder functions.<<ETX>>"
            },
            "slug": "Towards-minimal-network-architectures-with-growth-Romaniuk",
            "title": {
                "fragments": [],
                "text": "Towards minimal network architectures with evolutionary growth networks"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "This paper points out how simple learning rules such as perceptron and delta can be re-introduced as local learning techniques to yield an effective automatic network construction algorithm by choosing the right training set during network construction."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068822647"
                        ],
                        "name": "H. Andersen",
                        "slug": "H.-Andersen",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Andersen",
                            "middleNames": [
                                "Christian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Andersen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1733691"
                        ],
                        "name": "A. Tsoi",
                        "slug": "A.-Tsoi",
                        "structuredName": {
                            "firstName": "Ah",
                            "lastName": "Tsoi",
                            "middleNames": [
                                "Chung"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tsoi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 32
                            }
                        ],
                        "text": "One limitation of this approach [236] is that it could"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11360491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be3cfd6332d3aea52f37b23e1a3c004a567d9854",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A genetic algorit hm is proposed for the training and construction of a multilayer perceptron. The genetic algorit hm works on a layer-by-layer basis. For each layer, it automat ically chooses the number of neurons required, computes the synaptic weights between the present layer of neurons and the next layer, and gives a set of training patterns for the succeeding layer. The algorithm presented here const ruct s networks with neurons that implement a -threshold act ivation funct ion. This architecture is suitable for classificat ion problems with a single binary outp ut . The method is appl ied to the XOR problem, the n-bit pari ty problems, and the lVIONK's problems, and. its performance is found to be comparable to that of other techniques."
            },
            "slug": "A-Constructive-Algorithm-for-the-Training-of-a-on-Andersen-Tsoi",
            "title": {
                "fragments": [],
                "text": "A Constructive Algorithm for the Training of a Multilayer Perceptron Based on the Genetic Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The algorithm presented here const ruct s networks with neurons that implement a -threshold act ivation funct ion and its performance is found to be comparable to that of other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32092143"
                        ],
                        "name": "R. Berlich",
                        "slug": "R.-Berlich",
                        "structuredName": {
                            "firstName": "R\u00fcdiger",
                            "lastName": "Berlich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Berlich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50334917"
                        ],
                        "name": "M. Kunze",
                        "slug": "M.-Kunze",
                        "structuredName": {
                            "firstName": "Marcel",
                            "lastName": "Kunze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kunze"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 12991826,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "530448bf77c8e4060b2c57640eeb71d09d0c71d8",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-COMPARISON-BETWEEN-THE-PERFORMANCE-OF-FEED-NEURAL-Berlich-Kunze",
            "title": {
                "fragments": [],
                "text": "A COMPARISON BETWEEN THE PERFORMANCE OF FEED FORWARD NEURAL NETWORKS AND THE SUPERVISED GROWING NEURAL GAS ALGORITHM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49533680"
                        ],
                        "name": "N. Dodd",
                        "slug": "N.-Dodd",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Dodd",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Dodd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59632032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3aaf6b3674ae5fa46e98b229206654863f4a89d1",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "We present arguments that it is necessary to use structured neural networks for the solution of certain problem types. Structure is imposed on connectivity, activation functions and other parameters of the network to simultaneously optimise generalisation ability and compactness of the network. An analogy is made between the development of biological nervous systems from their genetic coding and the generation of artificial neural networks from a parametric description. Experiments are described which use genetic techniques to optimise network structure for a specific class of problem. Results are given which demonstrate the effectiveness of genetic optimisation of network specifications in comparison with other optimisation techniques. The parallel asynchronous implementation of genetic algorithms on a network of Sun\u2019s is briefly described."
            },
            "slug": "Optimisation-of-Neural-Network-Structure-using-Dodd",
            "title": {
                "fragments": [],
                "text": "Optimisation of Neural-Network Structure using Genetic Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 65,
                "text": "Arguments that it is necessary to use structured neural networks for the solution of certain problem types are presented and results demonstrate the effectiveness of genetic optimisation of network specifications in comparison with other optimisation techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015062"
                        ],
                        "name": "S. Nolfi",
                        "slug": "S.-Nolfi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Nolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nolfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762202"
                        ],
                        "name": "D. Parisi",
                        "slug": "D.-Parisi",
                        "structuredName": {
                            "firstName": "Domenico",
                            "lastName": "Parisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Parisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 41
                            }
                        ],
                        "text": "did not evolve learning rules explicitly [260], [275]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 39
                            }
                        ],
                        "text": "how learning can guide evolution [257]\u2013[260] and the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 69379,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "ec08113ca02b7b1532a462b02e30c0b5f7ff3f4f",
            "isKey": false,
            "numCitedBy": 421,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes simulations on populations of neural networks that both evolve at the population level and learn at the individual level. Unlike other simulations, the evolutionary task (finding food in the environment) and the learning task (predicting the next position of food on the basis of present position and planned network's movement) are different tasks. In these conditions, learning influences evolution (without Lamarckian inheritance of learned weight changes) and evolution influences learning. Average but not peak fitness has a better evolutionary growth with learning than without learning. After the initial generations, individuals that learn to predict during life also improve their food-finding ability during life. Furthermore, individuals that inherit an innate capacity to find food also inherit an innate predisposition to learn to predict the sensory consequences of their movements. They do not predict better at birth, but they do learn to predict better than individuals of the initial generation given the same learning experience. The results are interpreted in terms of a notion of dynamic correlation between the fitness surface and the learning surface. Evolution succeeds in finding both individuals that have high fitness and individuals that, although they do not have high fitness at birth, end up with high fitness because they learn to predict."
            },
            "slug": "Learning-and-Evolution-in-Neural-Networks-Nolfi-Parisi",
            "title": {
                "fragments": [],
                "text": "Learning and Evolution in Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Simulation on populations of neural networks that both evolve at the population level and learn at the individual level finds both individuals that have high fitness and individuals that, although they do not have high Fitness at birth, end up with high fitness because they learn to predict."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2761298"
                        ],
                        "name": "M. Mandischer",
                        "slug": "M.-Mandischer",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Mandischer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mandischer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "[118], [127], [128], [130], [138], [149]\u2013[225]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 17651091,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a38dba10d2785cb8dc3e4e88ea8a21882dd150e",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an evolutionary approach for the design of feedforward and recurrent neural networks. We show that evolutionary algorithms can be used for the construction of networks for real-world tasks. Therefore, a data structure based genotypic network representation, as well as genetic operators, are introduced. Results from the classification, function approximation and time-series domains are presented."
            },
            "slug": "Evolving-recurrent-neural-networks-with-non-binary-Mandischer",
            "title": {
                "fragments": [],
                "text": "Evolving recurrent neural networks with non-binary encoding"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "It is shown that evolutionary algorithms can be used for the construction of networks for real-world tasks and a data structure based genotypic network representation, as well as genetic operators, are introduced."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Conference on Evolutionary Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52427096"
                        ],
                        "name": "D. Prados",
                        "slug": "D.-Prados",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Prados",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Prados"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "\u201d For the three tests reported in his paper [34], the GA-based training algorithm \u201ctook a total of about 3 hours and 20 minutes, and the GDR took a total of about 23 hours and 40 minutes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "For some problems, evolutionary training can be significantly faster and more reliable than BP [30], [34], [40], [63], [83], [89]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 7
                            }
                        ],
                        "text": "Prados [34] described a GA-based training algorithm which is \u201csignificantly faster than methods that use the generalized delta rule (GDR)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 60880526,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d5f4ed46f387b5cdf9df5475f2f81f08ecfb6402",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "The author discusses a supervised-learning algorithm, called GenLearn, for training multilayered neural networks. GenLearn uses techniques from the field of genetic algorithms to perform a global search of weight space and, thereby, to avoid the common problem of getting stuck in local minima. GenLearn is based on survival of the fittest hidden neuron. In searching for the most fit hidden neurons, GenLearn searches for a globally optimal internal representation of the input data. A big advantage of the GenLearn procedure over the generalized delta rule (GDR) in training three-layered neural nets is that, during each iteration of GenLearn, each weight in the first matrix is modified only once, whereas, in the GDR procedure, each weight in the first matrix is modified once for each output-layer neuron. What makes this such a big advantage is that, although GenLearn often reaches the desired mean square error in about the same number of iterations as the GDR, each iteration takes considerably less time.<<ETX>>"
            },
            "slug": "Training-multilayered-neural-networks-by-replacing-Prados",
            "title": {
                "fragments": [],
                "text": "Training multilayered neural networks by replacing the least fit hidden neurons"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "The author discusses a supervised-learning algorithm, called GenLearn, for training multilayered neural networks, which uses techniques from the field of genetic algorithms to perform a global search of weight space and to avoid the common problem of getting stuck in local minima."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings IEEE Southeastcon '92"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47991737"
                        ],
                        "name": "D. Popovic",
                        "slug": "D.-Popovic",
                        "structuredName": {
                            "firstName": "Dobrivoje",
                            "lastName": "Popovic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Popovic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058975014"
                        ],
                        "name": "K.C.S. Murty",
                        "slug": "K.C.S.-Murty",
                        "structuredName": {
                            "firstName": "K.C.S.",
                            "lastName": "Murty",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K.C.S. Murty"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61614995,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9a2b981e709b6dd0dcaa801fd8f7210705f69a53",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Genetic algorithms (GA) have been used for training of fixed structure neural networks and for optimisation of network structure. The crucial issue of algorithms is their premature convergence that deteriorates the diversity of individual search points. Several techniques have being applied to retain the diversity of the search point distribution. In this paper the application of a breeder genetic algorithm (BGA) for neural network learning is considered as well as the problem of retaining diversity. Truncation selection, extended intermediate recombination, and variable mutation range are proposed. It is shown that the performance of BGA is superior to GA in retaining diversity."
            },
            "slug": "Retaining-diversity-of-search-point-distribution-a-Popovic-Murty",
            "title": {
                "fragments": [],
                "text": "Retaining diversity of search point distribution through a breeder genetic algorithm for neural network learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that the performance of BGA is superior to GA in retaining diversity, and truncation selection, extended intermediate recombination, and variable mutation range are proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 103
                            }
                        ],
                        "text": "A number of techniques were adopted to maintain the behavioral link between a parent and its offspring [190]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9854585,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "b2a5481391e10fed0791eddd61384d26b639623d",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In a study of evolutionary artificial neural networks (Yao and Liu, 1996), it has been argued that a partial training process after an architectural mutation plays an important role in maintaining the behavioural link between parents and their offspring and thus is beneficial to the simulated evolution. This paper investigates the issue further through a number of experiments. The experimental results show that a closer behavioural link between parents and their offspring due to the partial training process does lead to better performance, i.e., evolved ANNs generalise better. The results also illustrate that given a fixed amount of time there is an optimal balance of time between evolution and training (learning)."
            },
            "slug": "The-importance-of-maintaining-behavioural-link-and-Yao",
            "title": {
                "fragments": [],
                "text": "The importance of maintaining behavioural link between parents and offspring"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Experimental results show that a closer behavioural link between parents and their offspring due to the partial training process does lead to better performance, i.e., evolved ANNs generalise better and illustrate that given a fixed amount of time there is an optimal balance of time between evolution and training (learning)."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1997 IEEE International Conference on Evolutionary Computation (ICEC '97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32408261"
                        ],
                        "name": "Randall S. Sexton",
                        "slug": "Randall-S.-Sexton",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Sexton",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Randall S. Sexton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2642297"
                        ],
                        "name": "R. Dorsey",
                        "slug": "R.-Dorsey",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Dorsey",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dorsey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1492047826"
                        ],
                        "name": "John D. Johnson",
                        "slug": "John-D.-Johnson",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Johnson",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John D. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 125
                            }
                        ],
                        "text": "For some problems, evolutionary training can be significantly faster and more reliable than BP [30], [34], [40], [63], [83], [89]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8352913,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6966186854d1e4ab7245e9e789d08660b27541a0",
            "isKey": false,
            "numCitedBy": 254,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Toward-global-optimization-of-neural-networks:-A-of-Sexton-Dorsey",
            "title": {
                "fragments": [],
                "text": "Toward global optimization of neural networks: A comparison of the genetic algorithm and backpropagation"
            },
            "venue": {
                "fragments": [],
                "text": "Decis. Support Syst."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1682655"
                        ],
                        "name": "R. Belew",
                        "slug": "R.-Belew",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Belew",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Belew"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16029391,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a247233489e19c56592d26d4da3603d684f319b7",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Potential interactions between connectionist learning syst ems and algorithms modeled aft er evolutionary adapt ation are becoming of increasing interest . In a recent short and elegant paper Hinton and Nowlan extend a version of Holland's genetic algorithm (GA) t o consider ways in which the evolution of species and th e learning of individu als might in teract [17]. Their mod el is valuable both becaus e it provides insight into potenti al interactions between th e natu ral processes of evolution and learning and as a potential bridge between the arti ficial questions of efficient and effective machine learning using t he GA and connectioni st networks. Thi s pa per begins by describing the GA and Hinto n and Nowlan's simulati on . We then an alyze their model, use thi s analysis to explai n its nontrivial dynamical behavio rs, and consider the sensitivity of the simulation to several key parameters. Our next step is t o in terpose a thi rd adapt ive system culture between the learning of individuals and the evolut ion of pop ulations . Culture accumulates the \"wisdom\" of individuals' learning beyond the lifetime of any one indi vidual but adapts more responsively than the pace of evolut ion allows. We describe a series of exper iments in which the most minimal notion of culture has been ad ded to the Hinton and Nowlan mod el, and we use, this experience to comment on the functional value of cult ure, and similarities between an d interac tions among these three classes of adaptive systems."
            },
            "slug": "Evolution,-Learning,-and-Culture:-Computational-for-Belew",
            "title": {
                "fragments": [],
                "text": "Evolution, Learning, and Culture: Computational Metaphors for Adaptive Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A series of exper iments in which the most minimal notion of culture has been ad ded to the Hinton and Nowlan mod el, and this experience is used to comment on the functional value of cult ure, and similarities between an d interac tions among these three classes of adaptive systems."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1924170"
                        ],
                        "name": "Devil H. F. Yip",
                        "slug": "Devil-H.-F.-Yip",
                        "structuredName": {
                            "firstName": "Devil",
                            "lastName": "Yip",
                            "middleNames": [
                                "H.",
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Devil H. F. Yip"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709798"
                        ],
                        "name": "E. Hines",
                        "slug": "E.-Hines",
                        "structuredName": {
                            "firstName": "Evor",
                            "lastName": "Hines",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Hines"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70461342"
                        ],
                        "name": "W.W.H. Yu",
                        "slug": "W.W.H.-Yu",
                        "structuredName": {
                            "firstName": "W.W.H.",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W.W.H. Yu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 54076744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "250feee3b0220c831f3afac72ea7822810c9fc23",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The aim of the work presented in this paper is to forecast sales volumes as accurately as possible and as far into the future as possible. The choice of network topology was Silva's adaptive backpropagation algorithm and the network architectures were selected by genetic algorithms (GAs). The networks were trained to forecast from 1 month to 6 months in advance and the performance of the network was tested after training. The test results of artificial neural networks (ANNs) are compared with the time series smoothing methods of forecasting using several measures of accuracy. The outcome of the comparison proved that the ANNs generally perform better than the time series smoothing methods of forecasting. Further recommendations resulting from this paper are presented."
            },
            "slug": "Application-of-artificial-neural-networks-in-sales-Yip-Hines",
            "title": {
                "fragments": [],
                "text": "Application of artificial neural networks in sales forecasting"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The test results of artificial neural networks (ANNs) are compared with the time series smoothing methods of forecasting using several measures of accuracy and proved that the ANNs generally perform better than the time Series smoothing method of forecasting."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46199289"
                        ],
                        "name": "F. Heimes",
                        "slug": "F.-Heimes",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Heimes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Heimes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70344280"
                        ],
                        "name": "G. Zalesski",
                        "slug": "G.-Zalesski",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Zalesski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Zalesski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39619702"
                        ],
                        "name": "W. Land",
                        "slug": "W.-Land",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Land",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Land"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113621957"
                        ],
                        "name": "M. Oshima",
                        "slug": "M.-Oshima",
                        "structuredName": {
                            "firstName": "Minoru",
                            "lastName": "Oshima",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Oshima"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 13
                            }
                        ],
                        "text": "[117], [126]\u2013[128], higher order ANN\u2019s [52], [53], and"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 68
                            }
                        ],
                        "text": "There are some other different EP-based systems for designing ANN\u2019s [128], [129], [217], [223], but none has been tested on as many different benchmark problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 107
                            }
                        ],
                        "text": "Some researchers thus avoided crossover and adopted only mutations in the evolution of architectures [45], [128], [149], [179], [185]\u2013[197], [217], [223], although it has been shown that crossover may be useful and important in increasing the efficiency of evolution for some problems [48], [113], [212], [229]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 14
                            }
                        ],
                        "text": "[118], [127], [128], [130], [138], [149]\u2013[225]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61164448,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "85578738f86593998934e7fecb67cb3912a5f7cb",
            "isKey": true,
            "numCitedBy": 17,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents results in applying gradient and evolutionary programming (EP) techniques to training dynamic neural network models of aircraft response. The gradient methods modify the weights of predefined neural network structures to learn the desired mapping. We show that this approach is quite effective as long as the predefined network topology is capable of modeling the dynamic system. We examine several dynamic neural network structures: two recurrent architectures and the memory neuron network. The evolutionary programming algorithm determines not only the weights of a dynamic neural network, but also the topology. The EP algorithm is applicable to a much broader class of problems, since a predefined topology does not need to be in place beforehand and the dynamics of the system do not need to be known."
            },
            "slug": "Traditional-and-evolved-dynamic-neural-networks-for-Heimes-Zalesski",
            "title": {
                "fragments": [],
                "text": "Traditional and evolved dynamic neural networks for aircraft simulation"
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2471881"
                        ],
                        "name": "J. McDonnell",
                        "slug": "J.-McDonnell",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "McDonnell",
                            "middleNames": [
                                "Robert"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. McDonnell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3123199"
                        ],
                        "name": "D. Waagen",
                        "slug": "D.-Waagen",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Waagen",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Waagen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 134
                            }
                        ],
                        "text": "Some researchers thus avoided crossover and adopted only mutations in the evolution of architectures [45], [128], [149], [179], [185]\u2013[197], [217], [223], although it has been shown that crossover may be useful and important in increasing the efficiency of evolution for some problems [48], [113], [212], [229]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 24176621,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65a46593f9d36dd3a53bc65b00c6a80a321cd8c4",
            "isKey": false,
            "numCitedBy": 146,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary programming, a systematic multi-agent stochastic search technique, is used to generate recurrent perceptrons (nonlinear IIR filters). A hybrid optimization scheme is proposed that embeds a single-agent stochastic search technique, the method of Solis and Wets, into the evolutionary programming paradigm. The proposed hybrid optimization approach is further augmented by \"blending\" randomly selected parent vectors to create additional offspring. The first part of this work investigates the performance of the suggested hybrid stochastic search method. After demonstration on the Bohachevsky and Rosenbrock response surfaces, the hybrid stochastic optimization approach is applied in determining both the model order and the coefficients of recurrent perceptron time-series models. An information criterion is used to evaluate each recurrent perceptron structure as a candidate solution. It is speculated that the stochastic training method implemented in this study for training recurrent perceptrons can be used to train perceptron networks that have radically recurrent architectures."
            },
            "slug": "Evolving-recurrent-perceptrons-for-time-series-McDonnell-Waagen",
            "title": {
                "fragments": [],
                "text": "Evolving recurrent perceptrons for time-series modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is speculated that the stochastic training method implemented in this study for training recurrent perceptrons can be used to train perceptron networks that have radically recurrent architectures."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742807"
                        ],
                        "name": "H. Kitano",
                        "slug": "H.-Kitano",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Kitano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kitano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 45
                            }
                        ],
                        "text": "The local search algorithm could be BP [32], [133] or other random search algorithms [30], [135]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5797359,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6796a923d33bdd81f3f46dacb6620542f28b9de6",
            "isKey": false,
            "numCitedBy": 173,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports several experimental results on the speed of convergence of neural network training using genetic algorithms and back propagation. Recent excitement regarding genetic search lead some researchers to apply it to training neural networks. There are reports on both successful and faulty results, and, unfortunately, no systematic evaluation has been made. This paper reports results of systematic experiments designed to judge whether use of genetic algorithms provides any gain in neural network training over existing methods. Experimental results indicate that genetic search is, at best, equally efficient to faster variants of back propagation in very small scale networks, but far less efficient in larger networks."
            },
            "slug": "Empirical-Studies-on-the-Speed-of-Convergence-of-Kitano",
            "title": {
                "fragments": [],
                "text": "Empirical Studies on the Speed of Convergence of Neural Network Training Using Genetic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Experimental results indicate that genetic search is, at best, equally efficient to faster variants of back propagation in very small scale networks, but far less efficient in larger networks."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69005583"
                        ],
                        "name": "W. Kinnebrock",
                        "slug": "W.-Kinnebrock",
                        "structuredName": {
                            "firstName": "Werner",
                            "lastName": "Kinnebrock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Kinnebrock"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "which report excellent results using hybrid evolutionary and gradient descent algorithms [32], [67], [70], [71], [74], [80],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "Hybrid training has been used successfully in many application areas [32], [67], [70], [71], [74], [80], [81], [86],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31794729,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "90ad193caf53bcaf1a9eb0ca16512c29ba0c4ff0",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Accelerating-the-standard-backpropagation-method-a-Kinnebrock",
            "title": {
                "fragments": [],
                "text": "Accelerating the standard backpropagation method using a genetic approach"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1692756"
                        ],
                        "name": "Byoung-Tak Zhang",
                        "slug": "Byoung-Tak-Zhang",
                        "structuredName": {
                            "firstName": "Byoung-Tak",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Byoung-Tak Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "66483914"
                        ],
                        "name": "G. Veenker",
                        "slug": "G.-Veenker",
                        "structuredName": {
                            "firstName": "Gerd",
                            "lastName": "Veenker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Veenker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 18
                            }
                        ],
                        "text": "Zhang and Veenker [288] described an active learning paradigm where a training algorithm based on EA\u2019s can self-select training examples."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58139401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cab568f489037b1aef7b1fea80e23364454d2a3b",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors introduce an active learning paradigm for neural networks. In contrast to the passive paradigm, the learning in the active paradigm is initiated by the machine learner instead of its environment or teacher. The authors present a learning algorithm that uses a genetic algorithm for creating novel examples to teach multilayer feedforward networks. The creative learning networks, based on their own knowledge, discover new examples, criticize and select useful ones, train themselves, and thereby extend their existing knowledge. Experiments on function extrapolation show that the self-teaching neural networks not only reduce the teaching efforts of the human, but the genetically created examples also contribute robustly to the improvement of generalization performance and the interpretation of the connectionist knowledge.<<ETX>>"
            },
            "slug": "Neural-networks-that-teach-themselves-through-of-Zhang-Veenker",
            "title": {
                "fragments": [],
                "text": "Neural networks that teach themselves through genetic discovery of novel examples"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The authors present a learning algorithm that uses a genetic algorithm for creating novel examples to teach multilayer feedforward networks and shows that the self-teaching neural networks not only reduce the teaching efforts of the human, but the genetically created examples also contribute robustly to the improvement of generalization performance and the interpretation of the connectionist knowledge."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] 1991 IEEE International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760775"
                        ],
                        "name": "R. Beer",
                        "slug": "R.-Beer",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "Beer",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Beer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34741198"
                        ],
                        "name": "J. Gallagher",
                        "slug": "J.-Gallagher",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Gallagher",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gallagher"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "recurrent ANN\u2019s [41], [60], [65], [100], [102], [103], [106],"
                    },
                    "intents": []
                }
            ],
            "corpusId": 42196865,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "b03528ebfa23050a744da5a80fe85b7552e37a4d",
            "isKey": false,
            "numCitedBy": 624,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We would like the behavior of the artificial agents that we construct to be as well-adapted to their environments as natural animals are to theirs. Unfortunately, designing controllers with these properties is a very difficult task. In this article, we demonstrate that continuous-time recurrent neural networks are a viable mechanism for adaptive agent control and that the genetic algorithm can be used to evolve effective neural controllers. A significant advantage of this approach is that one need specify only a measure of an agent's overall performance rather than the precise motor output trajectories by which it is achieved. By manipulating the performance evaluation, one can place selective pressure on the development of controllers with desired properties. Several novel controllers have been evolved, including a chemotaxis controller that switches between different strategies depending on environmental conditions, and a locomotion controller that takes advantage of sensory feedback if available but that can operate in its absence if necessary."
            },
            "slug": "Evolving-Dynamical-Neural-Networks-for-Adaptive-Beer-Gallagher",
            "title": {
                "fragments": [],
                "text": "Evolving Dynamical Neural Networks for Adaptive Behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that continuous-time recurrent neural networks are a viable mechanism for adaptive agent control and that the genetic algorithm can be used to evolve effective neural controllers."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808919"
                        ],
                        "name": "N. Radcliffe",
                        "slug": "N.-Radcliffe",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Radcliffe",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Radcliffe"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 60
                            }
                        ],
                        "text": "As described in [71] and originally recognized by Radcliffe [53], the symmetry of neural network solutions allows the same functional unit to appear in different locations of different neural networks."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60951961,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4c3f0ebcd1733f6e6092c65d97acd6c70e609ca7",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 98,
            "paperAbstract": {
                "fragments": [],
                "text": "This work draws together two natural metaphors: \"genetic algorithms\" draw inspiration from natural evolution to attempt to provide a robust, efficient search technique, and \"neural networks\" form a crude model of information processing in brains which offer the prospect of training computers to solve tasks by example. It is very natural to think of applying the adaptive techniques of genetic algorithms to the problem of searching the space of neural networks, but doing so is extremely hard, and provides the motivation for this work. It is argued that the key determinant of the success of any particular genetic algorithm is the interaction between the underlying correlations in the search space, the representation of the space adopted, and the genetic operators used to manipulate the representatives of elements in the search space. It is further argued that genetic algorithms as usually formulated are not ideally suited to \"training\" neural networks, and that in order to make significant progress in this area a broadening of the standard \"schema\" analysis is required. Such a generalisation, based on the notion of imposing suitable nested sets of equivalence relations over arbitrary search spaces, is proposed and developed. \"Design principles\" to help construct genetic algorithms for arbitrary problems are suggested in the context of such knowledge of the regularities in the search space as are known. The techniques developed are applied to a number of problem domains and yield some new insights. Issues of linkage and convergence are also relevant to the application of adaptive genetic techniques to neural network problems. Studies of these are presented. Existing attempts to apply genetic algorithms are also reviewed in the light of the non-standard analysis developed, and the prospects for further progress are discussed. In recognition of the fact that much of this work was carried Out on a large, medium-grained, reconfigurable parallel computer, a study of connection strategies for such machines is also presented."
            },
            "slug": "Genetic-neural-networks-on-MIMD-computers-Radcliffe",
            "title": {
                "fragments": [],
                "text": "Genetic neural networks on MIMD computers"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This work draws together two natural metaphors: \"genetic algorithms\" draw inspiration from natural evolution to attempt to provide a robust, efficient search technique, and \"neural networks\" form a crude model of information processing in brains which offer the prospect of training computers to solve tasks by example."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747341"
                        ],
                        "name": "W. Spears",
                        "slug": "W.-Spears",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Spears",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Spears"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143730265"
                        ],
                        "name": "V. Anand",
                        "slug": "V.-Anand",
                        "structuredName": {
                            "firstName": "Vic",
                            "lastName": "Anand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Anand"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 310,
                                "start": 305
                            }
                        ],
                        "text": "Some researchers thus avoided crossover and adopted only mutations in the evolution of architectures [45], [128], [149], [179], [185]\u2013[197], [217], [223], although it has been shown that crossover may be useful and important in increasing the efficiency of evolution for some problems [48], [113], [212], [229]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14290849,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b367888bf6a6d4d16bebe26d271c7eb924ce7ec0",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Holland's analysis of the sources of power of genetic algorithms has served as guidance for the applications of genetic algorithms for more than 15 years. The technique of applying a recombination operator (crossover) to a population of individuals is a key to that power. Neverless, there have been a number of contradictory results concerning crossover operators with respect to overall performance. Recently, for example, genetic algorithms were used to design neural network modules and their control circuits. In these studies, a genetic algorithm without crossover outperformed a genetic algorithm with crossover. This report re-examines these studies, and concludes that the results were caused by a small population size. New results are presented that illustrate the effectiveness of crossover when the population size is larger. From a performance view, the results indicate that better neural networks can be evolved in a shorter time if the genetic algorithm uses crossover."
            },
            "slug": "A-Study-of-Crossover-Operators-in-Genetic-Spears-Anand",
            "title": {
                "fragments": [],
                "text": "A Study of Crossover Operators in Genetic Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This report re-examines these studies, and concludes that the results were caused by a small population size, and indicates that better neural networks can be evolved in a shorter time if the genetic algorithm uses crossover."
            },
            "venue": {
                "fragments": [],
                "text": "ISMIS"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015062"
                        ],
                        "name": "S. Nolfi",
                        "slug": "S.-Nolfi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Nolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nolfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2267481"
                        ],
                        "name": "O. Miglino",
                        "slug": "O.-Miglino",
                        "structuredName": {
                            "firstName": "Orazio",
                            "lastName": "Miglino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Miglino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762202"
                        ],
                        "name": "D. Parisi",
                        "slug": "D.-Parisi",
                        "structuredName": {
                            "firstName": "Domenico",
                            "lastName": "Parisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Parisi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 106
                            }
                        ],
                        "text": "Thus, a lot of the work into growing axonal connections or gradually duplicating neurons in ALife systems [1, 33, 52] may occupy a researchers no-man\u2019s land: too abstract to answer detailed questions in neurodevelopment (perhaps best modeled by work such as [70]), while rather superfluous and computationally expensive for AI."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 27
                            }
                        ],
                        "text": "Some elaborate PO systems, [33, 52] employ evolved developmental regimes in which neurons grow axonal projections to their targets, while one of the early PE systems [69] evolves separate learning rules (applied to each incoming synapse) for each neuron."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "For example, [52] exhibits a complex developmental process in which both internal ANN and external environmental signals influence neural activation, which, in turn, affects network connectivity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14019289,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "8a38793a02d5d87af001ffd0105815ed610ea020",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a model based on genetic algorithm and neural networks. The neural networks develop on the basis of an inherited genotype but they show phenotypic plasticity, i.e. they develop in ways that are adapted to the specific environment The genotype-to-phenotype mapping is not abstractly conceived as taking place in a single instant but is a temporal process that takes a substantial portion of an individual's lifetime to complete and is sensitive to the particular environment in which the individual happens to develop. Furthermore, the respective roles of the genotype and of the environment are not decided a priori but are part of what evolves. We show how such a model is able to evolve control systems for autonomous robots that can adapt to different types of environments."
            },
            "slug": "Phenotypic-plasticity-in-evolving-neural-networks-Nolfi-Miglino",
            "title": {
                "fragments": [],
                "text": "Phenotypic plasticity in evolving neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "It is shown how a model based on genetic algorithm and neural networks is able to evolve control systems for autonomous robots that can adapt to different types of environments."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of PerAc '94. From Perception to Action"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144928910"
                        ],
                        "name": "E. Alba",
                        "slug": "E.-Alba",
                        "structuredName": {
                            "firstName": "Enrique",
                            "lastName": "Alba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Alba"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743835"
                        ],
                        "name": "J. F. A. Montes",
                        "slug": "J.-F.-A.-Montes",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Montes",
                            "middleNames": [
                                "Francisco",
                                "Aldana"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. F. A. Montes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143981125"
                        ],
                        "name": "J. M. Troya",
                        "slug": "J.-M.-Troya",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Troya",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. M. Troya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 131
                            }
                        ],
                        "text": "The first separates the evolution of architectures from that of connection weights [24], [150], [153], [154], [165], [167], [169], [170]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 157
                            }
                        ],
                        "text": "In the first approach, each connection of an architecture is directly specified by its binary representation [24], [150], [153], [154], [165], [167], [169], [170], [202]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21123280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c1a0a0e06ff7b386f7635ad56fd0461de5d8ca6",
            "isKey": false,
            "numCitedBy": 46,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "ANN design is usually thought as a training problem to be solved for some predefined ANN structure and connectivity. Training methods arc very problem and ANN dependent. They are sometimes very accurate procedures but they work in narrow and restrictive domains. Thus the designer is faced to a wide diversity of multimodal and different training mechanisms. We have selected Genetic Algorithms as training procedures because of their robustness and their potential application to any ANN type training. Furthermore we have addressed the connectivity and structure definition problems in order to accomplish a full genetic ANN design. These three levels of design can work in parallel, thus achieving multilevel relationships to yield better ANNs. GRIAL is the tool used to test several new and known genetic techniques and operators. PARLOG is the Concurrent Logic Language used for the implementation in order to introduce new models for the genetic work and attain an intralevel distributed search as well as to parallelize any ANN evaluation."
            },
            "slug": "Full-Automatic-ANN-Design:-A-Genetic-Approach-Alba-Montes",
            "title": {
                "fragments": [],
                "text": "Full Automatic ANN Design: A Genetic Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work has addressed the connectivity and structure definition problems in order to accomplish a full genetic ANN design and selected Genetic Algorithms as training procedures because of their robustness and their potential application to any ANN type training."
            },
            "venue": {
                "fragments": [],
                "text": "IWANN"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766955"
                        ],
                        "name": "J. V. Hansen",
                        "slug": "J.-V.-Hansen",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Hansen",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. V. Hansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789479"
                        ],
                        "name": "R. D. Meservy",
                        "slug": "R.-D.-Meservy",
                        "structuredName": {
                            "firstName": "Rayman",
                            "lastName": "Meservy",
                            "middleNames": [
                                "David"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. D. Meservy"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 123,
                                "start": 119
                            }
                        ],
                        "text": "For some problems, evolutionary training can be significantly faster and more reliable than BP [30], [34], [40], [63], [83], [89]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 45642977,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "439d793d7100bf195a42afcbda68a49bfc8b41a1",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-experiments-with-genetic-optimization-of-a-Hansen-Meservy",
            "title": {
                "fragments": [],
                "text": "Learning experiments with genetic optimization of a generalized regression neural network"
            },
            "venue": {
                "fragments": [],
                "text": "Decis. Support Syst."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70860848"
                        ],
                        "name": "T. T. Maifeld",
                        "slug": "T.-T.-Maifeld",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Maifeld",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. T. Maifeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47245085"
                        ],
                        "name": "G. Shebl\u00e9",
                        "slug": "G.-Shebl\u00e9",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Shebl\u00e9",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Shebl\u00e9"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 110956429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fcd0cb169e9bb58e836d91a8d2971ff60e3d8377",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Short-term-load-forecasting-by-a-neural-network-and-Maifeld-Shebl\u00e9",
            "title": {
                "fragments": [],
                "text": "Short-term load forecasting by a neural network and a refined genetic algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774458"
                        ],
                        "name": "W. Schiffmann",
                        "slug": "W.-Schiffmann",
                        "structuredName": {
                            "firstName": "Wolfram",
                            "lastName": "Schiffmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Schiffmann"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71278113"
                        ],
                        "name": "M. Joost",
                        "slug": "M.-Joost",
                        "structuredName": {
                            "firstName": "Merten",
                            "lastName": "Joost",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Joost"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057407364"
                        ],
                        "name": "R. Werner",
                        "slug": "R.-Werner",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Werner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Werner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 117
                            }
                        ],
                        "text": "The first separates the evolution of architectures from that of connection weights [24], [150], [153], [154], [165], [167], [169], [170]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 148,
                                "start": 143
                            }
                        ],
                        "text": "In the first approach, each connection of an architecture is directly specified by its binary representation [24], [150], [153], [154], [165], [167], [169], [170], [202]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60655850,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d67e35ab205fb777469c699b43901344f1c6e0b3",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Unfortunately, the generated nets show oscillations in the course of error. These oscillations start typically after the number of epochs which was used for the genetic algorithm (here at 500 epochs). The genetic algorithm produced nets which can be trained quickly up to this epoch. The training behavior of the nets after this number of epoch is uncertain. In order to suppress these oscillations, a learning rate adaption would seem to be useful. Furthermore we have discovered that the number of training epochs can be reduced by using quickprop Fahlman, 1988]. We will also test this training procedure in our algorithm. In order to improve the generalization behavior of the nets, the performance of the produced net should not be determined with the utilized training set. It would be better to check the quality through an second pattern set. The real generalization behavior must then be determined on a third pattern set. Concerning the example of the thyroid data the nets could be trained with the extracted training set, but the quality measure should be determined with the complete training set. The test set could, in this case, be used as the generalization test. The references are sorted historically. Figure 11: The achieved classiication performance during the evolution process. Only the classiication performance of the best and the worst net of the population is shown."
            },
            "slug": "Synthesis-and-Performance-Analysis-of-Multilayer-Schiffmann-Joost",
            "title": {
                "fragments": [],
                "text": "Synthesis and Performance Analysis of Multilayer Neural Network Architectures"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The number of training epochs can be reduced by using quickprop Fahlman, 1988 and the performance of the produced net should not be determined with the utilized training set, in order to improve the generalization behavior of the nets."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144140767"
                        ],
                        "name": "D. Patel",
                        "slug": "D.-Patel",
                        "structuredName": {
                            "firstName": "Devesh",
                            "lastName": "Patel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Patel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 31
                            }
                        ],
                        "text": "Other researchers [32], [139], [213], [272] also used an evolutionary process to find parameters for BP but ANN\u2019s"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62650282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "809b95c9518f152ccccf838afb2af015dac3eeb9",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Traditional forecasting models such as the Box-Jenkins ARIMA model are almost all based on models that assume a linear relationship amongst variables and cannot approximate the non- linear relationship that exists amongst variables in real-world data such as stock-price data. Artificial neural networks, on the other hand, consist of two or more levels of nonlinearity that have been successfully used to approximate the underlying relationships of time series data. Neural networks however, pose a design problem: their optimum topology and training rule parameters including learning rate and momentum, for the problem at hand need to be determined. In this paper, we use genetic algorithms to determine these design parameters. In general genetic algorithms are an optimization method that find solutions to a problem by an evolutionary process based on natural selection. The genetic algorithm searches through the network parameter space and the neural network learning algorithm evaluates the selected parameters. We then use the optimally configured network to predict the stock market price of a blue-chip company on the UK market."
            },
            "slug": "Using-genetic-algorithms-to-construct-a-network-for-Patel",
            "title": {
                "fragments": [],
                "text": "Using genetic algorithms to construct a network for financial prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The genetic algorithm searches through the network parameter space and the neural network learning algorithm evaluates the selected parameters and the optimally configured network is used to predict the stock market price of a blue-chip company on the UK market."
            },
            "venue": {
                "fragments": [],
                "text": "Electronic Imaging"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70547733"
                        ],
                        "name": "Borut Mari\u010di\u0107",
                        "slug": "Borut-Mari\u010di\u0107",
                        "structuredName": {
                            "firstName": "Borut",
                            "lastName": "Mari\u010di\u0107",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Borut Mari\u010di\u0107"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61388189,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "596fbd896d282fd7c2c39cb95fc48d2b79adca2c",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "GENETICALLY-PROGRAMMED-NEURAL-NETWORK-FOR-SOLVING-Mari\u010di\u0107",
            "title": {
                "fragments": [],
                "text": "GENETICALLY PROGRAMMED NEURAL NETWORK FOR SOLVING POLE-BALANCING PROBLEM"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2199264"
                        ],
                        "name": "P. Harrald",
                        "slug": "P.-Harrald",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Harrald",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Harrald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2965826"
                        ],
                        "name": "M. Kamstra",
                        "slug": "M.-Kamstra",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Kamstra",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kamstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 13208205,
            "fieldsOfStudy": [
                "Computer Science",
                "Economics"
            ],
            "id": "973bc6c20041ee994ea706f708c5ff5d9c6cba53",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "We conduct evolutionary programming experiments to evolve artificial neural networks for forecast combination. Using stock price volatility forecast data we find evolved networks compare favorably with a naive average combination, a least squares method, and a kernel method on out-of-sample forecasting ability-the best evolved network showed strong superiority in statistical tests of encompassing. Further, we find that the result is not sensitive to the nature of the randomness inherent in the evolutionary optimization process."
            },
            "slug": "Evolving-artificial-neural-networks-to-combine-Harrald-Kamstra",
            "title": {
                "fragments": [],
                "text": "Evolving artificial neural networks to combine financial forecasts"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "Using stock price volatility forecast data, evolved networks compare favorably with a naive average combination, a least squares method, and a kernel method on out-of-sample forecasting ability-the best evolved network showed strong superiority in statistical tests of encompassing."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Evol. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1886460"
                        ],
                        "name": "S. Dreiseitl",
                        "slug": "S.-Dreiseitl",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Dreiseitl",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Dreiseitl"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705399"
                        ],
                        "name": "W. Jacak",
                        "slug": "W.-Jacak",
                        "structuredName": {
                            "firstName": "Witold",
                            "lastName": "Jacak",
                            "middleNames": [
                                "Aleksander"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Jacak"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59202707,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7e205c2f43c742a4e6ec3038d66c1e0e86b29aa",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The modeling of nonlinear dynamical systems is one of the emergent application areas of artificial neural networks. In this paper, we present a general methodology based on neural networks and genetic algorithms that can be applied to modeling of nonlinear dynamical systems. We describe a general methodology for modeling nonlinear systems with known rank (i.e. state-space dimension) by feedforward networks with external delay units. We point out the shortcomings of this approach when the rank of the system is not known a priori. In this case, it is beneficial to employ genetic algorithms to search for neural networks that can model the nonlinear dynamical systems. Two genetic algorithms are presented for this case: one that determines the best feedforward network with external delay, and one that searches for a network with arbitrary topology and memory cells within each neuron."
            },
            "slug": "Genetic-algorithm-based-neural-networks-for-system-Dreiseitl-Jacak",
            "title": {
                "fragments": [],
                "text": "Genetic algorithm based neural networks for dynamical system modeling"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A general methodology for modeling nonlinear systems with known rank by feedforward networks with external delay units is described, pointing out the shortcomings of this approach when the rank of the system is not known a priori."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Conference on Evolutionary Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1693885"
                        ],
                        "name": "Qiangfu Zhao",
                        "slug": "Qiangfu-Zhao",
                        "structuredName": {
                            "firstName": "Qiangfu",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiangfu Zhao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60676519,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8d3930038a27b2ca7499578188f6ab76a7b18b7b",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper studies the evolutionary learning of neural networks that can be decomposed into many homogeneous modules, and proposes a new algorithm by combining the individual evolutionary algorithm (IEA) and the co-evolutionary algorithm (CEA). The proposed algorithm has two parts. The first part, a modified version of the IEA, consists of four basic operations: evaluation, deletion, insertion and training. This part is to construct the system using as less modules as possible. The second part is CEA, and the purpose of this part is to evaluate and reproduce good candidate modules for constructing the system. The algorithm is called EditEr in this paper. In the EditEr, an individual is assigned to each module, and the fitness of an individual is defined according to its contribution to the system; a population is assigned to each class of individuals, and many individuals are to be found from each population. Some experimental results are provided to show the efficiency of the EditEr."
            },
            "slug": "EditEr:-a-combination-of-IEA-and-CEA-Zhao",
            "title": {
                "fragments": [],
                "text": "EditEr: a combination of IEA and CEA"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "A new algorithm is proposed by combining the individual evolutionary algorithm (IEA) and the co-evolutionary algorithm (CEA) for neural networks that can be decomposed into many homogeneous modules, called EditEr in this paper."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1997 IEEE International Conference on Evolutionary Computation (ICEC '97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308302"
                        ],
                        "name": "D. Ackley",
                        "slug": "D.-Ackley",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ackley",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Ackley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144885169"
                        ],
                        "name": "M. Littman",
                        "slug": "M.-Littman",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Littman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Littman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 61096220,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "675be3c1f8a57015a91be5cd191a8d262a9061fb",
            "isKey": false,
            "numCitedBy": 504,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Artificial Life (AL) test bed Results Decision Making Project Applicability Motivation This combines two completely different time scales Learning is a individual behaviour Evolution operates at the level of populations over extended periods of time \" An entire life of learning is but one tick of the clock for evolution. \" What is the motivation to create a system that combines genetic programming with reinforcement learning? Motivation Extremely difficult to study the interaction between evolution and learning in the \" real world \" fossils provide information about evolution but provide very little data about day-today life can study evolution of some life forms (viruses, bacteria, flies) that have a short enough life span to study a significant number of generations, but unfortunately these have little ability to learn Motivation"
            },
            "slug": "Interactions-between-learning-and-evolution-Ackley-Littman",
            "title": {
                "fragments": [],
                "text": "Interactions between learning and evolution"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2388305"
                        ],
                        "name": "D. Thierens",
                        "slug": "D.-Thierens",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Thierens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Thierens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 11
                            }
                        ],
                        "text": "\u201d Thierens [101] proposed a genetic encoding scheme of ANN\u2019s which can avoid the permutation problem, however, only very limited experimental results were presented."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15159460,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fde9a5e98958ed8126377780a07f756cd57682f5",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Feedforward neural networks have a number of functionally equivalent symmetries that make them difficult to optimise with genetic recombination operators. Although this problem has received considerable attention in the past, the proposed solutions all have a heuristic nature. We discuss a neural network genotype representation that completely eliminates the functional redundancies by transforming each neural network into its canonical form. This transformation is computationally extremely simple, since it only requires flipping the sign of some of the weights, followed by sorting the hidden neurons according to their bias. We have compared the redundant and non-redundant representations on the basis of their crossover correlation coefficient. As expected, the redundancy elimination results in a much higher crossover correlation coefficient, which shows that more information is now transmitted from the parents to the children. Finally, experimental results are given for the two-spirals classification problem."
            },
            "slug": "Non-redundant-genetic-coding-of-neural-networks-Thierens",
            "title": {
                "fragments": [],
                "text": "Non-redundant genetic coding of neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A neural network genotype representation that completely eliminates the functional redundancies by transforming each neural network into its canonical form is discussed, which is computationally extremely simple."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Evolutionary Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10751306,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5c06992e33866e83dc28fc96ef992481a19aa174",
            "isKey": false,
            "numCitedBy": 1552,
            "numCiting": 172,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural evolution is a population-based optimization process. Simulating this process on a computer results in stochastic optimization techniques that can often outperform classical methods of optimization when applied to difficult real-world problems. There are currently three main avenues of research in simulated evolution: genetic algorithms, evolution strategies, and evolutionary programming. Each method emphasizes a different facet of natural evolution. Genetic algorithms stress chromosomal operators. Evolution strategies emphasize behavioral changes at the level of the individual. Evolutionary programming stresses behavioral change at the level of the species. The development of each of these procedures over the past 35 years is described. Some recent efforts in these areas are reviewed."
            },
            "slug": "An-introduction-to-simulated-evolutionary-Fogel",
            "title": {
                "fragments": [],
                "text": "An introduction to simulated evolutionary optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The development of each of these procedures over the past 35 years is described and some recent efforts in these areas are reviewed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3247745"
                        ],
                        "name": "Thomas B\u00e4ck",
                        "slug": "Thomas-B\u00e4ck",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "B\u00e4ck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas B\u00e4ck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2641863"
                        ],
                        "name": "U. Hammel",
                        "slug": "U.-Hammel",
                        "structuredName": {
                            "firstName": "Ulrich",
                            "lastName": "Hammel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "U. Hammel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743344"
                        ],
                        "name": "H. Schwefel",
                        "slug": "H.-Schwefel",
                        "structuredName": {
                            "firstName": "Hans-Paul",
                            "lastName": "Schwefel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schwefel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[16] give a good introduction to various evolutionary algorithms for optimization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15623518,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9438a9e25e61366d407ce7dd74f32bff8192b8b0",
            "isKey": false,
            "numCitedBy": 1562,
            "numCiting": 301,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary computation has started to receive significant attention during the last decade, although the origins can be traced back to the late 1950's. This article surveys the history as well as the current state of this rapidly growing field. We describe the purpose, the general structure, and the working principles of different approaches, including genetic algorithms (GA) (with links to genetic programming (GP) and classifier systems (CS)), evolution strategies (ES), and evolutionary programming (EP) by analysis and comparison of their most important constituents (i.e. representations, variation operators, reproduction, and selection mechanism). Finally, we give a brief overview on the manifold of application domains, although this necessarily must remain incomplete."
            },
            "slug": "Evolutionary-computation:-comments-on-the-history-B\u00e4ck-Hammel",
            "title": {
                "fragments": [],
                "text": "Evolutionary computation: comments on the history and current state"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The purpose, the general structure, and the working principles of different approaches, including genetic algorithms (GA), evolution strategies (ES), and evolutionary programming (EP) are described by analysis and comparison of their most important constituents (i.e. representations, variation operators, reproduction, and selection mechanism)."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Evol. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37015648"
                        ],
                        "name": "L. Hsu",
                        "slug": "L.-Hsu",
                        "structuredName": {
                            "firstName": "Loke-Soo",
                            "lastName": "Hsu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Hsu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70832310"
                        ],
                        "name": "Zhi Biao Wu",
                        "slug": "Zhi-Biao-Wu",
                        "structuredName": {
                            "firstName": "Zhi",
                            "lastName": "Wu",
                            "middleNames": [
                                "Biao"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhi Biao Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62139292,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50f1cf3a05acacea995a8e1f334e85044d8981a5",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "In a neural network approach to a sequence prediction problem such as Chinese character prediction, if an orthogonal set is used to encode the Chinese characters, there will be more than 6000 units in the input layer. The authors demonstrate that the number of units in the input layer can be greatly reduced with proper encoding. A neural network maps a group of input vectors to a group of target vectors. It generalizes the responses for inputs that are similar to the inputs on which it has been trained. With this similarity property, if the input pattern vectors are encoded according to the interrelationship among the target patterns, the network may behave better, and fewer units will be needed in the input layer. The authors present such an input pattern encoding method for a neural network with recurrent connections. A modified genetic algorithm was used to do a generalized adaptive search for a good encoding.<<ETX>>"
            },
            "slug": "Input-pattern-encoding-through-generalized-adaptive-Hsu-Wu",
            "title": {
                "fragments": [],
                "text": "Input pattern encoding through generalized adaptive search"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that the number of units in the input layer can be greatly reduced with proper encoding, and an input pattern encoding method for a neural network with recurrent connections is presented."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723120"
                        ],
                        "name": "Kemal Oflazer",
                        "slug": "Kemal-Oflazer",
                        "structuredName": {
                            "firstName": "Kemal",
                            "lastName": "Oflazer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kemal Oflazer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59659031,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f736031bf5fe00e1a3e7c44635e2c134ab795ce",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents new unsupervised learning algorithms that have been synthesized using a genetic approach. A set of such learning algorithms has been compared with the classical Kohonen's Algorithm on the Self-Organizing Map and has been found to provide a better performance measure. This study indicates that there exist many unsupervised learning algorithms that lead to an organization similar to that of Kohonen's Algorithm, and that genetic algorithms can be used to search for optimal algorithms and optimal architectures for the unsupervised learning."
            },
            "slug": "Genetic-Synthesis-of-Unsupervised-Learning-Oflazer",
            "title": {
                "fragments": [],
                "text": "Genetic Synthesis of Unsupervised Learning Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This study indicates that there exist many unsupervised learning algorithms that lead to an organization similar to that of Kohonen's Algorithm, and that genetic algorithms can be used to search for optimal algorithms and optimal architectures for the unsuper supervised learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118665629"
                        ],
                        "name": "J. Zhou",
                        "slug": "J.-Zhou",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987807"
                        ],
                        "name": "D. Civco",
                        "slug": "D.-Civco",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Civco",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Civco"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15259709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9fda066b4c0cc3311ecc89d426491bbed0af007",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "the traditional gradient descent-based backpropagation. 1 Traditional approaches for suitability analysis in GIS are Through evolutionary learning from samples, a neural netoverlay ad the more complicated multicriteria evaluation work adapts its connection weights to approximate the de(m~). Despite being widely used, these methods have at sired output. Then, a successfully trained neural network can least three problems: (11 difficulties in handling spatial ,jato accomplish the suitability analysis task. A set of experiments possessing inaccumcy, multiple measurement scales, and is presented, The results show that the difficulties in tradifactor interdependency; (2) requirements of prior knowledge tional are by evO1utionq learning and the in identihng criteria, assigning scores, determining criteria abfiity the neural network. preference, and selecting aggregation functions; and (3) typically, an \"unfriendly\" user inte$ace. TO solve these prob- lntroduction to Traditional Methods lems, in this paper a neural network approach is presented. Overlay The neural network uses a genetic algorithm as its learning The application of digital map overlay for the purpose of mechanism. A set of experiments revealed that the afore- identifying suitable areas is a classic application of GIs. In mentioned dijlficulties are overcome by the evolutionary raster GIS, for example in IDRISI (Eastman, 1995), a suitability learning of neural networks. Our conclusion is that genetic map is produced from a series of Boolean images, where learning neural networks can provide an alternative for and each image represents all areas meeting the criterion being improvement over traditional suitability analysis methods in depicted. These images are then combined the overlay GIS. combination procedure to yield a final map that shows the sites meeting all the specified criteria. However, overlays"
            },
            "slug": "Using-Genetic-Learning-Neural-Networks-for-Spatial-Zhou-Civco",
            "title": {
                "fragments": [],
                "text": "Using Genetic Learning Neural Networks for Spatial Decision Making in GIs"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The conclusion is that genetic map is produced from a series of Boolean images, where learning neural networks can provide an alternative for and each image represents all areas meeting the criterion being improvement over traditional suitability analysis methods."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152891866"
                        ],
                        "name": "Yong Liu",
                        "slug": "Yong-Liu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 56
                            }
                        ],
                        "text": "in (3) may be replaced by Cauchy mutation [121], [122], [124] for faster evolution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16112032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32e1462301f63f95766457d09312b47f6e8a8859",
            "isKey": false,
            "numCitedBy": 531,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolution strategies are a class of general optimisation algorithms which are applicable to functions that are multimodal, non-differentiable, or even discontinuous. Although recombination operators have been introduced into evolution strategies, their primary search operator is still mutation. Classical evolution strategies rely on Gaussian mutations. A new mutation operator based on the Cauchy distribution is proposed in this paper. It is shown empirically that the new evolution strategy based on Cauchy mutation outperforms the classical evolution strategy on most of the 23 benchmark problems tested in this paper. These results, along with those obtained by fast evolutionary programming"
            },
            "slug": "Fast-Evolution-Strategies-Yao-Liu",
            "title": {
                "fragments": [],
                "text": "Fast Evolution Strategies"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown empirically that the new evolution strategy based on Cauchy mutation outperforms the classical evolution strategy on most of the 23 benchmark problems tested in this paper."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Programming"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2370215"
                        ],
                        "name": "J. Heistermann",
                        "slug": "J.-Heistermann",
                        "structuredName": {
                            "firstName": "Jochen",
                            "lastName": "Heistermann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Heistermann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 26395159,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ed1405107c7e457d38d713ab097ebb5098292b20",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The author discusses some of the capabilities of genetic algorithms (GAs). GAs are compared with other standard optimization methods like gradient descent or simulated annealing (SA). It is shown that SA is just a special case of GA. The role of a population in the optimization process is demonstrated by an example. GA was applied as a learning algorithm to neural networks.<<ETX>>"
            },
            "slug": "A-mixed-genetic-approach-to-the-optimization-of-Heistermann",
            "title": {
                "fragments": [],
                "text": "A mixed genetic approach to the optimization of neural controllers"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "GAs are compared with other standard optimization methods like gradient descent or simulated annealing (SA) and it is shown that SA is just a special case of GA."
            },
            "venue": {
                "fragments": [],
                "text": "CompEuro 1992 Proceedings Computer Systems and Software Engineering"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2342413"
                        ],
                        "name": "Guangming Lin",
                        "slug": "Guangming-Lin",
                        "structuredName": {
                            "firstName": "Guangming",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guangming Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152891866"
                        ],
                        "name": "Yong Liu",
                        "slug": "Yong-Liu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 49
                            }
                        ],
                        "text": "in (3) may be replaced by Cauchy mutation [121], [122], [124] for faster evolution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 57
                            }
                        ],
                        "text": "Other mutation operators, such as Cauchy mutation [121], [122], can also be used."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 44906665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8404a76a1356c2146ec43897a00c736d9f7a7a2",
            "isKey": false,
            "numCitedBy": 57,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary algorithms (EAs) can be regarded as algorithms based on neighbourhood search, where different search operators (such as crossover and mutation) determine different neighbourhood and step sizes. This paper analyses the efficiency of various mutations in evolutionary programming (EP) by examining their neighbourhood and step sizes. It shows analytically when and why Cauchy mutation-based fast EP (FEP) [1, 2] is better than Gaussian mutation-based classical EP (CEP). It also studies the relationship between the optimality of the solution and the time used to find the solution. Based on the theoretical analysis, an improved FEP (IFEP) is proposed, which combines the advantages of both Cauchy and Gaussian mutations in EP. Although IFEP is very simple and requires no extra parameters, it performs better than both FEP and CEP for a number of benchmark problems."
            },
            "slug": "An-Analysis-of-Evolutionary-Algorithms-Based-on-and-Yao-Lin",
            "title": {
                "fragments": [],
                "text": "An Analysis of Evolutionary Algorithms Based on Neighborhood and Step Sizes"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Based on the theoretical analysis, an improved FEP (IFEP) is proposed, which combines the advantages of both Cauchy and Gaussian mutations in EP and performs better than both FEP and CEP for a number of benchmark problems."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Programming"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1705072"
                        ],
                        "name": "E. Mjolsness",
                        "slug": "E.-Mjolsness",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Mjolsness",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Mjolsness"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11069815"
                        ],
                        "name": "D. Sharp",
                        "slug": "D.-Sharp",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Sharp",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sharp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35147437"
                        ],
                        "name": "B. Alpert",
                        "slug": "B.-Alpert",
                        "structuredName": {
                            "firstName": "Bradley",
                            "lastName": "Alpert",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Alpert"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[230] described a similar rule encoding method where rules are represented by recursive equations which specify the growth of connection matrices."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 43
                            }
                        ],
                        "text": "in chromosomes [151], [168], [184], [205], [230], [232]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 194,
                                "start": 189
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "[151], [230], [231] have argued that the indirect encoding scheme is biologically more plausible than the direct one, because it is impossible for genetic information encoded in chromosomes to specify independently the whole nervous"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 227,
                                "start": 222
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 9
                            }
                        ],
                        "text": "equation [230] or a generation rule similar to a production"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15155263,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee3d8fad2ce98f20b5a3cffb2a8deed3cb673479",
            "isKey": true,
            "numCitedBy": 102,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Scaling,-machine-learning,-and-genetic-neural-nets-Mjolsness-Sharp",
            "title": {
                "fragments": [],
                "text": "Scaling, machine learning, and genetic neural nets"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47380869"
                        ],
                        "name": "J. Aguilar",
                        "slug": "J.-Aguilar",
                        "structuredName": {
                            "firstName": "Jos\u00e9",
                            "lastName": "Aguilar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Aguilar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145376694"
                        ],
                        "name": "A. Colmenares",
                        "slug": "A.-Colmenares",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Colmenares",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Colmenares"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61684580,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9951972c7e7a73f08edf889664201c8d652760fe",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Gelenbe (1989) has modeled the neural network using an analogy with queuing theory. This model (called random neural network) calculates the probability of activation of the neurons in the network. Recently, we have proposed a recognition algorithm based on the random neural network. In this paper, we propose to solve the patterns recognition problem using an evolutionary learning on the random neural network. The evolutionary learning is based in a hybrid algorithm that trains the random neural network by integrating a genetic algorithm with the gradient descent rule-based learning algorithm of the random neural network. This hybrid learning algorithm optimizes the random neural network on the basis of its topology and its weights distribution."
            },
            "slug": "Recognition-algorithm-using-evolutionary-learning-Aguilar-Colmenares",
            "title": {
                "fragments": [],
                "text": "Recognition algorithm using evolutionary learning on the random neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The evolutionary learning is based in a hybrid algorithm that trains the random neural network by integrating a genetic algorithm with the gradient descent rule-based learning algorithm of therandom neural network."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694724"
                        ],
                        "name": "F. Gruau",
                        "slug": "F.-Gruau",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Gruau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gruau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145285040"
                        ],
                        "name": "L. D. Whitley",
                        "slug": "L.-D.-Whitley",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Whitley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Whitley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 69
                            }
                        ],
                        "text": "Figure 13: Generating a neural network from a Cellular Encoding (CE) [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 209
                            }
                        ],
                        "text": "Their GRNs produce chemicals that can diffuse within and between cells, and can stimulate different styles of neural-network growth, such as the serial and parallel duplication operations of Cellular Encoding [28]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 125
                            }
                        ],
                        "text": "One of the first generative encodings to get beyond the proof-of-principle stage was Frederic Gruau\u2019s Cellular Encoding (CE) [28, 27], an approach that also revolutionized genetic programming."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15048360,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "406e9e583450219318e1101bd02bb6a1e91f0b74",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "A grammar tree is used to encode a cellular developmental process that can generate whole families of Boolean neural networks for computing parity and symmetry. The development process resembles biological cell division. A genetic algorithm is used to find a grammar tree that yields both architecture and weights specifying a particular neural network for solving specific Boolean functions. The current study particularly focuses on the addition of learning to the development process and the evolution of grammar trees. Three ways of adding learning to the development process are explored. Two of these exploit the Baldwin effect by changing the fitness landscape without using Lamarckian evolution. The third strategy is Lamarckian in nature. Results for these three modes of combining learning with genetic search are compared against genetic search without learning. Our results suggest that merely using learning to change the fitness landscape can be as effective as Lamarckian strategies at improving search."
            },
            "slug": "Adding-Learning-to-the-Cellular-Development-of-and-Gruau-Whitley",
            "title": {
                "fragments": [],
                "text": "Adding Learning to the Cellular Development of Neural Networks: Evolution and the Baldwin Effect"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This study particularly focuses on the addition of learning to the development process and the evolution of grammar trees, and suggests that merely using learning to change the fitness landscape can be as effective as Lamarckian strategies at improving search."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47239520"
                        ],
                        "name": "A. Skinner",
                        "slug": "A.-Skinner",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Skinner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Skinner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145802420"
                        ],
                        "name": "J. Q. Broughton",
                        "slug": "J.-Q.-Broughton",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Broughton",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Q. Broughton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "which report excellent results using hybrid evolutionary and gradient descent algorithms [32], [67], [70], [71], [74], [80],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 93
                            }
                        ],
                        "text": "Hybrid training has been used successfully in many application areas [32], [67], [70], [71], [74], [80], [81], [86],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 135498460,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9544d456740a9a9abb84543936c97cb2bed40865",
            "isKey": false,
            "numCitedBy": 75,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks can be used in principle in an unbiased way for a multitude of pattern recognition and interpolation problems within computational material science. Reliably finding the weights of large feed-forward neural networks with both accuracy and speed is crucial to their use. In this paper, the rate of convergence of numerous optimization techniques that can be used to determine the weights is compared for two problems related to the construction of atomistic potentials. Techniques considered were back propagation (steepest descent), conjugate gradient methods, real-string genetic algorithms, simulated annealing and a new swarm search algorithm. For small networks, where only a few optimal solutions exist, we find that conjugate-gradient methods are most successful. However, for larger networks where the parameter space to be searched is more complex, a hybrid scheme is most effective; genetic algorithm or simulated annealing to find a good initial starting set of weights, followed by a conjugate-gradient approach to home in on a final solution. These hybrid approaches are now our method of choice for training large networks."
            },
            "slug": "Neural-networks-in-computational-materials-science:-Skinner-Broughton",
            "title": {
                "fragments": [],
                "text": "Neural networks in computational materials science: training algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The rate of convergence of numerous optimization techniques that can be used to determine the weights of large feed-forward neural networks is compared for two problems related to the construction of atomistic potentials."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "11501046"
                        ],
                        "name": "S. J. Marshall",
                        "slug": "S.-J.-Marshall",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Marshall",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. J. Marshall"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89207181"
                        ],
                        "name": "R. Harrison",
                        "slug": "R.-Harrison",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Harrison",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Harrison"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62512923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ac3789250592fe32ce11c53dcc2ff38c1fbf15e",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The training of feedforward neural networks by backpropagation requires much time-consuming experimentation by the network designer. The authors use the genetic algorithm formalism to optimize network structure and training parameters automatically, so as to allow successful back-propagation learning. Additionally, they describe a method to optimize network weights directly using the genetic algorithm, removing any need for a gradient-descent algorithm such as back-propagation."
            },
            "slug": "Optimization-and-training-of-feedforward-neural-by-Marshall-Harrison",
            "title": {
                "fragments": [],
                "text": "Optimization and training of feedforward neural networks by genetic algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The authors use the genetic algorithm formalism to optimize network structure and training parameters automatically, so as to allow successful back-propagation learning."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2876033"
                        ],
                        "name": "A. Wieland",
                        "slug": "A.-Wieland",
                        "structuredName": {
                            "firstName": "Alexis",
                            "lastName": "Wieland",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Wieland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 30
                            }
                        ],
                        "text": "[24], [26], [28], [37], [38], [41], [52], [53]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "recurrent ANN\u2019s [41], [60], [65], [100], [102], [103], [106],"
                    },
                    "intents": []
                }
            ],
            "corpusId": 62662319,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "8f67d2fbeab8b97b264fde1ee6e70c8cfd04052a",
            "isKey": false,
            "numCitedBy": 220,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The author describes how genetic algorithms (GAs) were used to create recurrent neural networks to control a series of unstable systems. The systems considered are variations of the pole balancing problem: network controllers with two, one, and zero inputs, variable length pole, multiple poles on one cart, and a jointed pole. GAs were able to quickly evolve networks for the one- and two-input pole balancing problems. Networks with zero inputs were only able to valance poles for a few seconds of simulated time due to the network's inability to maintain accurate estimates of their position and pole angle. Also, work in progress on a two-legged walker is briefly described.<<ETX>>"
            },
            "slug": "Evolving-neural-network-controllers-for-unstable-Wieland",
            "title": {
                "fragments": [],
                "text": "Evolving neural network controllers for unstable systems"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "The author describes how genetic algorithms were used to create recurrent neural networks to control a series of unstable systems, including network controllers with two, one, and zero inputs, and variations of the pole balancing problem."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48451266"
                        ],
                        "name": "E. DeRouin",
                        "slug": "E.-DeRouin",
                        "structuredName": {
                            "firstName": "Edward E.",
                            "lastName": "DeRouin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. DeRouin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158105372"
                        ],
                        "name": "Joe R. Brown",
                        "slug": "Joe-R.-Brown",
                        "structuredName": {
                            "firstName": "Joe R.",
                            "lastName": "Brown",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joe R. Brown"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61670430,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "412dfc16a8fee0f4d4e638e71a29c9d2e8156ea2",
            "isKey": false,
            "numCitedBy": 6,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks have proven very useful in the field of pattern classification by mapping input patterns into one of several categories. One widely used neural network paradigm is the multi- layer perceptron employing back-propagation of errors learning -- often called back- propagation networks (BPNs). Rather than being specifically programmed, BPNs `learn' this mapping by exposure to a training set, a collection of input pattern samples matched with their corresponding output classification. The proper construction of this training set is crucial to successful training of a BPN. One of the criteria to be met for proper construction of a training set is that each of the classes must be adequately represented. A class that is represented less often in the training data may not be learned as completely or correctly, impairing the network's discrimination ability. This is due to the implicit setting of a priori probabilities which results from unequal sample sizes. The degree of impairment is a function of (among other factors) the relative number of samples of each class used for training. This paper addresses the problem of unequal representation in training sets by proposing two alternative methods of learning. One adjusts the learning rate for each class to achieve user- specified goals. The other utilizes a genetic algorithm to set the connection weights with a fitness function based on these same goals. These methods are tested using both artificial and real-world training data."
            },
            "slug": "Alternative-learning-methods-for-training-neural-DeRouin-Brown",
            "title": {
                "fragments": [],
                "text": "Alternative learning methods for training neural network classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper addresses the problem of unequal representation in training sets by proposing two alternative methods of learning, one adjusts the learning rate for each class to achieve user- specified goals and the other utilizes a genetic algorithm to set the connection weights with a fitness function based on these same goals."
            },
            "venue": {
                "fragments": [],
                "text": "Defense, Security, and Sensing"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751569"
                        ],
                        "name": "Samy Bengio",
                        "slug": "Samy-Bengio",
                        "structuredName": {
                            "firstName": "Samy",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Samy Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909364"
                        ],
                        "name": "J. Cloutier",
                        "slug": "J.-Cloutier",
                        "structuredName": {
                            "firstName": "Jocelyn",
                            "lastName": "Cloutier",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cloutier"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 12
                            }
                        ],
                        "text": "\u2019s approach [265], [266] is slightly different from Chalmers\u2019 in the sense that gradient descent algorithms and simulated annealing, rather than EA\u2019s, were used to find near-optimal\u2019s."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "Similar experiments on the evolution of learning rules were also carried out by others [265], [266], [267], [269], [270]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1734973,
            "fieldsOfStudy": [
                "Computer Science",
                "Biology"
            ],
            "id": "d130325c41947a41a55a4431e9e8e15be89da8ea",
            "isKey": false,
            "numCitedBy": 248,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Summary form only given, as follows. The authors discuss an original approach to neural modeling based on the idea of searching, with learning methods, for a synaptic learning rule which is biologically plausible and yields networks that are able to learn to perform difficult tasks. The proposed method of automatically finding the learning rule relies on the idea of considering the synaptic modification rule as a parametric function. This function has local inputs and is the same in many neurons. The parameters that define this function can be estimated with known learning methods. For this optimization, particular attention is given to gradient descent and genetic algorithms. In both cases, estimation of this function consists of a joint global optimization of the synaptic modification function and the networks that are learning to perform some tasks. Both network architecture and the learning function can be designed within constraints derived from biological knowledge.<<ETX>>"
            },
            "slug": "Learning-a-synaptic-learning-rule-Bengio-Bengio",
            "title": {
                "fragments": [],
                "text": "Learning a synaptic learning rule"
            },
            "tldr": {
                "abstractSimilarityScore": 77,
                "text": "An original approach to neural modeling based on the idea of searching, with learning methods, for a synaptic learning rule which is biologically plausible and yields networks that are able to learn to perform difficult tasks is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IJCNN-91-Seattle International Joint Conference on Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67143724"
                        ],
                        "name": "A. Scherf",
                        "slug": "A.-Scherf",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Scherf",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Scherf"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71626891"
                        ],
                        "name": "L. Voelz",
                        "slug": "L.-Voelz",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Voelz",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Voelz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62151363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "36abe954aaa4d4b31f80cc1bc1e66f25faa18d09",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for training artificial neural networks, such as backpropagation, often employ some form of gradient descent in their search for an optimal weight set. The problem with such algorithms is their tendency to converge to local minima, or not to converge at all. Genetic algorithms simulate evolutionary operators in their search for optimality. The techniques of genetic search are applied to training a neural network for target detection in infrared imagery. The algorithm design, parameters, and experimental results are detailed. Testing verifies that genetic algorithms are a useful and effective approach for neural network training."
            },
            "slug": "Training-neural-networks-with-genetic-algorithms-Scherf-Voelz",
            "title": {
                "fragments": [],
                "text": "Training neural networks with genetic algorithms for target detection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The techniques of genetic search are applied to training a neural network for target detection in infrared imagery and testing verifies that genetic algorithms are a useful and effective approach for neural network training."
            },
            "venue": {
                "fragments": [],
                "text": "Defense, Security, and Sensing"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38833550"
                        ],
                        "name": "A. Imada",
                        "slug": "A.-Imada",
                        "structuredName": {
                            "firstName": "Akira",
                            "lastName": "Imada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Imada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053599719"
                        ],
                        "name": "K. Araki",
                        "slug": "K.-Araki",
                        "structuredName": {
                            "firstName": "Keijiro",
                            "lastName": "Araki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Araki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 16
                            }
                        ],
                        "text": "Imada and Araki [307] used EA\u2019s to evolve connection weights for Hopfield ANN\u2019s."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17589975,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e2eeac384ce803e343852a73cb4920b9dd014109",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We apply evolutionary computations to Hopfield's neural network model of associative memory. In the Hopfield model, an almost infinite number of combinations of synaptic weights gives a network an associative memory function. Furthermore, there is a trade-off between the storage capacity and the size of the basin of attraction. Therefore, the model can be thought of as a test suite of multi-modal and/or multi-objective function optimizations. As a preliminary stage, we investigate the basic behavior of an associative memory under simple evolutionary processes. In this paper, we present some experiments using an evolution strategy."
            },
            "slug": "Application-of-an-evolution-strategy-to-the-model-Imada-Araki",
            "title": {
                "fragments": [],
                "text": "Application of an evolution strategy to the Hopfield model of associative memory"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "This paper applies evolutionary computations to Hopfield's neural network model of associative memory, a model of neural networks that can be thought of as a test suite of multi-modal and/or multi-objective function optimizations."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1997 IEEE International Conference on Evolutionary Computation (ICEC '97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749342"
                        ],
                        "name": "C. Lebiere",
                        "slug": "C.-Lebiere",
                        "structuredName": {
                            "firstName": "Christian",
                            "lastName": "Lebiere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lebiere"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 30443043,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "995a3b11cc8a4751d8e167abc4aa937abc934df0",
            "isKey": false,
            "numCitedBy": 2940,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Cascade-Correlation is a new architecture and supervised learning algorithm for artificial neural networks. Instead of just adjusting the weights in a network of fixed topology. Cascade-Correlation begins with a minimal network, then automatically trains and adds new hidden units one by one, creating a multi-layer structure. Once a new hidden unit has been added to the network, its input-side weights are frozen. This unit then becomes a permanent feature-detector in the network, available for producing outputs or for creating other, more complex feature detectors. The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "slug": "The-Cascade-Correlation-Learning-Architecture-Fahlman-Lebiere",
            "title": {
                "fragments": [],
                "text": "The Cascade-Correlation Learning Architecture"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The Cascade-Correlation architecture has several advantages over existing algorithms: it learns very quickly, the network determines its own size and topology, it retains the structures it has built even if the training set changes, and it requires no back-propagation of error signals through the connections of the network."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 156
                            }
                        ],
                        "text": "On the classic pole-balancing control problem, it finds solutions 25 times faster than Cellular Encoding [27] (described below) and 5 times faster than ESP [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "The Enforced Sub-Populations (ESP) [26] extension to SANE remedies this problem by creating an explicit sub-population for each neuron in the network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2209222,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8939e9298eedb90aa4f040ab8e9f16872089a495",
            "isKey": false,
            "numCitedBy": 505,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Several researchers have demonstrated how complex action sequences can be learned through neuroevolution (i.e., evolving neural networks with genetic algorithms). However, complex general behavior such as evading predators or avoiding obstacles, which is not tied to specific environments, turns out to be very difficult to evolve. Often the system discovers mechanical strategies, such as moving back and forth, that help the agent cope but are not very effective, do not appear believable, and do not generalize to new environments. The problem is that a general strategy is too difficult for the evolution system to discover directly. This article proposes an approach wherein such complex general behavior is learned incrementally, by starting with simpler behavior and gradually making the task more challenging and general. The task transitions are implemented through successive stages of Delta coding (i.e., evolving modifications), which allows even converged populations to adapt to the new task. The method is tested in the stochastic, dynamic task of prey capture and is compared with direct evolution. The incremental approach evolves more effective and more general behavior and should also scale up to harder tasks."
            },
            "slug": "Incremental-Evolution-of-Complex-General-Behavior-Gomez-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Incremental Evolution of Complex General Behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article proposes an approach wherein complex general behavior is learned incrementally, by starting with simpler behavior and gradually making the task more challenging and general, which evolves more effective and more general behavior."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742807"
                        ],
                        "name": "H. Kitano",
                        "slug": "H.-Kitano",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Kitano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kitano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 15
                            }
                        ],
                        "text": "in chromosomes [151], [168], [184], [205], [230], [232]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[151], [230], [231] have argued that the indirect encoding scheme is biologically more plausible than the direct one, because it is impossible for genetic information encoded in chromosomes to specify independently the whole nervous"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 145
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 213
                            }
                        ],
                        "text": "the performance difference between the direct and indirect encoding schemes was not caused by the encoding scheme itself, but by how sparsely connected the initial ANN architectures were in the initial population [151]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 43
                            }
                        ],
                        "text": "encode the whole rule set as an individual [151] (the so-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 72,
                                "start": 67
                            }
                        ],
                        "text": "These definitions are slightly different from those used by others [151]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "Some good results from the developmental rule representation method have been reported [151] using various size encoder/decoder problems."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3109597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "211855f1de279c452858177331860cbc326351ab",
            "isKey": true,
            "numCitedBy": 812,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method of designing neural networks using the genetic algorithm. Recently there have been several reports claiming attempts to design neural networks using genetic algorithms were successful. However, these methods have a problem in scalability, i.e., the convergence characteristic degrades significantly as the size of the network increases. This is because these methods employ direct mapp ing of chromosomes into network connectivities. As an alternative approach, we propose a graph grammatical encoding that will encode graph generation grammar to the chromosome so that it generates more regular connectivity patterns with shorter chromosome length. Experimental results support that our new scheme provides magnitude of speedup in convergence of neural network design and exhibits desirable scaling property."
            },
            "slug": "Designing-Neural-Networks-Using-Genetic-Algorithms-Kitano",
            "title": {
                "fragments": [],
                "text": "Designing Neural Networks Using Genetic Algorithms with Graph Generation System"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A graph grammatical encoding is proposed that will encode graph generation grammar to the chromosome so that it generates more regular connectivity patterns with shorter chromosome length."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2755018"
                        ],
                        "name": "A. Topchy",
                        "slug": "A.-Topchy",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Topchy",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Topchy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075971975"
                        ],
                        "name": "O. Lebedko",
                        "slug": "O.-Lebedko",
                        "structuredName": {
                            "firstName": "Oleg",
                            "lastName": "Lebedko",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Lebedko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "which report excellent results using hybrid evolutionary and gradient descent algorithms [32], [67], [70], [71], [74], [80],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "Hybrid training has been used successfully in many application areas [32], [67], [70], [71], [74], [80], [81], [86],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 123
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 120351181,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9e91daf476fa47e24793f5b29c7dc6465d56bce9",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-network-training-by-means-of-cooperative-Topchy-Lebedko",
            "title": {
                "fragments": [],
                "text": "Neural network training by means of cooperative evolutionary search"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2830443"
                        ],
                        "name": "S. Hung",
                        "slug": "S.-Hung",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Hung",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750050"
                        ],
                        "name": "H. Adeli",
                        "slug": "H.-Adeli",
                        "structuredName": {
                            "firstName": "Hojjat",
                            "lastName": "Adeli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Adeli"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 107
                            }
                        ],
                        "text": "which report excellent results using hybrid evolutionary and gradient descent algorithms [32], [67], [70], [71], [74], [80],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": "Hybrid training has been used successfully in many application areas [32], [67], [70], [71], [74], [80], [81], [86],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8074873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55212c3934663fc825842e85980f74b4254eaeb0",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "A new algorithm is presented for training of multilayer feedforward neural networks by integrating a genetic algorithm with an adaptive conjugate gradient neural network learning algorithm. The parallel hybrid learning algorithm has been implemented in C on an MIMD shared memory machine (Cray Y-MP8/864 supercomputer). It has been applied to two different domains, engineering design and image recognition. The performance of the algorithm has been evaluated by applying it to three examples. The superior convergence property of the parallel hybrid neural network learning algorithm presented in this paper is demonstrated."
            },
            "slug": "A-parallel-genetic/neural-network-learning-for-MIMD-Hung-Adeli",
            "title": {
                "fragments": [],
                "text": "A parallel genetic/neural network learning algorithm for MIMD shared memory machines"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The superior convergence property of the parallel hybrid neural network learning algorithm presented in this paper is demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2038264"
                        ],
                        "name": "F. Dellaert",
                        "slug": "F.-Dellaert",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Dellaert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dellaert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704135"
                        ],
                        "name": "J. Vandewalle",
                        "slug": "J.-Vandewalle",
                        "structuredName": {
                            "firstName": "Joos",
                            "lastName": "Vandewalle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Vandewalle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6322249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "767b16cc1646ba4cec2b1520c45c119010deea39",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper aims to examine the use of genetic algorithms to optimize subsystems of cellular neural network architectures. The application at hand is character recognition: the aim is to evolve an optimal feature detector in order to aid a conventional classifier network to generalize across different fonts. To this end, a performance function and a genetic encoding for a feature detector are presented. An experiment is described where an optimal feature detector is indeed found by the genetic algorithm.<<ETX>>"
            },
            "slug": "Automatic-design-of-cellular-neural-networks-by-of-Dellaert-Vandewalle",
            "title": {
                "fragments": [],
                "text": "Automatic design of cellular neural networks by means of genetic algorithms: finding a feature detector"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The aim is to evolve an optimal feature detector in order to aid a conventional classifier network to generalize across different fonts by using genetic algorithms to optimize subsystems of cellular neural network architectures."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the Third IEEE International Workshop on Cellular Neural Networks and their Applications (CNNA-94)"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145728497"
                        ],
                        "name": "G. Mani",
                        "slug": "G.-Mani",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Mani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Mani"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 259,
                                "start": 254
                            }
                        ],
                        "text": "The transfer function of each node in the architecture has been assumed to be fixed and predefined by human experts, yet the transfer function has been shown to be an important part of an ANN architecture and have significant impact on ANN\u2019s performance [238]\u2013[240]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17366535,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aeda49d4330b189504ae18ed7e6737f1247c5249",
            "isKey": false,
            "numCitedBy": 29,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of connectionist networks in which each node executes a different function to achieve efficient supervised learning is demonstrated. A modified backpropagation algorithm for such networks, which performs gradient descent in function space, is presented, and its advantages are discussed. The benefits of the suggested paradigm include faster learning and ease of interpretation of the trained network. The potential for combining this approach with other related approaches, including traditional backpropagation or reweighting is explored.<<ETX>>"
            },
            "slug": "Learning-by-gradient-descent-in-function-space-Mani",
            "title": {
                "fragments": [],
                "text": "Learning by gradient descent in function space"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "A modified backpropagation algorithm for connectionist networks, which performs gradient descent in function space, is presented, and its advantages include faster learning and ease of interpretation of the trained network."
            },
            "venue": {
                "fragments": [],
                "text": "1990 IEEE International Conference on Systems, Man, and Cybernetics Conference Proceedings"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694724"
                        ],
                        "name": "F. Gruau",
                        "slug": "F.-Gruau",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Gruau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gruau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "number of hidden layers and nodes) a learning process determines the rest Developmental encoding [6] a developmental process is genetically encoded [10, 7, 12, 8, 13, 16] Uses: Indirect and developmental representations are more flexible tend to be used for evolving architectures Direct representations tend to be used for evolving weights alone"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16828119,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97b3386dd7c06841a1addc5903f6e251575264c2",
            "isKey": false,
            "numCitedBy": 313,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "This article illustrates an artificial developmental system that is a computationally efficient technique for the automatic generation of complex artificial neural networks (ANNs). The artificial developmental system can develop a graph grammar into a modular ANN made of a combination of simpler subnetworks. A genetic algorithm is used to evolve coded grammars that generate ANNs for controlling six-legged robot locomotion. A mechanism for the automatic definition of neural subnetworks is incorporated Using this mechanism, the genetic algorithm can automatically decompose a problem into subproblems, generate a subANN for solving the subproblem, and instantiate copies of this subANN to build a higher-level ANN that solves the problem. We report some simulation results showing that the same problem cannot be solved if the mechanism for automatic definition of subnetworks is suppressed. We support our argument with pictures that describe the steps of development, how ANN structures are evolved, and how the ANNs compute."
            },
            "slug": "Automatic-Definition-of-Modular-Neural-Networks-Gruau",
            "title": {
                "fragments": [],
                "text": "Automatic Definition of Modular Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "An artificial developmental system that is a computationally efficient technique for the automatic generation of complex artificial neural networks (ANNs) and some simulation results showing that the same problem cannot be solved if the mechanism for automatic definition of subnetworks is suppressed."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109465183"
                        ],
                        "name": "Chi-Ho Lee",
                        "slug": "Chi-Ho-Lee",
                        "structuredName": {
                            "firstName": "Chi-Ho",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chi-Ho Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699362"
                        ],
                        "name": "Jong-Hwan Kim",
                        "slug": "Jong-Hwan-Kim",
                        "structuredName": {
                            "firstName": "Jong-Hwan",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jong-Hwan Kim"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 82
                            }
                        ],
                        "text": "There are some other different EP-based systems for designing ANN\u2019s [128], [129], [217], [223], but none has been tested on as many different benchmark problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 141
                            }
                        ],
                        "text": "Some researchers thus avoided crossover and adopted only mutations in the evolution of architectures [45], [128], [149], [179], [185]\u2013[197], [217], [223], although it has been shown that crossover may be useful and important in increasing the efficiency of evolution for some problems [48], [113], [212], [229]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 33704755,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f81b11f36095fc80d1260de0b3abbcef67b79ceb",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper proposes an evolutionary design of a neural network architecture, with a one dimensional linked list encoding scheme. In this scheme, neurons are arranged in one dimensional array, and the order information of neurons play important roles in genetic operation. Due to one dimensional structure, encoding from neural network architecture to genotype becomes easy, and genetic operation can be easily applied. To avoid the permutation problem, we choose evolutionary programming (EP) rather than genetic algorithm (GA), i.e., we apply mutation operators only in order to generate offspring. The proposed scheme is applied to XOR and 3 parity problems, and optimal neural network architecture can be found with this encoding scheme."
            },
            "slug": "Evolutionary-ordered-neural-network-with-a-encoding-Lee-Kim",
            "title": {
                "fragments": [],
                "text": "Evolutionary ordered neural network with a linked-list encoding scheme"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "The paper proposes an evolutionary design of a neural network architecture, with a one dimensional linked list encoding scheme, which chooses evolutionary programming (EP) rather than genetic algorithm (GA), and applies mutation operators only in order to generate offspring."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Evolutionary Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40073871"
                        ],
                        "name": "Marcus Frean",
                        "slug": "Marcus-Frean",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Frean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcus Frean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17369445,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db408205c71237b3566c358ee3737b8bd0c4078a",
            "isKey": false,
            "numCitedBy": 584,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A general method for building and training multilayer perceptrons composed of linear threshold units is proposed. A simple recursive rule is used to build the structure of the network by adding units as they are needed, while a modified perceptron algorithm is used to learn the connection strengths. Convergence to zero errors is guaranteed for any boolean classification on patterns of binary variables. Simulations suggest that this method is efficient in terms of the numbers of units constructed, and the networks it builds can generalize over patterns not in the training set."
            },
            "slug": "The-Upstart-Algorithm:-A-Method-for-Constructing-Frean",
            "title": {
                "fragments": [],
                "text": "The Upstart Algorithm: A Method for Constructing and Training Feedforward Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "Simulations suggest that this method for building and training multilayer perceptrons composed of linear threshold units is efficient in terms of the numbers of units constructed, and the networks it builds can generalize over patterns not in the training set."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1767244"
                        ],
                        "name": "S. Baluja",
                        "slug": "S.-Baluja",
                        "structuredName": {
                            "firstName": "Shumeet",
                            "lastName": "Baluja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Baluja"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 27946067,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "14e72559ede023c4419fe8d04e084b2907163e53",
            "isKey": false,
            "numCitedBy": 135,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents an evolutionary method for creating an artificial neural network based autonomous land vehicle controller. The evolved controllers perform better in unseen situations than those trained with an error backpropagation learning algorithm designed for this task. In this paper, an overview of the previous connectionist based approaches to this task is given, and the evolutionary algorithms used in this study are described in detail. Methods for reducing the high computational costs of training artificial neural networks with evolutionary algorithms are explored. Error metrics specific to the task of autonomous vehicle control are introduced; the evolutionary algorithms guided by these error metrics reveal improved performance over those guided by the standard sum-squared error metric. Finally, techniques for integrating evolutionary search and error backpropagation are presented. The evolved networks are designed to control Carnegie Mellon University's NAVLAB vehicles in road following tasks."
            },
            "slug": "Evolution-of-an-artificial-neural-network-based-Baluja",
            "title": {
                "fragments": [],
                "text": "Evolution of an artificial neural network based autonomous land vehicle controller"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An overview of the previous connectionist based approaches to this task is given, and the evolutionary algorithms used in this study are described in detail, and techniques for integrating evolutionary search and error backpropagation are presented."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern. Part B"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115379395"
                        ],
                        "name": "Jian Fang",
                        "slug": "Jian-Fang",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Fang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Fang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145185123"
                        ],
                        "name": "Y. Xi",
                        "slug": "Y.-Xi",
                        "structuredName": {
                            "firstName": "Yugeng",
                            "lastName": "Xi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Xi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 121
                            }
                        ],
                        "text": "Some researchers thus avoided crossover and adopted only mutations in the evolution of architectures [45], [128], [149], [179], [185]\u2013[197], [217], [223], although it has been shown that crossover may be useful and important in increasing the efficiency of evolution for some problems [48], [113], [212], [229]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 155
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 87
                            }
                        ],
                        "text": "The second approach evolves architectures and connection weights simultaneously [149], [179], [180], [182], [185]\u2013[200]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38454976,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4fec74ec028d25cd959d6a2995f027f5ec32f401",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-network-design-based-on-evolutionary-Fang-Xi",
            "title": {
                "fragments": [],
                "text": "Neural network design based on evolutionary programming"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell. Eng."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10375621"
                        ],
                        "name": "B. Whitehead",
                        "slug": "B.-Whitehead",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Whitehead",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Whitehead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3135886"
                        ],
                        "name": "Timothy D. Choate",
                        "slug": "Timothy-D.-Choate",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Choate",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy D. Choate"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 31049006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2a7407651f386d2e5576e86f6886aa59e0606503",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "An evolutionary neural network training algorithm is proposed for radial basis function (RBF) networks. The locations of basis function centers are not directly encoded in a genetic string, but are governed by space-filling curves whose parameters evolve genetically. This encoding causes each group of codetermined basis functions to evolve to fit a region of the input space. A network produced from this encoding is evaluated by training its output connections only. Networks produced by this evolutionary algorithm appear to have better generalization performance on the Mackey-Glass time series than corresponding networks whose centers are determined by k-means clustering."
            },
            "slug": "Evolving-space-filling-curves-to-distribute-radial-Whitehead-Choate",
            "title": {
                "fragments": [],
                "text": "Evolving space-filling curves to distribute radial basis functions over an input space"
            },
            "tldr": {
                "abstractSimilarityScore": 64,
                "text": "An evolutionary neural network training algorithm is proposed for radial basis function (RBF) networks that appear to have better generalization performance on the Mackey-Glass time series than corresponding networks whose centers are determined by k-means clustering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3091735"
                        ],
                        "name": "L. Meeden",
                        "slug": "L.-Meeden",
                        "structuredName": {
                            "firstName": "Lisa",
                            "lastName": "Meeden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Meeden"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1756069,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "e9246682b318114cc10b371091f108c850e45e1c",
            "isKey": false,
            "numCitedBy": 104,
            "numCiting": 56,
            "paperAbstract": {
                "fragments": [],
                "text": "By beginning with simple reactive behaviors and gradually building up to more memory-dependent behaviors, it may be possible for connectionist systems to eventually achieve the level of planning. This paper focuses on an intermediate step in this incremental process, where the appropriate means of providing guidance to adapting controllers is explored. A local and a global method of reinforcement learning are contrasted-a special form of back-propagation and an evolutionary algorithm. These methods are applied to a neural network controller for a simple robot. A number of experiments are described where the presence of explicit goals and the immediacy of reinforcement are varied. These experiments reveal how various types of guidance can affect the final control behavior. The results show that the respective advantages and disadvantages of these two adaptation methods are complementary, suggesting that some hybrid of the two may be the most effective method. Concluding remarks discuss the next incremental steps toward more complex control behaviors."
            },
            "slug": "An-incremental-approach-to-developing-intelligent-Meeden",
            "title": {
                "fragments": [],
                "text": "An incremental approach to developing intelligent neural network controllers for robots"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "A local and a global method of reinforcement learning are contrasted-a special form of back-propagation and an evolutionary algorithm are applied to a neural network controller for a simple robot."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern. Part B"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 32
                            }
                        ],
                        "text": "EPNet uses rank-based selection [125] and five mutations: hybrid training; node deletion; connection deletion; connection addition; and node addition [188], [194], [254]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 52
                            }
                        ],
                        "text": "may be replaced by other selection schemes, such as [125]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13240228,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ae04469ed61a798722e005adae600721fedb9fe8",
            "isKey": false,
            "numCitedBy": 99,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-empirical-study-of-genetic-operators-in-genetic-Yao",
            "title": {
                "fragments": [],
                "text": "An empirical study of genetic operators in genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Microprocess. Microprogramming"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98816741"
                        ],
                        "name": "L. Iu",
                        "slug": "L.-Iu",
                        "structuredName": {
                            "firstName": "L",
                            "lastName": "Iu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Iu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 63897991,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "502378062746a1b1c91e0c71f55ef2b1d3226204",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "A geneiic algorithm to optimize both neural network and related \nconnection weights in a given task is proposed. A neural network having arbitrary connections is regarded as a virtural living thing which has genes respresenting ita connections among neural units. An XOR problem ia dealt with. The proposed method is available for designing a neural network whose adequate structure ia unknown."
            },
            "slug": "OPTIMIZATION-OF-NEURAL-NETWORKS-BY-GENETIC-Iu",
            "title": {
                "fragments": [],
                "text": "OPTIMIZATION OF NEURAL NETWORKS BY GENETIC ALGORITHM"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "A geneiic algorithm to optimize both neural network and related connection weights in a given task is proposed and is available for designing a neural network whose adequate structure is unknown."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39399199"
                        ],
                        "name": "J. Sietsma",
                        "slug": "J.-Sietsma",
                        "structuredName": {
                            "firstName": "Jocelyn",
                            "lastName": "Sietsma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sietsma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145011433"
                        ],
                        "name": "R. Dow",
                        "slug": "R.-Dow",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Dow",
                            "middleNames": [
                                "J.",
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Dow"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 37977852,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e354ec85b8287bf15ed596be16ef6e422ccc29e7",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Creating-artificial-neural-networks-that-generalize-Sietsma-Dow",
            "title": {
                "fragments": [],
                "text": "Creating artificial neural networks that generalize"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2122321058"
                        ],
                        "name": "P.J.B. Hancock",
                        "slug": "P.J.B.-Hancock",
                        "structuredName": {
                            "firstName": "P.J.B.",
                            "lastName": "Hancock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P.J.B. Hancock"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "Hancock [113] suggested that the permutation problem might \u201cnot be as severe as had been supposed\u201d with the population size and the selection mechanism he used because \u201cThe increased number of ways of solving the problem outweigh the difficulties of bringing building blocks together."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 296,
                                "start": 291
                            }
                        ],
                        "text": "Some researchers thus avoided crossover and adopted only mutations in the evolution of architectures [45], [128], [149], [179], [185]\u2013[197], [217], [223], although it has been shown that crossover may be useful and important in increasing the efficiency of evolution for some problems [48], [113], [212], [229]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 44,
                                "start": 39
                            }
                        ],
                        "text": "ANN\u2019s is the permutation problem [32], [113], also known"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62752273,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "050b24afa54ff75f5e3e549412c43815333645bc",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The specification of neural net architectures by genetic algorithm (GA) is thought to be hampered by difficulties with crossover. This is the 'permutation' or 'competing conventions' problem: similar nets may have the hidden units defined in different orders so that they have very dissimilar genetic strings, preventing successful recombination of building blocks. Previous empirical tests of a number of recombination operators using a simulated net-building task indicated the superiority of one that sorts hidden unit definitions by overlap prior to crossover. However, simple crossover also fared well, suggesting that the permutation problem is not serious in practice. This is supported by an observed reduction in performance when the permutation problem is removed. The GA is shown to be able to resolve the permutations, so that the advantages of an increase in the number of maxima outweigh the difficulties of recombination.<<ETX>>"
            },
            "slug": "Genetic-algorithms-and-permutation-problems:-a-of-Hancock",
            "title": {
                "fragments": [],
                "text": "Genetic algorithms and permutation problems: a comparison of recombination operators for neural net structure specification"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The GA is shown to be able to resolve the permutations, so that the advantages of an increase in the number of maxima outweigh the difficulties of recombination."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10375621"
                        ],
                        "name": "B. Whitehead",
                        "slug": "B.-Whitehead",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Whitehead",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Whitehead"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3135886"
                        ],
                        "name": "Timothy D. Choate",
                        "slug": "Timothy-D.-Choate",
                        "structuredName": {
                            "firstName": "Timothy",
                            "lastName": "Choate",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timothy D. Choate"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17037588,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0fdec442a9c911d916d1a89c342e18b0c090dbf5",
            "isKey": false,
            "numCitedBy": 275,
            "numCiting": 132,
            "paperAbstract": {
                "fragments": [],
                "text": "In a radial basis function (RBF) network, the RBF centers and widths can be evolved by a cooperative-competitive genetic algorithm. The set of genetic strings in one generation of the algorithm represents one REP network, not a population of competing networks. This leads to moderate computation times for the algorithm as a whole. Selection operates on individual RBFs rather than on whole networks. Selection therefore requires a genetic fitness function that promotes competition among RBFs which are doing nearly the same job while at the same time promoting cooperation among RBFs which cover different parts of the domain of the function to be approximated. Niche creation resulting from a fitness function of the form |w(i)|(beta)/E(|w(i')|(beta)), 1<beta<2 can facilitate the desired cooperative-competitive behavior. The feasibility of the resulting algorithm to evolve networks of Gaussian, inverse multiquadric, and thin-plate spline RBFs is demonstrated by predicting the Mackey-Glass time series. For each type of RBF, and for networks of 25, 50, 75, 100, 125, and 150 RBF units, prediction errors for the evolved Gaussian RBF networks are 50-70% lower than RBF networks obtained by k-means clustering."
            },
            "slug": "Cooperative-competitive-genetic-evolution-of-radial-Whitehead-Choate",
            "title": {
                "fragments": [],
                "text": "Cooperative-competitive genetic evolution of radial basis function centers and widths for time series prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "In a radial basis function (RBF) network, the RBF centers and widths can be evolved by a cooperative-competitive genetic algorithm, and the feasibility of the resulting algorithm to evolve networks of Gaussian, inverse multiquadric, and thin-plate spline RBFs is demonstrated by predicting the Mackey-Glass time series."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859405"
                        ],
                        "name": "David E. Moriarty",
                        "slug": "David-E.-Moriarty",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Moriarty",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David E. Moriarty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 35
                            }
                        ],
                        "text": "In 1997, Moriarty and Miikkulainen [49] devised the SANE system, which effectively evolves individual neurons, thus circumventing CCP entirely."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "Figure 5: The SANE system [49] for designing neural networks by evolving populations of individual hidden-layer neurons."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7885480,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "3564c49d34c1642cf52606665440b8d2754510d8",
            "isKey": false,
            "numCitedBy": 341,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "This article demonstrates the advantages of a cooperative, coevolutionary search in difficult control problems. The symbiotic adaptive neuroevolution (SANE) system coevolves a population of neurons that cooperate to form a functioning neural network. In this process, neurons assume different but overlapping roles, resulting in a robust encoding of control behavior. SANE is shown to be more efficient and more adaptive and to maintain higher levels of diversity than the more common network-based population approaches. Further empirical studies illustrate the emergent neuron specializations and the different roles the neurons assume in the population."
            },
            "slug": "Forming-Neural-Networks-Through-Efficient-and-Moriarty-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Forming Neural Networks Through Efficient and Adaptive Coevolution"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "The symbiotic adaptive neuroevolution system coevolves a population of neurons that cooperate to form a functioning neural network to be more efficient and more adaptive and to maintain higher levels of diversity than the more common network-based population approaches."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "A gradient descent-based optimization algorithm such as backpropagation (BP) [6] can then be used to adjust connection weights in the ANN iteratively in order to minimize the error."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7840452,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a57c6d627ffc667ae3547073876c35d6420accff",
            "isKey": false,
            "numCitedBy": 1582,
            "numCiting": 122,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Connectionist-Learning-Procedures-Hinton",
            "title": {
                "fragments": [],
                "text": "Connectionist Learning Procedures"
            },
            "venue": {
                "fragments": [],
                "text": "Artif. Intell."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2248653"
                        ],
                        "name": "S. Jockusch",
                        "slug": "S.-Jockusch",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Jockusch",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Jockusch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30258243"
                        ],
                        "name": "H. Ritter",
                        "slug": "H.-Ritter",
                        "structuredName": {
                            "firstName": "Helge",
                            "lastName": "Ritter",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ritter"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 90,
                                "start": 85
                            }
                        ],
                        "text": "There has been some work in this area where good results were reported [119], [120], [245]\u2013[253]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 19473249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9658c1b7124784ed72c1f548b0234757523dfa5b",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Self-organizing-maps:-Local-competition-and-Jockusch-Ritter",
            "title": {
                "fragments": [],
                "text": "Self-organizing maps: Local competition and evolutionary optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694198"
                        ],
                        "name": "J. Urzelai",
                        "slug": "J.-Urzelai",
                        "structuredName": {
                            "firstName": "Joseba",
                            "lastName": "Urzelai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Urzelai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 166
                            }
                        ],
                        "text": "Some elaborate PO systems, [33, 52] employ evolved developmental regimes in which neurons grow axonal projections to their targets, while one of the early PE systems [69] evolves separate learning rules (applied to each incoming synapse) for each neuron."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13546513,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "0562b5dc160f9f4469a3b9d9d5748516cac0be04",
            "isKey": false,
            "numCitedBy": 109,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract This paper is concerned with adaptation capabilities of evolved neural controllers. We propose to evolve mechanisms for parameter self-organization instead of evolving the parameters themselves. The method consists of encoding a set of local adaptation rules that synapses follow while the robot freely moves in the environment. In the experiments presented here, the performance of the robot is measured in environments that are different in significant ways from those used during evolution. The results show that evolutionary adaptive controllers solve the task much faster and better than evolutionary standard fixed-weight controllers, that the method scales up well to large architectures, and that evolutionary adaptive controllers can adapt to environmental changes that involve new sensory characteristics (including transfer from simulation to reality and across different robotic platforms) and new spatial relationships."
            },
            "slug": "Evolution-of-Adaptive-Synapses:-Robots-with-Fast-in-Urzelai-Floreano",
            "title": {
                "fragments": [],
                "text": "Evolution of Adaptive Synapses: Robots with Fast Adaptive Behavior in New Environments"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The results show that evolutionary adaptive controllers solve the task much faster and better than evolutionary standard fixed-weight controllers, that the method scales up well to large architectures, and that they can adapt to environmental changes that involve new sensory characteristics and new spatial relationships."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790928"
                        ],
                        "name": "R. A. Zitar",
                        "slug": "R.-A.-Zitar",
                        "structuredName": {
                            "firstName": "Raed",
                            "lastName": "Zitar",
                            "middleNames": [
                                "Abu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. A. Zitar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2679783"
                        ],
                        "name": "M. Hassoun",
                        "slug": "M.-Hassoun",
                        "structuredName": {
                            "firstName": "Mohamad",
                            "lastName": "Hassoun",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Hassoun"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 31
                            }
                        ],
                        "text": "For example, Zitar and Hassoun [306] used EA\u2019s to extract rules in a reinforcement learning system and then used them to train ANN\u2019s."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 9437246,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89cc48c1826c7c6076ebaa8bd53931c7492847ba",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel system for rule extraction of temporal control problems and presents a new way of designing neurocontrollers. The system employs a hybrid genetic search and reinforcement learning strategy for extracting the rules. The learning strategy requires no supervision and no reference model. The extracted rules are weighted micro rules that operate on small neighborhoods of the admissable control space. A further refinement of the extracted rules is achieved by applying additional genetic search and reinforcement to reduce the number of extracted micro rules. This process results in a smaller set of macro rules which can be used to train a feedforward multilayer perceptron neurocontroller. The micro rules or the macro rules may also be utilized directly in a table look-up controller. As an example of the macro rules-based neurocontroller, we chose four benchmarks. In the first application we verify the capability of our system to learn optimal linear control strategies. The other three applications involve engine idle speed control, bioreactor control, and stabilizing two poles on a moving cart. These problems are highly nonlinear, unstable, and may include noise and delays in the plant dynamics. In terms of retrievals; the neurocontrollers generally outperform the controllers using a table look-up method. Both controllers, though, show robustness against noise disturbances and plant parameter variations."
            },
            "slug": "Neurocontrollers-trained-with-rules-extracted-by-a-Zitar-Hassoun",
            "title": {
                "fragments": [],
                "text": "Neurocontrollers trained with rules extracted by a genetic assisted reinforcement learning system"
            },
            "tldr": {
                "abstractSimilarityScore": 97,
                "text": "This paper proposes a novel system for rule extraction of temporal control problems and presents a new way of designing neurocontrollers that employs a hybrid genetic search and reinforcement learning strategy for extracting the rules."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70314519"
                        ],
                        "name": "H. Ishigami",
                        "slug": "H.-Ishigami",
                        "structuredName": {
                            "firstName": "Hideyuki",
                            "lastName": "Ishigami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Ishigami"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144883796"
                        ],
                        "name": "T. Fukuda",
                        "slug": "T.-Fukuda",
                        "structuredName": {
                            "firstName": "Toshio",
                            "lastName": "Fukuda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Fukuda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40528071"
                        ],
                        "name": "T. Shibata",
                        "slug": "T.-Shibata",
                        "structuredName": {
                            "firstName": "Takanori",
                            "lastName": "Shibata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Shibata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144223018"
                        ],
                        "name": "F. Arai",
                        "slug": "F.-Arai",
                        "structuredName": {
                            "firstName": "Fumihito",
                            "lastName": "Arai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Arai"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 123524662,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee0cc68379b0f9ce74458a25a4d16703b230c5fd",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Structure-optimization-of-fuzzy-neural-network-by-Ishigami-Fukuda",
            "title": {
                "fragments": [],
                "text": "Structure optimization of fuzzy neural network by genetic algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144404817"
                        ],
                        "name": "J. Holland",
                        "slug": "J.-Holland",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Holland",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Holland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "[12], and genetic algorithms (GA\u2019s) [13], [14]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 37
                            }
                        ],
                        "text": "The canonical genetic algorithm (GA) [13], [14] has"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58781161,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4b4279db68b16e20fbc56f9d41980a950191d30a",
            "isKey": false,
            "numCitedBy": 38845,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "Name of founding work in the area. Adaptation is key to survival and evolution. Evolution implicitly optimizes organisims. AI wants to mimic biological optimization { Survival of the ttest { Exploration and exploitation { Niche nding { Robust across changing environments (Mammals v. Dinos) { Self-regulation,-repair and-reproduction 2 Artiicial Inteligence Some deenitions { \"Making computers do what they do in the movies\" { \"Making computers do what humans (currently) do best\" { \"Giving computers common sense; letting them make simple deci-sions\" (do as I want, not what I say) { \"Anything too new to be pidgeonholed\" Adaptation and modiication is root of intelligence Some (Non-GA) branches of AI: { Expert Systems (Rule based deduction)"
            },
            "slug": "Adaptation-in-natural-and-artificial-systems-Holland",
            "title": {
                "fragments": [],
                "text": "Adaptation in natural and artificial systems"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "Names of founding work in the area of Adaptation and modiication, which aims to mimic biological optimization, and some (Non-GA) branches of AI."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1975
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145084328"
                        ],
                        "name": "Jeng-Sheng Huang",
                        "slug": "Jeng-Sheng-Huang",
                        "structuredName": {
                            "firstName": "Jeng-Sheng",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeng-Sheng Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368435"
                        ],
                        "name": "Hsiao-Chung Liu",
                        "slug": "Hsiao-Chung-Liu",
                        "structuredName": {
                            "firstName": "Hsiao-Chung",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsiao-Chung Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121226935,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6f60ea05d642e6811e4d91420993274a9742697a",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Object-recognition-using-genetic-algorithms-with-a-Huang-Liu",
            "title": {
                "fragments": [],
                "text": "Object recognition using genetic algorithms with a Hopfield's neural model"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "122185897"
                        ],
                        "name": "A. Siddiqi",
                        "slug": "A.-Siddiqi",
                        "structuredName": {
                            "firstName": "Ansar",
                            "lastName": "Siddiqi",
                            "middleNames": [
                                "Ahmed"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Siddiqi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145815031"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Lucas",
                            "middleNames": [
                                "M.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 157
                            }
                        ],
                        "text": "The direct encoding scheme achieved the same performance as that achieved by the developmental rule representation when the initial conditions were the same [233]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 6
                            }
                        ],
                        "text": "Lucas [233] shows that the direct encoding scheme can be"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15788622,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e11ef91575eddff818ae26b600dbea2116d7bf5",
            "isKey": false,
            "numCitedBy": 92,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "The intuitive expectation is that the scheme used to encode the neural network in the chromosome should be critical to the success of evolving neural networks to solve difficult problems. In 1990 Kitano published an encoding scheme based on context-free parallel matrix rewriting. The method allowed compact, finite, chromosomes to grow neural networks of potentially infinite size. Results were presented that demonstrated superior evolutionary properties of the matrix rewriting method compared to a simple direct encoding. The authors present results that contradict those findings, and demonstrate that a genetic algorithm (GA) using a direct encoding can find good individuals just as efficiently as a GA using matrix rewriting."
            },
            "slug": "A-comparison-of-matrix-rewriting-versus-direct-for-Siddiqi-Lucas",
            "title": {
                "fragments": [],
                "text": "A comparison of matrix rewriting versus direct encoding for evolving neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The authors present results that contradict findings, and demonstrate that a genetic algorithm (GA) using a direct encoding can find good individuals just as efficiently as a GA using matrix rewriting."
            },
            "venue": {
                "fragments": [],
                "text": "1998 IEEE International Conference on Evolutionary Computation Proceedings. IEEE World Congress on Computational Intelligence (Cat. No.98TH8360)"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728171"
                        ],
                        "name": "P. E. Hotz",
                        "slug": "P.-E.-Hotz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hotz",
                            "middleNames": [
                                "Eggenberger"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. E. Hotz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 89
                            }
                        ],
                        "text": "The upper right network of Figure 18 is the cornerstone of Eggenberger\u2019s pioneering work [17, 16] into GRN-based EvoDevo systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16777373,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "37accad54a446f369a349e6cbbecbf655943860c",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we propose a biological inspired model to develop the structure of artiicial neural networks. The model is based on an artiicial genetic regualtory system, which controls the development of the neural network. The model allows for diierent cell types which are the result of diierent intercellular communication processes. Having diierent cells will lead to diierent development of connection patterns. The goal of the proposed model is to investigate the question how the local genetic processes are able to construct the structure of a neural network."
            },
            "slug": "Creation-of-Neural-Networks-Based-on-Developmental-Hotz",
            "title": {
                "fragments": [],
                "text": "Creation of Neural Networks Based on Developmental and Evolutionary Principles"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The goal of the proposed model is to investigate the question how the local genetic processes are able to construct the structure of a neural network."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2760607"
                        ],
                        "name": "R. Smalz",
                        "slug": "R.-Smalz",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Smalz",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Smalz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145937506"
                        ],
                        "name": "M. Conrad",
                        "slug": "M.-Conrad",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Conrad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Conrad"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 10495822,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "301dd9a43efe042540f8ca495aa6232374e8eded",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Combining-evolution-with-credit-apportionment:-A-Smalz-Conrad",
            "title": {
                "fragments": [],
                "text": "Combining evolution with credit apportionment: A new learning algorithm for neural nets"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116997934"
                        ],
                        "name": "Sandeep Jain",
                        "slug": "Sandeep-Jain",
                        "structuredName": {
                            "firstName": "Sandeep",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sandeep Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143989014"
                        ],
                        "name": "P. Peng",
                        "slug": "P.-Peng",
                        "structuredName": {
                            "firstName": "Pei-Yuan",
                            "lastName": "Peng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Peng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2359960"
                        ],
                        "name": "A. Tzes",
                        "slug": "A.-Tzes",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Tzes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tzes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880767"
                        ],
                        "name": "F. Khorrami",
                        "slug": "F.-Khorrami",
                        "structuredName": {
                            "firstName": "Farshad",
                            "lastName": "Khorrami",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Khorrami"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5261076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "39754ba8e60db3f5321dae8bae7d734eb4f30a09",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of neural networks for active control of lightly damped systems is considered in this article. The training process of the neural-network controller is based on the genetic learning algorithm. The schemes imitates nature's cleansing phenomena of natural selection and survival of the fittest to generate individual controllers withe best fitness values. It essentially incorporates an exhaustive search in the weight-space governed by the rituals of crossover and mutation to seek the optimum neural-network weights to satisfy certain performance criteria. Several appropriate modifications of the classical genetic algorithm for neural-network control purposes are discussed. The genetic-trained neural-network controller is applied for tip position tracking and vibration suppression of a single-link flexible arm. Simulation studies are presented to validate the effectiveness of the advocated algorithms."
            },
            "slug": "Neural-network-design-with-genetic-learning-for-of-Jain-Peng",
            "title": {
                "fragments": [],
                "text": "Neural network design with genetic learning for control of a single link flexible manipulator"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "The application of neural networks for active control of lightly damped systems is considered and several appropriate modifications of the classical genetic algorithm for neural-network control purposes are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Intell. Robotic Syst."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715339"
                        ],
                        "name": "D. Goldberg",
                        "slug": "D.-Goldberg",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goldberg"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "Although evolution has been introduced into ANN\u2019s at various levels, they can roughly be divided into three: the evolution of connection weights, architectures, and learning rules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 96
                            }
                        ],
                        "text": "The architecture of an ANN includes its topological structure, i.e., connectivity, and the transfer function of each node in the ANN."
                    },
                    "intents": []
                }
            ],
            "corpusId": 38613589,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e62d1345b340d5fda3b092c460264b9543bc4b5",
            "isKey": false,
            "numCitedBy": 58251,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book brings together - in an informal and tutorial fashion - the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields. \n \nMajor concepts are illustrated with running examples, and major algorithms are illustrated by Pascal computer programs. No prior knowledge of GAs or genetics is assumed, and only a minimum of computer programming and mathematics background is required."
            },
            "slug": "Genetic-Algorithms-in-Search-Optimization-and-Goldberg",
            "title": {
                "fragments": [],
                "text": "Genetic Algorithms in Search Optimization and Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "This book brings together the computer techniques, mathematical tools, and research results that will enable both students and practitioners to apply genetic algorithms to problems in many fields."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117289777"
                        ],
                        "name": "Brad Fullmer and Risto Miikkulainen",
                        "slug": "Brad-Fullmer-and-Risto-Miikkulainen",
                        "structuredName": {
                            "firstName": "Brad",
                            "lastName": "Miikkulainen",
                            "middleNames": [
                                "Fullmer",
                                "and",
                                "Risto"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Brad Fullmer and Risto Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8809224,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8a030483afa171bbe9df0d6bb402808b13944708",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "A new mechanism for genetic encoding of neural networks is proposed, which is loosely based on the marker structure of biological DNA. The mechanism allows all aspects of the network structure, including the number of nodes and their connectivity, to be evolved through genetic algorithms. The e ectiveness of the encoding scheme is demonstrated in an object recognition task that requires arti cial creatures (whose behaviour is driven by a neural network) to develop high-level nite-state exploration and discrimination strategies. The task requires solving the sensory-motor grounding problem, i.e. developing a functional understanding of the e ects that a creature's movement has on its sensory input."
            },
            "slug": "Using-Marker-Based-Genetic-Encoding-Of-Neural-To-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Using Marker-Based Genetic Encoding Of Neural Networks To Evolve Finite-State Behaviour"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "A new mechanism for genetic encoding of neural networks is proposed, which is loosely based on the marker structure of biological DNA, which allows all aspects of the network structure, including the number of nodes and their connectivity, to be evolved through genetic algorithms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69409595"
                        ],
                        "name": "S. Omatu",
                        "slug": "S.-Omatu",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Omatu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omatu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2593208"
                        ],
                        "name": "M. Yoshioka",
                        "slug": "M.-Yoshioka",
                        "structuredName": {
                            "firstName": "Michifumi",
                            "lastName": "Yoshioka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Yoshioka"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62683067,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "32d2906654de17a76a8e3460631581649a9c06fb",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a method to use the neural networks to tune the PID (proportional plus integral plus derivative) gains such that human operators tune the gains adaptively according to the environmental condition and systems specification. The tuning method is based on the error backpropagation method and hence it may be trapped in a local minimum. In order to avoid the local minimum problem, we use the genetic algorithm to find the initial values of the connection weights of the neural network and initial values of PID gains. The experimental results show the effectiveness of the present approach."
            },
            "slug": "Self-tuning-neuro-PID-control-and-applications-Omatu-Yoshioka",
            "title": {
                "fragments": [],
                "text": "Self-tuning neuro-PID control and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 90,
                "text": "A method to use the neural networks to tune the PID (proportional plus integral plus derivative) gains such that human operators tune the gains adaptively according to the environmental condition and systems specification is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152688643"
                        ],
                        "name": "Dong Wang",
                        "slug": "Dong-Wang",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143773766"
                        ],
                        "name": "Jin-wu Xu",
                        "slug": "Jin-wu-Xu",
                        "structuredName": {
                            "firstName": "Jin-wu",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jin-wu Xu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 135
                            }
                        ],
                        "text": "Similar work on the evolution of initial weights has also been done on competitive learning neural networks [139] and Kohonen networks [140]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61048832,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6ee213b4123ee8f317e06da1b88a2c43c850e382",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a novel approach of employing genetic algorithms (GA) to set initial reference vectors in the Kohonen layer of learning vector quantization (LVQ) neural networks. The aim of this investigation is to improve the learning characteristics of LVQ so as to get more accurate classification results. In the proposed scheme, the reference vectors are set to the locations mostly matching the probability distribution of training vectors. Genetic algorithms are applied to optimize the locations and distribution of the reference vectors. After competitive learning of LVQ, the reference vectors are employed to be representatives of various patterns to determine the categories which testing vectors belong to a comparison study is reported based on LVQ with random initial reference vectors, LVQ with GA learning and LVQ with initial reference vectors set by GA. Experimental results of a case study have shown that the proposed method is promising for machine fault classification."
            },
            "slug": "Fault-detection-based-on-evolving-LVQ-neural-Wang-Xu",
            "title": {
                "fragments": [],
                "text": "Fault detection based on evolving LVQ neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "Experimental results of a case study have shown that the proposed method of employing genetic algorithms to set initial reference vectors in the Kohonen layer of learning vector quantization (LVQ) neural networks is promising for machine fault classification."
            },
            "venue": {
                "fragments": [],
                "text": "1996 IEEE International Conference on Systems, Man and Cybernetics. Information Intelligence and Systems (Cat. No.96CH35929)"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2638008"
                        ],
                        "name": "T. Ragg",
                        "slug": "T.-Ragg",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Ragg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Ragg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3337656"
                        ],
                        "name": "S. Gutjahr",
                        "slug": "S.-Gutjahr",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Gutjahr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Gutjahr"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 215
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14702894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "22dca4fe7dcc3a8d7df22476a2ed3995cc00cb71",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "Presents a new approach to determine the optimal topology of multilayer perceptrons for a given learning task, based on information theory and evolution. Our method exploits the mutual information of the input-output relation to sort the units into a list with respect to their information content. Embedded in a evolutionary algorithm, a mutation operator is proposed which removes or adds input units from given networks based on their ranking. The power of the approach is demonstrated on several benchmarks. We conclude that using an evolutionary algorithm as a framework in conjunction with intelligent mutation operators is concurrently the most efficient optimization technique with regard to network size and performance as well as scalability."
            },
            "slug": "Automatic-determination-of-optimal-network-based-on-Ragg-Gutjahr",
            "title": {
                "fragments": [],
                "text": "Automatic determination of optimal network topologies based on information theory and evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is concluded that using an evolutionary algorithm as a framework in conjunction with intelligent mutation operators is concurrently the most efficient optimization technique with regard to network size and performance as well as scalability."
            },
            "venue": {
                "fragments": [],
                "text": "EUROMICRO 97. Proceedings of the 23rd EUROMICRO Conference: New Frontiers of Information Technology (Cat. No.97TB100167)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48890329"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Miller",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "14024873"
                        ],
                        "name": "P. Todd",
                        "slug": "P.-Todd",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Todd",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Todd"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1983880"
                        ],
                        "name": "Shailesh U. Hegde",
                        "slug": "Shailesh-U.-Hegde",
                        "structuredName": {
                            "firstName": "Shailesh",
                            "lastName": "Hegde",
                            "middleNames": [
                                "U."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shailesh U. Hegde"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[150] which make EA\u2019s a better candidate for"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 89
                            }
                        ],
                        "text": "The first separates the evolution of architectures from that of connection weights [24], [150], [153], [154], [165], [167], [169], [170]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 115
                            }
                        ],
                        "text": "In the first approach, each connection of an architecture is directly specified by its binary representation [24], [150], [153], [154], [165], [167], [169], [170], [202]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 23166548,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e89cb97bc83badf8c6cc0e2439ee4a035cba72d9",
            "isKey": false,
            "numCitedBy": 934,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "RoboCup has come a long way since it\u2019s creation in \u201997 [1] and is a respected place for machine learning researchers to try out new algorithms in a competitive fashion. RoboCup is now an international competition that draws many teams and respected researchers looking for a chance to create the best team. Originally we set out to create a team to compete in RoboCup. This was an ambitious project, and we had hopes to finish within the next year. For this semester, we chose to scale down the RoboCup team towards a smaller research area to try our learning algorithm on. The scaled down version of the RoboCup soccer environment is known as the \u201dKeepaway Testbed\u201d and was started by Peter Stone, University of Texas [2]. Here the task is simple, you have two teams on the field each with the same number of players. Instead of trying to score a goal on the opponent the teams are given tasks, and one team is labeled the keepers and the other is labeled the takers. It is the task of the keepers to maintain possesion of the ball and it is the task of the takers to take the ball. The longer the keepers are able to maintain possesion of the ball the better the team. There are several advantages to this environment. First, it provides some of the essential characteristics of a real soccer game. Typically it is believed that if a team is able to maintain possesion of the ball for long periods of time they will win the match. Secondly, it provides realistic behavior much the same as the original RoboCup server. This is accomplished by introducing noise into the system similar to the original RoboCup, and similar to what would be received by real robots. Finally, when you want to go through the learning process this environment is capable of stopping play once the takers have touched the ball, and the environment is capable of starting a new trial based on that occurrence. Although the RoboCup Keepaway Machine Learning testbed provided an excellent environment to train our agents, we still needed to scale down the problem in order to do a feasibility study. Based on the Keepaway testbed, we created a simulation world with one simple task. One agent is placed into the world and has to locate the position of the goal. This can be thought of as an agent in a soccer environment needing to locate either the ball or another teammate. It was in this environment where we tested our methods for learning autonomous agents."
            },
            "slug": "Designing-Neural-Networks-using-Genetic-Algorithms-Miller-Todd",
            "title": {
                "fragments": [],
                "text": "Designing Neural Networks using Genetic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This semester, the RoboCup Keepaway Machine Learning testbed provided an excellent environment to train the authors' agents, but still needed to scale down the problem in order to do a feasibility study."
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694724"
                        ],
                        "name": "F. Gruau",
                        "slug": "F.-Gruau",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Gruau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gruau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 228
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 50
                            }
                        ],
                        "text": "in chromosomes [151], [168], [184], [205], [230], [232]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 201,
                                "start": 196
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 122784493,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ba1534f60d97991669a2f2fc1521d76f3fa1d5c1",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "Genetic algorithms (GAS) are used to generate neural networks that implement Boolean functions. Neural networks both involve an architecture that is a graph of connections, and a set of weights. The algorithm that is put forward yields both the architecture and the weights by using chromosomes that encode an algorithmic description based upon a cell rewriting grammar. The developmental process interprets the grammar for l cycles and develops a neural net parametrized by l. The encoding along with the developmental process have been designed in order to improve the existing approaches. They implement the following key-properties. The representation on the chromosome is abstract and compact. Any chromosome develops a valid phenotype. The developmental process gives modular and interpretable architectures with a powerful scalability property. The GA finds a neural net for the 50 inputs parity function, and for the 40 inputs symmetry function.<<ETX>>"
            },
            "slug": "Genetic-synthesis-of-Boolean-neural-networks-with-a-Gruau",
            "title": {
                "fragments": [],
                "text": "Genetic synthesis of Boolean neural networks with a cell rewriting developmental process"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Genetic algorithms (GAS) are used to generate neural networks that implement Boolean functions by using chromosomes that encode an algorithmic description based upon a cell rewriting grammar to give modular and interpretable architectures with a powerful scalability property."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727799"
                        ],
                        "name": "F. Mondada",
                        "slug": "F.-Mondada",
                        "structuredName": {
                            "firstName": "Francesco",
                            "lastName": "Mondada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Mondada"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742820"
                        ],
                        "name": "D. Floreano",
                        "slug": "D.-Floreano",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Floreano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Floreano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 814595,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5df2e7acaccd97949e45b7d0c04009479be6158b",
            "isKey": false,
            "numCitedBy": 103,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Evolution-of-neural-control-structures:-some-on-Mondada-Floreano",
            "title": {
                "fragments": [],
                "text": "Evolution of neural control structures: some experiments on mobile robots"
            },
            "venue": {
                "fragments": [],
                "text": "Robotics Auton. Syst."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34644976"
                        ],
                        "name": "R. Hutchins",
                        "slug": "R.-Hutchins",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hutchins",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hutchins"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 93
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 41
                            }
                        ],
                        "text": "recurrent ANN\u2019s [41], [60], [65], [100], [102], [103], [106],"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 147
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16579727,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f460436c9f56cb5285ff621267547f45bcc4a09",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "Nonlinear system behavior is not always well characterized by linearized system models, especially if the system is chaotic. This research studies the use of a neural network algorithm structure to model two nonlinear systems, a quadratic system and a chaotic system. An evolutionary programming approach is employed to train the neural nets so that the training process might better avoid selecting weighting parameters that represent a local minimum rather than a global minimum. This training approach is compared with the more standard backpropagation technique.<<ETX>>"
            },
            "slug": "Identifying-nonlinear-dynamic-systems-using-neural-Hutchins",
            "title": {
                "fragments": [],
                "text": "Identifying nonlinear dynamic systems using neural nets and evolutionary programming"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "This research studies the use of a neural network algorithm structure to model two nonlinear systems, a quadratic system and a chaotic system, and an evolutionary programming approach is employed to train the neural nets to better avoid selecting weighting parameters that represent a local minimum rather than a global minimum."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 28th Asilomar Conference on Signals, Systems and Computers"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3048336"
                        ],
                        "name": "Y. Hirose",
                        "slug": "Y.-Hirose",
                        "structuredName": {
                            "firstName": "Yoshio",
                            "lastName": "Hirose",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hirose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8944642"
                        ],
                        "name": "K. Yamashita",
                        "slug": "K.-Yamashita",
                        "structuredName": {
                            "firstName": "Koichiro",
                            "lastName": "Yamashita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Yamashita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072036348"
                        ],
                        "name": "Shimpei Hijiya",
                        "slug": "Shimpei-Hijiya",
                        "structuredName": {
                            "firstName": "Shimpei",
                            "lastName": "Hijiya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shimpei Hijiya"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 5959611,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "2db2bae5d798b04aebc0848ce5bc6d040476d229",
            "isKey": false,
            "numCitedBy": 452,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Backpropagation-algorithm-which-varies-the-number-Hirose-Yamashita",
            "title": {
                "fragments": [],
                "text": "Backpropagation algorithm which varies the number of hidden units"
            },
            "venue": {
                "fragments": [],
                "text": "International 1989 Joint Conference on Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152891866"
                        ],
                        "name": "Yong Liu",
                        "slug": "Yong-Liu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 42
                            }
                        ],
                        "text": "in (3) may be replaced by Cauchy mutation [121], [122], [124] for faster evolution."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 50
                            }
                        ],
                        "text": "Other mutation operators, such as Cauchy mutation [121], [122], can also be used."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14808044,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ecab42ac8dda1dbd40002ef4f1b06f5ebef5182f",
            "isKey": false,
            "numCitedBy": 331,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary programming (EP) has been applied to many numerical and combinatorial optimisation problems successfully in recent years. One disadvantage of EP is its slow convergence to a good near optimum for some function optimisation problems. In this paper, we propose a fast EP (FEP) which uses a Cauchy instead of Gaussian mutation operator as the primary search operator. The relationship between FEP and classical EP (CEP) is similar to that between the fast simulated annealing and the classical version. Extensive empirical studies have been carried out to evaluate the performance of FEP for diierent function optimisation problems. Fifty runs have been conducted for each of the 23 test functions in our studies. Our experimental results show that FEP performs much better than CEP for multi-modal functions with many local minima while being comparable to CEP in performance for unimodal and multi-modal functions with only a few local minima. We emphasise in the paper that no single algorithm can be the best for all problems. What we need is to identify the relationship between an algorithm and a class of problems which are most amenable to the algorithm."
            },
            "slug": "Fast-Evolutionary-Programming-Yao-Liu",
            "title": {
                "fragments": [],
                "text": "Fast Evolutionary Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper proposes a fast EP (FEP) which uses a Cauchy instead of Gaussian mutation operator as the primary search operator and shows that FEP performs much better than CEP for multi-modal functions with many local minima while being comparable to CEP in performance for unimodal and multi- modal function optimisation problems with only a fewLocal minima."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Programming"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107782112"
                        ],
                        "name": "Z. Guo",
                        "slug": "Z.-Guo",
                        "structuredName": {
                            "firstName": "Z.",
                            "lastName": "Guo",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3243274"
                        ],
                        "name": "R. Uhrig",
                        "slug": "R.-Uhrig",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Uhrig",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Uhrig"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 64
                            }
                        ],
                        "text": "EA\u2019s have been used to perform such a search effectively [267], [277]\u2013[287]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 110602694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e4406d506b2320fdad34f41a79d16f145762ac02",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The application of neural networks to nuclear power plants for fault diagnostics is a very challenging task. How to select proper input variables for neural networks from hundreds of plant processing variables is crucially important to the success. Genetic algorithms are used in this study to guide the search for optimal combination of inputs for the neural networks to reach the criteria of fewer inputs, faster training, and more accurate recall. Data from Tennessee Valley Authority (TVA) Watts Bar Nuclear Power Plant simulator are used to demonstrate the potential applications of genetic algorithms and neural networks to nuclear power plants.<<ETX>>"
            },
            "slug": "Using-genetic-algorithms-to-select-inputs-for-Guo-Uhrig",
            "title": {
                "fragments": [],
                "text": "Using genetic algorithms to select inputs for neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Genetic algorithms are used in this study to guide the search for optimal combination of inputs for the neural networks to reach the criteria of fewer inputs, faster training, and more accurate recall."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] COGANN-92: International Workshop on Combinations of Genetic Algorithms and Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37642115"
                        ],
                        "name": "A. Sebald",
                        "slug": "A.-Sebald",
                        "structuredName": {
                            "firstName": "Anthony",
                            "lastName": "Sebald",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Sebald"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1809791"
                        ],
                        "name": "K. Chellapilla",
                        "slug": "K.-Chellapilla",
                        "structuredName": {
                            "firstName": "Kumar",
                            "lastName": "Chellapilla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Chellapilla"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 46
                            }
                        ],
                        "text": "YAO: EVOLVING ARTIFICIAL NEURAL NETWORKS 1433\nSebald and Chellapilla [242] used the evolution of node transfer function as an example to show the importance of evolving representations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 23
                            }
                        ],
                        "text": "Sebald and Chellapilla [242] used the evolution of node transfer function as an example to show the importance of evolving representations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7434432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c01b645f69ff8888fee28454cd599b9c52d1ca4a",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "The idea of evolutionary friendliness recognizes that problem representations have a significant impact on the performance of evolutionary algorithms. There are two aspects of these representations. Different solution schemes exploit different natural symmetries. Very commonly, problems also possess symmetries that are determined by the coordinate systems used to represent them. Solution symmetries are typically specified by the user and are not allowed to evolve. The problem coordinate system is again typically chosen by the user and not evolved. In this first paper, the most appropriate solution symmetry is evolved. In the second paper, the coordinate system is evolved. In this paper, common detection problems with decision boundaries that possess special symmetries are solved using an evolutionary programming (EP) framework that is capable of exploiting these symmetries to quickly generate solutions. In particular, neural networks possessing appropriate symmetries are evolved by optimizing both their bases and their parameters. Simulation results indicate that the EP procedure is capable of selecting appropriate basis functions for different regions of the input space as well as optimizing the associated set of parameters."
            },
            "slug": "On-Making-Problems-Evolutionarily-Friendly-Part-1:-Sebald-Chellapilla",
            "title": {
                "fragments": [],
                "text": "On Making Problems Evolutionarily Friendly - Part 1: Evolving the Most Convenient Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Simulation results indicate that the EP procedure is capable of selecting appropriate basis functions for different regions of the input space as well as optimizing the associated set of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Programming"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143787550"
                        ],
                        "name": "E. Vonk",
                        "slug": "E.-Vonk",
                        "structuredName": {
                            "firstName": "E.",
                            "lastName": "Vonk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Vonk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687772"
                        ],
                        "name": "L. Jain",
                        "slug": "L.-Jain",
                        "structuredName": {
                            "firstName": "Lakhmi",
                            "lastName": "Jain",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108926187"
                        ],
                        "name": "Richard C. Johnson",
                        "slug": "Richard-C.-Johnson",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Johnson",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Richard C. Johnson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 206,
                                "start": 201
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 36
                            }
                        ],
                        "text": "in chromosomes [151], [168], [184], [205], [230], [232]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 13176050,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4fbdd1cce5694a22c283c488a866875270d83ee",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Kitano's approach to neural network design is extended in the sense that not just the neural network structure, but also the values of the weights are coded in the chromosome. Experimental results are presented demonstrating the capability of the technique in the solution of a standard test problem."
            },
            "slug": "Using-genetic-algorithms-with-grammar-encoding-to-Vonk-Jain",
            "title": {
                "fragments": [],
                "text": "Using genetic algorithms with grammar encoding to generate neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Kitano's approach to neural network design is extended in the sense that not just the neural network structure, but also the values of the weights are coded in the chromosome."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of ICNN'95 - International Conference on Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3363621"
                        ],
                        "name": "M. Kamo",
                        "slug": "M.-Kamo",
                        "structuredName": {
                            "firstName": "Masashi",
                            "lastName": "Kamo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kamo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46292154"
                        ],
                        "name": "Takuya Kubo",
                        "slug": "Takuya-Kubo",
                        "structuredName": {
                            "firstName": "Takuya",
                            "lastName": "Kubo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takuya Kubo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3148737"
                        ],
                        "name": "Y. Iwasa",
                        "slug": "Y.-Iwasa",
                        "structuredName": {
                            "firstName": "Yoh",
                            "lastName": "Iwasa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Iwasa"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 85245618,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "fd717cc4a5399ac549da93be5035c1e303caeb39",
            "isKey": false,
            "numCitedBy": 23,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In some animals, males evolve exaggerated traits (e.g. the peacock's conspicuous tail and display) because of female preference. Recently Enquist and Arak presented a simple neural network model for a visual system in female birds that acquires the ability to discriminate males of the correct species from those of the wrong species by training. They reported that the trained networks were attracted by 'supernormal stimuli' where there was a greater response to an exaggerated form than to the images used as the correct species for training. They suggested that signal recognition mechanisms have an inevitable bias in response, which in turn causes selection on signal form. We here examine the Enquist and Arak model in detail. A three-layered neural network is used to represent the female's mate preference, which consists of 6 by 6 receptor cells arranged on a regular square lattice, ten hidden cells, and one output cell. Connection weights of the network were modified by a genetic algorithm, in which the female's fitness increases if she accepts a conspecific male but decreases if she accepts a male of a different species or a random image. We found that: (i) after the training period the evolved network was able to discriminate male images. Female preference evolves to favour unfamiliar patterns if they are similar to the images of the correct species (generalization); (ii) the speed and the final degree of learning depended critically on the choice of the random images that are rejected. The learning was much less successful if the random images were changed every generation than if 20 random images were fixed throughout the training period; (iii) the male of the same species used for training achieved the highest probability of being accepted by the trained network. Hence, contrary to Enquist and Arak, the evolved network was not attracted by supernormal stimuli."
            },
            "slug": "Neural-network-for-female-mate-preference,-trained-Kamo-Kubo",
            "title": {
                "fragments": [],
                "text": "Neural network for female mate preference, trained by a genetic algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "A simple neural network model for a visual system in female birds that acquires the ability to discriminate males of the correct species from those of the wrong species by training was presented and found that after the training period the evolved network was able to discriminate male images."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157250502"
                        ],
                        "name": "Yan Chen",
                        "slug": "Yan-Chen",
                        "structuredName": {
                            "firstName": "Yan",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yan Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "92820519"
                        ],
                        "name": "M. Narita",
                        "slug": "M.-Narita",
                        "structuredName": {
                            "firstName": "Masakuni",
                            "lastName": "Narita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Narita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3170168"
                        ],
                        "name": "Takayoshi Yamada",
                        "slug": "Takayoshi-Yamada",
                        "structuredName": {
                            "firstName": "Takayoshi",
                            "lastName": "Yamada",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takayoshi Yamada"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 109664632,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0f7f75773da81e7d3eb68277f47185bd21d9a28",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Several nuclear reactor diagnostic systems using neural networks have been proposed in recent years. Neural networks trained by backpropagation, the standard training algorithm, have certain problems such as local minima and long training times. \n \nIn this paper, neural networks trained by genetic algorithms are used in a nuclear reactor diagnostic system to solve these problems. The system is tested by simulated data modeled on the experimental fast reactor JOYO, and two categories of abnormality (abnormal control rod vibration and abnormal coolant flow) are identified. The comparisons to networks trained by back-propagation also are discussed."
            },
            "slug": "Nuclear-reactor-diagnostic-system-using-genetic-Chen-Narita",
            "title": {
                "fragments": [],
                "text": "Nuclear reactor diagnostic system using genetic algorithm (GA)\u2010trained neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "In this paper, neural networks trained by genetic algorithms are used in a nuclear reactor diagnostic system to solve problems such as local minima and long training times."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121091352"
                        ],
                        "name": "C. P\u00e9rez",
                        "slug": "C.-P\u00e9rez",
                        "structuredName": {
                            "firstName": "Claudio",
                            "lastName": "P\u00e9rez",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. P\u00e9rez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "121936990"
                        ],
                        "name": "C. Holzmann",
                        "slug": "C.-Holzmann",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Holzmann",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Holzmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 213,
                                "start": 208
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 57772628,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aaa150bf5852255614ea4a75cc86a307d43ef81a",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper presents two ways of improving the handwritten digit recognition ability of a neural network. First by selection of the number of hidden units in the neural network, and second, by training using an augmented set of patterns. The handwritten digit recognition application is performed on feed-forward, fully connected neural networks with two hidden layer architectures. A genetic algorithm is used to search among configurations of two unequal hidden layer networks to find the optimum number of hidden units. Training procedures involving augmented sets of training patterns are produced by two methods: by shifting and by magnification every handwritten digit of the original training set. Results show that the best two hidden layer network, 178/spl times/26 units, trained for 10 different random starting weight sets in centered mode, i.e., no shifting, resulted in 82.74% average recognition rate with standard deviation, STD=0.66. The best two hidden layer network, 154/spl times/58, trained for 11 different random starting weight sets in shifting mode, resulted in 92.09% average recognition rate with STD=0.37. A comparison of recognition rates for centered versus shifting training modes for the genetic selection resulted statistically significant with p<0.001."
            },
            "slug": "Improvements-on-handwritten-digit-recognition-by-of-P\u00e9rez-Holzmann",
            "title": {
                "fragments": [],
                "text": "Improvements on handwritten digit recognition by genetic selection of neural network topology and by augmented training"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Two ways of improving the handwritten digit recognition ability of a neural network are presented, first by selection of the number of hidden units in the neural network, and second, by training using an augmented set of patterns."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 91
                            }
                        ],
                        "text": "The local search algorithm could be BP [32], [133] or other random search algorithms [30], [135]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53880993,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eeeafa833b771b8b71a74781569a56565d181609",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Simulated Annealing (SA) is a general stochastic search algorithm. It is usually employed as an optimization method to nd a near optimal solution for hard combinatorial optimization problems , but it is very diicult to give the accuracy of the solution found. In order to nd a better solution, an often used strategy is to run the algorithm many times and select the best solution as the nal one. This paper gives an algorithm called Genetic Annealing (GA), which connects each run of SA and gradually improve the solution. It introduces the concept of evolution into the annealing process. The basic idea is to use genetic operations adopted in genetic algorithms to inherit the possible beneets of the solutions found in former runs. Experiments have shown that GA is better than classical SA. The paral-lelization of GA is also discussed in the paper."
            },
            "slug": "Optimization-by-Genetic-Annealing-Yao",
            "title": {
                "fragments": [],
                "text": "Optimization by Genetic Annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An algorithm called Genetic Annealing is given, which connects each run of SA and gradually improve the solution, and introduces the concept of evolution into the annealing process."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771100"
                        ],
                        "name": "N. Funabiki",
                        "slug": "N.-Funabiki",
                        "structuredName": {
                            "firstName": "Nobuo",
                            "lastName": "Funabiki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Funabiki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057718"
                        ],
                        "name": "J. Kitamichi",
                        "slug": "J.-Kitamichi",
                        "structuredName": {
                            "firstName": "Junji",
                            "lastName": "Kitamichi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kitamichi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145852999"
                        ],
                        "name": "Seishi Nishikawa",
                        "slug": "Seishi-Nishikawa",
                        "structuredName": {
                            "firstName": "Seishi",
                            "lastName": "Nishikawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seishi Nishikawa"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 185
                            }
                        ],
                        "text": "Many others used EA\u2019s and ANN\u2019s for combinatorial or global (numerical) optimization in order to combine EA\u2019s global search capability with ANN\u2019s fast convergence to local optima [308]\u2013[318]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123021615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e9ad892ce0c06f8f1a96180b25074c1000c0aa23",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An \"evolutionary neural network (ENN)\" is presented for the max cut problem of an undirected graph G(V, E) in this paper. The goal of the NP-hard problem is to find a partition of V into two disjoint subsets such that the cut size be maximized. The cut size is the sum of weights on edges in E whose endpoints belong to different subsets. The ENN combines the evolutionary initialization scheme of the neural state into the energy minimization criteria of the binary neural network. The performance of ENN is evaluated through simulations in randomly weighted complete graphs and unweighted random graphs with up to 1000 vertices. The results show that the evolutionary initialization scheme drastically improves the solution quality. ENN can always find better solutions than the maximum neural network, the mean field annealing, the simulated annealing, and the greedy algorithm."
            },
            "slug": "An-evolutionary-neural-network-algorithm-for-max-Funabiki-Kitamichi",
            "title": {
                "fragments": [],
                "text": "An evolutionary neural network algorithm for max cut problems"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "An \"evolutionary neural network (ENN)\" is presented for the max cut problem of an undirected graph G(V, E) and the results show that the evolutionary initialization scheme drastically improves the solution quality."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48175537"
                        ],
                        "name": "Jia Lei",
                        "slug": "Jia-Lei",
                        "structuredName": {
                            "firstName": "Jia",
                            "lastName": "Lei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jia Lei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70150691"
                        ],
                        "name": "Guangdong He",
                        "slug": "Guangdong-He",
                        "structuredName": {
                            "firstName": "Guangdong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guangdong He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118240389"
                        ],
                        "name": "J. Jiang",
                        "slug": "J.-Jiang",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Jiang",
                            "middleNames": [
                                "Ping"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Jiang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 48
                            }
                        ],
                        "text": "recurrent ANN\u2019s [41], [60], [65], [100], [102], [103], [106],"
                    },
                    "intents": []
                }
            ],
            "corpusId": 61569544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d4ea3160274b1791fae4995698b0ff54c2ca5df",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The CSTR system (continuous stirred tank reactor system) is a typical nonlinear system. At present, one of its states, reaction consistence, can not be measured. In this paper, a recurrent neural network is used to estimate the value of the state. Nevertheless, due to the strong nonlinearity of the system, traditional training method such as BP algorithm usually converges in local optimum. Genetic algorithms (GAs), as a global optimization search method, can solve the problem, but the conventional GAs converge very slowly. To improve the learning speed of the neural network, a hybrid genetic algorithm (HGA) is employed. The results demonstrate the proposed HGA can get a very good effect."
            },
            "slug": "The-state-estimation-of-the-CSTR-system-based-on-a-Lei-He",
            "title": {
                "fragments": [],
                "text": "The state estimation of the CSTR system based on a recurrent neural network trained by HGAs"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "To improve the learning speed of the neural network, a hybrid genetic algorithm (HGA) is employed and the results demonstrate the proposed HGA can get a very good effect."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513927"
                        ],
                        "name": "C. Dagli",
                        "slug": "C.-Dagli",
                        "structuredName": {
                            "firstName": "Cihan",
                            "lastName": "Dagli",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dagli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1710428"
                        ],
                        "name": "P. Poshyanonda",
                        "slug": "P.-Poshyanonda",
                        "structuredName": {
                            "firstName": "Pipatpong",
                            "lastName": "Poshyanonda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Poshyanonda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8939563,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6afe763687f1f69ef385da2c260f02d31616937",
            "isKey": false,
            "numCitedBy": 67,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "In this study, two approaches are explored for the solution of the rectangular stock cutting problem: neuro-optimization, which integrates artificial neural networks and optimization methods; and genetic neuro-nesting, which combines artificial neural networks and genetic algorithms. In the first approach, an artificial neural network architecture is used to generate rectangular pattern configurations, to be used by the optimization model, with an acceptable scrap. Rectangular patterns of different sizes are selected as input to the network to generate the location and rotation of each pattern after they are combined. A mathematical programming model is used to determine the nesting of different sizes of rectangular patterns to meet the demand for rectangular blanks for a given planning horizon. The test data used in this study is generated randomly from a specific normal distribution. The average scrap percentage obtained is within acceptable limits. In the second approach, a genetic algorithm is used to generate sequences of the input patterns to be allocated on a finite width with infinite-length material. Each gene represents the sequence in which the patterns are to be allocated using the allocation algorithm developed. The scrap percentage of each allocation is used as an evaluation criterion for each gene for determining the best allocation while considering successive generations. The allocation algorithm uses the sliding method integrated with an artificial neural network based on the adaptive resonance theory (ART1) paradigm to allocate the patterns according to the sequence generated by the genetic algorithm. It slides an incoming pattern next to the allocated ones and keeps all scrap areas produced, which can be utilized in allocating a new pattern through the ART1 network. If there is a possible match with an incoming pattern and one of the scrap areas, the neural network selects the best match area and assigns the pattern. Both approaches gave satisfactory results. The second approach generated nests having packing densities in the range 95\u201397%. Improvement in packing densities was possible at the expense of excessive computational time. Parallel implementation of this unconventional approach could well bring a quick and satisfactory solution to this classical problem."
            },
            "slug": "New-approaches-to-nesting-rectangular-patterns-Dagli-Poshyanonda",
            "title": {
                "fragments": [],
                "text": "New approaches to nesting rectangular patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "Two approaches are explored for the solution of the rectangular stock cutting problem: neuro-optimization, which integrates artificial neural networks and optimization methods; and genetic neuro-nesting, which combines artificial Neural networks and genetic algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Intell. Manuf."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2495230"
                        ],
                        "name": "Jacek Jarmulak",
                        "slug": "Jacek-Jarmulak",
                        "structuredName": {
                            "firstName": "Jacek",
                            "lastName": "Jarmulak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacek Jarmulak"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746390"
                        ],
                        "name": "P. Spronck",
                        "slug": "P.-Spronck",
                        "structuredName": {
                            "firstName": "Pieter",
                            "lastName": "Spronck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Spronck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3307476"
                        ],
                        "name": "E. Kerckhoffs",
                        "slug": "E.-Kerckhoffs",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Kerckhoffs",
                            "middleNames": [
                                "J.",
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Kerckhoffs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62666366,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f6505f77abad45b3fcc8f6281f9a11b737ab00f7",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-in-process-control:-model-based-and-Jarmulak-Spronck",
            "title": {
                "fragments": [],
                "text": "Neural networks in process control: model-based and reinforcement trained controllers"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754328"
                        ],
                        "name": "D. Hush",
                        "slug": "D.-Hush",
                        "structuredName": {
                            "firstName": "Don",
                            "lastName": "Hush",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35216199"
                        ],
                        "name": "B. Horne",
                        "slug": "B.-Horne",
                        "structuredName": {
                            "firstName": "Bill",
                            "lastName": "Horne",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Horne"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 75
                            }
                        ],
                        "text": "A detailed review of BP and other learning algorithms can be found in [7], [17], and [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "Most training algorithms, such as BP and conjugate gradient algorithms [7], [17]\u2013[19], are based on gradient descent."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3191120,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "56e02786bad4cf8781950b5df615729a417b31d7",
            "isKey": false,
            "numCitedBy": 1263,
            "numCiting": 148,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical results concerning the capabilities and limitations of various neural network models are summarized, and some of their extensions are discussed. The network models considered are divided into two basic categories: static networks and dynamic networks. Unlike static networks, dynamic networks have memory. They fall into three groups: networks with feedforward dynamics, networks with output feedback, and networks with state feedback, which are emphasized in this work. Most of the networks discussed are trained using supervised learning.<<ETX>>"
            },
            "slug": "Progress-in-supervised-neural-networks-Hush-Horne",
            "title": {
                "fragments": [],
                "text": "Progress in supervised neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Theoretical results concerning the capabilities and limitations of various neural network models are summarized, and some of their extensions are discussed."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Signal Processing Magazine"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1768336"
                        ],
                        "name": "I. Erkmen",
                        "slug": "I.-Erkmen",
                        "structuredName": {
                            "firstName": "Ismet",
                            "lastName": "Erkmen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Erkmen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2079273698"
                        ],
                        "name": "A. Ozdogan",
                        "slug": "A.-Ozdogan",
                        "structuredName": {
                            "firstName": "Ahmet",
                            "lastName": "Ozdogan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Ozdogan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 37
                            }
                        ],
                        "text": "Lee [81] and many others [32], [136]\u2013[138] used GA\u2019s to search for a near-optimal set of initial connection weights and then used BP to perform local search from these initial weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 28
                            }
                        ],
                        "text": "[118], [127], [128], [130], [138], [149]\u2013[225]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 61902075,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5be0474633d917d5dd1427848fe3735aa0e03b90",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A new intelligent approach is developed for short-term load forecasting (STLF). The technique consists of three basic modules. The first module employs the clustering of daily load curves using a modified Kohonen algorithm (MKA). The second module determines the most appropriate supervised neural network topology and associated initial weight values for each cluster extracted from a historical database, by using a genetic algorithm (GA). In the third module, a genetically optimized three-layered backpropagation (BP) network is trained and run to perform hourly load forecasting. The effects of each module on the forecasting accuracy are considered separately. The proposed system is tested extensively with the load curves of the Turkish electrical power system in 1993 using different day types from different times of the year, and promising results are obtained with approximately 1% mean error for days distributed throughout the year."
            },
            "slug": "Short-term-load-forecasting-using-genetically-with-Erkmen-Ozdogan",
            "title": {
                "fragments": [],
                "text": "Short term load forecasting using genetically optimized neural network cascaded with a modified Kohonen clustering process"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new intelligent approach is developed for short-term load forecasting (STLF), which employs the clustering of daily load curves using a modified Kohonen algorithm and a genetically optimized three-layered backpropagation (BP) network is trained and run to perform hourly load forecasting."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 12th IEEE International Symposium on Intelligent Control"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153843799"
                        ],
                        "name": "S. Yao",
                        "slug": "S.-Yao",
                        "structuredName": {
                            "firstName": "Susu",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34751306"
                        ],
                        "name": "Chengjian Wei",
                        "slug": "Chengjian-Wei",
                        "structuredName": {
                            "firstName": "Chengjian",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chengjian Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72181620"
                        ],
                        "name": "Zuhong He",
                        "slug": "Zuhong-He",
                        "structuredName": {
                            "firstName": "Zuhong",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zuhong He"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 119982288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ab6d947e7c562b38584d5e305559ad2b5f5341c",
            "isKey": false,
            "numCitedBy": 44,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "A wavelet-based neural network with evolutionary programming is proposed. Unlike conventional backpropagation training algorithms, the evolutionary programming does not require gradient information and can provide a stochastic optimal search. Evolutionary programming is applied to optimise the wavelet neural network for function approximation. Experimental results are presented to show the potential of the evolutionary wavelet neural networks."
            },
            "slug": "Evolving-wavelet-neural-networks-for-function-Yao-Wei",
            "title": {
                "fragments": [],
                "text": "Evolving wavelet neural networks for function approximation"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A wavelet-based neural network with evolutionary programming is proposed that can provide a stochastic optimal search and experimental results are presented to show the potential of the evolutionary wavelet neural networks."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "72707179"
                        ],
                        "name": "B. Deo",
                        "slug": "B.-Deo",
                        "structuredName": {
                            "firstName": "Brahma",
                            "lastName": "Deo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Deo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "69364087"
                        ],
                        "name": "A. Datta",
                        "slug": "A.-Datta",
                        "structuredName": {
                            "firstName": "Amlan",
                            "lastName": "Datta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Datta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97891746"
                        ],
                        "name": "Basant Kukreja",
                        "slug": "Basant-Kukreja",
                        "structuredName": {
                            "firstName": "Basant",
                            "lastName": "Kukreja",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Basant Kukreja"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46801066"
                        ],
                        "name": "R. Rastogi",
                        "slug": "R.-Rastogi",
                        "structuredName": {
                            "firstName": "Ravi",
                            "lastName": "Rastogi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Rastogi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145080287"
                        ],
                        "name": "K. Deb",
                        "slug": "K.-Deb",
                        "structuredName": {
                            "firstName": "Kalyanmoy",
                            "lastName": "Deb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Deb"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 110248441,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21d9c158f28e9fb992d1e7d370cdc6d32b34466d",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Adaptive neural net (ANN) model of hot metal desulphurization is first optimized by various search methods including the golden section search and Davies-Swann-Campey methods. Logarithmic preprocessing of input data leads to a further improvement in generalizaton ability of the net. Genetic adaptive search (GAS) method is used to optimize the mathematical model for desulphurization and when the input data are preprocessed with this optimized model and fed into an artificial neural net, the generalization ability of the net becomes even better. Best results are obtained when using GAS to optimize the interconnection weights during the training phase, while training data are proprocessed through a mathematical model already optimized by GAS."
            },
            "slug": "Optimization-of-back-propagation-algorithm-and-ANN-Deo-Datta",
            "title": {
                "fragments": [],
                "text": "Optimization of back propagation algorithm and GAS-assisted ANN models for hot metal desulphurization"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Genetic adaptive search (GAS) method is used to optimize the mathematical model for desulphurization and when the input data are preprocessed with this optimized model and fed into an artificial neural net, the generalization ability of the net becomes even better."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40371087"
                        ],
                        "name": "E. Johansson",
                        "slug": "E.-Johansson",
                        "structuredName": {
                            "firstName": "Erik",
                            "lastName": "Johansson",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Johansson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50247236"
                        ],
                        "name": "F. Dowla",
                        "slug": "F.-Dowla",
                        "structuredName": {
                            "firstName": "Farid",
                            "lastName": "Dowla",
                            "middleNames": [
                                "U."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Dowla"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46821262"
                        ],
                        "name": "D. Goodman",
                        "slug": "D.-Goodman",
                        "structuredName": {
                            "firstName": "Dennis",
                            "lastName": "Goodman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Goodman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 137
                            }
                        ],
                        "text": "Evolutionary training can be slow for some problems in comparison with fast variants of BP [131] and conjugate gradient algorithms [19], [132]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 40190917,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "74a063a505ec68d7e6558abbf8ead5b783074c90",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In many applications, the number of interconnects or weights in a neural network is so large that the learning time for the conventional backpropagation algorithm can become excessively long. Numerical optimization theory offers a rich and robust set of techniques which can be applied to neural networks to improve learning rates. In particular, the conjugate gradient method is easily adapted to the backpropagation learning problem. This paper describes the conjugate gradient method, its application to the backpropagation learning problem and presents results of numerical tests which compare conventional backpropagation, steepest descent and the conjugate gradient methods. For the parity problem, we find that the conjugate gradient method is an order of magnitude faster than conventional backpropagation with momentum."
            },
            "slug": "Backpropagation-Learning-for-Multilayer-Neural-the-Johansson-Dowla",
            "title": {
                "fragments": [],
                "text": "Backpropagation Learning for Multilayer Feed-Forward Neural Networks Using the Conjugate Gradient Method"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "For the parity problem, it is found that the conjugate gradient method is an order of magnitude faster than conventional backpropagation with momentum."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2706899"
                        ],
                        "name": "M. Gargano",
                        "slug": "M.-Gargano",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Gargano",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gargano"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859506"
                        ],
                        "name": "R. Marose",
                        "slug": "R.-Marose",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Marose",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Marose"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150195921"
                        ],
                        "name": "L. von Kleeck",
                        "slug": "L.-von-Kleeck",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "von Kleeck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. von Kleeck"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61366198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3299b702174a6b0044013d6f0d4bc97b5cfdf057",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Personnel selection in the financial industry can improve the performance of this critical corporate resource when done properly. This paper introduces a neural net classifier to aid this selection process. A novel application of a genetic algorithm to speed up the neural learning process is presented.<<ETX>>"
            },
            "slug": "An-application-of-artificial-neural-networks-and-to-Gargano-Marose",
            "title": {
                "fragments": [],
                "text": "An application of artificial neural networks and genetic algorithms to personnel selection in the financial industry"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "A neural net classifier is introduced to aid personnel selection in the financial industry and a novel application of a genetic algorithm to speed up the neural learning process is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings First International Conference on Artificial Intelligence Applications on Wall Street"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1793453"
                        ],
                        "name": "B. Back",
                        "slug": "B.-Back",
                        "structuredName": {
                            "firstName": "Barbro",
                            "lastName": "Back",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Back"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2492338"
                        ],
                        "name": "Teija Laitinen",
                        "slug": "Teija-Laitinen",
                        "structuredName": {
                            "firstName": "Teija",
                            "lastName": "Laitinen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Teija Laitinen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691231"
                        ],
                        "name": "K. Sere",
                        "slug": "K.-Sere",
                        "structuredName": {
                            "firstName": "Kaisa",
                            "lastName": "Sere",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sere"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62563555,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "33575dd0c972719758318e3db91f980bc46d6f39",
            "isKey": false,
            "numCitedBy": 195,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Neural-networks-and-genetic-algorithms-for-Back-Laitinen",
            "title": {
                "fragments": [],
                "text": "Neural networks and genetic algorithms for bankruptcy predictions"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157210358"
                        ],
                        "name": "Ping Zhang",
                        "slug": "Ping-Zhang",
                        "structuredName": {
                            "firstName": "Ping",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ping Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741417"
                        ],
                        "name": "Y. Sankai",
                        "slug": "Y.-Sankai",
                        "structuredName": {
                            "firstName": "Yoshiyuki",
                            "lastName": "Sankai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Sankai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052832181"
                        ],
                        "name": "M. Ohta",
                        "slug": "M.-Ohta",
                        "structuredName": {
                            "firstName": "Michio",
                            "lastName": "Ohta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ohta"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 234
                            }
                        ],
                        "text": "Because EA\u2019s can treat large, complex, nondifferentiable, and multimodal spaces, which are the typical case in the real world, considerable research and application has been conducted on the evolution of connection weights [24], [26]\u2013[112]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 60448373,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38343ae2edf81f738ecd9ebccc05fbba5b259026",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "A hybrid adaptive learning control for nonlinear dynamical systems is proposed. Feedforward multilayer neural networks are used to construct a controller. Parameters of the neural networks are adjusted by a dynamic backpropagation algorithm and a genetic algorithm. The genetic algorithm manages to escape local minima and reach the neighborhood of the global minimum on the squared error surface. The dynamic backpropagation algorithm is used to search the global minimum from its neighborhood. Computer simulations show that the tracking control performance of nonlinear dynamical systems can be enhanced by the proposed method."
            },
            "slug": "Hybrid-adaptive-learning-control-of-nonlinear-Zhang-Sankai",
            "title": {
                "fragments": [],
                "text": "Hybrid adaptive learning control of nonlinear system"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Computer simulations show that the tracking control performance of nonlinear dynamical systems can be enhanced by the proposed hybrid adaptive learning control method."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 American Control Conference - ACC'95"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1448287667"
                        ],
                        "name": "Shyh-Jier-Huang",
                        "slug": "Shyh-Jier-Huang",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Shyh-Jier-Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shyh-Jier-Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47396617"
                        ],
                        "name": "C. Huang",
                        "slug": "C.-Huang",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Huang",
                            "middleNames": [
                                "Lien"
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 109993122,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2fc04a8a36ae1ed0bc8be1143a8afe9adf5d2a83",
            "isKey": false,
            "numCitedBy": 77,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "A new approach using genetic algorithms based neural networks and dynamic programming (GANN-DP) to solve power system unit commitment problems is proposed in this paper. A set of feasible generator commitment schedules is first formulated by genetic-enhanced neural networks. These pre-committed schedules are then optimized by the dynamic programming technique. By the proposed approach, learning stagnation is avoided. The neural network stability and accuracy are significantly increased. The computational performance of unit commitment in a power system is therefore highly improved. The proposed method has been tested on a practical Taiwan Power (Taipower) thermal system through the utility data. The results demonstrate the feasibility and practicality of this approach."
            },
            "slug": "Application-of-genetic-based-neural-networks-to-Shyh-Jier-Huang-Huang",
            "title": {
                "fragments": [],
                "text": "Application of genetic-based neural networks to thermal unit commitment"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "By the proposed approach, learning stagnation is avoided, the neural network stability and accuracy are significantly increased, and the computational performance of unit commitment in a power system is therefore highly improved."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1718584"
                        ],
                        "name": "P. Angeline",
                        "slug": "P.-Angeline",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Angeline",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Angeline"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 78
                            }
                        ],
                        "text": "There has been some work in this area where good results were reported [119], [120], [245]\u2013[253]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 182
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 122709268,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b31adf7ca1027b2919c65e642cd357e0d692acc0",
            "isKey": false,
            "numCitedBy": 9,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks using radial basis functions (RBFs) are a popular representation for inducing classification schemes. However, RBF neural networks often require a large number of hidden units (basis functions) in order to adequately model the class distinctions. This is due to the static nature of each basis function. This paper uses an evolutionary program to induce dynamic basis functions whose receptive fields are dependent on the input vector. This technique requires only a single basis function per class to perform on par with RBF networks."
            },
            "slug": "Evolving-basis-functions-with-dynamic-receptive-Angeline",
            "title": {
                "fragments": [],
                "text": "Evolving basis functions with dynamic receptive fields"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "An evolutionary program is used to induce dynamic basis functions whose receptive fields are dependent on the input vector and requires only a single basis function per class to perform on par with RBF networks."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144441192"
                        ],
                        "name": "M. A. Lewis",
                        "slug": "M.-A.-Lewis",
                        "structuredName": {
                            "firstName": "M.",
                            "lastName": "Lewis",
                            "middleNames": [
                                "Anthony"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1712136"
                        ],
                        "name": "A. Fagg",
                        "slug": "A.-Fagg",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Fagg",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Fagg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1927968"
                        ],
                        "name": "Alan Solidum",
                        "slug": "Alan-Solidum",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Solidum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alan Solidum"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 12833418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "908f836b4d2a019dcb72acbd07004b03a9d27210",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "The authors describe the staged evolution of a complex motor pattern generator (MPG) for the control of a walking robot. The experiments were carried out on a six-legged, Brooks-style insect robot. The MPG was composed of a network of neurons with weights determined by genetic algorithm optimization. Staged evolution was used to improve the convergence rate of the algorithm. First, an oscillator for the individual leg movements was evolved. Then, a network of these oscillators was evolved to coordinate the movements of the different legs. By introducing a staged set of manageable challenges, the algorithm's performance was improved.<<ETX>>"
            },
            "slug": "Genetic-programming-approach-to-the-construction-of-Lewis-Fagg",
            "title": {
                "fragments": [],
                "text": "Genetic programming approach to the construction of a neural network for control of a walking robot"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The authors describe the staged evolution of a complex motor pattern generator (MPG) for the control of a walking robot that was composed of a network of neurons with weights determined by genetic algorithm optimization."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1992 IEEE International Conference on Robotics and Automation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762202"
                        ],
                        "name": "D. Parisi",
                        "slug": "D.-Parisi",
                        "structuredName": {
                            "firstName": "Domenico",
                            "lastName": "Parisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Parisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1896018"
                        ],
                        "name": "F. Cecconi",
                        "slug": "F.-Cecconi",
                        "structuredName": {
                            "firstName": "Federico",
                            "lastName": "Cecconi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Cecconi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015062"
                        ],
                        "name": "S. Nolfi",
                        "slug": "S.-Nolfi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Nolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nolfi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 48
                            }
                        ],
                        "text": "did not evolve learning rules explicitly [260], [275]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 123110150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fbc2a63a3ef19f6c0ce0148323dd9493ba7c9e8e",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Ecological networks are networks that learn in an environment. It is the environment, and not the researcher, that determines the conditions in which learning takes place such as which input patterns are seen, what the teaching input is, etc. Furthermore, input patterns at time N+1 are often a function of the output of the network at time N. Two hypotheses are explored with reference to ecological networks. One is that predicting the sensory consequences (input) for an organism of the organism's actions (output) on the environment is one of the basic tasks of this type of network\u2014basic for constructing an environmental map or world model. The other is that learning to predict the sensory consequences of the organism's actions favourably predisposes the organism to learn to attain goals with those actions. Some data from simulations that support these two hypotheses are reported."
            },
            "slug": "Econets:-Neural-networks-that-learn-in-an-Parisi-Cecconi",
            "title": {
                "fragments": [],
                "text": "Econets: Neural networks that learn in an environment"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Two hypotheses are explored with reference to ecological networks: that predicting the sensory consequences for an organism of the organism's actions (output) on the environment is one of the basic tasks of this type of network\u2014basic for constructing an environmental map or world model."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "28617572"
                        ],
                        "name": "A. Schultz",
                        "slug": "A.-Schultz",
                        "structuredName": {
                            "firstName": "Abraham",
                            "lastName": "Schultz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Schultz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143979395"
                        ],
                        "name": "H. Wechsler",
                        "slug": "H.-Wechsler",
                        "structuredName": {
                            "firstName": "Harry",
                            "lastName": "Wechsler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Wechsler"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 55
                            }
                        ],
                        "text": "recurrent ANN\u2019s [41], [60], [65], [100], [102], [103], [106],"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 154
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 58782449,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b879ac5609fe3110222ac33c5bb08056211d95fd",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Pattern recognition systems commonly employ a single representation of the sensor data. For hard classification problems it is unlikely that a single representation will be able to capture all the relevant information in the sensor field. For a given input, the goal is to fuse the information contained in multiple representations to compute the associated pattern class. For each representation, the learning vector quantization network is first used to establish a transformation to an associated feature space. A recurrent network is then used to fuse the information generated by each of the representations. The weights for the recurrent network are learned using an evolutionary strategy. This network is multi-stable and its equilibrium states are associated with different pattern classes. For a specified input, the system relaxes to an equilibrium state associated with an underlying pattern class. The class decision boundaries generated by the recurrent neural network are compared to the boundaries generated by nearest neighbor recall.<<ETX>>"
            },
            "slug": "Data-fusion-in-neural-networks-via-computational-Schultz-Wechsler",
            "title": {
                "fragments": [],
                "text": "Data fusion in neural networks via computational evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "A recurrent neural network is used to fuse the information contained in multiple representations to compute the associated pattern class in pattern recognition systems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1738017"
                        ],
                        "name": "S. Wesolkowski",
                        "slug": "S.-Wesolkowski",
                        "structuredName": {
                            "firstName": "Slawomir",
                            "lastName": "Wesolkowski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Wesolkowski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145128671"
                        ],
                        "name": "K. Hassanein",
                        "slug": "K.-Hassanein",
                        "structuredName": {
                            "firstName": "Khaled",
                            "lastName": "Hassanein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Hassanein"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 126
                            }
                        ],
                        "text": "There have been some very successful experiments which show that EA\u2019s can be used to evolve ANN ensembles [192], [193], [302]\u2013[305]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56752082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5089cd38a61fbb70e5ee9ced89659477505be896",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Many researchers have been concerned with classifier combination techniques for complex pattern recognition problems such as digit recognition. Several combination approaches are compared with respect to achieving very high accuracy digit recognition. Both nonadaptive and adaptive combining schemes were considered. Non adaptive combiners studied included: majority voting, highest confidence, Borda count, and vector addition. Adaptive combination schemes investigated included: four different neural net combiners and weighted voting optimized with a genetic algorithm. These different schemes were tested on the task of combining three neural network digit classifiers, identical in architecture but trained using different training sets. Significant increases in recognition performance on blind test sets containing non digit blobs are reported in the region of very high accuracy (0.2% to. 1.0% error) for the neural network and genetic algorithm approaches on a database of digits extracted from financial documents."
            },
            "slug": "A-comparative-study-of-combination-schemes-for-an-Wesolkowski-Hassanein",
            "title": {
                "fragments": [],
                "text": "A comparative study of combination schemes for an ensemble of digit recognition neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Significant increases in recognition performance on blind test sets containing non digit blobs are reported in the region of very high accuracy for the neural network and genetic algorithm approaches on a database of digits extracted from financial documents."
            },
            "venue": {
                "fragments": [],
                "text": "1997 IEEE International Conference on Systems, Man, and Cybernetics. Computational Cybernetics and Simulation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765522"
                        ],
                        "name": "J. Sklansky",
                        "slug": "J.-Sklansky",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Sklansky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sklansky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3009428"
                        ],
                        "name": "M. Vriesenga",
                        "slug": "M.-Vriesenga",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Vriesenga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Vriesenga"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 207116803,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd2244e8e64da1db78da332c700b0d2c80aea8d9",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Piecewise-linear mathematical structures form a convenient and important framework for implementing trainable and adaptive pattern classifiers. Neural networks and genetic algorithms offer additional approaches with important benefits for the design of such classifiers. In this paper we show how neural modeling and genetic selection can be applied to piecewise-linear structures to optimize both the topology and the parameter values of the network forming the classifier. Such a classifier will tend to have a low error rate and high robustness. We describe applications of these techniques to an adaptive detector of abnormal tissue in mammograms and a detector of straight lines and edges in noisy aerial images."
            },
            "slug": "Genetic-Selection-and-Neural-Modeling-of-Sklansky-Vriesenga",
            "title": {
                "fragments": [],
                "text": "Genetic Selection and Neural Modeling of Piecewise-Linear Classifiers"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper shows how neural modeling and genetic selection can be applied to piecewise-linear structures to optimize both the topology and the parameter values of the network forming the classifier, which will tend to have a low error rate and high robustness."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Pattern Recognit. Artif. Intell."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46312376"
                        ],
                        "name": "K. Wu",
                        "slug": "K.-Wu",
                        "structuredName": {
                            "firstName": "Kun",
                            "lastName": "Wu",
                            "middleNames": [
                                "Hsiang"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2171477235"
                        ],
                        "name": "C. Chen",
                        "slug": "C.-Chen",
                        "structuredName": {
                            "firstName": "Chin",
                            "lastName": "Chen",
                            "middleNames": [
                                "Hsing"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146055693"
                        ],
                        "name": "Jiann-Der Lee",
                        "slug": "Jiann-Der-Lee",
                        "structuredName": {
                            "firstName": "Jiann-Der",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jiann-Der Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 75
                            }
                        ],
                        "text": "There are some other different EP-based systems for designing ANN\u2019s [128], [129], [217], [223], but none has been tested on as many different benchmark problems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62250554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fac4805936a1e734c9bfd6bf3cab40599568995",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a modular fuzzy neural network (MFNN) based on cache genetic learning process. In this model, we use cache genetic algorithm to generate the possible patterns of the structure and the parameters directly for the MFNN. Using the proposed cache genetic algorithm, small population can be held to speed up the genetic process from cache pool and keep chromosomes fresh by extracting new blood from auxiliary pool. A modular fuzzy neural network is able to learn the set of simpler functions faster than a multilayer perceptron can learn the undecomposed function in complex systems. Combined with the cache genetic algorithm and the modular neural network to synthesize the fuzzy logic controller, the performance is better than usual fuzzy neural networks. We use the proposed model to solve the problem of the robot path planning and compare it with the other methods to realize its performance by considering four factors: safety factor, smoothness factor, length factor, and time factor. Experiments results show the proposed model is superior than other approaches."
            },
            "slug": "A-cache-genetic-based-modular-fuzzy-neural-network-Wu-Chen",
            "title": {
                "fragments": [],
                "text": "A cache-genetic-based modular fuzzy neural network for robot path planning"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The proposed model is used to solve the problem of the robot path planning and compare it with the other methods to realize its performance by considering four factors: safety factor, smoothness factor, length factor, and time factor."
            },
            "venue": {
                "fragments": [],
                "text": "1996 IEEE International Conference on Systems, Man and Cybernetics. Information Intelligence and Systems (Cat. No.96CH35929)"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859405"
                        ],
                        "name": "David E. Moriarty",
                        "slug": "David-E.-Moriarty",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Moriarty",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David E. Moriarty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 23133387,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8e2ddb6c045dc7ad283373c06ef54f993e675f4d",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks were evolved to constrain minimax search in the game of Othello. At each level of the search tree, such focus networks decide which moves are to be explored. Based on the evolved knowledge of the minimax algorithm's advantages and limitations the networks hide problem nodes from minimax. Focus networks were encoded in marker-based chromosomes and evolved against a full-width minimax opponent using the same heuristic board evaluation function. The focus network was able to guide the minimax search away from poor information, resulting in stronger play while examining far fewer nodes.<<ETX>>"
            },
            "slug": "Improving-game-tree-search-with-evolutionary-neural-Moriarty-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Improving game-tree search with evolutionary neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Focus networks were encoded in marker-based chromosomes and evolved against a full-width minimax opponent using the same heuristic board evaluation function, resulting in stronger play while examining far fewer nodes."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the First IEEE Conference on Evolutionary Computation. IEEE World Congress on Computational Intelligence"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716802"
                        ],
                        "name": "D. Coit",
                        "slug": "D.-Coit",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Coit",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Coit"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116706862"
                        ],
                        "name": "Alice E. Smith",
                        "slug": "Alice-E.-Smith",
                        "structuredName": {
                            "firstName": "Alice",
                            "lastName": "Smith",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alice E. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 14283721,
            "fieldsOfStudy": [
                "Business",
                "Computer Science"
            ],
            "id": "01af6884cccef1b9b15a37f5a1039381bc08017c",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Solving-the-redundancy-allocation-problem-using-a-Coit-Smith",
            "title": {
                "fragments": [],
                "text": "Solving the redundancy allocation problem using a combined neural network/genetic algorithm approach"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Oper. Res."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117114985"
                        ],
                        "name": "Wei Yan",
                        "slug": "Wei-Yan",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Yan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2245765"
                        ],
                        "name": "Zhaoda Zhu",
                        "slug": "Zhaoda-Zhu",
                        "structuredName": {
                            "firstName": "Zhaoda",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhaoda Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2354989"
                        ],
                        "name": "Rongchun Hu",
                        "slug": "Rongchun-Hu",
                        "structuredName": {
                            "firstName": "Rongchun",
                            "lastName": "Hu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rongchun Hu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 100
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 110141036,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13014669bba0da64b1bae85fb257ab30ffce552d",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a general purposed real valued genetic algorithm model is presented. For the training of neural networks, the hybrid algorithm integrates the real valued algorithm with the well known BP algorithm. It is used to the training of a feedforward neural networks for radar target classification based on 1-D range profile. 50 range profile samples from the real radar data of each of the three aircrafts are used to train the neural network and another 50 range profile samples are used to test the classification performance. The proposed method can also be used to other optimization problems."
            },
            "slug": "A-hybrid-genetic/BP-algorithm-and-its-application-Yan-Zhu",
            "title": {
                "fragments": [],
                "text": "A hybrid genetic/BP algorithm and its application for radar target classification"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The hybrid algorithm integrates the real valued algorithm with the well known BP algorithm to training of a feedforward neural networks for radar target classification based on 1-D range profile."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE 1997 National Aerospace and Electronics Conference. NAECON 1997"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144215175"
                        ],
                        "name": "R. Jacobs",
                        "slug": "R.-Jacobs",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jacobs",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jacobs"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9947500,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a9ef2995e8e1bd57a74343073219364811c2ace0",
            "isKey": false,
            "numCitedBy": 1991,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Increased-rates-of-convergence-through-learning-Jacobs",
            "title": {
                "fragments": [],
                "text": "Increased rates of convergence through learning rate adaptation"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2863003"
                        ],
                        "name": "S. Omatu",
                        "slug": "S.-Omatu",
                        "structuredName": {
                            "firstName": "Sigeru",
                            "lastName": "Omatu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Omatu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2096278"
                        ],
                        "name": "S. Deris",
                        "slug": "S.-Deris",
                        "structuredName": {
                            "firstName": "Safaai",
                            "lastName": "Deris",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Deris"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 31
                            }
                        ],
                        "text": "Lee [81] and many others [32], [136]\u2013[138] used GA\u2019s to search for a near-optimal set of initial connection weights and then used BP to perform local search from these initial weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5241710,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7e37b4c77c845e8a2e2bd898fcb91c5f46c71bbf",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": "Considers the stabilization of an inverted pendulum which can be controlled by moving a cart in an intelligent way. We adopt a PID (proportional + integral + derivative) control method to stabilize the pendulum. This controller requires the determination of PID control gains, but it is difficult to select the best gains theoretically. Thus, there have been many approaches to determine them empirically; most of these are based on the experience of operators and knowledge. We propose a method to use neural networks to tune the PID gains in the same way that human operators tune the gains adaptively according to the environmental condition and systems specification. The tuning method is based on the error backpropagation method, and hence it may be trapped in a local minimum. In order to avoid the local minimum problem, we use a genetic algorithm to find the initial values of the connection weights of the neural network and the initial values of the PID gains. Experimental results show the effectiveness of the approach."
            },
            "slug": "Stabilization-of-inverted-pendulum-by-the-genetic-Omatu-Deris",
            "title": {
                "fragments": [],
                "text": "Stabilization of inverted pendulum by the genetic algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A method to use neural networks to tune the PID gains in the same way that human operators tune the gains adaptively according to the environmental condition and systems specification is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE International Conference on Evolutionary Computation"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2938111"
                        ],
                        "name": "N. Noguchi",
                        "slug": "N.-Noguchi",
                        "structuredName": {
                            "firstName": "Noboru",
                            "lastName": "Noguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Noguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70403042"
                        ],
                        "name": "H. Terao",
                        "slug": "H.-Terao",
                        "structuredName": {
                            "firstName": "Hideo",
                            "lastName": "Terao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Terao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 108949990,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd96d601526d98c1061b437e9217841829ef03b4",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Path-planning-of-an-agricultural-mobile-robot-by-Noguchi-Terao",
            "title": {
                "fragments": [],
                "text": "Path planning of an agricultural mobile robot by neural network and genetic algorithm"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643923853"
                        ],
                        "name": "PujolJo\u00e3o Carlos Figueira",
                        "slug": "PujolJo\u00e3o-Carlos-Figueira",
                        "structuredName": {
                            "firstName": "PujolJo\u00e3o",
                            "lastName": "Figueira",
                            "middleNames": [
                                "Carlos"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "PujolJo\u00e3o Carlos Figueira"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1643834361"
                        ],
                        "name": "PoliRiccardo",
                        "slug": "PoliRiccardo",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "PoliRiccardo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "PoliRiccardo"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 6461198,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "08ffc45da941e384658ef1cf2b9badb950b5c2df",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary computation is a class of global search techniques based on the learning process of a population of potential solutions to a given problem, that has been successfully applied to a vari..."
            },
            "slug": "Evolving-the-Topology-and-the-Weights-of-Neural-a-Figueira-PoliRiccardo",
            "title": {
                "fragments": [],
                "text": "Evolving the Topology and the Weights of Neural Networks Using a Dual Representation"
            },
            "tldr": {
                "abstractSimilarityScore": 96,
                "text": "Evolutionary computation is a class of global search techniques based on the learning process of a population of potential solutions to a given problem, that has been successfully applied to a variate of problems."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7491121"
                        ],
                        "name": "Daniel Gariglio",
                        "slug": "Daniel-Gariglio",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Gariglio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Gariglio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1971864"
                        ],
                        "name": "J. Heidepriem",
                        "slug": "J.-Heidepriem",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Heidepriem",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Heidepriem"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113165717"
                        ],
                        "name": "A. Helget",
                        "slug": "A.-Helget",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Helget",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Helget"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 13687212,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "317c8b7795f1c968950d58470ec32b5f1ad78b4c",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper deals with the application of neural networks and evolutionary techniques to the area of process identification and control. A distillation process is simulated with the dynamic flow-sheet simulator DIVA which employs a first-principle-based model. Several neural paradigms were implemented to adaptively model the concentration dynamics. A combined PI-Neural Net controller for concentration control is presented. Using genetic algorithms it was possible to optimize the network structure and reduce the size of the training set. Finally, some parallelization methods for neural and evolutionary algorithms, implemented on Connection Machine architectures, are briefly explained."
            },
            "slug": "Identification-and-Control-of-a-Simulated-Plant-and-Gariglio-Heidepriem",
            "title": {
                "fragments": [],
                "text": "Identification and Control of a Simulated Distillation Plant using Connectionist and Evolutionary Techniques"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "This paper deals with the application of neural networks and evolutionary techniques to the area of process identification and control and a combined PI-Neural Net controller for concentration control is presented."
            },
            "venue": {
                "fragments": [],
                "text": "Simul."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10375621"
                        ],
                        "name": "B. Whitehead",
                        "slug": "B.-Whitehead",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Whitehead",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Whitehead"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 8288544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f60f5ae708096077b0de97d4181d2f305f3aaeb",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "A well-performing set of radial basis functions (RBFs) can emerge from genetic competition among individual RBFs. Genetic selection of the individual RBFs is based on credit sharing which localizes competition within orthogonal niches. These orthogonal niches are derived using singular value decomposition and are used to apportion credit for the overall performance of the RBF network among individual nonorthogonal RBFs. Niche-based credit apportionment facilitates competition to fill each niche and hence to cover the training data. The resulting genetic algorithm yields RBF networks with better prediction performance on the Mackey-Glass chaotic time series than RBF networks produced by the orthogonal least squares method and by k-means clustering."
            },
            "slug": "Genetic-evolution-of-radial-basis-function-coverage-Whitehead",
            "title": {
                "fragments": [],
                "text": "Genetic evolution of radial basis function coverage using orthogonal niches"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A well-performing set of radial basis functions (RBFs) can emerge from genetic competition among individual RBFs which yields RBF networks with better prediction performance on the Mackey-Glass chaotic time series thanRBF networks produced by the orthogonal least squares method and by k-means clustering."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48795843"
                        ],
                        "name": "M. Narayanan",
                        "slug": "M.-Narayanan",
                        "structuredName": {
                            "firstName": "Maheshwari",
                            "lastName": "Narayanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Narayanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056629353"
                        ],
                        "name": "S. Lucas",
                        "slug": "S.-Lucas",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Lucas",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Lucas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20507434,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a92ff2e6866497c81f2e8bac72bd85f017edc150",
            "isKey": false,
            "numCitedBy": 55,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability of neural networks to predict the international normalised ratio (INR) for patients treated with Warfarin was investigated. Neural networks were obtained by using all the predictor variables in the neural network, or by using a genetic algorithm to select an optimal subset of predictor variables in a neural network. The use of a genetic algorithm gave a marked and significant improvement in the prediction of the INR in two of the three cases investigated. The mean error in these cases, typically, reduced from 1.02 +/- 0.29 to 0.28 +/- 0.25 (paired t-test, t = -4.71, p < 0.001, n = 30). The use of a genetic algorithm with Warfarin data offers a significant enhancement of the predictive ability of a neural network with Warfarin data, identifies significant predictor variables, reduces the size of the neural network and thus the speed at which the reduced network can be trained, and reduces the sensitivity of a network to over-training."
            },
            "slug": "A-genetic-algorithm-to-improve-a-neural-network-to-Narayanan-Lucas",
            "title": {
                "fragments": [],
                "text": "A genetic algorithm to improve a neural network to predict a patient's response to warfarin."
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The use of a genetic algorithm with WarFarin data offers a significant enhancement of the predictive ability of a neural network with Warfarin data, identifies significant predictor variables, reduces the size of the neural network and thus the speed at which the reduced network can be trained, and reduces the of a network to over-training."
            },
            "venue": {
                "fragments": [],
                "text": "Methods of information in medicine"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723246"
                        ],
                        "name": "L. Belfore",
                        "slug": "L.-Belfore",
                        "structuredName": {
                            "firstName": "Lee",
                            "lastName": "Belfore",
                            "middleNames": [
                                "A."
                            ],
                            "suffix": "II"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Belfore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153293477"
                        ],
                        "name": "Abdul-Rahman A. Arkadan",
                        "slug": "Abdul-Rahman-A.-Arkadan",
                        "structuredName": {
                            "firstName": "Abdul-Rahman",
                            "lastName": "Arkadan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Abdul-Rahman A. Arkadan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 40141255,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "1ccd61a47bbc30c8c0ec44844a0e08f776354a2b",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The work presented examines the feasibility of using artificial neural networks (ANNs) and evolutionary algorithms (EAs) to model fault free and faulted switched reluctance motor (SRM) drive systems. SRMs are capable of functioning despite the presence of faults. Faults impart transient changes to machine inductances in a manner that is difficult to model analytically. After this transient period, SRMs are capable of functioning at a reduced level of performance. ANNs are applied for their well known interpolation capabilities for highly nonlinear systems. EAs are employed for their ability to search a complex structural and parametric space as necessary to find good ANN solutions. In this paper, the ANN structure and training regimen are described for application to an example SRM drive system under normal and abnormal operating conditions.<<ETX>>"
            },
            "slug": "Modeling-faulted-switched-reluctance-motors-using-Belfore-Arkadan",
            "title": {
                "fragments": [],
                "text": "Modeling faulted switched reluctance motors using evolutionary neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper examines the feasibility of using artificial neural networks (ANNs) and evolutionary algorithms (EAs) to develop discrete time dynamic models for fault-free and faulted switched reluctance motor (SRM) drive systems."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IECON'94 - 20th Annual Conference of IEEE Industrial Electronics"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2107686778"
                        ],
                        "name": "S. S. Wilson",
                        "slug": "S.-S.-Wilson",
                        "structuredName": {
                            "firstName": "S.",
                            "lastName": "Wilson",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. S. Wilson"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "Wilson [234] also used simulated annealing in ANN architecture design."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 62673236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ef7dfc1001914dabaac17814de374b1ef32e48a7",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "A simulated annealing technique for automatically training a machine vision system to recognize and locate complex objects is described. In this method, the training is used to find an optimum connectivity pattern of a fixed number of inputs that have fixed weights, rather than the usual technique of finding the optimum weights for a fixed connectivity. The recognition model uses a two-layer artificial neural network, where the first layer consists of image edge vectors in four directions. Each neuron in the second layer has a fixed number of connections that connect only to those first layer edges that are best for distinguishing the object from a confusing background. Simulated annealing is used to find the best parameters for defining edges in the first layer, as well as the pattern of connections from the first to the second layer. Weights of the connections are either plus or minus one, so that multiplications are avoided, and the system speed is considerably enhanced. In industrial applications on a low-cost parallel SIMD (single instruction multiple data) architecture, objects can be trained by an unskilled user in less than 1 min, and after training, parts can be located in about 100 ms. This method has been found to work very well on integrated circuit patterns. >"
            },
            "slug": "Teaching-network-connectivity-using-simulated-on-a-Wilson",
            "title": {
                "fragments": [],
                "text": "Teaching network connectivity using simulated annealing on a massively parallel processor"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "In industrial applications on a low-cost parallel SIMD (single instruction multiple data) architecture, objects can be trained by an unskilled user in less than 1 min, and after training, parts can be located in about 100 ms."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46515453"
                        ],
                        "name": "E. Chang",
                        "slug": "E.-Chang",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Chang",
                            "middleNames": [
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144990248"
                        ],
                        "name": "R. Lippmann",
                        "slug": "R.-Lippmann",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Lippmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lippmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18629302,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "579d1134df253899a775c1b5857030d8c5023ac1",
            "isKey": false,
            "numCitedBy": 84,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Genetic algorithms were used to select and create features and to select reference exemplar patterns for machine vision and speech pattern classification tasks. For a complex speech recognition task, genetic algorithms required no more computation time than traditional approaches to feature selection but reduced the number of input features required by a factor of five (from 153 to 33 features). On a difficult artificial machine-vision task, genetic algorithms were able to create new features (polynomial functions of the original features) which reduced classification error rates from 19% to almost 0%. Neural net and k nearest neighbor (KNN) classifiers were unable to provide such low error rates using only the original features. Genetic algorithms were also used to reduce the number of reference exemplar patterns for a KNN classifier. On a 338 training pattern vowel-recognition problem with 10 classes, genetic algorithms reduced the number of stored exemplars from 338 to 43 without significantly increasing classification error rate. In all applications, genetic algorithms were easy to apply and found good solutions in many fewer trials than would be required by exhaustive search. Run times were long, but not unreasonable. These results suggest that genetic algorithms are becoming practical for pattern classification problems as faster serial and parallel computers are developed."
            },
            "slug": "Using-Genetic-Algorithms-to-Improve-Pattern-Chang-Lippmann",
            "title": {
                "fragments": [],
                "text": "Using Genetic Algorithms to Improve Pattern Classification Performance"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results suggest that genetic algorithms are becoming practical for pattern classification problems as faster serial and parallel computers are developed."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 16063799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6114b4a7a3f8f6f0b7118ff8a93447d1ca49c726",
            "isKey": false,
            "numCitedBy": 58,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Applying-evolutionary-programming-to-selected-Fogel",
            "title": {
                "fragments": [],
                "text": "Applying evolutionary programming to selected control problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3247745"
                        ],
                        "name": "Thomas B\u00e4ck",
                        "slug": "Thomas-B\u00e4ck",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "B\u00e4ck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas B\u00e4ck"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 54
                            }
                        ],
                        "text": "The architecture of an ANN includes its topological structure, i.e., connectivity, and the transfer function of each node in the ANN."
                    },
                    "intents": []
                }
            ],
            "corpusId": 27611267,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3e9cc58291f6e94ab9c47bbb8b4fa79b1e3613d",
            "isKey": false,
            "numCitedBy": 1692,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The Third Edition of this internationally acclaimed publication provides the latest theory and techniques for using simulated evolution to achieve machine intelligence. As a leading advocate for evolutionary computation, the author has successfully challenged the traditional notion of artificial intelligence, which essentially programs human knowledge fact by fact, but does not have the capacity to learn or adapt as evolutionary computation does."
            },
            "slug": "Evolutionary-computation:-Toward-a-new-philosophy-B\u00e4ck",
            "title": {
                "fragments": [],
                "text": "Evolutionary computation: Toward a new philosophy of machine intelligence"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The Third Edition of this internationally acclaimed publication provides the latest theory and techniques for using simulated evolution to achieve machine intelligence."
            },
            "venue": {
                "fragments": [],
                "text": "Complex."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2937027,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f9197ff9fdabd2b78bfe0602365011c6699b0d66",
            "isKey": false,
            "numCitedBy": 1188,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "The assumption that acquired character istics are not in\u00ad herited is ofte n taken to imply t hat t he adaptations t hat an organism learns dur ing its lifeti me cannot guide t he course of evolut ion . This infere nce is incor rec t (2). Learni ng alt ers the shape of t he search space in which evolu tio n operates and thereby pro vides good evolut ion ar y paths towa rds sets of co-adapted alleles. We demonst r at e t hat th is effect allows learning organisms to evolve much faster than their 000 \u00ad learning equivalents, even though the characteris tics acquired by t he phenotype are not communicated to the genotype."
            },
            "slug": "How-Learning-Can-Guide-Evolution-Hinton-Nowlan",
            "title": {
                "fragments": [],
                "text": "How Learning Can Guide Evolution"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2978631"
                        ],
                        "name": "S. Bornholdt",
                        "slug": "S.-Bornholdt",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Bornholdt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Bornholdt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2582796"
                        ],
                        "name": "D. Graudenz",
                        "slug": "D.-Graudenz",
                        "structuredName": {
                            "firstName": "Dirk",
                            "lastName": "Graudenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Graudenz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 162
                            }
                        ],
                        "text": "In fact, can represent real-valued connection weights from node to node so that the architecture and connection weights can be evolved simultaneously [37], [42], [45], [165], [166], [169]\u2013[171]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 49
                            }
                        ],
                        "text": "has been carried out in recent years [33], [42], [45],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 105,
                                "start": 101
                            }
                        ],
                        "text": "Some researchers thus avoided crossover and adopted only mutations in the evolution of architectures [45], [128], [149], [179], [185]\u2013[197], [217], [223], although it has been shown that crossover may be useful and important in increasing the efficiency of evolution for some problems [48], [113], [212], [229]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18002169,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f53e9dea86632a8d64daabaabb6f8adecf86aa95",
            "isKey": true,
            "numCitedBy": 148,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "A learning algorithm based on genetic algorithms for asymmetric neural networks with an arbitrary structure is presented. It is suited for the learning of temporal patterns and leads to stable neural networks with feedback.<<ETX>>"
            },
            "slug": "General-asymmetric-neural-networks-and-structure-by-Bornholdt-Graudenz",
            "title": {
                "fragments": [],
                "text": "General asymmetric neural networks and structure design by genetic algorithms: a learning rule for temporal patterns"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "A learning algorithm based on genetic algorithms for asymmetric neural networks with an arbitrary structure is presented, suited for the learning of temporal patterns and leads to stable Neural networks with feedback."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of IEEE Systems Man and Cybernetics Conference - SMC"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7373730"
                        ],
                        "name": "J. Bongard",
                        "slug": "J.-Bongard",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "Bongard",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bongard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145576002"
                        ],
                        "name": "R. Pfeifer",
                        "slug": "R.-Pfeifer",
                        "structuredName": {
                            "firstName": "Rolf",
                            "lastName": "Pfeifer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Pfeifer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 277,
                                "start": 274
                            }
                        ],
                        "text": "The match must be a perfect one, which defeats weak linkage, but the potentially-long gap between gene products (and corresponding lengthy source of possible regulatory matches) not only saves it, but puts weak linkage under direct evolutionary control! Bongard and Pfeifer [6] based their (more complex) GRN upon this model, although in retrospect, theirs more closely resembles that of AGE, developed several years after their own."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 102
                            }
                        ],
                        "text": "Bongard and Pfeifer have one of the most elaborate and impressive uses of this bio-inspired mechanism [6, 5]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 75
                            }
                        ],
                        "text": "In a recent personal communication, Bongard elaborated upon the results in [6] with recalled sightings of telltale footprints in the genetic record."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 112
                            }
                        ],
                        "text": "Using the same GRN-based developmental approach to evolving morphologies and neural networks as discussed above [6], Bongard has also investigated the modularity of the genotype-phenotype mapping [5]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 154
                            }
                        ],
                        "text": "Nearly a decade passed before 3-d simulators of equal power became commonplace enough that other ALife researchers could begin to produce similar results [6, 5], many of which show the intricate relationships in the co-evolution of body and brain."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8663857,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "e5f2b4072537bd94e7332ac2db39fe6e47362114",
            "isKey": true,
            "numCitedBy": 171,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, a minimal model of ontogenetic development, combined with differential gene expression and a genetic algorithm, is used to evolve both the morphology and neural control of agents that perform a block-pushing task in a physically-realistic, virtual environment. We refer to this methodology as artificial ontogeny (AO). It is demonstrated that evolved genetic regulatory networks in AO give rise to hierarchical, repeated phenotypic structures. Moreover, it is shown that the indirect genotype to phenotype mapping results in a dissociation between the information content in the genome, and the complexity of the evolved agent. It is argued that these findings support the claim that artificial ontogeny is a useful design tool for the evolutionary design of virtual agents and real-world robots."
            },
            "slug": "Repeated-structure-and-dissociation-of-genotypic-in-Bongard-Pfeifer",
            "title": {
                "fragments": [],
                "text": "Repeated structure and dissociation of genotypic and phenotypic complexity in artificial ontogeny"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is demonstrated that evolved genetic regulatory networks in AO give rise to hierarchical, repeated phenotypic structures, and the claim that artificial ontogeny is a useful design tool for the evolutionary design of virtual agents and real-world robots is supported."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145468098"
                        ],
                        "name": "M. M\u00f8ller",
                        "slug": "M.-M\u00f8ller",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "M\u00f8ller",
                            "middleNames": [
                                "Fodslette"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. M\u00f8ller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "Evolutionary training can be slow for some problems in comparison with fast variants of BP [131] and conjugate gradient algorithms [19], [132]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "Most training algorithms, such as BP and conjugate gradient algorithms [7], [17]\u2013[19], are based on gradient descent."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8029054,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f4a097b2131784d7ac3fc3c47d1e9283e9ac207",
            "isKey": false,
            "numCitedBy": 3774,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-scaled-conjugate-gradient-algorithm-for-fast-M\u00f8ller",
            "title": {
                "fragments": [],
                "text": "A scaled conjugate gradient algorithm for fast supervised learning"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1859405"
                        ],
                        "name": "David E. Moriarty",
                        "slug": "David-E.-Moriarty",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Moriarty",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David E. Moriarty"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 182
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 114
                            }
                        ],
                        "text": "The second approach evolves architectures and connection weights simultaneously [149], [179], [180], [182], [185]\u2013[200]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11302202,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb63418eb819b6e752081fc9867ed45ed8bea73f",
            "isKey": false,
            "numCitedBy": 121,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "An approach to develop new game-playing strategies based on artificial evolution of neural networks is presented. Evolution was directed to discover strategies in Othello against a random-moving opponent and later against an \u03b1-\u03b2 search program. The networks discovered first a standard positional strategy, and subsequently a mobility strategy, an advanced strategy rarely seen outside of tournaments. The latter discovery demonstrates how evolutionary neural networks can develop novel solutions by turning an initial disadvantage into an advantage in a changed environment."
            },
            "slug": "Discovering-Complex-Othello-Strategies-through-Moriarty-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Discovering Complex Othello Strategies through Evolutionary Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 94,
                "text": "An approach to develop new game-playing strategies based on artificial evolution of neural networks is presented, directed to discover strategies in Othello against a random-moving opponent and later against an \u03b1-\u03b2 search program."
            },
            "venue": {
                "fragments": [],
                "text": "Connect. Sci."
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15058655"
                        ],
                        "name": "S. Knerr",
                        "slug": "S.-Knerr",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Knerr",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Knerr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3078169"
                        ],
                        "name": "L. Personnaz",
                        "slug": "L.-Personnaz",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Personnaz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Personnaz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097910"
                        ],
                        "name": "G. Dreyfus",
                        "slug": "G.-Dreyfus",
                        "structuredName": {
                            "firstName": "G\u00e9rard",
                            "lastName": "Dreyfus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Dreyfus"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "various areas [20]\u2013[22], but BP has drawbacks due to its use of gradient descent [23], [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 196511,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02c76c01bef98edbd8a2f2041454035f77837ace",
            "isKey": false,
            "numCitedBy": 208,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "It is shown that neural network classifiers with single-layer training can be applied efficiently to complex real-world classification problems such as the recognition of handwritten digits. The STEPNET procedure, which decomposes the problem into simpler subproblems which can be solved by linear separators, is introduced. Provided appropriate data representations and learning rules are used, performance comparable to that obtained by more complex networks can be achieved. Results from two different databases are presented: an European database comprising 8700 isolated digits and a zip code database from the US Postal Service comprising 9000 segmented digits. A hardware implementation of the classifier is briefly described."
            },
            "slug": "Handwritten-digit-recognition-by-neural-networks-Knerr-Personnaz",
            "title": {
                "fragments": [],
                "text": "Handwritten digit recognition by neural networks with single-layer training"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "It is shown that neural network classifiers with single-layer training can be applied efficiently to complex real-world classification problems such as the recognition of handwritten digits and provided appropriate data representations and learning rules are used, performance comparable to that obtained by more complex networks can be achieved."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33827014"
                        ],
                        "name": "R. Hochman",
                        "slug": "R.-Hochman",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Hochman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hochman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725285"
                        ],
                        "name": "T. Khoshgoftaar",
                        "slug": "T.-Khoshgoftaar",
                        "structuredName": {
                            "firstName": "Taghi",
                            "lastName": "Khoshgoftaar",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Khoshgoftaar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1684557"
                        ],
                        "name": "E. B. Allen",
                        "slug": "E.-B.-Allen",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Allen",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. B. Allen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2489811"
                        ],
                        "name": "J. Hudepohl",
                        "slug": "J.-Hudepohl",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hudepohl",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hudepohl"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 7400703,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "15196475400410267b69cab1882e5446ce071fc6",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "In this empirical study, from a large data set of software metrics for program modules, thirty distinct partitions into training and validation sets are automatically generated with approximately equal distributions of fault prone and not fault prone modules. Thirty classification models are built for each of the two approaches considered-discriminant analysis and the evolutionary neural network (ENN) approach-and their performances on corresponding data sets are compared. The lower error proportions for ENNs on fault prone, not fault prone, and overall classification were found to be statistically significant. The robustness of ENNs follows from their superior performance on the range of data configurations used. It is suggested that ENNs can be effective in other software reliability problem domains, where they have been largely ignored."
            },
            "slug": "Evolutionary-neural-networks:-a-robust-approach-to-Hochman-Khoshgoftaar",
            "title": {
                "fragments": [],
                "text": "Evolutionary neural networks: a robust approach to software reliability problems"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "From a large data set of software metrics for program modules, thirty distinct partitions into training and validation sets are automatically generated with approximately equal distributions of fault prone and not fault prone modules, showing the robustness of ENNs follows from their superior performance on the range of data configurations used."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings The Eighth International Symposium on Software Reliability Engineering"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3045514"
                        ],
                        "name": "Sangbong Park",
                        "slug": "Sangbong-Park",
                        "structuredName": {
                            "firstName": "Sangbong",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sangbong Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144963342"
                        ],
                        "name": "Lae-Jeong Park",
                        "slug": "Lae-Jeong-Park",
                        "structuredName": {
                            "firstName": "Lae-Jeong",
                            "lastName": "Park",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lae-Jeong Park"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9383401"
                        ],
                        "name": "C. Park",
                        "slug": "C.-Park",
                        "structuredName": {
                            "firstName": "Cheol",
                            "lastName": "Park",
                            "middleNames": [
                                "Hoon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Park"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 30747913,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "a4651b0cf47100e49e0f6ae70986c3ca13ce1573",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper investigates a neurocontroller for nonminimum phase systems which is trained off-line with genetic algorithm (GA) and is combined in parallel with a conventional linear controller of proportional plus integral plus derivative (PID) type. Training of this kind of a neuro-genetic controller provides a solution under a given global evaluation function, which is devised based on the desired control performance during the whole training time interval. Empirical simulation results illustrate the efficacy of the proposed controller compared with a conventional linear controller in point of learning capability of adaptation and improvement of performances of a step response like fast settling time, small undershoot, and small overshoot."
            },
            "slug": "A-neuro-genetic-controller-for-nonminimum-phase-Park-Park",
            "title": {
                "fragments": [],
                "text": "A neuro-genetic controller for nonminimum phase systems"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "Empirical simulation results illustrate the efficacy of the proposed neurocontroller for nonminimum phase systems compared with a conventional linear controller in point of learning capability of adaptation and improvement of performances of a step response like fast settling time, small undershoot, and small overshoot."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145285040"
                        ],
                        "name": "L. D. Whitley",
                        "slug": "L.-D.-Whitley",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Whitley",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Whitley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 233,
                                "start": 229
                            }
                        ],
                        "text": "Because EA\u2019s can treat large, complex, nondifferentiable, and multimodal spaces, which are the typical case in the real world, considerable research and application has been conducted on the evolution of connection weights [24], [26]\u2013[112]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "[24], [26], [28], [37], [38], [41], [52], [53]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11730923,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81dc8ca76afa41d2c49865068c46114178bdf558",
            "isKey": false,
            "numCitedBy": 1429,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper reports work done over the past three years using rank-based allocation of reproductive trials. New evidence and arguments are presented which suggest that allocating reproductive trials according to rank is superior to tness proportionate reproduction. Ranking can not only be used to slow search speed, but also to increase search speed when appropriate. Furthermore, the use of ranking provides a degree of control over selective pressure that is not possible with tness proportionate reproduction. The use of rank-based allocation of reproductive trials is discussed in the context of 1) Holland's schema theorem, 2) DeJong's standard test suite, and 3) a set of neural net optimization problems that are larger than the problems in the standard test suite. The GENITOR algorithm is also discussed; this algorithm is speciically designed to allocate reproductive trials according to rank."
            },
            "slug": "The-GENITOR-Algorithm-and-Selection-Pressure:-Why-Whitley",
            "title": {
                "fragments": [],
                "text": "The GENITOR Algorithm and Selection Pressure: Why Rank-Based Allocation of Reproductive Trials is Best"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "This paper reports work done over the past three years using rank-based allocation of reproductive trials to suggest that allocating reproductive trials according to rank is superior to tness proportionate reproduction."
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "[149] and Fogel [12], [243] have provided a more general discussion on the mapping between genotypes"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 17755853,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "1db9a59ebe9b4808722fee55601beb8e2eda5064",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Evolutionary computation can be conducted at various levels of abstraction (e.g., genes, individuals, species). Recent claims have been made that simulated evolution can be made more biologically accurate by applying specific genetic operators that mimic low-level transformations to DNA. This paper argues instead that the appropriateness of particular variation operators depends on the level of abstraction of the simulation. Further, including spec@ random variation operators simply because they have a similar form as genetic operators that occur in nature does not, in general, lead lo greater fdelity in simulation."
            },
            "slug": "Phenotypes,-genotypes,-and-operators-in-computation-Fogel",
            "title": {
                "fragments": [],
                "text": "Phenotypes, genotypes, and operators in evolutionary computation"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is argued that the appropriateness of particular variation operators depends on the level of abstraction of the simulation, and including spec@ random variation operators simply because they have a similar form as genetic operators that occur in nature does not, in general, lead to greater popularity in simulation."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1995 IEEE International Conference on Evolutionary Computation"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736103"
                        ],
                        "name": "S. Pal",
                        "slug": "S.-Pal",
                        "structuredName": {
                            "firstName": "Sankar",
                            "lastName": "Pal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Pal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144194050"
                        ],
                        "name": "Dinabandhu Bhandari",
                        "slug": "Dinabandhu-Bhandari",
                        "structuredName": {
                            "firstName": "Dinabandhu",
                            "lastName": "Bhandari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dinabandhu Bhandari"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3838615,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e94db57d9438f9b6e6ee2e3b5ce2c8a47b97be57",
            "isKey": false,
            "numCitedBy": 28,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Genetic-algorithms-with-fuzzy-fitness-function-for-Pal-Bhandari",
            "title": {
                "fragments": [],
                "text": "Genetic algorithms with fuzzy fitness function for object extraction using cellular networks"
            },
            "venue": {
                "fragments": [],
                "text": "CVPR 1994"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2500478"
                        ],
                        "name": "Hsi-Chieh Lee",
                        "slug": "Hsi-Chieh-Lee",
                        "structuredName": {
                            "firstName": "Hsi-Chieh",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hsi-Chieh Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144513927"
                        ],
                        "name": "C. Dagli",
                        "slug": "C.-Dagli",
                        "structuredName": {
                            "firstName": "Cihan",
                            "lastName": "Dagli",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dagli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62183212,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "6d4b63c585bb92166ab49b33740f2d97d0259164",
            "isKey": false,
            "numCitedBy": 33,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-parallel-genetic-neuro-scheduler-for-job-shop-Lee-Dagli",
            "title": {
                "fragments": [],
                "text": "A parallel genetic-neuro scheduler for job-shop scheduling problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1893129"
                        ],
                        "name": "M. Bichsel",
                        "slug": "M.-Bichsel",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Bichsel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bichsel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062816546"
                        ],
                        "name": "Peter Seitz",
                        "slug": "Peter-Seitz",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Seitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Seitz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 40
                            }
                        ],
                        "text": "on the information theory or statistics [226]\u2013[228] can"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4377879,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c58f57c8bb8677139edfd33ca9f1b386c1d617e0",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Minimum-class-entropy:-A-maximum-information-to-Bichsel-Seitz",
            "title": {
                "fragments": [],
                "text": "Minimum class entropy: A maximum information approach to layered networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "89799463"
                        ],
                        "name": "Y. H. Song",
                        "slug": "Y.-H.-Song",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Song",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. H. Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2965546"
                        ],
                        "name": "A. Johns",
                        "slug": "A.-Johns",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Johns",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Johns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144619803"
                        ],
                        "name": "Q. Xuan",
                        "slug": "Q.-Xuan",
                        "structuredName": {
                            "firstName": "Q.",
                            "lastName": "Xuan",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Q. Xuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46701018"
                        ],
                        "name": "J. Liu",
                        "slug": "J.-Liu",
                        "structuredName": {
                            "firstName": "Jiao",
                            "lastName": "Liu",
                            "middleNames": [
                                "Yu"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 110841538,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "91ae976b96b4d4c28e063c4580739e36631819be",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "The paper proposes a novel fault detection and classification scheme for EHV power transmission lines using genetic algorithm-based neural networks. The application concerned is fault classification for EHV lines with a unified power factor corrector (UPFC), since fault classification is a key part of protective relaying schemes. After the genetic algorithm-based neural network is briefly discussed in general, EMTP based digital simulation results of a UPFC transmission system are presented. The generation of training/test data and preprocessing of these data for neural networks are then described. The paper places special emphasis on the performance comparison between a genetic algorithm-based neural network and a backpropagation network-based scheme."
            },
            "slug": "Genetic-algorithm-based-neural-networks-applied-to-Song-Johns",
            "title": {
                "fragments": [],
                "text": "Genetic algorithm based neural networks applied to fault classification for EHV transmission lines with a UPFC"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "A novel fault detection and classification scheme for EHV power transmission lines using genetic algorithm-based neural networks and a backpropagation network-based scheme is proposed."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2341152"
                        ],
                        "name": "Stewart W. Wilson",
                        "slug": "Stewart-W.-Wilson",
                        "structuredName": {
                            "firstName": "Stewart",
                            "lastName": "Wilson",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stewart W. Wilson"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 68
                            }
                        ],
                        "text": "The length of chromosome can be reduced greatly in this case [153], [154]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 103
                            }
                        ],
                        "text": "The first separates the evolution of architectures from that of connection weights [24], [150], [153], [154], [165], [167], [169], [170]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 129
                            }
                        ],
                        "text": "In the first approach, each connection of an architecture is directly specified by its binary representation [24], [150], [153], [154], [165], [167], [169], [170], [202]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14894095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6a22cadc3573b135a5f77411da4555a35c594ae7",
            "isKey": false,
            "numCitedBy": 52,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Perceptron-redux:-emergence-of-structure-Wilson",
            "title": {
                "fragments": [],
                "text": "Perceptron redux: emergence of structure"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1699108"
                        ],
                        "name": "G. Tesauro",
                        "slug": "G.-Tesauro",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Tesauro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Tesauro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 175,
                                "start": 171
                            }
                        ],
                        "text": "As detailed earlier, the computational complexities of Reinforcement Learning (RL) can be tamed by ANNs trained as search-state critics, as in Tesauro\u2019s backgammon player [66]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6023746,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5ed59f49c1bb7de06cfa2a9467d5efb535103277",
            "isKey": false,
            "numCitedBy": 906,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Ever since the days of Shannon's proposal for a chess-playing algorithm [12] and Samuel's checkers-learning program [10] the domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning. Such board games offer the challenge of tremendous complexity and sophistication required to play at expert level. At the same time, the problem inputs and performance measures are clear-cut and well defined, and the game environment is readily automated in that it is easy to simulate the board, the rules of legal play, and the rules regarding when the game is over and determining the outcome."
            },
            "slug": "Temporal-difference-learning-and-TD-Gammon-Tesauro",
            "title": {
                "fragments": [],
                "text": "Temporal difference learning and TD-Gammon"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "The domain of complex board games such as Go, chess, checkers, Othello, and backgammon has been widely regarded as an ideal testing ground for exploring a variety of concepts and approaches in artificial intelligence and machine learning."
            },
            "venue": {
                "fragments": [],
                "text": "CACM"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1940971"
                        ],
                        "name": "Shyh-Jier Huang",
                        "slug": "Shyh-Jier-Huang",
                        "structuredName": {
                            "firstName": "Shyh-Jier",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shyh-Jier Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47396617"
                        ],
                        "name": "C. Huang",
                        "slug": "C.-Huang",
                        "structuredName": {
                            "firstName": "Ching",
                            "lastName": "Huang",
                            "middleNames": [
                                "Lien"
                            ],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Huang"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62732258,
            "fieldsOfStudy": [
                "Computer Science",
                "Engineering"
            ],
            "id": "035059f3fbc2125caded9615c198d00cf4cbd1b2",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Genetic-based-multilayered-perception-for-Taiwan-Huang-Huang",
            "title": {
                "fragments": [],
                "text": "Genetic-based multilayered perception for Taiwan power system short-term load forecasting"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2011013"
                        ],
                        "name": "Li-Der Chou",
                        "slug": "Li-Der-Chou",
                        "structuredName": {
                            "firstName": "Li-Der",
                            "lastName": "Chou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Der Chou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30545717"
                        ],
                        "name": "J.-L.C. Wu",
                        "slug": "J.-L.C.-Wu",
                        "structuredName": {
                            "firstName": "J.-L.C.",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J.-L.C. Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 53749768,
            "fieldsOfStudy": [
                "Business",
                "Computer Science"
            ],
            "id": "80f9fa89bb8f5d1c0c058c8ced851091b8373ee6",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The concept of virtual paths has become the key technology in ATM networks. A control scheme based on genetic algorithms and neural networks is proposed and applied to the bandwidth allocation of virtual paths in ATM networks. The proposed control scheme is capable of selecting adaptively optimal step sizes of virtual paths according to the traffic characteristics and network environment. As the optimisation problem is constrained, traditional genetic algorithms are no longer applicable to this problem. The authors propose the masked genetic algorithms with seeds (MGAS) to solve the optimisation problem. Simulation results demonstrate the superiority of the MGAS algorithm. To achieve better performance, the relationships among the QOS measures, the evaluation of seed scores, and the selection of relearning data records are discussed. Finally, a simplified control scheme is proposed to reduce not only the complexity of the neural networks but also the processing time."
            },
            "slug": "Bandwidth-allocation-of-virtual-paths-using-genetic-Chou-Wu",
            "title": {
                "fragments": [],
                "text": "Bandwidth allocation of virtual paths using neural-network-based genetic algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The authors propose the masked genetic algorithms with seeds (MGAS) to solve the optimisation problem and a simplified control scheme is proposed to reduce not only the complexity of the neural networks but also the processing time."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108050600"
                        ],
                        "name": "Sang-Kyung Lee",
                        "slug": "Sang-Kyung-Lee",
                        "structuredName": {
                            "firstName": "Sang-Kyung",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sang-Kyung Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2052637923"
                        ],
                        "name": "D. Jang",
                        "slug": "D.-Jang",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Jang",
                            "middleNames": [
                                "Pyo"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59182227,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "98e1bc0f5a4afa630be84c930d04917ff8525b63",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Translation,-rotation-and-scale-invariant-pattern-Lee-Jang",
            "title": {
                "fragments": [],
                "text": "Translation, rotation and scale invariant pattern recognition using spectral analysis and hybrid genetic-neural-fuzzy networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3236303"
                        ],
                        "name": "Percy P. C. Yip",
                        "slug": "Percy-P.-C.-Yip",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Yip",
                            "middleNames": [
                                "P.",
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy P. C. Yip"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144763324"
                        ],
                        "name": "Y. Pao",
                        "slug": "Y.-Pao",
                        "structuredName": {
                            "firstName": "Yoh-Han",
                            "lastName": "Pao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Pao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 179
                            }
                        ],
                        "text": "Many others used EA\u2019s and ANN\u2019s for combinatorial or global (numerical) optimization in order to combine EA\u2019s global search capability with ANN\u2019s fast convergence to local optima [308]\u2013[318]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 24886101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bb85c6b7e659419869b7eab3a13a6cbe09cae007",
            "isKey": false,
            "numCitedBy": 156,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Feasible approaches to the task of solving NP-complete problems usually entails the incorporation of heuristic procedures so as to increase the efficiency of the methods used. We propose a new technique, which incorporates the idea of simulated annealing into the practice of simulated evolution, in place of arbitrary heuristics. The proposed technique is called guided evolutionary simulated annealing (GESA). We report on the use of GESA approach primarily for combinatorial optimization. In addition, we report the case of function optimization, treating the task as a search problem. The traveling salesman problem is taken as a benchmark problem in the first case. Simulation results are reported. The results show that the GESA approach can discover a very good near optimum solution after examining an extremely small fraction of possible solutions. A very complicated function with many local minima is used in the second case. The results in both cases indicate that the GESA technique is a practicable method which yields consistent and good near optimal solutions, superior to simulated evolution."
            },
            "slug": "Combinatorial-optimization-with-use-of-guided-Yip-Pao",
            "title": {
                "fragments": [],
                "text": "Combinatorial optimization with use of guided evolutionary simulated annealing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new technique is proposed, which incorporates the idea of simulated annealing into the practice of simulated evolution, in place of arbitrary heuristics, called GESA, which is used primarily for combinatorial optimization."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3280950"
                        ],
                        "name": "J. Bobbin",
                        "slug": "J.-Bobbin",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Bobbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Bobbin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 1441995,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9e76ddd7554a54ae83afc1b723682a339f98bfe",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Many mathematical solutions to certain classes of optimal control problems, particularly problems which give rise to 'chattering controls', make some physically unrealistic assumptions in order to solve the problems. These solutions often ignore the cost of changing control and thus fail to give physically realistic results due to the physical reality of this cost in many applications. When this cost is incorporated into the problem, the problem can become very difficult to solve numerically. The paper considers an evolutionary approach to solving optimal control problems which take the cost of changing control into account. A novel chromosome representation and an insert mutation have been proposed and tested against three different problems. The experimental results show that the evolutionary approach is quite competitive in comparison with the existing method based on dynamic programming."
            },
            "slug": "Solving-optimal-control-problems-with-a-cost-by-Bobbin-Yao",
            "title": {
                "fragments": [],
                "text": "Solving optimal control problems with a cost changing control by evolutionary algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "The paper considers an evolutionary approach to solving optimal control problems which take the cost of changing control into account and shows that the evolutionary approach is quite competitive in comparison with the existing method based on dynamic programming."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1997 IEEE International Conference on Evolutionary Computation (ICEC '97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1995280"
                        ],
                        "name": "G. Edelman",
                        "slug": "G.-Edelman",
                        "structuredName": {
                            "firstName": "Gerald",
                            "lastName": "Edelman",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Edelman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 132
                            }
                        ],
                        "text": "Despite all of this detail, the model circumvents the simulation of axonal growth via algorithms (based loosely on Neural Darwinism [15] and Deacon\u2019s Law [11]) that iteratively introduce connections and then use a group\u2019s total connectivity (along with that of other, competing groups) to compute a biasing factor that affects the connections generated in the next developmental round."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 143166578,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "0c95da468bcd557eace230af26134a2f0d348044",
            "isKey": false,
            "numCitedBy": 2101,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Somatic Selection * A Summary and Historical Introduction * Structure, Function, and Perception * Neuronal Group Selection Epigenetic Mechanisms * Developmental Bases of Diversity: The Primary Repertoire * Cellular Dynamics of Neural Maps * Evolution and Function of Distributed Systems * Synapses as Populations: The Bases of the Secondary Repertoire Global Functions * Action and Perception * Categorization and Memory * Selective Networks and Recognition Automata * Selection, Learning, and Behavior Conclusion * Summary, Predictions, and Implications"
            },
            "slug": "Neural-Darwinism:-The-Theory-Of-Neuronal-Group-Edelman",
            "title": {
                "fragments": [],
                "text": "Neural Darwinism: The Theory Of Neuronal Group Selection"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Somatic Selection helps clarify the role of language in the selection process and provides a scaffolding for future generations of researchers to study language-based learning and cognition."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1917876"
                        ],
                        "name": "P. Husbands",
                        "slug": "P.-Husbands",
                        "structuredName": {
                            "firstName": "Phil",
                            "lastName": "Husbands",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Husbands"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145472053"
                        ],
                        "name": "I. Harvey",
                        "slug": "I.-Harvey",
                        "structuredName": {
                            "firstName": "Inman",
                            "lastName": "Harvey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "I. Harvey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145676014"
                        ],
                        "name": "D. Cliff",
                        "slug": "D.-Cliff",
                        "structuredName": {
                            "firstName": "Dave",
                            "lastName": "Cliff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Cliff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2114424160"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "G.",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 106
                            }
                        ],
                        "text": "Thus, a lot of the work into growing axonal connections or gradually duplicating neurons in ALife systems [1, 33, 52] may occupy a researchers no-man\u2019s land: too abstract to answer detailed questions in neurodevelopment (perhaps best modeled by work such as [70]), while rather superfluous and computationally expensive for AI."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 27
                            }
                        ],
                        "text": "Some elaborate PO systems, [33, 52] employ evolved developmental regimes in which neurons grow axonal projections to their targets, while one of the early PE systems [69] evolves separate learning rules (applied to each incoming synapse) for each neuron."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14950099,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eede1a8e761c73e33020b141cd9ae8db5e216793",
            "isKey": false,
            "numCitedBy": 54,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "Provides a high-level review of current and recent work in the use of genetic algorithm-based techniques to develop sensorimotor control systems for autonomous agents. It focuses on network-based controllers and genetic encoding issues associated with them. The paper closes with a discussion of the possibility of using artificial evolutionary techniques to help tackle more specifically scientific questions about natural sensorimotor systems."
            },
            "slug": "The-use-of-genetic-algorithms-for-the-development-Husbands-Harvey",
            "title": {
                "fragments": [],
                "text": "The use of genetic algorithms for the development of sensorimotor control systems"
            },
            "tldr": {
                "abstractSimilarityScore": 95,
                "text": "A high-level review of current and recent work in the use of genetic algorithm-based techniques to develop sensorimotor control systems for autonomous agents focuses on network-based controllers and genetic encoding issues associated with them."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of PerAc '94. From Perception to Action"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7540793,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "796df7ff5550ab473ddc93bbf8e3c1ede0c2f426",
            "isKey": false,
            "numCitedBy": 152,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "The choice of an optimal neural network design for a given problem is addressed. A relationship between optimal network design and statistical model identification is described. A derivative of Akaike's information criterion (AIC) is given. This modification yields an information statistic which can be used to objectively select a ;best' network for binary classification problems. The technique can be extended to problems with an arbitrary number of classes."
            },
            "slug": "AN-INFORMATION-CRITERION-FOR-OPTIMAL-NEURAL-NETWORK-Fogel",
            "title": {
                "fragments": [],
                "text": "AN INFORMATION CRITERION FOR OPTIMAL NEURAL NETWORK SELECTION"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A derivative of Akaike's information criterion (AIC) is given which can be used to objectively select a ;best' network for binary classification problems and can be extended to problems with an arbitrary number of classes."
            },
            "venue": {
                "fragments": [],
                "text": "1990 Conference Record Twenty-Fourth Asilomar Conference on Signals, Systems and Computers, 1990."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728743"
                        ],
                        "name": "Z. Michalewicz",
                        "slug": "Z.-Michalewicz",
                        "structuredName": {
                            "firstName": "Zbigniew",
                            "lastName": "Michalewicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Michalewicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2751408"
                        ],
                        "name": "C. Janikow",
                        "slug": "C.-Janikow",
                        "structuredName": {
                            "firstName": "Cezary",
                            "lastName": "Janikow",
                            "middleNames": [
                                "Z."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Janikow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144206182"
                        ],
                        "name": "J. Krawczyk",
                        "slug": "J.-Krawczyk",
                        "structuredName": {
                            "firstName": "Jacek",
                            "lastName": "Krawczyk",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Krawczyk"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 120602965,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab36885fe66d08409b999ced5eec8c85858ace23",
            "isKey": false,
            "numCitedBy": 243,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-modified-genetic-algorithm-for-optimal-control-Michalewicz-Janikow",
            "title": {
                "fragments": [],
                "text": "A modified genetic algorithm for optimal control problems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1391181439"
                        ],
                        "name": "K. Sims",
                        "slug": "K.-Sims",
                        "structuredName": {
                            "firstName": "Karl",
                            "lastName": "Sims",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sims"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "Considered a true classic of Evolutionary Computation and Artificial Life is the ground-breaking work of Karl Sims [60]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "In contrast to Karl Sims [60], Bongard and Pfeifer put very few constraints on the evolved bodies and controllers, which grow together during a sophisticated morphogenesis that begins with a single cell."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3261121,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "7550e7381bc6f11d0b49a3dcfde9f0d3b9a89c93",
            "isKey": false,
            "numCitedBy": 621,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This article describes a system for the evolution and coevolution of virtual creatures that compete in physically simulated three-dimensional worlds. Pairs of individuals enter one-on-one contests in which they contend to gain control of a common resource. The winners receive higher relative fitness scores allowing them to survive and reproduce. Realistic dynamics simulation including gravity, collisions, and friction, restricts the actions to physically plausible behaviors. The morphology of these creatures and the neural systems for controlling their muscle forces are both genetically determined, and the morphology and behavior can adapt to each other as they evolve simultaneously. The genotypes are structured as directed graphs of nodes and connections, and they can efficiently but flexibly describe instructions for the development of creatures' bodies and control systems with repeating or recursive components. When simulated evolutions are performed with populations of competing creatures, interesting and diverse strategies and counterstrategies emerge."
            },
            "slug": "Evolving-3D-Morphology-and-Behavior-by-Competition-Sims",
            "title": {
                "fragments": [],
                "text": "Evolving 3D Morphology and Behavior by Competition"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "This article describes a system for the evolution and coevolution of virtual creatures that compete in physically simulated three-dimensional worlds that can adapt to each other as they evolve simultaneously."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Life"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47962020"
                        ],
                        "name": "T. Morimoto",
                        "slug": "T.-Morimoto",
                        "structuredName": {
                            "firstName": "Tetsuo",
                            "lastName": "Morimoto",
                            "middleNames": [],
                            "suffix": "M.D."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Morimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "15297921"
                        ],
                        "name": "J. Baerdemaeker",
                        "slug": "J.-Baerdemaeker",
                        "structuredName": {
                            "firstName": "Josse",
                            "lastName": "Baerdemaeker",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baerdemaeker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48168101"
                        ],
                        "name": "Y. Hashimoto",
                        "slug": "Y.-Hashimoto",
                        "structuredName": {
                            "firstName": "Yasushi",
                            "lastName": "Hashimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hashimoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 154
                            }
                        ],
                        "text": "When an EA is used to search for a near-optimal set of control parameters, the ANN will be used in fitness evaluation rather than the real control system [293]\u2013[297]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 108830691,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "0e9c718117768a040edc241f6fcc216570c58907",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "An-intelligent-approach-for-optimal-control-of-and-Morimoto-Baerdemaeker",
            "title": {
                "fragments": [],
                "text": "An intelligent approach for optimal control of fruit-storage process using neural networks and genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887716"
                        ],
                        "name": "E. C. Wasson",
                        "slug": "E.-C.-Wasson",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Wasson",
                            "middleNames": [
                                "C."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. C. Wasson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49007965"
                        ],
                        "name": "E. Boughton",
                        "slug": "E.-Boughton",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Boughton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Boughton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 87
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 21489125,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b99587096b2cbfa47d4981968ad532c9165e1fe",
            "isKey": false,
            "numCitedBy": 137,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Evolving-neural-networks-for-detecting-breast-Fogel-Wasson",
            "title": {
                "fragments": [],
                "text": "Evolving neural networks for detecting breast cancer."
            },
            "venue": {
                "fragments": [],
                "text": "Cancer letters"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17328656"
                        ],
                        "name": "A. Roy",
                        "slug": "A.-Roy",
                        "structuredName": {
                            "firstName": "Asim",
                            "lastName": "Roy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Roy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "123798980"
                        ],
                        "name": "Lark-Sang Kim",
                        "slug": "Lark-Sang-Kim",
                        "structuredName": {
                            "firstName": "Lark-Sang",
                            "lastName": "Kim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lark-Sang Kim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145033054"
                        ],
                        "name": "S. Mukhopadhyay",
                        "slug": "S.-Mukhopadhyay",
                        "structuredName": {
                            "firstName": "Somnath",
                            "lastName": "Mukhopadhyay",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mukhopadhyay"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 2459964,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c58c5c3ffd078bfbcd696adc6fb28cfa1071cfa",
            "isKey": false,
            "numCitedBy": 100,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-polynomial-time-algorithm-for-the-construction-of-Roy-Kim",
            "title": {
                "fragments": [],
                "text": "A polynomial time algorithm for the construction and training of a class of multilayer perceptrons"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2363971"
                        ],
                        "name": "J. Hertz",
                        "slug": "J.-Hertz",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Hertz",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Hertz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46486898"
                        ],
                        "name": "A. Krogh",
                        "slug": "A.-Krogh",
                        "structuredName": {
                            "firstName": "Anders",
                            "lastName": "Krogh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krogh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50760571"
                        ],
                        "name": "R. Palmer",
                        "slug": "R.-Palmer",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Palmer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Palmer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 71
                            }
                        ],
                        "text": "Most training algorithms, such as BP and conjugate gradient algorithms [7], [17]\u2013[19], are based on gradient descent."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 50
                            }
                        ],
                        "text": "More detailed discussion of ANN\u2019s can be found in [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 70
                            }
                        ],
                        "text": "A detailed review of BP and other learning algorithms can be found in [7], [17], and [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 136,
                                "start": 133
                            }
                        ],
                        "text": "Examples of popular learning rules include the delta rule, the Hebbian rule, the antiHebbian rule, and the competitive learning rule [7]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 86
                            }
                        ],
                        "text": "randomly generated learning rules, the evolution discovered the well-known delta rule [7], [274] and some of its vari-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 38623065,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6c0cbbd275bb43e09f0527a31ddd61824eca295b",
            "isKey": true,
            "numCitedBy": 6632,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Publisher: \nThis book is a comprehensive introduction to the neural network models currently under intensive study for computational applications. It is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning. It also provides coverage of neural network applications in a variety of problems of both theoretical and practical interest."
            },
            "slug": "Introduction-to-the-theory-of-neural-computation-Hertz-Krogh",
            "title": {
                "fragments": [],
                "text": "Introduction to the theory of neural computation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This book is a detailed, logically-developed treatment that covers the theory and uses of collective computational networks, including associative memory, feed forward networks, and unsupervised learning."
            },
            "venue": {
                "fragments": [],
                "text": "The advanced book program"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1808919"
                        ],
                        "name": "N. Radcliffe",
                        "slug": "N.-Radcliffe",
                        "structuredName": {
                            "firstName": "Nicholas",
                            "lastName": "Radcliffe",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Radcliffe"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 104
                            }
                        ],
                        "text": "Formal analysis of nonstandard representations and operators based on the concept of equivalent classes [115], [116] has given representations other than ary strings a more solid theoretical foundation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8985012,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "739cdeb06f9bfb011654391757101a536b41d17b",
            "isKey": false,
            "numCitedBy": 310,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "The conventional understanding of genetic algorithms depends upon analysis by schemata and the notion of intrinsic parallelism. For this reason, only -ary string representations have had any formal basis and non-standard representations and operators have been regarded largely as heuristics, rather than principled algorithms. This paper extends the analysis to general representations through identification of schemata as equivalence classes induced by implicit equivalence relations over the space of chromosomes."
            },
            "slug": "Equivalence-Class-Analysis-of-Genetic-Algorithms-Radcliffe",
            "title": {
                "fragments": [],
                "text": "Equivalence Class Analysis of Genetic Algorithms"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper extends the analysis to general representations through identification of schemata as equivalence classes induced by implicit equivalence relations over the space of chromosomes."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2328501"
                        ],
                        "name": "Adam Ghozeil",
                        "slug": "Adam-Ghozeil",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Ghozeil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Ghozeil"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 111
                            }
                        ],
                        "text": "Formal analysis of nonstandard representations and operators based on the concept of equivalent classes [115], [116] has given representations other than ary strings a more solid theoretical foundation."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 37807033,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "19dfd862185e49f1cdb82a213473966cba429ec8",
            "isKey": false,
            "numCitedBy": 72,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Consideration is given to the effects of representations and operators in evolutionary algorithms. In particular, theorems are presented which establish, under some general assumptions, that no choice of cardinality of a representation offers any intrinsic advantage over another. Functionally equivalent algorithms can be constructed regardless of the chosen representation. Further, a similar effective equivalence of variation operators is shown such that no intrinsic advantage accrues to any particular one-parent operator or any particular two-parent operator."
            },
            "slug": "A-note-on-representations-and-variation-operators-Fogel-Ghozeil",
            "title": {
                "fragments": [],
                "text": "A note on representations and variation operators"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Theorems are presented which establish that no choice of cardinality of a representation offers any intrinsic advantage over another, and that Functionally equivalent algorithms can be constructed regardless of the chosen representation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Evol. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398192342"
                        ],
                        "name": "Una-May O\u2019Reilly",
                        "slug": "Una-May-O\u2019Reilly",
                        "structuredName": {
                            "firstName": "Una-May",
                            "lastName": "O\u2019Reilly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Una-May O\u2019Reilly"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 29
                            }
                        ],
                        "text": "John Koza\u2019s genetic programs [40, 41], have been enlisted in the design of everything from electrical circuits to antennas to automatic controllers, some of which have been patented and many of which infringe upon existing patents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 31795221,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "465418b258a94b8cd44fc255654103b824714f2d",
            "isKey": false,
            "numCitedBy": 1705,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Reading Genetic Programming IE Automatic Discovery ofReusable Programs (GPII) in its entirety is not a task for the weak-willed because the book without appendices is about 650 pages. An entire previous book by the same author [1] is devoted to describing Genetic Programming (GP), while this book is a sequel extolling an extension called Automatically Defined Functions (ADFs). The author, John R. Koza, argues that ADFs can be used in conjunction with GP to improve its efficacy on large problems. \"An automatically defined function (ADF) is a function (i.e., subroutine, procedure, module) that is dynamically evolved during a run of genetic programming and which may be called by a calling program (e.g., a main program) that is simultaneously being evolved\" (p. 1). Dr. Koza recommends adding the ADF technique to the \"GP toolkit.\" The book presents evidence that it is possible to interpret GP with ADFs as performing either a top-down process of problem decomposition or a bottom-up process of representational change to exploit identified regularities. This is stated as Main Point 1. Main Point 2 states that ADFs work by exploiting inherent regularities, symmetries, patterns, modularities, and homogeneities within a problem, though perhaps in ways that are very different from the style of programmers. Main Points 3 to 7 are appropriately qualified statements to the effect that, with a variety of problems, ADFs pay off be-"
            },
            "slug": "Genetic-Programming-II:-Automatic-Discovery-of-O\u2019Reilly",
            "title": {
                "fragments": [],
                "text": "Genetic Programming II: Automatic Discovery of Reusable Programs."
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This book presents evidence that it is possible to interpret GP with ADFs as performing either a top-down process of problem decomposition or a bottom-up process of representational change to exploit identified regularities."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Life"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7785881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "isKey": false,
            "numCitedBy": 3518,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application."
            },
            "slug": "Optimal-Brain-Damage-LeCun-Denker",
            "title": {
                "fragments": [],
                "text": "Optimal Brain Damage"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A class of practical and nearly optimal schemes for adapting the size of a neural network by using second-derivative information to make a tradeoff between network complexity and training set error is derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2887716"
                        ],
                        "name": "E. C. Wasson",
                        "slug": "E.-C.-Wasson",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Wasson",
                            "middleNames": [
                                "C."
                            ],
                            "suffix": "III"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. C. Wasson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49007965"
                        ],
                        "name": "E. Boughton",
                        "slug": "E.-Boughton",
                        "structuredName": {
                            "firstName": "Edward",
                            "lastName": "Boughton",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Boughton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143700679"
                        ],
                        "name": "V. W. Porto",
                        "slug": "V.-W.-Porto",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Porto",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. W. Porto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 135
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 606980,
            "fieldsOfStudy": [
                "Medicine",
                "Computer Science"
            ],
            "id": "8d705786eec92d23cab108903fdb8d7ea78ba625",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-step-toward-computer-assisted-mammography-using-Fogel-Wasson",
            "title": {
                "fragments": [],
                "text": "A step toward computer-assisted mammography using evolutionary programming and neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "Cancer letters"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145160279"
                        ],
                        "name": "P. Hancock",
                        "slug": "P.-Hancock",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Hancock",
                            "middleNames": [
                                "J.",
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Hancock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144708497"
                        ],
                        "name": "Leslie S. Smith",
                        "slug": "Leslie-S.-Smith",
                        "structuredName": {
                            "firstName": "Leslie",
                            "lastName": "Smith",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Leslie S. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34721714"
                        ],
                        "name": "W. A. Phillips",
                        "slug": "W.-A.-Phillips",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Phillips",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. A. Phillips"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9313977,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "6b41f18729a3459f536dd7945f00115a8354cf38",
            "isKey": false,
            "numCitedBy": 63,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We show that a form of synaptic plasticity recently discovered in slices of the rat visual cortex (Artola et al. 1990) can support an error-correcting learning rule. The rule increases weights when both pre- and postsynaptic units are highly active, and decreases them when pre-synaptic activity is high and postsynaptic activation is less than the threshold for weight increment but greater than a lower threshold. We show that this rule corrects false positive outputs in feedforward associative memory, that in an appropriate opponent-unit architecture it corrects misses, and that it performs better than the optimal Hebbian learning rule reported by Willshaw and Dayan (1990)."
            },
            "slug": "A-Biologically-Supported-Error-Correcting-Learning-Hancock-Smith",
            "title": {
                "fragments": [],
                "text": "A Biologically Supported Error-Correcting Learning Rule"
            },
            "tldr": {
                "abstractSimilarityScore": 70,
                "text": "It is shown that a form of synaptic plasticity recently discovered in slices of the rat visual cortex can support an error-correcting learning rule and that this rule performs better than the optimal Hebbian learning rule reported by Willshaw and Dayan (1990)."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2716460"
                        ],
                        "name": "R. Calabretta",
                        "slug": "R.-Calabretta",
                        "structuredName": {
                            "firstName": "Raffaele",
                            "lastName": "Calabretta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Calabretta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015062"
                        ],
                        "name": "S. Nolfi",
                        "slug": "S.-Nolfi",
                        "structuredName": {
                            "firstName": "Stefano",
                            "lastName": "Nolfi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nolfi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1762202"
                        ],
                        "name": "D. Parisi",
                        "slug": "D.-Parisi",
                        "structuredName": {
                            "firstName": "Domenico",
                            "lastName": "Parisi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Parisi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143737778"
                        ],
                        "name": "G. Wagner",
                        "slug": "G.-Wagner",
                        "structuredName": {
                            "firstName": "G\u00fcnter",
                            "lastName": "Wagner",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Wagner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[7] in which portions (modules) of direct-encoding genomes can duplicate to form competing control units for a robot."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[7] shows clear advantages of modularity (of genotype, phenotype and mapping) for tasks that require functional specialization."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 19,
                                "start": 16
                            }
                        ],
                        "text": "The networks of [7] have a human-designed framework tailored for neutral complexification, including a binary duplicate gene associated specifically with a hand-crafted group of neurons and connections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7535038,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "10e4faabca3d8f6172303a06a5d3c2d226b85f6b",
            "isKey": true,
            "numCitedBy": 129,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "The evolution of simulated robots with three different architectures is studied in this article. We compare a nonmodular feed-forward network, a hardwired modular, and a duplication-based modular motor control network. We conclude that both modular architectures outperform the non-modular architecture, both in terms of rate of adaptation as well as the level of adaptation achieved. The main difference between the hardwired and duplication-based modular architectures is that in the latter the modules reached a much higher degree of functional specialization of their motor control units with regard to high-level behavioral functions. The hardwired architectures reach the same level of performance, but have a more distributed assignment of functional tasks to the motor control units. We conclude that the mechanism through which functional specialization is achieved is similar to the mechanism proposed for the evolution of duplicated genes. It is found that the duplication of multifunctional modules first leads to a change in the regulation of the module, leading to a differentiation of the functional context in which the module is used. Then the module adapts to the new functional context. After this second step the system is locked into a functionally specialized state. We suggest that functional specialization may be an evolutionary absorption state."
            },
            "slug": "Duplication-of-Modules-Facilitates-the-Evolution-of-Calabretta-Nolfi",
            "title": {
                "fragments": [],
                "text": "Duplication of Modules Facilitates the Evolution of Functional Specialization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Both modular architectures outperform the non-modular architecture, both in terms of rate of adaptation as well as the level of adaptation achieved, and suggest that functional specialization may be an evolutionary absorption state."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Life"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47962020"
                        ],
                        "name": "T. Morimoto",
                        "slug": "T.-Morimoto",
                        "structuredName": {
                            "firstName": "Tetsuo",
                            "lastName": "Morimoto",
                            "middleNames": [],
                            "suffix": "M.D."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Morimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2105637077"
                        ],
                        "name": "J. Suzuki",
                        "slug": "J.-Suzuki",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Suzuki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Suzuki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48168101"
                        ],
                        "name": "Y. Hashimoto",
                        "slug": "Y.-Hashimoto",
                        "structuredName": {
                            "firstName": "Yasushi",
                            "lastName": "Hashimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hashimoto"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62632401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a5257b7351d7694e7ed1f20e1603ef49cfdcf141",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Optimization-of-a-fuzzy-controller-for-fruit-using-Morimoto-Suzuki",
            "title": {
                "fragments": [],
                "text": "Optimization of a fuzzy controller for fruit storage using neural networks and genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3214740"
                        ],
                        "name": "Torsten Reil",
                        "slug": "Torsten-Reil",
                        "structuredName": {
                            "firstName": "Torsten",
                            "lastName": "Reil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Torsten Reil"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Another often-cited GRN model, introduced in [54], appears at the bottom left of Figure 18."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13642057,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "f77f0d3643cf2a8b43e4f17ea0b780c899980c82",
            "isKey": false,
            "numCitedBy": 168,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "An artificial genome with biologically plausible properties was developed and the dynamics of gene expression were studied. The model differs from previous approaches, such as Random Boolean Nets [1], in that it is entirely based on template matching in a nucleotide-like sequence. Genes activate or inhibit other genes by binding to their regulatory sequences. \n \nThe results of the experiments suggest that many features of real-life development, such as cyclic gene activity, differentiation into multiple cell types, and robusteness may be inherent properties of a template-matching system rather than necessarily designed from scratch by Natural Selection. Moreover, the system may provide a new hypothesis about the role of junk DNA in real genomes. In addition to these biological implications, the approach used here is thought to provide a flexible basis for future simulations of morphogenesis."
            },
            "slug": "Dynamics-of-Gene-Expression-in-an-Artificial-Genome-Reil",
            "title": {
                "fragments": [],
                "text": "Dynamics of Gene Expression in an Artificial Genome - Implications for Biological and Artificial Ontogeny"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The results of the experiments suggest that many features of real-life development, such as cyclic gene activity, differentiation into multiple cell types, and robusteness may be inherent properties of a template-matching system rather than necessarily designed from scratch by Natural Selection."
            },
            "venue": {
                "fragments": [],
                "text": "ECAL"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952703"
                        ],
                        "name": "Y. Chauvin",
                        "slug": "Y.-Chauvin",
                        "structuredName": {
                            "firstName": "Yves",
                            "lastName": "Chauvin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Chauvin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56530205,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23e95aa03b5ddf4380af437581372e4d8414c121",
            "isKey": false,
            "numCitedBy": 520,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Contents: D.E. Rumelhart, R. Durbin, R. Golden, Y. Chauvin, Backpropagation: The Basic Theory. A. Waibel, T. Hanazawa, G. Hinton, K. Shikano, K.J. Lang, Phoneme Recognition Using Time-Delay Neural Networks. C. Schley, Y. Chauvin, V. Henkle, Automated Aircraft Flare and Touchdown Control Using Neural Networks. F.J. Pineda, Recurrent Backpropagation Networks. M.C. Mozer, A Focused Backpropagation Algorithm for Temporal Pattern Recognition. D.H. Nguyen, B. Widrow, Nonlinear Control with Neural Networks. M.I. Jordan, D.E. Rumelhart, Forward Models: Supervised Learning with a Distal Teacher. S.J. Hanson, Backpropagation: Some Comments and Variations. A. Cleeremans, D. Servan-Schreiber, J.L. McClelland, Graded State Machines: The Representation of Temporal Contingencies in Feedback Networks. S. Becker, G.E. Hinton, Spatial Coherence as an Internal Teacher for a Neural Network. J.R. Bachrach, M.C. Mozer, Connectionist Modeling and Control of Finite State Systems Given Partial State Information. P. Baldi, Y. Chauvin, K. Hornik, Backpropagation and Unsupervised Learning in Linear Networks. R.J. Williams, D. Zipser, Gradient-Based Learning Algorithms for Recurrent Networks and Their Computational Complexity. P. Baldi, Y. Chauvin, When Neural Networks Play Sherlock Homes. P. Baldi, Gradient Descent Learning Algorithms: A Unified Perspective."
            },
            "slug": "Backpropagation:-theory,-architectures,-and-Chauvin-Rumelhart",
            "title": {
                "fragments": [],
                "text": "Backpropagation: theory, architectures, and applications"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This chapter discusses Backpropagation and Unsupervised Learning in Linear Networks, a model for Spatial Coherence as an Internal Teacher for a Neural Network, and Gradient Descent Learning Algorithms for Recurrent Networks and Their Computational Complexity."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2120847483"
                        ],
                        "name": "Mukesh J. Patel",
                        "slug": "Mukesh-J.-Patel",
                        "structuredName": {
                            "firstName": "Mukesh",
                            "lastName": "Patel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mukesh J. Patel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145513516"
                        ],
                        "name": "Vasant G Honavar",
                        "slug": "Vasant-G-Honavar",
                        "structuredName": {
                            "firstName": "Vasant",
                            "lastName": "Honavar",
                            "middleNames": [
                                "G"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vasant G Honavar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31276291"
                        ],
                        "name": "K. Balakrishnan",
                        "slug": "K.-Balakrishnan",
                        "structuredName": {
                            "firstName": "Karthik",
                            "lastName": "Balakrishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Balakrishnan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "A detailed account of G2L can be found in [4], which includes a description of their complex binary chromosome for encoding grammar rules."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64420987,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "849316317ed4975d6e450183fa5288b2adc21a1b",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: Introduction, Artificial Neural Network Architectures, Evolutionary Computation and Neural Network Architectures, G2L-Systems Coding Neural Networks, Lamarckian Evolution and the Baldwin Effect, Evolving Network Architectures Directed by Local Search, Conclusion, Acknowledgments, References"
            },
            "slug": "Combined-Biological-Metaphors-Patel-Honavar",
            "title": {
                "fragments": [],
                "text": "Combined Biological Metaphors"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This chapter contains sections titled: Introduction, Artificial Neural Network Architectures, Evolutionary Computation and Neural Networkitectures, G2L-Systems Coding Neural Networks, Lamarckian Evolution and the Baldwin Effect, Evolving Network Architecture Directed by Local Search, and Conclusion."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144239030"
                        ],
                        "name": "C. Nikolopoulos",
                        "slug": "C.-Nikolopoulos",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Nikolopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Nikolopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52323333"
                        ],
                        "name": "P. Fellrath",
                        "slug": "P.-Fellrath",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Fellrath",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Fellrath"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 167,
                                "start": 162
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 94
                            }
                        ],
                        "text": "The second approach evolves architectures and connection weights simultaneously [149], [179], [180], [182], [185]\u2013[200]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 56522887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "be837bdf46604d92235e2742b388ab524e21b448",
            "isKey": false,
            "numCitedBy": 27,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": "Investor is a hybrid expert system which incorporates both the connectionist and the logic programming paradigms in a unified model. Investor employs a neural network model to detect the interest rate trends and this information is then used as input to its deductive reasoning component, which infers the appropriate investment strategy.<<ETX>>"
            },
            "slug": "A-hybrid-expert-system-for-investment-advising-Nikolopoulos-Fellrath",
            "title": {
                "fragments": [],
                "text": "A hybrid expert system for investment advising"
            },
            "tldr": {
                "abstractSimilarityScore": 84,
                "text": "Investor is a hybrid expert system which incorporates both the connectionist and the logic programming paradigms in a unified model that infers the appropriate investment strategy."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of 1994 IEEE International Conference on Neural Networks (ICNN'94)"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50112753"
                        ],
                        "name": "Seong-Whan Lee",
                        "slug": "Seong-Whan-Lee",
                        "structuredName": {
                            "firstName": "Seong-Whan",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seong-Whan Lee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 8,
                                "start": 4
                            }
                        ],
                        "text": "Lee [81] and many others [32], [136]\u2013[138] used GA\u2019s to search for a near-optimal set of initial connection weights and then used BP to perform local search from these initial weights."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "Hybrid training has been used successfully in many application areas [32], [67], [70], [71], [74], [80], [81], [86],"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11743607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1a59fa263dc38d26a804af93789bb391cddeb250",
            "isKey": false,
            "numCitedBy": 149,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a simple multilayer cluster neural network with five independent subnetworks for off-line recognition of totally unconstrained handwritten numerals. We also show that the use of genetic algorithms for avoiding the problem of finding local minima in training the multilayer cluster neural network with gradient descent technique reduces error rates."
            },
            "slug": "Off-line-recognition-of-totally-unconstrained-using-Lee",
            "title": {
                "fragments": [],
                "text": "Off-line recognition of totally unconstrained handwritten numerals using multilayer cluster neural network"
            },
            "tldr": {
                "abstractSimilarityScore": 87,
                "text": "A simple multilayer cluster neural network with five independent subnetworks for off-line recognition of totally unconstrained handwritten numerals and it is shown that the use of genetic algorithms for avoiding the problem of finding local minima in training the multi-layer network with gradient descent technique reduces error rates."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 12th IAPR International Conference on Pattern Recognition, Vol. 3 - Conference C: Signal Processing (Cat. No.94CH3440-5)"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34675248"
                        ],
                        "name": "P. Eggenberger",
                        "slug": "P.-Eggenberger",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Eggenberger",
                            "middleNames": [
                                "Heinrich"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Eggenberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12866385,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d492fb007ceaf53f9a165313d8fd918a22e8f5b4",
            "isKey": false,
            "numCitedBy": 198,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Most simulations of biological evolu tion depend on a rather restricted set of properties In this paper a richer model based on di erential gene expres sion is introduced to control develop mental processes in an arti cial evolu tionary system Di erential gene expres sion is used to get di erent cell types and to modulate cell division and cell death One of the advantages using developmental processes in evolutionary systems is the reduction of the informa tion needed in the genome to encode e g shapes or cell types which results in bet ter scaling behavior of the system My result showed that the shaping of multi cellular organisms in d is possible with the proposed system"
            },
            "slug": "Evolving-Morphologies-of-Simulated-3d-Organisms-on-Eggenberger",
            "title": {
                "fragments": [],
                "text": "Evolving Morphologies of Simulated 3d Organisms Based on Differential Gene Expression"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The result showed that the shaping of multi cellular organisms in d is possible with the proposed model and the reduction of the information needed in the genome to encode shapes or cell types which results in scaling behavior of the system."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145184042"
                        ],
                        "name": "J. W. Merrill",
                        "slug": "J.-W.-Merrill",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Merrill",
                            "middleNames": [
                                "W.",
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. W. Merrill"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35190494"
                        ],
                        "name": "R. Port",
                        "slug": "R.-Port",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Port",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Port"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20803829,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f4ac1c0a349e6dc787ad596ddd6e828688b9022a",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fractally-configured-neural-networks-Merrill-Port",
            "title": {
                "fragments": [],
                "text": "Fractally configured neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "108275637"
                        ],
                        "name": "J. Baldwin",
                        "slug": "J.-Baldwin",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Baldwin",
                            "middleNames": [
                                "Mark"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Baldwin"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 144
                            }
                        ],
                        "text": "In [13], a similar dynamic layer, this time in a Kohonen network [39], is used to investigate an alternate interpretation of the Baldwin Effect [2], a theory concerning interactions between learning and evolution that has a strong following among EA researchers [68]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7059820,
            "fieldsOfStudy": [
                "Geology"
            ],
            "id": "0c1386d88cb54eb17c6a2745cd4bf15dbb3f2b09",
            "isKey": false,
            "numCitedBy": 1624,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "In several recent publications I have developed, from different points of view, some considerations which tend to bring out a certain influence at work in organic evolutionwhich I venture to call \u201ca new factor.\u201d I give below a list of references1 to these publications and shall refer to them by number as this paper proceeds. The object of the present paper is to gather into one sketch an outline of the view of the process of development which these different publications have hinged upon. The problems involved in a theory of organic development may be gathered up under three great heads: Ontogeny, Phylogeny, Heredity. The general consideration, the \u201cfactor\u201d which I propose to bring out, is operative in the first instance, in the field of Ontogeny; I shall consequently speak first of the problem of Ontogeny, then of that of Phylogeny, in so far as the topic dealt with makes it necessary, then of that of Heredity, under the same limitation, and finally, give some definitions and conclusions."
            },
            "slug": "A-New-Factor-in-Evolution-Baldwin",
            "title": {
                "fragments": [],
                "text": "A New Factor in Evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "The object of the present paper is to gather into one sketch an outline of the view of the process of development which these different publications have hinged upon."
            },
            "venue": {
                "fragments": [],
                "text": "The American Naturalist"
            },
            "year": 1896
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082896"
                        ],
                        "name": "J. Utans",
                        "slug": "J.-Utans",
                        "structuredName": {
                            "firstName": "Joachim",
                            "lastName": "Utans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Utans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145016534"
                        ],
                        "name": "J. Moody",
                        "slug": "J.-Moody",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Moody",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Moody"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 46
                            }
                        ],
                        "text": "on the information theory or statistics [226]\u2013[228] can"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18824280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "81d635505b1d4a6b0a04697c5b04c586613c6824",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "The notion of generalization can be defined precisely as the prediction risk, the expected performance of an estimator on new observations. The authors propose the prediction risk as a measure of the generalization ability of multi-layer perceptron networks and use it to select the optimal network architecture. The prediction risk must be estimated from the available data. The authors approximate the prediction risk by v-fold cross-validation and asymptotic estimates of generalized cross-validation or H. Akaike's (1970) final prediction error. They apply the technique to the problem of predicting corporate bond ratings. This problem is very attractive as a case study, since it is characterized by the limited availability of the data and by the lack of complete a priori information that could be used to impose a structure to the network architecture.<<ETX>>"
            },
            "slug": "Selecting-neural-network-architectures-via-the-to-Utans-Moody",
            "title": {
                "fragments": [],
                "text": "Selecting neural network architectures via the prediction risk: application to corporate bond rating prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The authors propose the prediction risk as a measure of the generalization ability of multi-layer perceptron networks and use it to select the optimal network architecture."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings First International Conference on Artificial Intelligence Applications on Wall Street"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "They include evolution strategies (ES) [8], [9], evolutionary programming (EP) [10], [11],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62755706,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0bd665f5deb3449b03d60bfa147449d41bc21539",
            "isKey": false,
            "numCitedBy": 313,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "System-Identification-Through-Simulated-Evolution:-Fogel",
            "title": {
                "fragments": [],
                "text": "System Identification Through Simulated Evolution: A Machine Learning Approach to Modeling"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1837034"
                        ],
                        "name": "Z. Bo",
                        "slug": "Z.-Bo",
                        "structuredName": {
                            "firstName": "Zhi-qian",
                            "lastName": "Bo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Bo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153175392"
                        ],
                        "name": "H. Li",
                        "slug": "H.-Li",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Li",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30393665"
                        ],
                        "name": "R. Aggarwal",
                        "slug": "R.-Aggarwal",
                        "structuredName": {
                            "firstName": "Raj",
                            "lastName": "Aggarwal",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Aggarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2965546"
                        ],
                        "name": "A. Johns",
                        "slug": "A.-Johns",
                        "structuredName": {
                            "firstName": "Allan",
                            "lastName": "Johns",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Johns"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50347335"
                        ],
                        "name": "P. Moore",
                        "slug": "P.-Moore",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Moore",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Moore"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 110689506,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "21a17305af8282e7dbb710dcf39384908e31dda5",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper first introduces a novel fault transient measurement arrangement, and the simulation studies relating to faults on typical 400 kV vertical construction transmission lines to examine and analyse the fault generated current signals. The application of the ANN based fault measurement technique is then described. An Evolving Connectionist Network (ECN) with a GA is investigated to solve the problem of internal and external fault identification. The technique is based on utilising fault generated high frequency current signals to essentially recognise the various patterns generated within the frequency spectra of the signals, for the purposes of accurately distinguishing between internal and external faults under various system and fault conditions. In particular the effect on the higher frequency current signals (and therefore on the performance of the ANN-based protection technique) due, for example, to fault type, fault position and fault inception angle for nonlinear arcing faults is highlighted. The paper clearly demonstrates that the new ECN method can successfully optimise a connectionist network for fault identification in EHV transmission lines thereby significantly improving performance over backpropagation learning algorithm based NN."
            },
            "slug": "Non-communication-protection-of-transmission-line-Bo-Li",
            "title": {
                "fragments": [],
                "text": "Non-communication protection of transmission line based on genetic evolved neural network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737243"
                        ],
                        "name": "L. Fogel",
                        "slug": "L.-Fogel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Fogel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46400209"
                        ],
                        "name": "A. J. Owens",
                        "slug": "A.-J.-Owens",
                        "structuredName": {
                            "firstName": "Alvin",
                            "lastName": "Owens",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. J. Owens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1446900976"
                        ],
                        "name": "M. J. Walsh",
                        "slug": "M.-J.-Walsh",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Walsh",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. J. Walsh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 79
                            }
                        ],
                        "text": "They include evolution strategies (ES) [8], [9], evolutionary programming (EP) [10], [11],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 62252283,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a9e41a611b3b57b828775a45a7d74a1c75ed3f20",
            "isKey": false,
            "numCitedBy": 3238,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This chapter contains sections titled: References Artificial Intelligence through a Simulation of Evolution Natural Automata and Prosthetic Devices"
            },
            "slug": "Artificial-Intelligence-through-Simulated-Evolution-Fogel-Owens",
            "title": {
                "fragments": [],
                "text": "Artificial Intelligence through Simulated Evolution"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "This chapter contains sections titled: References Artificial Intelligence through a Simulation of Evolution Natural Automata and Prosthetic Devices and Artificial intelligence through a simulation of Evolution natural automata and prosthetic devices."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1966
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46917325"
                        ],
                        "name": "A. Chilingarian",
                        "slug": "A.-Chilingarian",
                        "structuredName": {
                            "firstName": "Ashot",
                            "lastName": "Chilingarian",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Chilingarian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380891951"
                        ],
                        "name": "S. Ter-Antonyan",
                        "slug": "S.-Ter-Antonyan",
                        "structuredName": {
                            "firstName": "Samvel",
                            "lastName": "Ter-Antonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ter-Antonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145181300"
                        ],
                        "name": "A. Vardanyan",
                        "slug": "A.-Vardanyan",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "Vardanyan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vardanyan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 121096346,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b5a7db7e0c571ef1de8c0f89deaafb623acc90e4",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 4,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "THE-COMPARISON-OF-BAYESIAN-AND-NEURAL-TECHNIQUES-IN-Chilingarian-Ter-Antonyan",
            "title": {
                "fragments": [],
                "text": "THE COMPARISON OF BAYESIAN AND NEURAL TECHNIQUES IN PROBLEMS OF CLASSIFICATION TO MULTIPLE CATEGORIES"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144473519"
                        ],
                        "name": "M. Mozer",
                        "slug": "M.-Mozer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mozer",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Mozer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1748557"
                        ],
                        "name": "P. Smolensky",
                        "slug": "P.-Smolensky",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Smolensky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Smolensky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17651092,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a87953825b0bea2a5d52bfccf09d2518295c5053",
            "isKey": false,
            "numCitedBy": 662,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper proposes a means of using the knowledge in a network to determine the functionality or relevance of individual units, both for the purpose of understanding the network's behavior and improving its performance. The basic idea is to iteratively train the network to a certain performance criterion, compute a measure of relevance that identifies which input or hidden units are most critical to performance, and automatically trim the least relevant units. This skeletonization technique can be used to simplify networks by eliminating units that convey redundant information; to improve learning performance by first learning with spare hidden units and then trimming the unnecessary ones away, thereby constraining generalization; and to understand the behavior of networks in terms of minimal \"rules.\""
            },
            "slug": "Skeletonization:-A-Technique-for-Trimming-the-Fat-a-Mozer-Smolensky",
            "title": {
                "fragments": [],
                "text": "Skeletonization: A Technique for Trimming the Fat from a Network via Relevance Assessment"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The basic idea is to iteratively train the network to a certain performance criterion, compute a measure of relevance that identifies which input or hidden units are most critical to performance, and automatically trim the least relevant units."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36117718"
                        ],
                        "name": "S. Sette",
                        "slug": "S.-Sette",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Sette",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Sette"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3175041"
                        ],
                        "name": "L. Boullart",
                        "slug": "L.-Boullart",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Boullart",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Boullart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73777628"
                        ],
                        "name": "L. Van Langenhove",
                        "slug": "L.-Van-Langenhove",
                        "structuredName": {
                            "firstName": "Lieva",
                            "lastName": "Van Langenhove",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Van Langenhove"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152453868"
                        ],
                        "name": "P. Kiekens",
                        "slug": "P.-Kiekens",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Kiekens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Kiekens"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 113223380,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b1835f21ecf9ee9a6b18abd01c55b4177faae9a",
            "isKey": false,
            "numCitedBy": 70,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "An important aspect of the fiber-to-yam production process is the quality of the resulting yarn. The yarn should have optimal product characteristics (and minimal faults). In theory, this objective can be realized using an optimization algorithm. The complexity of a fiber-to-yarn process is very high, however, and no mathematical function is known to exist that represents the whole process. This paper presents a method to simulate and optimize the fiber-to-yam production process using a neural network combined with a genetic algorithm. The neural network is used to model the process, with the machine settings and fiber quality parameters as input and the yarn tenacity and elongation as output. The genetic algorithm is used afterward to optimize the input parameters for obtaining the best yarns. Since this is a multi-objective optimization, the genetic algorithm is enforced with a sharing function and a Pareto optimization. The paper shows that simultaneous optimization of yarn qualities is easily achieved as a function of the necessary (optimal) input parameters, and that the results are considerably better than current manual machine intervention. The last part of the paper is dedicated to finding an optimal mixture of available fiber qualities based on the predictions of the genetic algorithm."
            },
            "slug": "Optimizing-the-Fiber-to-Yarn-Production-Process-a-Sette-Boullart",
            "title": {
                "fragments": [],
                "text": "Optimizing the Fiber-to-Yarn Production Process with a Combined Neural Network/Genetic Algorithm Approach"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The paper shows that simultaneous optimization of yarn qualities is easily achieved as a function of the necessary (optimal) input parameters, and that the results are considerably better than current manual machine intervention."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35020713"
                        ],
                        "name": "Seung-Soo Han",
                        "slug": "Seung-Soo-Han",
                        "structuredName": {
                            "firstName": "Seung-Soo",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Seung-Soo Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "41211872"
                        ],
                        "name": "G. May",
                        "slug": "G.-May",
                        "structuredName": {
                            "firstName": "Gary",
                            "lastName": "May",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. May"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 110623300,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "9810b8268e9727eb400d99366b9511b45a425f12",
            "isKey": false,
            "numCitedBy": 49,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Silicon oxide (SiO/sub 2/) films have extensive applications in integrated circuit fabrication technology, including passivation layers for integrated circuits, diffusion or photolithographic masks, and interlayer dielectrics for metal-insulator structures such as MOS transistors or multichip modules. The properties of SiO/sub 2/ films deposited by plasma enhanced chemical vapor deposition (PECVD) are determined by the nature and composition of the plasma, which is in turn controlled by the deposition variables involved in the PECVD process. The complex nature of particle dynamics within a plasma makes it very difficult to quantify the exact relationship between deposition conditions and critical output parameters reflecting film quality. In this study, the synthesis and optimization of process recipes using genetic algorithms is introduced. In order to characterize the PECVD of SiO/sub 2/ films deposited under varying conditions, a central composite designed experiment has been performed. Data from this experiment was then used to develop neural network based process models. A recipe synthesis procedure was then performed using the optimized neural network models to generate the necessary deposition conditions to obtain several novel film qualities, including zero stress, 100% uniformity, low permittivity, and minimal impurity concentration. This synthesis procedure utilized genetic algorithms, Powell's algorithm, the simplex method, and hybrid combinations thereof. Recipes predicted by these techniques were verified by experiment, and the performance of each synthesis method are compared. It was found that the genetic algorithm-based recipes generally produced films of superior quality. Deposition was carried out in a Plasma Therm 700 series PECVD system."
            },
            "slug": "Using-neural-network-process-models-to-perform-via-Han-May",
            "title": {
                "fragments": [],
                "text": "Using neural network process models to perform PECVD silicon dioxide recipe synthesis via genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701656"
                        ],
                        "name": "James L. McClelland",
                        "slug": "James-L.-McClelland",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "McClelland",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James L. McClelland"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60899176,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "249ce7a85b158c16ba108451070c07aa1156e7eb",
            "isKey": false,
            "numCitedBy": 1286,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "1. Very rarely, a book is published which not only advances our knowledge of a particular topic, but fundamentally recasts our methods of investigating and thinking about large tracts of the map of learning. Linguists remember 1957 as the publication year of Noam Chomsky's Syntactic structures-a book whose ostensible subjects were the structure of English grammatical rules and the goals of grammatical description, but which can be seen with hindsight as the first shot in an intellectual revolution which ended by radically changing the texture of day-to-day research activity and discourse throughout almost all of linguistics, and in substantial parts of other cognition-related disciplines. In decades to come, perhaps 1986 will be remembered by academics as the year of publication of the pair of volumes reviewed here: they constitute the first large-scale public statement of an intellectual paradigm fully as revolutionary as the generative paradigm ever was (there have been scattered journal articles in the preceding four or five years). I would go further and suggest that, if the promises of this book can be redeemed, the contrast in linguistics and neighboring disciplines between the 1990's and the 1970's will be significantly greater than the contrast between the 1970's and the 1950's. (I need hardly add, of course, that it is one thing to fire an opening salvo, but another to achieve ultimate predominance.) The new paradigm is called Parallel Distributed Processing by the sixteen writers who contributed to this book, many of whom work either at the University of California, San Diego, or at Carnegie-Mellon University in Pittsburgh. Some other researchers (e.g. Feldman 1985) use the term 'connectionism' for the same concept. These two volumes comprise 26 chapters which, among them, (i) explain the over-all nature and aims of PDP/connectionist models, (ii) define a family of specific variants of the general paradigm, and (iii) exemplify it by describing experiments in which PDP models were used to simulate human performance in various cognitive domains. The experiments, inevitably, treat their respective domains in a simplified, schematic way by comparison with the endless complexity found in any real-life cognitive area; but simplification in this case does not mean trivialization. There are also auxiliary chapters on relevant related topics; thus Chap. 9, by M. I. JORDAN, is a tutorial on linear algebra, a branch of mathematics having special significance for the PDP paradigm. (Each chapter is attributed to a particular author or"
            },
            "slug": "Parallel-Distributed-Processing:-Explorations-in-of-Rumelhart-McClelland",
            "title": {
                "fragments": [],
                "text": "Parallel Distributed Processing: Explorations in the Microstructures of Cognition"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "In decades to come, perhaps 1986 will be remembered by academics as the year of publication of the pair of volumes reviewed here: they constitute the first large-scale public statement of an intellectual paradigm fully as revolutionary as the generative paradigm ever was."
            },
            "venue": {
                "fragments": [],
                "text": "Language"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "23111666"
                        ],
                        "name": "S. Fels",
                        "slug": "S.-Fels",
                        "structuredName": {
                            "firstName": "Sidney",
                            "lastName": "Fels",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fels"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 41170404,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "703b3e9dc9ad014cb1baa840f5a03b33c67021d8",
            "isKey": false,
            "numCitedBy": 427,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "To illustrate the potential of multilayer neural networks for adaptive interfaces, a VPL Data-Glove connected to a DECtalk speech synthesizer via five neural networks was used to implement a hand-gesture to speech system. Using minor variations of the standard backpropagation learning procedure, the complex mapping of hand movements to speech is learned using data obtained from a single ;speaker' in a simple training phase. With a 203 gesture-to-word vocabulary, the wrong word is produced less than 1% of the time, and no word is produced about 5% of the time. Adaptive control of the speaking rate and word stress is also available. The training times and final performance speed are improved by using small, separate networks for each naturally defined subtask. The system demonstrates that neural networks can be used to develop the complex mappings required in a high bandwidth interface that adapts to the individual user."
            },
            "slug": "Glove-Talk:-a-neural-network-interface-between-a-a-Fels-Hinton",
            "title": {
                "fragments": [],
                "text": "Glove-Talk: a neural network interface between a data-glove and a speech synthesizer"
            },
            "tldr": {
                "abstractSimilarityScore": 76,
                "text": "To illustrate the potential of multilayer neural networks for adaptive interfaces, a VPL Data-Glove connected to a DECtalk speech synthesizer via five neural networks was used to implement a hand-gesture to speech system, demonstrating that neural networks can be used to develop the complex mappings required in a high bandwidth interface that adapts to the individual user."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143700679"
                        ],
                        "name": "V. W. Porto",
                        "slug": "V.-W.-Porto",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Porto",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. W. Porto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737243"
                        ],
                        "name": "L. Fogel",
                        "slug": "L.-Fogel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Fogel",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fogel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45693699,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a39a4876651fccbb1f3892007f7a5396d8f2996d",
            "isKey": false,
            "numCitedBy": 148,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Investigates three potential neural network training algorithms in processing active sonar returns. Although all three methods generate reasonable probabilities of detection and false alarm in discriminating between man-made objects and background events, the stochastic training methods of simulated annealing and evolutionary programming outperform backpropagation. >"
            },
            "slug": "Alternative-Neural-Network-Training-Methods-Porto-Fogel",
            "title": {
                "fragments": [],
                "text": "Alternative Neural Network Training Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "Investigates three potential neural network training algorithms in processing active sonar returns and finds that the stochastic training methods of simulated annealing and evolutionary programming outperform backpropagation."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Expert"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2737945"
                        ],
                        "name": "H. Akaike",
                        "slug": "H.-Akaike",
                        "structuredName": {
                            "firstName": "Hirotugu",
                            "lastName": "Akaike",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Akaike"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 265,
                                "start": 260
                            }
                        ],
                        "text": "Unfortunately, measuring generalization quantitatively and accurately is almost impossible in practice [298] although there are many theories and criteria on generalization, such as the minimum description length (MDL) [299], Akaike information criteria (AIC) [300], and minimum message length (MML) [301]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 411526,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "50a42ed2f81b9fe150883a6c89194c88a9647106",
            "isKey": false,
            "numCitedBy": 42248,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The history of the development of statistical hypothesis testing in time series analysis is reviewed briefly and it is pointed out that the hypothesis testing procedure is not adequately defined as the procedure for statistical model identification. The classical maximum likelihood estimation procedure is reviewed and a new estimate minimum information theoretical criterion (AIC) estimate (MAICE) which is designed for the purpose of statistical identification is introduced. When there are several competing models the MAICE is defined by the model and the maximum likelihood estimates of the parameters which give the minimum of AIC defined by AIC = (-2)log-(maximum likelihood) + 2(number of independently adjusted parameters within the model). MAICE provides a versatile procedure for statistical model identification which is free from the ambiguities inherent in the application of conventional hypothesis testing procedure. The practical utility of MAICE in time series analysis is demonstrated with some numerical examples."
            },
            "slug": "A-new-look-at-the-statistical-model-identification-Akaike",
            "title": {
                "fragments": [],
                "text": "A new look at the statistical model identification"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1974
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696678"
                        ],
                        "name": "D. Wolpert",
                        "slug": "D.-Wolpert",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Wolpert",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Wolpert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2957256"
                        ],
                        "name": "W. Macready",
                        "slug": "W.-Macready",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Macready",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Macready"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5553697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8315dff3d304baf47c025f4b33535b9d693350c1",
            "isKey": false,
            "numCitedBy": 9370,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving. A number of \"no free lunch\" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class. These theorems result in a geometric interpretation of what it means for an algorithm to be well suited to an optimization problem. Applications of the NFL theorems to information-theoretic aspects of optimization and benchmark measures of performance are also presented. Other issues addressed include time-varying optimization problems and a priori \"head-to-head\" minimax distinctions between optimization algorithms, distinctions that result despite the NFL theorems' enforcing of a type of uniformity over all algorithms."
            },
            "slug": "No-free-lunch-theorems-for-optimization-Wolpert-Macready",
            "title": {
                "fragments": [],
                "text": "No free lunch theorems for optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 99,
                "text": "A framework is developed to explore the connection between effective optimization algorithms and the problems they are solving and a number of \"no free lunch\" (NFL) theorems are presented which establish that for any algorithm, any elevated performance over one class of problems is offset by performance over another class."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Evol. Comput."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1395867398"
                        ],
                        "name": "A. O'neill",
                        "slug": "A.-O'neill",
                        "structuredName": {
                            "firstName": "A.",
                            "lastName": "O'neill",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. O'neill"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 111191440,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "9cb9a0fe7b42075e72973d8ecd7b435f6b831233",
            "isKey": false,
            "numCitedBy": 8,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "For the first time, the supervised training of a high-speed, two-layer, optoelectronic neural network using a genetic algorithm is demonstrated, and results for the 3 bit exclusiveor function are presented."
            },
            "slug": "Genetic-based-training-of-two-layer,-optoelectronic-O'neill",
            "title": {
                "fragments": [],
                "text": "Genetic based training of two-layer, optoelectronic neural network"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143901532"
                        ],
                        "name": "X. Yao",
                        "slug": "X.-Yao",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Yao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Yao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152891866"
                        ],
                        "name": "Yong Liu",
                        "slug": "Yong-Liu",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Liu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16733583,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "e505cfacc09aa4c154fc9b0e8b16bab16a1245e8",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "A tool for micro-soldering the connecting tags of an integrated circuit chip to corresponding terminals on a substrate by Joule heating includes a high conductivity bit having a planar bottom face adapted to contact the tags and press them against the terminals while applying sufficient heat to the tags to solder them to the terminals. The face has a perimeter and a centrally apertured portion adapted to receive and accommodate the circuit chip. The apertured portion forms a geometric loop on the planar face having opposite sides spaced from each other and adapted to contact the tags of the chip. A continuous high electrical conductivity flange extends upwardly from all portions of the perimeter of the face to provide rigidity to the bit. A pair of high conductivity strips extend from facing segments of the flange for applying current to and removing current from the flange on opposite sides of the loop so that a pair of symmetrical current half loops extend about the face between the opposite sides thereof. Each strip has a reduced cross-sectional area symmetrical with the side of the face and removed from the intersection of the strip with the flange."
            },
            "slug": "Evolving-Artificial-Neural-Networks-through-Yao-Liu",
            "title": {
                "fragments": [],
                "text": "Evolving Artificial Neural Networks through Evolutionary Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "A tool for micro-soldering the connecting tags of an integrated circuit chip to corresponding terminals on a substrate by Joule heating includes a high conductivity bit having a planar bottom face adapted to contact the tags and press them against the terminals while applying sufficient heat to the tags to solder them to the terminals."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Programming"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2854554"
                        ],
                        "name": "T. Szir\u00e1nyi",
                        "slug": "T.-Szir\u00e1nyi",
                        "structuredName": {
                            "firstName": "Tam\u00e1s",
                            "lastName": "Szir\u00e1nyi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Szir\u00e1nyi"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Sziranyi [99] and Pal and Bhandari [98] used EA\u2019s to tune circuit"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 4786260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "172379ecfaed88a53a3d13d75785a9aa008b2a1b",
            "isKey": false,
            "numCitedBy": 14,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Template parameters of cellular neural networks (CNNs) should be robust enough to random variability of VLSI tolerances and noise. Using the CNN for image processing, one of the main problems is the robustness of a given task in a real VLSI chip. \n \n \n \nIt will be shown that very different tasks such as 2D or 3D deconvolution and texture segmentation can be solved in a real VLSI CNN environment without significant loss of efficiency and accuracy under low precision (about 6\u20138 bits) and random variability of the VLSI parameters. The CNN turns out to be very robust against template noise, image noise, imperfect estimation of templates and parameter accuracy. \n \n \n \nThe parameters of a template are tuned using genetic learning. These optimized parameters depend on the precision of the architecture. It was found that about 6\u20138 bits of precision is enough for a complicated multilayer deconvolution, while only 4 bits of precision is enough for difficult texture segmentation in the presence of noise and parameter variances. The tolerance sensitivity of template parameters is considered for VLSI implementation. Theory and examples are demonstrated by many results using real-life microscopic images and natural textures."
            },
            "slug": "Robustness-of-cellular-neural-networks-in-image-and-Szir\u00e1nyi",
            "title": {
                "fragments": [],
                "text": "Robustness of cellular neural networks in image deblurring and texture segmentation"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It will be shown that very different tasks such as 2D or 3D deconvolution and texture segmentation can be solved in a real VLSI CNN environment without significant loss of efficiency and accuracy under low precision (about 6\u20138 bits) and random variability of the V LSI parameters."
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Circuit Theory Appl."
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743344"
                        ],
                        "name": "H. Schwefel",
                        "slug": "H.-Schwefel",
                        "structuredName": {
                            "firstName": "Hans-Paul",
                            "lastName": "Schwefel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Schwefel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 7
                            }
                        ],
                        "text": "The architecture of an ANN includes its topological structure, i.e., connectivity, and the transfer function of each node in the ANN."
                    },
                    "intents": []
                }
            ],
            "corpusId": 43418053,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9006a35af873bcbb181dff7041309456b2999889",
            "isKey": false,
            "numCitedBy": 2468,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Problems and Methods of Optimization Hill Climbing Strategies Random Strategies Evolution Strategies for Numerical Optimization Comparison of Direct Search Strategies for Parameter Optimization."
            },
            "slug": "Evolution-and-optimum-seeking-Schwefel",
            "title": {
                "fragments": [],
                "text": "Evolution and optimum seeking"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Problems and Methods of Optimization Hill Climbing Strategies Random Strategies Evolution Strategies for Numerical Optimization Comparison of Direct Search Strategies for Parameter Optimization."
            },
            "venue": {
                "fragments": [],
                "text": "Sixth-generation computer technology series"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2863627"
                        ],
                        "name": "M. Kupinski",
                        "slug": "M.-Kupinski",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Kupinski",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kupinski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144692532"
                        ],
                        "name": "M. Giger",
                        "slug": "M.-Giger",
                        "structuredName": {
                            "firstName": "Maryellen",
                            "lastName": "Giger",
                            "middleNames": [
                                "Lissak"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Giger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62269894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a074c097f63929fb6355e19f995da1085118ea08",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "We have investigated various methods of feature selection for two different data classifiers used in the computerized detection of mass lesions in digital mammograms. Numerous features were extracted from abnormal and normal breast regions from a database consisting of 210 individual mammograms. A step-wise method, a genetic algorithm and individual feature analysis were employed to select a subset of features to be used with linear discriminants. Similar techniques were also employed for an artificial neural network classifier. In both tests the genetic algorithm was able to either outperform or equal the performance of other methods."
            },
            "slug": "Feature-selection-and-classifiers-for-the-detection-Kupinski-Giger",
            "title": {
                "fragments": [],
                "text": "Feature selection and classifiers for the computerized detection of mass lesions in digital mammography"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Various methods of feature selection for two different data classifiers used in the computerized detection of mass lesions in digital mammograms were investigated, with the genetic algorithm able to either outperform or equal the performance of other methods."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of International Conference on Neural Networks (ICNN'97)"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1729447"
                        ],
                        "name": "Yaow-Ming Chen",
                        "slug": "Yaow-Ming-Chen",
                        "structuredName": {
                            "firstName": "Yaow-Ming",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yaow-Ming Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403179152"
                        ],
                        "name": "R. O\u2019Connell",
                        "slug": "R.-O\u2019Connell",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "O\u2019Connell",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. O\u2019Connell"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 77489404,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "8d90b92a1ce46b62dd411b6c3a4e04f1eaac286e",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Harmonics in a power system can lead to communication interference, transformer heating or solid-state device malfunctions. Active power line conditioners (APLC) are one important method of achieving harmonic reduction. The purpose of this paper is to propose a novel voltage-type APLC which cancels harmonic currents by injecting a compensation current. The proposed APLC consists of a variable DC voltage source, an inverter and a neural network controller that is trained with the genetic algorithm and backpropagation. Computer simulations for two load current test cases show that the neural net can provide switch control signals for the proposed APLC to generate compensation currents that reduce line current THD significantly."
            },
            "slug": "Active-power-line-conditioner-with-a-neural-network-Chen-O\u2019Connell",
            "title": {
                "fragments": [],
                "text": "Active power line conditioner with a neural network control"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Computer simulations for two load current test cases show that the neural net can provide switch control signals for the proposed APLC to generate compensation currents that reduce line current THD significantly."
            },
            "venue": {
                "fragments": [],
                "text": "IAS '96. Conference Record of the 1996 IEEE Industry Applications Conference Thirty-First IAS Annual Meeting"
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49464494"
                        ],
                        "name": "Kevin J. Lang",
                        "slug": "Kevin-J.-Lang",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lang",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin J. Lang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724972"
                        ],
                        "name": "A. Waibel",
                        "slug": "A.-Waibel",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Waibel",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Waibel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 14
                            }
                        ],
                        "text": "various areas [20]\u2013[22], but BP has drawbacks due to its use of gradient descent [23], [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1234937,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e08d090d1e586610d636a46004876e9f3ded8209",
            "isKey": false,
            "numCitedBy": 640,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-time-delay-neural-network-architecture-for-word-Lang-Waibel",
            "title": {
                "fragments": [],
                "text": "A time-delay neural network architecture for isolated word recognition"
            },
            "venue": {
                "fragments": [],
                "text": "Neural Networks"
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7211457"
                        ],
                        "name": "Donald Favareau",
                        "slug": "Donald-Favareau",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Favareau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald Favareau"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "Despite all of this detail, the model circumvents the simulation of axonal growth via algorithms (based loosely on Neural Darwinism [15] and Deacon\u2019s Law [11]) that iteratively introduce connections and then use a group\u2019s total connectivity (along with that of other, competing groups) to compute a biasing factor that affects the connections generated in the next developmental round."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 55239623,
            "fieldsOfStudy": [
                "Linguistics"
            ],
            "id": "75ad326821587b1baf98725582a700621782c353",
            "isKey": false,
            "numCitedBy": 888,
            "numCiting": 2,
            "paperAbstract": {
                "fragments": [],
                "text": "The Symbolic Species: The Co-evolution of Language and the Brain by Terrence W. Deacon. New York: W.W. Norton, 1997, 527 pp. Reviewed by Donald Favareau University of California, Los Angeles In 866, the recently formed Societe Linguistique de Paris passed an official resolution banning the presentation of any further papers regarding the origins of human language. The nature of the inquiry itself, it was felt, it lacked even the dis- possibility of scientific certainty, and all work pertaining to was likewise missed on the grounds of being empirically irresolvable, incorrigibly speculative, and unproductively divisive. Terrence W. Deacon, almost a century and a half vocative polemic that will doubtlessly incur even later, has authored a pro- more violent censure on the part of his detractors. Empirically vigorous, incisively speculative and with the poten- tial to be productively divisive, Deacon's The Symbolic Species: The Co-Evolu- if tion of Language and the Brain challenges many, not most, of the assumptions underlying modern linguistic theory. Of particular interest to linguists will be Deacon's refutation of Chomsky's (1972) Universal Grammar paradigm, as well as his corollary rejection of the pos- sibility of innate syntactic processing or language-learning modules nestled deep within the human brain. bolic representation at all, Instead, claims Deacon, language itself and the sym- which it evinces and encodes lies not inside individual brains but at the interface between biology and culture. A biological anthropologist with extensive experience in neurology. Deacon supports this argument not, in itself, first with an appeal to evolutionary theory. Universality reliable indicator of is Deacon proposes, a what evolution has built into human brains (p. 339). Accordingly, the universal grammatical Chomskian notion that some kind of knowledge must be innate in human beings in order to ac- Precisely because count for certain otherwise unexplainable universal features regarding language is an argument which Deacon considers specious. some ver- sion of Chomsky's model is so deeply embedded in contemporary devoted to linguistic theory, a considerable portion of this The Symbolic Species is its refutation. It is argument, to the exclusion of so many other fascinating and corollary argu- ments presented throughout the work, struct. that this review will endeavor to recon- Fundamental gist ral selection to Deacon's argument is nineteenth century American psycholo- James Mark Baldwin's that this modification 895; 1902) theory that the very context wherein natu- its takes place can itself be modified by the behavior of inhabitants and may, in turn, generate subsequent new sets of selection Issues in Applied Linguistics ISSN 1050-4273 Vol. 9 No. 2, 1998, Regents of the University of California"
            },
            "slug": "The-Symbolic-Species:-The-Co-evolution-of-Language-Favareau",
            "title": {
                "fragments": [],
                "text": "The Symbolic Species: The Co-evolution of Language and the Brain"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144567818"
                        ],
                        "name": "N. Saravanan",
                        "slug": "N.-Saravanan",
                        "structuredName": {
                            "firstName": "N.",
                            "lastName": "Saravanan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Saravanan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144330013"
                        ],
                        "name": "D. Fogel",
                        "slug": "D.-Fogel",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Fogel",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Fogel"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[117], [126]\u2013[128], higher order ANN\u2019s [52], [53], and"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 114
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 168
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 31293164,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "37d6a212344e5d098f4c3abe9291dfd2d1b31ab9",
            "isKey": false,
            "numCitedBy": 129,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Controlling unstable nonlinear systems with neural networks can be problematic. Two examples show that evolutionary programming provides a feasible method for addressing such control problems. >"
            },
            "slug": "Evolving-Neural-Control-Systems-Saravanan-Fogel",
            "title": {
                "fragments": [],
                "text": "Evolving Neural Control Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Two examples show that evolutionary programming provides a feasible method for addressing such control problems as controlling unstable nonlinear systems with neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Expert"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50314842"
                        ],
                        "name": "M. Kishimoto",
                        "slug": "M.-Kishimoto",
                        "structuredName": {
                            "firstName": "Maki",
                            "lastName": "Kishimoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kishimoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "90483705"
                        ],
                        "name": "K. Sakasai",
                        "slug": "K.-Sakasai",
                        "structuredName": {
                            "firstName": "Kaoru",
                            "lastName": "Sakasai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Sakasai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3792391"
                        ],
                        "name": "K. Ara",
                        "slug": "K.-Ara",
                        "structuredName": {
                            "firstName": "Katsuyuki",
                            "lastName": "Ara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Ara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2058059"
                        ],
                        "name": "T. Fujita",
                        "slug": "T.-Fujita",
                        "structuredName": {
                            "firstName": "Takaaki",
                            "lastName": "Fujita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Fujita"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2151892941"
                        ],
                        "name": "Y. Suzuki",
                        "slug": "Y.-Suzuki",
                        "structuredName": {
                            "firstName": "Y",
                            "lastName": "Suzuki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Suzuki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 121791271,
            "fieldsOfStudy": [
                "Physics"
            ],
            "id": "6bd197f9067ee414853738e2e4c0c1e61ac6e325",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "New methods to reconstruct plasma shape and plasma current distribution from magnetic measurements are proposed. The reconstruction of plasma current profile from magnetic measurements is regarded as an optimum allocation problem of currents into cross section of the vacuum vessel of the tokamak. For solving this optimization problem, we use two types of solutions: a genetic algorithm and a combined method of a Hopfield neural network and a genetic algorithm. The effectiveness of these methods is shown by the application of these techniques to JT-60U plasmas."
            },
            "slug": "Reconstruction-of-plasma-current-profile-of-using-Kishimoto-Sakasai",
            "title": {
                "fragments": [],
                "text": "Reconstruction of plasma current profile of tokamaks using combinatorial optimization techniques"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2516166"
                        ],
                        "name": "J. Rissanen",
                        "slug": "J.-Rissanen",
                        "structuredName": {
                            "firstName": "Jorma",
                            "lastName": "Rissanen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Rissanen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 219
                            }
                        ],
                        "text": "Unfortunately, measuring generalization quantitatively and accurately is almost impossible in practice [298] although there are many theories and criteria on generalization, such as the minimum description length (MDL) [299], Akaike information criteria (AIC) [300], and minimum message length (MML) [301]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 30140639,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d382b9c11e5c6a8e173fbeb442545e3be8d3e3a5",
            "isKey": false,
            "numCitedBy": 6271,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Modeling-By-Shortest-Data-Description*-Rissanen",
            "title": {
                "fragments": [],
                "text": "Modeling By Shortest Data Description*"
            },
            "venue": {
                "fragments": [],
                "text": "Autom."
            },
            "year": 1978
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072595826"
                        ],
                        "name": "R. Stocker",
                        "slug": "R.-Stocker",
                        "structuredName": {
                            "firstName": "R.",
                            "lastName": "Stocker",
                            "middleNames": [
                                "Dimsdale."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Stocker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145669585"
                        ],
                        "name": "H. Jelinek",
                        "slug": "H.-Jelinek",
                        "structuredName": {
                            "firstName": "Herbert",
                            "lastName": "Jelinek",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Jelinek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1394948589"
                        ],
                        "name": "B. Burnota",
                        "slug": "B.-Burnota",
                        "structuredName": {
                            "firstName": "B",
                            "lastName": "Burnota",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Burnota"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774394"
                        ],
                        "name": "T. Bossomaier",
                        "slug": "T.-Bossomaier",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Bossomaier",
                            "middleNames": [
                                "R.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bossomaier"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59834887,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe47a5e4aaf0aa91e1c8c8bd3bf28da43c4ac514",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Addresses one of the central issues of complexity, namely how systems are put together. Topics include: organization and behaviour of computational systems; criticality and complexity; nonlinear dynamics and fractals; and evolution, learning and artificial neural networks."
            },
            "slug": "Complex-systems-:-from-local-interactions-to-global-Stocker-Jelinek",
            "title": {
                "fragments": [],
                "text": "Complex systems : from local interactions to global phenomena"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1996
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34340513"
                        ],
                        "name": "H. Szu",
                        "slug": "H.-Szu",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Szu",
                            "middleNames": [],
                            "suffix": ""
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Szu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2062884264"
                        ],
                        "name": "R. Hartley",
                        "slug": "R.-Hartley",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Hartley",
                            "middleNames": [
                                "Lunn"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Hartley"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 33160570,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "4ec54afc661d9fe0caebafb80a3de3482de3b545",
            "isKey": false,
            "numCitedBy": 143,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent advances in the solution of nonconvex optimization problems use simulated annealing techniques that are considerably faster than exhaustive global search techniques. This letter presents a simulated annealing technique, which is t/log (t) times faster than conventional simulated annealing, and applies it to a multisensor location and tracking problem."
            },
            "slug": "Nonconvex-optimization-by-fast-simulated-annealing-Szu-Hartley",
            "title": {
                "fragments": [],
                "text": "Nonconvex optimization by fast simulated annealing"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the IEEE"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2681887"
                        ],
                        "name": "D. Rumelhart",
                        "slug": "D.-Rumelhart",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Rumelhart",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Rumelhart"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2116648700"
                        ],
                        "name": "Ronald J. Williams",
                        "slug": "Ronald-J.-Williams",
                        "structuredName": {
                            "firstName": "Ronald",
                            "lastName": "Williams",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ronald J. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 62245742,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "111fd833a4ae576cfdbb27d87d2f8fc0640af355",
            "isKey": false,
            "numCitedBy": 19405,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Learning-internal-representations-by-error-Rumelhart-Hinton",
            "title": {
                "fragments": [],
                "text": "Learning internal representations by error propagation"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801252"
                        ],
                        "name": "P. Mussio",
                        "slug": "P.-Mussio",
                        "structuredName": {
                            "firstName": "Piero",
                            "lastName": "Mussio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Mussio"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58810717,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "01bbf1e83c0a0a52f69814b4e12982f981a3688b",
            "isKey": false,
            "numCitedBy": 398,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Toward a Practice of Autonomous Systems --Proc. of the First European Conference on Artificial Life, edited by F. Varela and P. Bourgine, The MIT Press, 1992, ISBN 0 262 72019 1, 515 pp."
            },
            "slug": "Toward-a-Practice-of-Autonomous-Systems-Mussio",
            "title": {
                "fragments": [],
                "text": "Toward a Practice of Autonomous Systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1880555"
                        ],
                        "name": "D. Sanes",
                        "slug": "D.-Sanes",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Sanes",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Sanes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5467420"
                        ],
                        "name": "T. Reh",
                        "slug": "T.-Reh",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Reh",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Reh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144424862"
                        ],
                        "name": "W. Harris",
                        "slug": "W.-Harris",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Harris",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Harris"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "For example, many studies, summarized in [58], find high levels of long-term potentiation (LTP) and long-term depression (LTD) - both forms of synaptic tuning - during prenatal development."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 26366287,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "a49b20db2121f221bc369ff8b1d23006f127e584",
            "isKey": false,
            "numCitedBy": 529,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "[Development-of-the-nervous-system].-Sanes-Reh",
            "title": {
                "fragments": [],
                "text": "[Development of the nervous system]."
            },
            "venue": {
                "fragments": [],
                "text": "Ceskoslovenska fysiologie"
            },
            "year": 1983
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152251452"
                        ],
                        "name": "John Maynard Smith",
                        "slug": "John-Maynard-Smith",
                        "structuredName": {
                            "firstName": "John Maynard",
                            "lastName": "Smith",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "John Maynard Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[257]\u2013[271], but most of them deal with the issue of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 33
                            }
                        ],
                        "text": "how learning can guide evolution [257]\u2013[260] and the"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5476916,
            "fieldsOfStudy": [
                "Computer Science",
                "Medicine"
            ],
            "id": "89ff03feb5a587eec331b5333556424bdfeef143",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "When-learning-guides-evolution-Smith",
            "title": {
                "fragments": [],
                "text": "When learning guides evolution"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "In their seminal ER textbook[51], Nolfi and Floreano cite several reasons for the popularity of ANNs and EANNs for robotic control:"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 65
                            }
                        ],
                        "text": "With fixed-topology, direct-encoded EANNs, it is common practice [51] to group together the genes coding for all of a neuron\u2019s incoming weights, and to forbid the choice of crossover points within these weight packages."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 75,
                                "start": 71
                            }
                        ],
                        "text": "They are also the default representation in Evolutionary Robotics (ER) [51], the field that most tangibly illustrates the true potential of biologically-inspired routes to AI."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 29
                            }
                        ],
                        "text": "Several lengthy publications [73, 51, 21, 20, 23] have been wholly or partially devoted to illustrating and classifying the breadth of EANN research."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "One of the prime showcases for EANNS, the field of Evolutionary Robotics, made formidable progress in the mid 1990\u2019s (as fully documented in [51])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolutionary Robotics: The Biology"
            },
            "venue": {
                "fragments": [],
                "text": "Intelligence, and Technology of Self- Organizing Machines, The MIT Press, Cambridge, MA"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "The local search algorithm could be BP [32], [133] or other random search algorithms [30], [135]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 21
                            }
                        ],
                        "text": "\u201d Bartlett and Downs [30] also gave a modified GA which was \u201can order of magnitude\u201d faster than BP for the 7-bit parity problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "For some problems, evolutionary training can be significantly faster and more reliable than BP [30], [34], [40], [63], [83], [89]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Bartlett and Downs [30] also demonstrated that the evolutionary approach was faster and had better scalability than BP."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 18,
                                "start": 0
                            }
                        ],
                        "text": "Bartlett and Downs [30] also gave a modified GA which was \u201can order of magnitude\u201d faster than BP for the 7-bit parity problem."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training a neural network with a genetic algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Dep. Elect. Eng., Univ. Queensland, Australia, Tech. Rep., Jan. 1990."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "As a remedy, Montana and Davis [48] proposed the evolution of ANN weight vectors for a standard supervisedlearning task: classification of patterns in sonar data."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 89
                            }
                        ],
                        "text": "Figure 1: Comparison of genotype-phenotype conversions in the EANNs of Montana and Davis [48] and Miller et."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 0
                            }
                        ],
                        "text": "Montana and Davis [27] defined a large number of tailored genetic operators which incorporated many heuristics about training ANN\u2019s."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training feedforward networks using genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings the Eleventh International Joint Conference on Artificial Intelligence, "
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 182
                            }
                        ],
                        "text": "In fact, can represent real-valued connection weights from node to node so that the architecture and connection weights can be evolved simultaneously [37], [42], [45], [165], [166], [169]\u2013[171]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 147,
                                "start": 142
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 124
                            }
                        ],
                        "text": "The first separates the evolution of architectures from that of connection weights [24], [150], [153], [154], [165], [167], [169], [170]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 155,
                                "start": 150
                            }
                        ],
                        "text": "In the first approach, each connection of an architecture is directly specified by its binary representation [24], [150], [153], [154], [165], [167], [169], [170], [202]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Genetic synthesis of discretetime recurrent neural network"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int. Workshop Artificial Neural Networks (IWANN\u201993), Lecture Notes in Computer Science, vol. 686. Berlin, Germany: Springer-Verlag, 1993, pp. 179\u2013184."
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1758714"
                        ],
                        "name": "S. Fahlman",
                        "slug": "S.-Fahlman",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Fahlman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Fahlman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 91
                            }
                        ],
                        "text": "Evolutionary training can be slow for some problems in comparison with fast variants of BP [131] and conjugate gradient algorithms [19], [132]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 238073001,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "dab1baffed954d5d25d0a4b03a3999af02d0b3f9",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Fast-learning-variations-on-back-propagation:-an-Fahlman",
            "title": {
                "fragments": [],
                "text": "Fast-learning variations on back propagation: an empirical study."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103216941"
                        ],
                        "name": "Y. Ikuno",
                        "slug": "Y.-Ikuno",
                        "structuredName": {
                            "firstName": "Yasumasa",
                            "lastName": "Ikuno",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Ikuno"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102447862"
                        ],
                        "name": "Hiroaki Hawabata",
                        "slug": "Hiroaki-Hawabata",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Hawabata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hiroaki Hawabata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "97335433"
                        ],
                        "name": "Yoshiaki Shirao",
                        "slug": "Yoshiaki-Shirao",
                        "structuredName": {
                            "firstName": "Yoshiaki",
                            "lastName": "Shirao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshiaki Shirao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053856628"
                        ],
                        "name": "Masaya Hirata",
                        "slug": "Masaya-Hirata",
                        "structuredName": {
                            "firstName": "Masaya",
                            "lastName": "Hirata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masaya Hirata"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "94897182"
                        ],
                        "name": "Toshikuni Nagahara",
                        "slug": "Toshikuni-Nagahara",
                        "structuredName": {
                            "firstName": "Toshikuni",
                            "lastName": "Nagahara",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Toshikuni Nagahara"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056747985"
                        ],
                        "name": "Yashio Inagaki",
                        "slug": "Yashio-Inagaki",
                        "structuredName": {
                            "firstName": "Yashio",
                            "lastName": "Inagaki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yashio Inagaki"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 117740285,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ad9e09719f49d216c9ea606b3faf1ff07fb7999",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Application-of-an-Improved-Genetic-Algorithm-to-the-Ikuno-Hawabata",
            "title": {
                "fragments": [],
                "text": "Application of an Improved Genetic Algorithm to the Learning of Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "71606688"
                        ],
                        "name": "S. Carlos",
                        "slug": "S.-Carlos",
                        "structuredName": {
                            "firstName": "Sio",
                            "lastName": "Carlos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Carlos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 61219842,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3edf872e2d4cefdca46591d0c7b3727cb10a6e0e",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 1,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Evolving-a-learning-algorithm-for-the-binary-Carlos",
            "title": {
                "fragments": [],
                "text": "Evolving a learning algorithm for the binary perceptron"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060983236"
                        ],
                        "name": "D. E. Goldberg",
                        "slug": "D.-E.-Goldberg",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Goldberg",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. E. Goldberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "For example, fitness sharing [25] divides an individual\u2019s raw fitness by the number of individuals that it resembles, where resemblance can entail similar fitness values, genotypes or phenotypes."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61406969,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc2806a373aefe9ec9fd02e6369fdf9b6a6ff8e4",
            "isKey": false,
            "numCitedBy": 12663,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Genetic-Algorithms-in-Search-Goldberg",
            "title": {
                "fragments": [],
                "text": "Genetic Algorithms in Search"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2011013"
                        ],
                        "name": "Li-Der Chou",
                        "slug": "Li-Der-Chou",
                        "structuredName": {
                            "firstName": "Li-Der",
                            "lastName": "Chou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Der Chou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2110402817"
                        ],
                        "name": "Jean-Lien C. Wu",
                        "slug": "Jean-Lien-C.-Wu",
                        "structuredName": {
                            "firstName": "Jean-Lien",
                            "lastName": "Wu",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Lien C. Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60393333,
            "fieldsOfStudy": [
                "Business"
            ],
            "id": "a31256a7e118e895fd4edf1ccefaa7efb138b42d",
            "isKey": false,
            "numCitedBy": 15,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Parameter-Adjustment-Using-Neural-Network-Based-for-Chou-Wu",
            "title": {
                "fragments": [],
                "text": "Parameter Adjustment Using Neural-Network-Based Genetic Algorithms for Guaranteed QOS in ATM Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2805285"
                        ],
                        "name": "J. Paredis",
                        "slug": "J.-Paredis",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Paredis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Paredis"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60048587,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "3e2dba5f1ca185acc0611d94aab5901f2bb24d89",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-evolution-of-behavior:-some-experiments-Paredis",
            "title": {
                "fragments": [],
                "text": "The evolution of behavior: some experiments"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39605316"
                        ],
                        "name": "J. Horny\u00e1k",
                        "slug": "J.-Horny\u00e1k",
                        "structuredName": {
                            "firstName": "J\u00f3zsef",
                            "lastName": "Horny\u00e1k",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Horny\u00e1k"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152236629"
                        ],
                        "name": "L. Monostori",
                        "slug": "L.-Monostori",
                        "structuredName": {
                            "firstName": "L\u00e1szl\u00f3",
                            "lastName": "Monostori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Monostori"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 59824133,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "750129b70c9f8402ba4b02957036705ffc6c4a2c",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Feature-extraction-technique-for-ANN-based-Horny\u00e1k-Monostori",
            "title": {
                "fragments": [],
                "text": "Feature extraction technique for ANN-based financial forecasting"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144113993"
                        ],
                        "name": "A. Bergman",
                        "slug": "A.-Bergman",
                        "structuredName": {
                            "firstName": "Aviv",
                            "lastName": "Bergman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Bergman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2553065"
                        ],
                        "name": "M. Kerszberg",
                        "slug": "M.-Kerszberg",
                        "structuredName": {
                            "firstName": "Michel",
                            "lastName": "Kerszberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kerszberg"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 58348468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eb204bfeb2ab61ad6e87738aa2bedd3074c62cf5",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "BREEDING-INTELLIGENT-AUTOMATA.-Bergman-Kerszberg",
            "title": {
                "fragments": [],
                "text": "BREEDING INTELLIGENT AUTOMATA."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "100550660"
                        ],
                        "name": "H. C. Andersen",
                        "slug": "H.-C.-Andersen",
                        "structuredName": {
                            "firstName": "Hans",
                            "lastName": "Andersen",
                            "middleNames": [
                                "Christian"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. C. Andersen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 153,
                                "start": 148
                            }
                        ],
                        "text": "One way to alleviate this problem is to evolve ANN architectures and connection weights simultaneously [37], [42], [45], [149], [165], [166], [169]\u2013[172], [179], [180], [182], [185]\u2013[200], [230], [232]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58732142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b40283b49a6fb2208ba62e86daa8e824ced2e2bf",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-constructive-algorithm-for-a-multilayer-based-on-Andersen",
            "title": {
                "fragments": [],
                "text": "A constructive algorithm for a multilayer perceptron based on co-operative population concepts in genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694724"
                        ],
                        "name": "F. Gruau",
                        "slug": "F.-Gruau",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Gruau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gruau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "On the classic pole-balancing control problem, it finds solutions 25 times faster than Cellular Encoding [27] (described below) and 5 times faster than ESP [26]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "For example, in Cellular Encoding [27], the P (parallel) and S (serial) commands both duplicate a neuron, and P copies all input and output connections as well."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 125
                            }
                        ],
                        "text": "One of the first generative encodings to get beyond the proof-of-principle stage was Frederic Gruau\u2019s Cellular Encoding (CE) [28, 27], an approach that also revolutionized genetic programming."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53809999,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c3f23318474971b8fce4db3099e6d760508c9949",
            "isKey": false,
            "numCitedBy": 86,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Genetic-micro-programming-of-neural-networks-Gruau",
            "title": {
                "fragments": [],
                "text": "Genetic micro programming of neural networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48703219"
                        ],
                        "name": "N. Snoad",
                        "slug": "N.-Snoad",
                        "structuredName": {
                            "firstName": "Nigel",
                            "lastName": "Snoad",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Snoad"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774394"
                        ],
                        "name": "T. Bossomaier",
                        "slug": "T.-Bossomaier",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Bossomaier",
                            "middleNames": [
                                "R.",
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Bossomaier"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11004643,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "87d8cd604db03f3a8859f98b004dbb81de16a509",
            "isKey": false,
            "numCitedBy": 7,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "MONSTER-the-Ghost-in-the-Connection-Machine:-of-in-Snoad-Bossomaier",
            "title": {
                "fragments": [],
                "text": "MONSTER - the Ghost in the Connection Machine: Modularity of Neural Systems in Theoretical Evolutionary Research"
            },
            "venue": {
                "fragments": [],
                "text": "SC"
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145559418"
                        ],
                        "name": "J. Torreele",
                        "slug": "J.-Torreele",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Torreele",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Torreele"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "[117], [126]\u2013[128], higher order ANN\u2019s [52], [53], and"
                    },
                    "intents": []
                }
            ],
            "corpusId": 52852842,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8003367117cc9f3982dfcf6dc5d10d3866e9e89",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Temporal-Processing-with-Recurrent-Networks:-An-Torreele",
            "title": {
                "fragments": [],
                "text": "Temporal Processing with Recurrent Networks: An Evolutionary Approach"
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843614"
                        ],
                        "name": "Jim Antonisse",
                        "slug": "Jim-Antonisse",
                        "structuredName": {
                            "firstName": "Jim",
                            "lastName": "Antonisse",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jim Antonisse"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 62,
                                "start": 57
                            }
                        ],
                        "text": ", the binary representation, might not be the best [48], [114]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52855129,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b48189c33a9a96de4b0173cea68671b9a8fd7c37",
            "isKey": false,
            "numCitedBy": 227,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-New-Interpretation-of-Schema-Notation-that-the-Antonisse",
            "title": {
                "fragments": [],
                "text": "A New Interpretation of Schema Notation that Overtums the Binary Encoding Constraint"
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789858"
                        ],
                        "name": "A. Lindenmayer",
                        "slug": "A.-Lindenmayer",
                        "structuredName": {
                            "firstName": "Aristid",
                            "lastName": "Lindenmayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Lindenmayer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725594"
                        ],
                        "name": "P. Prusinkiewicz",
                        "slug": "P.-Prusinkiewicz",
                        "structuredName": {
                            "firstName": "Przemyslaw",
                            "lastName": "Prusinkiewicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Prusinkiewicz"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 34
                            }
                        ],
                        "text": "Inspired by both CE and L systems [42], Boers and Sprinkhuizen-Kuyper designed a generative encoding, G2L, for producing network topologies."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 33802856,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "668a527162ca3b31fdf3b87ee2e8ecf7689c4d3b",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Developmental-Models-of-Multicellular-Organisms:-A-Lindenmayer-Prusinkiewicz",
            "title": {
                "fragments": [],
                "text": "Developmental Models of Multicellular Organisms: A Computer Graphics Perspective"
            },
            "venue": {
                "fragments": [],
                "text": "ALIFE"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144924401"
                        ],
                        "name": "C. R. Chow",
                        "slug": "C.-R.-Chow",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Chow",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. R. Chow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145516202"
                        ],
                        "name": "C. Chu",
                        "slug": "C.-Chu",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Chu",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 45884026,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7887bf8b26951845276ac5a40d9d4180a6c89ce9",
            "isKey": false,
            "numCitedBy": 4,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-Concurrent-Training-Algorithm-for-Supervised-in-Chow-Chu",
            "title": {
                "fragments": [],
                "text": "A Concurrent Training Algorithm for Supervised Learning in Artificial Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "J. Inf. Sci. Eng."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2032124"
                        ],
                        "name": "T. Caudell",
                        "slug": "T.-Caudell",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Caudell",
                            "middleNames": [
                                "Preston"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Caudell"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764931"
                        ],
                        "name": "C. Dolan",
                        "slug": "C.-Dolan",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Dolan",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dolan"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 12
                            }
                        ],
                        "text": "[24], [26], [28], [37], [38], [41], [52], [53]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 11053754,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "352e698e83bd7a337801b206628e886ae5c61eb2",
            "isKey": false,
            "numCitedBy": 69,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Parametric-Connectivity:-Training-of-Constrained-Caudell-Dolan",
            "title": {
                "fragments": [],
                "text": "Parametric Connectivity: Training of Constrained Networks using Genetic Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1764931"
                        ],
                        "name": "C. Dolan",
                        "slug": "C.-Dolan",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Dolan",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Dolan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2806570"
                        ],
                        "name": "M. Dyer",
                        "slug": "M.-Dyer",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Dyer",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Dyer"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 180
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 41488215,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5231eff378b1eec2459cf27f9f1bb3ee46cbd84d",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Toward-the-Evolution-of-Symbols-Dolan-Dyer",
            "title": {
                "fragments": [],
                "text": "Toward the Evolution of Symbols"
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "Olmez [97] used EA\u2019s to optimize a modified restricted Coulomb energy (RCE) ANN."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Classification of EEG waveforms by using RCE neural networks and genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": " Electron. Lett. , vol. 33, no. 18, pp. 1561\u20131562, 1997."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A polynomial time algorithm for the construction and training of a class of multilayer perceptrons,\u201dNeural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "vol. 6,"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training weights of neural networks by genetic algorithms and messy genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": " Proc. 2nd IASTED Int. Symp. Expert Systems and Neural Networks , M. H. Hamza, Ed. Anaheim, CA: Acta, 1990, pp. 74\u201377."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 39
                            }
                        ],
                        "text": "[117], [126]\u2013[128], higher order ANN\u2019s [52], [53], and"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "[24], [26], [28], [37], [38], [41], [52], [53]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Application of genetic algorithms to the training of higher order neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "J. Syst. Eng., vol. 2, pp. 272\u2013276, 1992."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Genetic synthesis of discretetime recurrent neural network"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. Int. Workshop Artificial Neural Networks (IWANN'93)"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 192,
                                "start": 187
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 22
                            }
                        ],
                        "text": "in chromosomes [151], [168], [184], [205], [230], [232]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolutionary structuring of artificial neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Bionics and Evolution Techniques Lab., Tech. Univ. Berlin, Germany, Tech. Rep., 1993."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 91
                            }
                        ],
                        "text": "There has been some work in this area where good results were reported [119], [120], [245]\u2013[253]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "RBF neural network, basis functions and genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 1997 IEEE Int. Conf. Neural Networks. Part 4 (of 4) , pp. 2187\u20132190."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 53
                            }
                        ],
                        "text": "sets of parameters have also been proposed by others [155],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 159
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization of artificial neural network structure using genetic techniques implemented on multiple transputers"
            },
            "venue": {
                "fragments": [],
                "text": " Proc. Transputing\u201991, P. Welch, D. Stiles, T. L. Kunii, and A. Bakkers, Eds. Amsterdam, The Netherlands: IOS, 1991, pp. 687\u2013700."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 115
                            }
                        ],
                        "text": "Similar experiments on the evolution of learning rules were also carried out by others [265], [266], [267], [269], [270]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The artificial evolution of a generalized class of adaptive processes"
            },
            "venue": {
                "fragments": [],
                "text": " Preprints AI\u201993 Workshop Evolutionary Computation, Nov. 1993, pp. 18\u201336."
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 29
                            }
                        ],
                        "text": "John Koza\u2019s genetic programs [40, 41], have been enlisted in the design of everything from electrical circuits to antennas to automatic controllers, some of which have been patented and many of which infringe upon existing patents."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 217,
                                "start": 213
                            }
                        ],
                        "text": "Few EA publications highlight their use of duplication and differentiation, but it appears in many implementations, particularly in genetic programming, where duplication of subtrees is a common genetic operation [41]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "KEANE, Genetic Programming 3: Darwinian Invention and Problem Solving"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Steerable genNets: The genetic programming of steerable behaviors in genNets"
            },
            "venue": {
                "fragments": [],
                "text": "Toward a Practice of Autonomous Systems: Proc. 1st Europ. Conf. Artificial Life"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Intell. Syst"
            },
            "venue": {
                "fragments": [],
                "text": "J. Intell. Syst"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 266,
                                "start": 262
                            }
                        ],
                        "text": "In [13], a similar dynamic layer, this time in a Kohonen network [39], is used to investigate an alternate interpretation of the Baldwin Effect [2], a theory concerning interactions between learning and evolution that has a strong following among EA researchers [68]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Introduction to the special issue: Evolution"
            },
            "venue": {
                "fragments": [],
                "text": "learning, and instinct: 100 years of the Baldwin Effect, Evolutionary Computation, 4 "
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Training neural networks by means of genetic YAO: EVOLVING ARTIFICIAL NEURAL NETWORKS  1441  algorithms working on very long chromosomes"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Neural Syst. , vol. 6, no. 3, pp. 299\u2013316, 1995."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optical field - scale groundwater remediation using neural networks and the genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Environmental Sci . Technol ."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "Support for an overlapping model comes from very early work on Cascade Correlation ANNs [18]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "LEBIERE, The cascade-correlation learning architecture"
            },
            "venue": {
                "fragments": [],
                "text": "Tech. Rep. CMU-CS-90-100,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 108,
                                "start": 103
                            }
                        ],
                        "text": "Unfortunately, measuring generalization quantitatively and accurately is almost impossible in practice [298] although there are many theories and criteria on generalization, such as the minimum description length (MDL) [299], Akaike information criteria (AIC) [300], and minimum message length (MML) [301]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A mathematical theory of generalization"
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst.  , vol. 4, no. 2, pp. 151\u2013249, 1990."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Yao was the Program Committee Cochair of CEC'99, ICCIMA'99, IEEE ICEC'98, SEAL'98, IEEE ICEC'97, and SEAL'96. He is an Associate Editor"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE TRANSACTIONS ON EVOLUTIONARY COMPUTATION and Knowledge and Information Systems: An International Journal"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 166
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Designing applicationspecific neural networks using the genetic algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Neural Information Processing Systems 2  , D. S. Touretzky, Ed. San Mateo, CA: Morgan Kaufmann, 1990, pp. 447\u2013454."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Proc. 8th Annual Conf"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 8th Annual Conf"
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Genetic-based fuzzy net controller and its application"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Modeling and Anal. B, vol. 38, nos. 1\u20132, pp. 49\u201358, 1997."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Genetic-based fuzzy net controller and its application"
            },
            "venue": {
                "fragments": [],
                "text": "Advances in Modeling and Anal. B, vol. 38, nos. 1\u20132, pp. 49\u201358, 1997."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Helget, \u201cIdentification and control of a simulated distillation plant using connectionist and evolutionary techniques,\u201dSimulation"
            },
            "venue": {
                "fragments": [],
                "text": "vol. 63,"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Current transients based faulted phase selection technique using a genetic algorithm evolved neural network"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 1997 32nd Universities Power Engineering Conf., UPEC\u201997, Part 2 (of 2). Iraklio, Greece: Technological Educational Inst., 1997, pp. 959\u2013962."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 205
                            }
                        ],
                        "text": "Many EAs and EANNs have chromosomes that can grow in the course of evolution, and this has obvious advantages in terms of gradual complexification, as illustrated quite early by Inman Harvey\u2019s SAGE system [29], and more recently by NEAT."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Artficial Evolution of Adaptive Behavior"
            },
            "venue": {
                "fragments": [],
                "text": "PhD thesis, Sussex University, Sussex, England"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Toward designing neural network ensembles by evolution,\" in Parallel Problem Solving from"
            },
            "venue": {
                "fragments": [],
                "text": "Lecture Notes in Computer Science"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Fast genetic selection of features for neural network classifiers"
            },
            "venue": {
                "fragments": [],
                "text": " IEEE Trans. Neural Networks  , vol. 3, pp. 324\u2013328, Mar. 1992."
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Using the genetic algorithm to train time dependent behaviors in neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 1st Int. Workshop Multistrategy Learning (MSL-91)  , R. S. Michalski and G. Tecuci, Eds. Fairfax, VA: Center for Artificial Intelligence, 1991, pp. 273\u2013280."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 36
                            }
                        ],
                        "text": "Second, this combination provides safer evolution of control systems."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Backpropagation: Theory , Architectures, and Applications. Hillsdale, NJ: Erlbaum"
            },
            "venue": {
                "fragments": [],
                "text": "Backpropagation: Theory , Architectures, and Applications. Hillsdale, NJ: Erlbaum"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The Performance of the Neocognitron with Various S-Cell and C-Cell Transfer Functions"
            },
            "venue": {
                "fragments": [],
                "text": "Intell. Machines Lab., Dep. Elect. Eng., Univ. Queensland,"
            },
            "year": 1992
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 173
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "GANNET: Design of a neural net for face recognition by genetic algorithm"
            },
            "venue": {
                "fragments": [],
                "text": "Center for Cognitive and Computational Neuroscience, Dep. Comput. Sci. Psychology, Stirling Univ., Stirling, U.K., Tech. Rep. CCCN-6, Aug. 1990."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 39
                            }
                        ],
                        "text": "They include evolution strategies (ES) [8], [9], evolutionary programming (EP) [10], [11],"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optimization of Computer Models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1981
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptive switching circuits , \u201d in 1960"
            },
            "venue": {
                "fragments": [],
                "text": "IRE WESTCON Convention Rec ."
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolving cascadecorrelation networks for time-series forecasting"
            },
            "venue": {
                "fragments": [],
                "text": "Int. J. Artificial Intell. Tools, vol. 3, no. 3, pp. 327\u2013338, 1995."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Interactions between learning and evolution, \" in Artificial Life II, SFI Studies in the Sciences of Complexity"
            },
            "venue": {
                "fragments": [],
                "text": "Interactions between learning and evolution, \" in Artificial Life II, SFI Studies in the Sciences of Complexity"
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "A genetic breeding algorithm which exhibits self-organizing in neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IASTED Int. Symp. Artificial Intelligence Application and Neural Networks\u2014AINN\u201990 , M. H. Hamza, Ed. Anaheim, CA: ACTA, 1990, pp. 293\u2013296."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optical fieldscale groundwater remediation using neural networks and the genetic algorithms,\u201dEnvironmental"
            },
            "venue": {
                "fragments": [],
                "text": "Sci. Technol. , vol. 29,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "Yaeger\u2019s [72] PolyWorld system was the first to employ a complete POE approach in an artificial life (ALife) environment: one in which a population of agents move about, eat, mate and fight."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "Figure 3: Yaeger\u2019s [72] PolyWorld system."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 154,
                                "start": 150
                            }
                        ],
                        "text": "In [12], an intermediate abstraction level, that of neuron groups, serves as the atomic unit during development (in much the same way as in Polyworld [72])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computational genetics"
            },
            "venue": {
                "fragments": [],
                "text": "physiology, metabolism, neural systems, learning, vision and behavior or polyworld: Life in a new context, in Artificial Life III, Proceedings Volume XVII, C. G. Langton, ed., Reading, Massachusetts"
            },
            "year": 1994
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Adaptation in open systems: Learning and evolution"
            },
            "venue": {
                "fragments": [],
                "text": "Workshop Konnektionismus  , J. Kindermann and C. Lischka, Eds. Germany: GMD, 1988, pp. 122\u2013130."
            },
            "year": 1988
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 164
                            }
                        ],
                        "text": "In the first approach, each connection of an architecture is directly specified by its binary representation [24], [150], [153], [154], [165], [167], [169], [170], [202]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Machinery fault diagnostics using direct encoding graph syntax for optimizing artificial neural network structure"
            },
            "venue": {
                "fragments": [],
                "text": "inProc. 1996 3rd Biennial Joint Conf. Engineering Systems Design and Analysis, ESDA, Part 7 (of 9). New York: ASME, 1996, pp. 205\u2013210."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Applications of neural networks and genetic algorithms in the classification of enothelial cells"
            },
            "venue": {
                "fragments": [],
                "text": "Pattern Recognition Lett ."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 187,
                                "start": 182
                            }
                        ],
                        "text": "The adaptive adjustment of BP parameters (such as the learning rate and momentum) through evolution could be considered as the first attempt of the evolution of learning rules [32], [152], [272]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[152] encoded BP\u2019s parameters in chromosomes together with ANN\u2019s architecture."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 152
                            }
                        ],
                        "text": "In order to reduce the length of the genotypical representation of architectures, the indirect encoding scheme has been used by many researchers [151], [152], [155], [156], [159], [160], [168], [184], [205], [208], [211], [230]\u2013[232]"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Toward the genetic synthesis of neural networks"
            },
            "venue": {
                "fragments": [],
                "text": " Proc. 3rd Int. Conf. Genetic Algorithms and Their Applications  , J. D. Schaffer, Ed. San Mateo, CA: Morgan Kaufmann, 1989, pp. 360\u2013369."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 170,
                                "start": 148
                            }
                        ],
                        "text": "number of hidden layers and nodes) a learning process determines the rest Developmental encoding [6] a developmental process is genetically encoded [10, 7, 12, 8, 13, 16] Uses: Indirect and developmental representations are more flexible tend to be used for evolving architectures Direct representations tend to be used for evolving weights alone"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Designing neural networks by genetic algorithms using graph generation system"
            },
            "venue": {
                "fragments": [],
                "text": "Journal of Complex System,"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pattern recognition based on the fuzzy neural networks and their learning by modified genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": " Neural Network World , vol. 5, no. 1, pp. 91\u201397, 1995."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "various areas [20]\u2013[22], but BP has drawbacks due to its use of gradient descent [23], [24]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Two problems with backpropagation and other steepest-descent learning procedures for networks"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 8th Annual Conf. Cognitive Science Society . Hillsdale, NJ: Erlbaum, 1986, pp. 823\u2013831."
            },
            "year": 1986
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "He is a Professor of Computer Science at the School of Computer Science, University of Birmingham, U.K. He was an Associate Professor in the School of Computer Science"
            },
            "venue": {
                "fragments": [],
                "text": "Senior Member, IEEE) received the B.Sc. degree from the University of Science and Technology of ChinaSc. degree from the North China Institute of Computing Technologies (NCI), Beijing, in 1985, and the Ph.D. degree from USTC in Canberra, before joining the University of Birmingham. He held postdocto"
            },
            "year": 1982
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Darwinian optimization of synthetic neural systems"
            },
            "venue": {
                "fragments": [],
                "text": "inProc. 1st IEEE Int. Conf. Neural Networks  , vol. 3, 1987, pp. 769\u2013775."
            },
            "year": 1987
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolutionary learning of nearestneighbor MLP"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks , vol. 7, pp. 762\u2013767, May 1996."
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolving sequential machines in amorphous neural networks"
            },
            "venue": {
                "fragments": [],
                "text": " Artificial Neural Networks: Proc. Int. Conf. Artificial Neural Networks\u2014ICANN-91 , vol. 1, T. Kohonen, K. M\u0308akisara, O. Simula, and J. Kangas, Eds. Amsterdam, The Netherlands: North-Holland, 1991, pp. 973\u2013978."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Optical fieldscale groundwater remediation using neural networks and the genetic algorithms"
            },
            "venue": {
                "fragments": [],
                "text": "Environmental Sci. Technol"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolutionary design of application-specific neural networks: A genetic approach"
            },
            "venue": {
                "fragments": [],
                "text": " Neural Network World  , vol. 5, no. 1, pp. 41\u201350, 1995."
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 11,
                                "start": 6
                            }
                        ],
                        "text": "[257]\u2013[271], but most of them deal with the issue of"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolutionary Computation (Special Issue on the Baldwin Effect"
            },
            "venue": {
                "fragments": [],
                "text": "vol. 4,"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Genetic determination of largesignal HEMT model"
            },
            "venue": {
                "fragments": [],
                "text": "inProc. 1997 27th Europ. Microwave Conf, Part 1 (of 2)  , Turnbridge Wells, U.K., pp. 432\u2013436."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Steerable genNets: The genetic programming of steerable behaviors in genNets"
            },
            "venue": {
                "fragments": [],
                "text": " Toward a Practice of Autonomous Systems: Proc. 1st Europ. Conf. Artificial Life  , F. J. Varela and P. Bourgine, Eds. Cambridge, MA: MIT Press, 1991, pp. 272\u2013281."
            },
            "year": 1991
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Genetic 1444  PROCEEDINGS OF THE IEEE, VOL. 87, NO. 9, SEPTEMBER 1999  algorithm based neural networks applied to fault classification for EHV transmission lines with a UPFC"
            },
            "venue": {
                "fragments": [],
                "text": "Proc. 1997 6th Int. Conf. Developments in Power System Protection , Stevenage, U.K., Inst. Elect. Eng. Conf. Pub. 434, 1997, pp. 278\u2013281."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining neural and evolutionary learning: Aspects and approaches"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 5,
                                "start": 0
                            }
                        ],
                        "text": "[241] were, to our best knowledge, the first to apply EA\u2019s to the evolution of both topological structures and node transfer functions even though only simple ANN\u2019s with seven nodes were considered."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Preadaptation in neural circuits"
            },
            "venue": {
                "fragments": [],
                "text": "inProc. Int. Joint Conf. Neural Networks, vol. I, Washington, DC, 1990, pp. 202\u2013205."
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Combining neural and evolutionary learning: Aspects and approaches Institut f\u00fcr Informatik, Technische Univ"
            },
            "venue": {
                "fragments": [],
                "text": "Combining neural and evolutionary learning: Aspects and approaches Institut f\u00fcr Informatik, Technische Univ"
            },
            "year": 1990
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Evolutionary Programming, L"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "[12], and genetic algorithms (GA\u2019s) [13], [14]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "The canonical genetic algorithm (GA) [13], [14] has"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Algorithms in Search, Optimization, and Machine Learning"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 64
                            }
                        ],
                        "text": ", one real number per connection weight [27], [29], [30], [48], [63]\u2013[65], [74], [95], [96], [102], [110], [111], [117], [118]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 113
                            }
                        ],
                        "text": "For some problems, evolutionary training can be significantly faster and more reliable than BP [30], [34], [40], [63], [83], [89]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "have been a number of successful examples of applying EP or ES to the evolution of ANN connection weights [29], [63]\u2013[65], [67], [68], [95], [96], [102], [106], [111], [117], [119], [120]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Alternative neural network training methods,\u201dIEEE"
            },
            "venue": {
                "fragments": [],
                "text": "Expert  ,"
            },
            "year": 1995
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Parallel algorithms for learning in neural networks with evolution strategy"
            },
            "venue": {
                "fragments": [],
                "text": " Proc. Parallel  Computing\u201989, D. J. Evans, G. R. Joubert, and F. J. Peters, Eds. Amsterdam, The Netherlands: Elsevier, 1989, pp. 275\u2013280."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The dynamics of evolution and learning\u2014Toward genetic neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "Connectionism in Perspective  , R. Pfeifer et al., Eds. Amsterdam, The Netherlands: Elsevier, pp. 173\u2013198, 1989."
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Different voltagedependent thresholds for inducing long-term depression and long-term potentiation in slices of rat visual cortex"
            },
            "venue": {
                "fragments": [],
                "text": "Nature, vol. 347, no. 6288, pp. 69\u201372, Sept. 1990."
            },
            "year": 1990
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 117,
            "methodology": 55,
            "result": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 363,
        "totalPages": 37
    },
    "page_url": "https://www.semanticscholar.org/paper/Evolving-Artificial-Neural-Networks-Yao/5ac303258fd7f522fd3e4f172b97bb17eb888598?sort=total-citations"
}