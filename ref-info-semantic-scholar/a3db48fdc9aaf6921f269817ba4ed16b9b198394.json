{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40698158"
                        ],
                        "name": "A. Booker",
                        "slug": "A.-Booker",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Booker",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Booker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704765"
                        ],
                        "name": "P. Frank",
                        "slug": "P.-Frank",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Frank",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Frank"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144582109"
                        ],
                        "name": "D. Serafini",
                        "slug": "D.-Serafini",
                        "structuredName": {
                            "firstName": "D.",
                            "lastName": "Serafini",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Serafini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776399"
                        ],
                        "name": "V. Torczon",
                        "slug": "V.-Torczon",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "Torczon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Torczon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3142467"
                        ],
                        "name": "M. Trosset",
                        "slug": "M.-Trosset",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Trosset",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Trosset"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Some progress has already been made in handling constraints for the expected improvement approach (see Schonlau et al. (1997) and  Booker et al. (1999) )."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "For example,  Booker et al. (1999)  use kriging response surfaces to accelerate the General Pattern Search algorithm of Dennis and Torczon (1991)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3028611,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "eca468f5a60b8120724be266e8f4ecc8e54abe00",
            "isKey": false,
            "numCitedBy": 1027,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": "The goal of the research reported here is to develop rigorous optimization algorithms to apply to some engineering design problems for which direct application of traditional optimization approaches is not practical. This paper presents and analyzes a framework for generating a sequence of approximations to the objective function and managing the use of these approximations as surrogates for optimization. The result is to obtain convergence to a minimizer of an expensive objective function subject to simple constraints. The approach is widely applicable because it does not require, or even explicitly approximate, derivatives of the objective. Numerical results are presented for a 31-variable helicopter rotor blade design example and for a standard optimization test example."
            },
            "slug": "A-rigorous-framework-for-optimization-of-expensive-Booker-Dennis",
            "title": {
                "fragments": [],
                "text": "A rigorous framework for optimization of expensive functions by surrogates"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "The goal of the research reported here is to develop rigorous optimization algorithms to apply to some engineering design problems for which direct application of traditional optimization approaches is not practical."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118096690"
                        ],
                        "name": "Donald R. Jones",
                        "slug": "Donald-R.-Jones",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Jones",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald R. Jones"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815604"
                        ],
                        "name": "M. Schonlau",
                        "slug": "M.-Schonlau",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Schonlau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schonlau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145228938"
                        ],
                        "name": "W. Welch",
                        "slug": "W.-Welch",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Welch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Welch"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Because the approach starts with an experimental design, statistical analyses can be done to identify which input variables are the most important (highest contribution to the variance of the output) and \u2018main effect plots\u2019 can be created to visualize input-output relationships (see Booker, 1998;  Jones et al., 1998 )."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 294,
                                "start": 276
                            }
                        ],
                        "text": "\u2026starts with an experimental design, statistical analyses can be done to identify which input variables are the most important (highest contribution to the variance of the output) and \u2018main effect plots\u2019 can be created to visualize input-output relationships (see Booker, 1998; Jones et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13068209,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "daa63f57c3fbe994c4356f8d986a22e696e776d2",
            "isKey": false,
            "numCitedBy": 5764,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "In many engineering optimization problems, the number of function evaluations is severely limited by time or cost. These problems pose a special challenge to the field of global optimization, since existing methods often require more function evaluations than can be comfortably afforded. One way to address this challenge is to fit response surfaces to data collected by evaluating the objective and constraint functions at a few points. These surfaces can then be used for visualization, tradeoff analysis, and optimization. In this paper, we introduce the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering. We then show how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule. The key to using response surfaces for global optimization lies in balancing the need to exploit the approximating surface (by sampling where it is minimized) with the need to improve the approximation (by sampling where prediction error may be high). Striking this balance requires solving certain auxiliary problems which have previously been considered intractable, but we show how these computational obstacles can be overcome."
            },
            "slug": "Efficient-Global-Optimization-of-Expensive-Jones-Schonlau",
            "title": {
                "fragments": [],
                "text": "Efficient Global Optimization of Expensive Black-Box Functions"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper introduces the reader to a response surface methodology that is especially good at modeling the nonlinear, multimodal functions that often occur in engineering and shows how these approximating functions can be used to construct an efficient global optimization algorithm with a credible stopping rule."
            },
            "venue": {
                "fragments": [],
                "text": "J. Glob. Optim."
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2563797"
                        ],
                        "name": "C. D. Perttunen",
                        "slug": "C.-D.-Perttunen",
                        "structuredName": {
                            "firstName": "Cary",
                            "lastName": "Perttunen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. D. Perttunen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 70
                            }
                        ],
                        "text": "Kushner\u2019s original algorithm was one dimensional, but Stuckman (1988), Perttunen (1991), Elder (1992), and Mockus (1994), have all heuristically extended the method to higher dimensions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 124842496,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3667bc1e478a8801d98ffafc493fbdf356574c00",
            "isKey": false,
            "numCitedBy": 13,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Proposes the use of the Delaunay triangulation for feasible region division in constrained global optimization. The mathematical foundations for its use, along with the practical considerations for its implementation, are presented. The Delaunay triangulation algorithm is implemented in C.D. Perttunen's nonparametric method (1989). Results of this application are shown through the use of a standard set of test functions. The use of Delaunay triangulation is shown to yield a search in which the scatter plot of search points mimics the contour plot of the objective function under consideration.<<ETX>>"
            },
            "slug": "A-computational-geometric-approach-to-feasible-in-Perttunen",
            "title": {
                "fragments": [],
                "text": "A computational geometric approach to feasible region division in constrained global optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "The use of Delaunay triangulation is proposed for feasible region division in constrained global optimization and shown to yield a search in which the scatter plot of search points mimics the contour plot of the objective function under consideration."
            },
            "venue": {
                "fragments": [],
                "text": "Conference Proceedings 1991 IEEE International Conference on Systems, Man, and Cybernetics"
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2251706"
                        ],
                        "name": "N. Alexandrov",
                        "slug": "N.-Alexandrov",
                        "structuredName": {
                            "firstName": "Natalia",
                            "lastName": "Alexandrov",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Alexandrov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27975036"
                        ],
                        "name": "R. Lewis",
                        "slug": "R.-Lewis",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Lewis",
                            "middleNames": [
                                "Michael"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Lewis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "67288183"
                        ],
                        "name": "C. Gumbert",
                        "slug": "C.-Gumbert",
                        "structuredName": {
                            "firstName": "Clyde",
                            "lastName": "Gumbert",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gumbert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145397314"
                        ],
                        "name": "L. Green",
                        "slug": "L.-Green",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Green",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Green"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46737137"
                        ],
                        "name": "P. A. Newman",
                        "slug": "P.-A.-Newman",
                        "structuredName": {
                            "firstName": "Perry",
                            "lastName": "Newman",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. A. Newman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1490,
                                "start": 15
                            }
                        ],
                        "text": "In particular, Alexandrov et al. (2000) develop an algorithm that uses a \u2018correction factor\u2019 approach to force gradient matching between the surface and function at the current iterate. To get the next iterate, the surface is optimized within a trust region around the incumbant solution. The optimum point is then sampled and, if the objective function fails to decrease, the trust region is contracted. They prove that this approach must converge to a critical point of the function. It is perhaps best to think of Alexandrov\u2019s approach as a way of using response surfaces to accelerate an existing, provably convergent local optimization method\u2014as opposed to thinking of it as a new, response-surface-based method. Alexandrov starts with the well-known trust-region algorithm; however, instead of finding the next iterate by minimizing a second-order Taylor-series approximation within the trust region, she minimizes the response surface within the trust region. Because the response surface will usually be more accurate than the Taylor approximation, one will usually be able to take longer steps; hence, the algorithm will proceed faster. Gradient matching is necessary to preserve the convergence properties of the trust region approach. Response surfaces have also been used to accelerate derivative-free methods for local optimization. For example, Booker et al. (1999) use kriging response surfaces to accelerate the General Pattern Search algorithm of Dennis and Torczon (1991). In the original, un-accelerated version of the Pattern Search algorithm, one searches over a lattice of points around the current iterate until either one finds an"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 15
                            }
                        ],
                        "text": "In particular, Alexandrov et al. (2000) develop an algorithm that uses a \u2018correction factor\u2019 approach to force gradient matching between the surface and function at the current iterate."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 1380,
                                "start": 15
                            }
                        ],
                        "text": "In particular, Alexandrov et al. (2000) develop an algorithm that uses a \u2018correction factor\u2019 approach to force gradient matching between the surface and function at the current iterate. To get the next iterate, the surface is optimized within a trust region around the incumbant solution. The optimum point is then sampled and, if the objective function fails to decrease, the trust region is contracted. They prove that this approach must converge to a critical point of the function. It is perhaps best to think of Alexandrov\u2019s approach as a way of using response surfaces to accelerate an existing, provably convergent local optimization method\u2014as opposed to thinking of it as a new, response-surface-based method. Alexandrov starts with the well-known trust-region algorithm; however, instead of finding the next iterate by minimizing a second-order Taylor-series approximation within the trust region, she minimizes the response surface within the trust region. Because the response surface will usually be more accurate than the Taylor approximation, one will usually be able to take longer steps; hence, the algorithm will proceed faster. Gradient matching is necessary to preserve the convergence properties of the trust region approach. Response surfaces have also been used to accelerate derivative-free methods for local optimization. For example, Booker et al. (1999) use kriging response surfaces to accelerate the General Pattern Search algorithm of Dennis and Torczon (1991)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1458598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4370c644fee12369c51f40d6264d2401062698c8",
            "isKey": true,
            "numCitedBy": 251,
            "numCiting": 66,
            "paperAbstract": {
                "fragments": [],
                "text": "This work discusses an approach, the Approximation Management Framework (AMF), for solving optimization problems that involve computationally expensive simulations. AMF aims to maximize the use of lower-fidelity, cheaper models in iterative procedures with occasional, but systematic, recourse to higher-fidelity, more expensive models for monitoring the progress of the algorithm. The method is globally convergent to a solution of the original, high-fidelity problem. Three versions of AMF, based on three nonlinear programming algorithms, are demonstrated on a 3D aerodynamic wing optimization problem and a 2D airfoil optimization problem. In both cases Euler analysis solved on meshes of various refinement provides a suite of variable-fidelity models. Preliminary results indicate threefold savings in terms of high-fidelity analyses in case of the 3D problem and twofold savings for the 2D problem."
            },
            "slug": "Optimization-with-variable-fidelity-models-applied-Alexandrov-Lewis",
            "title": {
                "fragments": [],
                "text": "Optimization with variable-fidelity models applied to wing design"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Three versions of AMF, based on three nonlinear programming algorithms, are demonstrated on a 3D aerodynamic wing optimization problem and a 2D airfoil optimization problem, and preliminary results indicate threefold savings in terms of high-fidelity analyses in case of the 3D problem and twofold savings for the 2D problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2210196"
                        ],
                        "name": "J. Mockus",
                        "slug": "J.-Mockus",
                        "structuredName": {
                            "firstName": "Jonas",
                            "lastName": "Mockus",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Mockus"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 106
                            }
                        ],
                        "text": "Kushner\u2019s original algorithm was one dimensional, but Stuckman (1988), Perttunen (1991), Elder (1992), and Mockus (1994), have all heuristically extended the method to higher dimensions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 42695024,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82d51fac26b83f50ceb242b45fd6b1c88e94f867",
            "isKey": false,
            "numCitedBy": 277,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper a review of application of Bayesian approach to global and stochastic optimization of continuous multimodal functions is given. Advantages and disadvantages of Bayesian approach (average case analysis), comparing it with more usual minimax approach (worst case analysis) are discussed. New interactive version of software for global optimization is discussed. Practical multidimensional problems of global optimization are considered"
            },
            "slug": "Application-of-Bayesian-approach-to-numerical-of-Mockus",
            "title": {
                "fragments": [],
                "text": "Application of Bayesian approach to numerical methods of global and stochastic optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "Advantages and disadvantages of Bayesian approach (average case analysis), comparing it with more usual minimax approach (worst case analysis) are discussed and new interactive version of software for global optimization is discussed."
            },
            "venue": {
                "fragments": [],
                "text": "J. Glob. Optim."
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6050918"
                        ],
                        "name": "J. Elder",
                        "slug": "J.-Elder",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Elder",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elder"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "Kushner\u2019s original algorithm was one dimensional, but Stuckman (1988), Perttunen (1991), Elder (1992), and Mockus (1994), have all heuristically extended the method to higher dimensions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "On the left of the figure we show the contours of the two-dimensional Branin test function  Dixon and Szego, 1978 ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 123629619,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "396e4c51c8a9344d70429777c6d1a7a5a7357a48",
            "isKey": false,
            "numCitedBy": 17,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A global optimization algorithm is introduced which generalizes H.J. Kushner's (1964) univariate search. It aims to minimize the number of probes required for a given confidence in the results. All known probes contribute to a stochastic model of the underlying score surface, and this model is interrogated for the location most likely to exceed the current result goal. The surface is assumed to be fractal, leading to a piecewise Gaussian model, where the local regions are defined by the Delaunay triangulation of the probes. The algorithm balances the competing aims of (1) sampling in the vicinity of known peaks, and (2) exploring new regions. Preliminary tests on a standard 2-D search problem were very encouraging.<<ETX>>"
            },
            "slug": "Global-R/sup-d/-optimization-when-probes-are-the-Elder",
            "title": {
                "fragments": [],
                "text": "Global R/sup d/ optimization when probes are expensive: the GROPE algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A global optimization algorithm is introduced which generalizes H.J. Kushner's (1964) univariate search and balances the competing aims of sampling in the vicinity of known peaks, and exploring new regions."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] 1992 IEEE International Conference on Systems, Man, and Cybernetics"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102631067"
                        ],
                        "name": "D. D. Cox",
                        "slug": "D.-D.-Cox",
                        "structuredName": {
                            "firstName": "Denis",
                            "lastName": "Cox",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. D. Cox"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149717463"
                        ],
                        "name": "S. John",
                        "slug": "S.-John",
                        "structuredName": {
                            "firstName": "Staddon",
                            "lastName": "John",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. John"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 122348801,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea506a0a281eab154fa4d9c99b9c6e6ecb32974c",
            "isKey": false,
            "numCitedBy": 264,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for finding global optima using statistical prediction is presented. Assuming a random function model, lower confidence bounds on predicted values are used for sequential selection of evaluation points and as a convergence criterion. Comparison with published results for several test functions indicates that the procedure is very efficient in finding the global optimum of a multimodal function, and in terminating with relatively few evaluations.<<ETX>>"
            },
            "slug": "A-statistical-method-for-global-optimization-Cox-John",
            "title": {
                "fragments": [],
                "text": "A statistical method for global optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Comparison with published results for several test functions indicates that the procedure is very efficient in finding the global optimum of a multimodal function, and in terminating with relatively few evaluations."
            },
            "venue": {
                "fragments": [],
                "text": "[Proceedings] 1992 IEEE International Conference on Systems, Man, and Cybernetics"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9421473"
                        ],
                        "name": "M. Sasena",
                        "slug": "M.-Sasena",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Sasena",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sasena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1999696"
                        ],
                        "name": "P. Papalambros",
                        "slug": "P.-Papalambros",
                        "structuredName": {
                            "firstName": "Panos",
                            "lastName": "Papalambros",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Papalambros"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2289987"
                        ],
                        "name": "P. Goovaerts",
                        "slug": "P.-Goovaerts",
                        "structuredName": {
                            "firstName": "Pierre",
                            "lastName": "Goovaerts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Goovaerts"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [],
            "corpusId": 28393288,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fc4e8886fff2da4566934a7438a0038b6251c544",
            "isKey": false,
            "numCitedBy": 51,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The use of approximate models or metamodeling has lead to new areas of research in the optimization of computer simulations. Metamodeling approaches have advantages over traditional techniques when dealing with the noisy responses and/or high computational cost characteristic of many computer simulations, most notably those in MDO. While a number of methods in the literature discuss how to exploit the benefits of metamodeling approaches, one particular algorithm, Efficient Global Optimization (EGO), is the focus of this paper. Specifically, we look at the criteria used by the algorithm to select additional points to add to the data set used in fitting the metamodel. In addition to modifications to the original criterion, three criteria originally proposed for use in infill sampling in the field of geostatistics are explored. The impact of these criteria on the search strategy of EGO is examined through several analytical examples. In addition, several enhancements to EGO are explored. Finally, a case study is presented using a computer simulation to predict the fuel economy of hybrid vehicles."
            },
            "slug": "Metamodeling-sampling-criteria-in-a-global-Sasena-Papalambros",
            "title": {
                "fragments": [],
                "text": "Metamodeling sampling criteria in a global optimization framework"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A look at the criteria used by the algorithm to select additional points to add to the data set used in fitting the metamodel of Efficient Global Optimization, and three criteria originally proposed for use in infill sampling in the field of geostatistics are explored."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424124"
                        ],
                        "name": "B. Stuckman",
                        "slug": "B.-Stuckman",
                        "structuredName": {
                            "firstName": "Bruce",
                            "lastName": "Stuckman",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Stuckman"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 53
                            }
                        ],
                        "text": "Kushner\u2019s original algorithm was one dimensional, but Stuckman (1988), Perttunen (1991), Elder (1992), and Mockus (1994), have all heuristically extended the method to higher dimensions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 31720281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b2c742a51e7d06ede251a2e7e82e37df1ecd59a",
            "isKey": false,
            "numCitedBy": 82,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "The theory and implementation of a global search method of optimization in n dimensions, inspired by Kushner's method in one dimension, are presented. This method is meant to address optimization problems where the function has many extrema, where it may or may not be differentiable, and where it is important to reduce the number of evaluations of the function at the expense of increased computation. Comparisons are made to the performance of other global optimization techniques on a set of standard differentiable test functions. A new class of discrete-valued test functions is introduced, and the performance of the method is determined on a randomly generated set of these functions. Overall, this method has the power of other Bayesian/sampling techniques without the need for a separate local optimization technique for improved convergence. This makes it possible for the search to operate on unknown functions that may contain one or more discrete components. >"
            },
            "slug": "A-global-search-method-for-optimizing-nonlinear-Stuckman",
            "title": {
                "fragments": [],
                "text": "A global search method for optimizing nonlinear systems"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "The theory and implementation of a global search method of optimization in n dimensions, inspired by Kushner's method in one dimension, are presented, which has the power of other Bayesian/sampling techniques without the need for a separate local optimization technique for improved convergence."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1988
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057802095"
                        ],
                        "name": "Hans-Martin Gutmann",
                        "slug": "Hans-Martin-Gutmann",
                        "structuredName": {
                            "firstName": "Hans-Martin",
                            "lastName": "Gutmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hans-Martin Gutmann"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Gutmann (2001) proves this result for Kushner\u2019s original one-dimensional algorithm (it is a special case of a more general convergence theorem)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Gutmann (2001) has implemented this approach using thin-plate splines instead of kriging."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 0
                            }
                        ],
                        "text": "Gutmann (2001) reports excellent numerical results for a spline-based implementation of Method 7 and proves the convergence of the method."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 41245344,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "b8ea85f7b1a10c2ed6c4c90dc6443a997e972e95",
            "isKey": true,
            "numCitedBy": 649,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "AbstractWe introduce a method that aims to find the global minimum of a continuous nonconvex function on a compact subset of \n$$\\mathbb{R}^d $$\n. It is assumed that function evaluations are expensive and that no additional information is available. Radial basis function interpolation is used to define a utility function. The maximizer of this function is the next point where the objective function is evaluated. We show that, for most types of radial basis functions that are considered in this paper, convergence can be achieved without further assumptions on the objective function. Besides, it turns out that our method is closely related to a statistical global optimization method, the P-algorithm. A general framework for both methods is presented. Finally, a few numerical examples show that on the set of Dixon-Szeg\u00f6 test functions our method yields favourable results in comparison to other global optimization methods."
            },
            "slug": "A-Radial-Basis-Function-Method-for-Global-Gutmann",
            "title": {
                "fragments": [],
                "text": "A Radial Basis Function Method for Global Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that, for most types of radial basis functions that are considered in this paper, convergence can be achieved without further assumptions on the objective function."
            },
            "venue": {
                "fragments": [],
                "text": "J. Glob. Optim."
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3269957"
                        ],
                        "name": "A. Zilinskas",
                        "slug": "A.-Zilinskas",
                        "structuredName": {
                            "firstName": "Antanas",
                            "lastName": "Zilinskas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zilinskas"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 16,
                                "start": 0
                            }
                        ],
                        "text": "Zilinskas (1992) introduced an axiomatic treatment of the method, which he calls the \u2018Palgorithm.\u2019"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43055364,
            "fieldsOfStudy": [
                "Engineering"
            ],
            "id": "6d036bc27098ff01f19790a764b12bff02aa53fc",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "A review of statistical models for global optimization is presented. Rationality of the search for a global minimum is formulated axiomatically and the features of the corresponding algorithm are derived from the axioms. Furthermore the results of some applications of the proposed algorithm are presented and the perspectives of the approach are discussed."
            },
            "slug": "A-review-of-statistical-models-for-global-Zilinskas",
            "title": {
                "fragments": [],
                "text": "A review of statistical models for global optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "Rationality of the search for a global minimum is formulated axiomatically and the features of the corresponding algorithm are derived from the axioms."
            },
            "venue": {
                "fragments": [],
                "text": "J. Glob. Optim."
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6050918"
                        ],
                        "name": "J. Elder",
                        "slug": "J.-Elder",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Elder",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elder"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "Kushner\u2019s original algorithm was one dimensional, but Stuckman (1988), Perttunen (1991), Elder (1992), and Mockus (1994), have all heuristically extended the method to higher dimensions."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 428,
                                "start": 89
                            }
                        ],
                        "text": "Kushner\u2019s original algorithm was one dimensional, but Stuckman (1988), Perttunen (1991), Elder (1992), and Mockus (1994), have all heuristically extended the method to higher dimensions. Zilinskas (1992) introduced an axiomatic treatment of the method, which he calls the \u2018Palgorithm.\u2019 The key advantage of using the probability of improvement is that, under certain mild assumptions, the iterates will be dense. Gutmann (2001) proves this result for Kushner\u2019s original one-dimensional algorithm (it is a special case of a more general convergence theorem)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 89
                            }
                        ],
                        "text": "Kushner\u2019s original algorithm was one dimensional, but Stuckman (1988), Perttunen (1991), Elder (1992), and Mockus (1994), have all heuristically extended the method to higher dimensions. Zilinskas (1992) introduced an axiomatic treatment of the method, which he calls the \u2018Palgorithm."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15251414,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "55692d2bed5b98c89a3314bb0e0df89650d95f4e",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "A global optimization algorithm i s introduced which generalizes Kushner\u2019s univariate search [1]. It aims to minimize the number o f probes (function evaluations) required for a g i v e n confidence in the results. All known p r o b e s contribute to a stochastic model of the underly ing \u201cscore surface\u201d; this model is interrogated for the location most likely to exceed the current result goal. The surface is assumed to be fractal, l e a d i n g to a piecewise Gaussian model, where the local regions are defined by the Delaunay triangulation o f the probes. The algorithm balances the c o m p e t i n g aims of 1) sampling in the vicinity of known p e a k s , and 2) exploring new regions. Preliminary tests o n a standard 2-d search problem are very encouraging."
            },
            "slug": "Global-Rd-Optimization-when-Probes-are-Expensive-:-Elder",
            "title": {
                "fragments": [],
                "text": "Global Rd Optimization when Probes are Expensive : the GROPE Algorithm"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "A global optimization algorithm introduced which generalizes Kushner\u2019s univariate search and aims to minimize the number of probes (function evaluations) required for confidence in the results."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075355113"
                        ],
                        "name": "D. Dennis",
                        "slug": "D.-Dennis",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Dennis",
                            "middleNames": [
                                "Mack"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2081260358"
                        ],
                        "name": "Coxy",
                        "slug": "Coxy",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Coxy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Coxy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2054222936"
                        ],
                        "name": "Susan",
                        "slug": "Susan",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "Susan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2098105830"
                        ],
                        "name": "JohnzAbstractAn",
                        "slug": "JohnzAbstractAn",
                        "structuredName": {
                            "firstName": "",
                            "lastName": "JohnzAbstractAn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "JohnzAbstractAn"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 9899433,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1825c07187eddd2a3dc26b16d525524730a4feb4",
            "isKey": false,
            "numCitedBy": 170,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "An algorithm for nding global optima using statistical prediction is presented. Assuming a random function model, lower conndence bounds on predicted values are used for sequential selection of evaluation points and as a convergence criterion. Performance comparision with published results on several test functions indicate that the procedure is very eecient in nding the global optimum of a multimodal function, and in terminating with relatively few evaluations. The statistical computations involve linear algebra operations which are readily vectorized and is easy to adapt to parallel processing environments."
            },
            "slug": "SDO-:-A-Statistical-Method-for-Global-Optimization-Dennis-Coxy",
            "title": {
                "fragments": [],
                "text": "SDO : A Statistical Method for Global Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "Performance comparision with published results on several test functions indicate that the procedure is very eecient in nding the global optimum of a multimodal function, and in terminating with relatively few evaluations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1800149"
                        ],
                        "name": "J. Dennis",
                        "slug": "J.-Dennis",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Dennis",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dennis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776399"
                        ],
                        "name": "V. Torczon",
                        "slug": "V.-Torczon",
                        "structuredName": {
                            "firstName": "Virginia",
                            "lastName": "Torczon",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Torczon"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "For example, Booker et al. (1999) use kriging response surfaces to accelerate the General Pattern Search algorithm of  Dennis and Torczon (1991) ."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3127630,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "023933a8bc9378c94c715dcf7338430c0eb1e934",
            "isKey": false,
            "numCitedBy": 194,
            "numCiting": 19,
            "paperAbstract": {
                "fragments": [],
                "text": "Direct search methods are methods designed to solve unconstrained minimization problems of the form: \n \n$$\\mathop {\\min }\\limits_{x \\in {\\mathbb{R}^n}} f(x),$$ \n \nwhere f: \u211dn \u2192 \u211d. These methods are distinguished by the fact that they neither use nor require explicit derivative information; the search for a local minimizer is driven solely by function information. Popular methods in this class include the factorial design algorithm of Box [1], the pattern search algorithm of Hooke and Jeeves [4], and the simplex method of Neider and Mead [5]."
            },
            "slug": "Direct-Search-Methods-on-Parallel-Machines-Dennis-Torczon",
            "title": {
                "fragments": [],
                "text": "Direct Search Methods on Parallel Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 82,
                "text": "Direct search methods are methods designed to solve unconstrained minimization problems of the form min x in R n f(x), distinguished by the fact that they neither use nor require explicit derivative information; the search for a local minimizer is driven solely by function information."
            },
            "venue": {
                "fragments": [],
                "text": "SIAM J. Optim."
            },
            "year": 1991
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49600153"
                        ],
                        "name": "M. Locatelli",
                        "slug": "M.-Locatelli",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Locatelli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Locatelli"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 35799038,
            "fieldsOfStudy": [
                "Mathematics",
                "Computer Science"
            ],
            "id": "b94597ae958ed215bf57035774759fa8e5d89156",
            "isKey": false,
            "numCitedBy": 122,
            "numCiting": 18,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper Bayesian analysis and Wiener process are used in orderto build an algorithm to solve the problem of globaloptimization.The paper is divided in two main parts.In the first part an already known algorithm is considered: a new (Bayesian)stopping ruleis added to it and some results are given, such asan upper bound for the number of iterations under the new stopping rule.In the second part a new algorithm is introduced in which the Bayesianapproach is exploited not onlyin the choice of the Wiener model but also in the estimationof the parameter \u03c32 of the Wiener process, whose value appears to bequite crucial.Some results about this algorithm are also given."
            },
            "slug": "Bayesian-Algorithms-for-One-Dimensional-Global-Locatelli",
            "title": {
                "fragments": [],
                "text": "Bayesian Algorithms for One-Dimensional Global Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "In this paper Bayesian analysis and Wiener process are used in orderto build an algorithm to solve the problem of globaloptimization and the Bayesian approach is exploited not only in the choice of the Wiener model but also in the estimation of the parameter \u03c32 of theWiener process."
            },
            "venue": {
                "fragments": [],
                "text": "J. Glob. Optim."
            },
            "year": 1997
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 2537,
                                "start": 13
                            }
                        ],
                        "text": "Furthermore, Booker et.al. (1999) have shown how response surfaces can be used to accelerate a derivative-free method of local optimization. We then moved on to discuss methods that attempt to make the search global by exploiting kriging\u2019s ability to estimate potential error in its predictions. The first of these methods (Method 3) determines the next iterate by minimizing a \u2018statistical lower bounding function\u2019, based on the predictor minus several standard errors. Unfortunately, this method was found to have the fatal flaw that the iterates would not be dense, and hence the search could fail to find the global minimum. We then considered Method 4 in which the next iterate maximizes the (estimated) probability that the function value at a point will be better than a target T . This method was found to be convergent, but sensitive to the choice of T . However, one can finesse this problem by computing several search points per iteration, using several values of T . The resulting \u2018Enhanced Method 4\u2019 appears to be a highly promising approach. Finally, we considered Method 5 in which the next iterate maximizes the expected improvement from sampling at a point. Methods 3\u20135 all rely upon the standard error computed in kriging and can perform poorly if initial sample is highly deceptive (this is especially true of Method 5). Deceptive samples can cause the kriging standard error to underestimate the true error in the predictor and, as a result, Methods 3\u20135 may converge prematurely or slowly. The use of several targets in \u2018Enhanced Method 4\u2019 seems to reduce the negative impact of a deceptive initial sample\u2014which is one reason why we consider this approach so promising. We then turned our attention to \u2018one-stage\u2019 methods that can avoid being deceived by a deceptive initial sample. These methods use the mathematical machinery of response surfaces to directly evaluate hypotheses about the location of the optimum. In Method 6 we assumed that we merely want to achieve a known goal f \u2217 for the objective function. The next iterate was the point x\u2217 where, in a sense that we made precise, it is \u2018most credible\u2019 that the function obtains the value f \u2217. This method was found to converge quickly to the goal. In Method 7, we assumed that we want to find the global minimum of the objective function and that the minimal function value is not known in advance. This case is handled by computing several search points using several values of f \u2217, just as we used several values of T in Enhanced Method 4. Gutmann (2001) reports excellent numerical results for a spline-based implementation of Method 7 and proves the convergence of the method."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 262
                            }
                        ],
                        "text": "\u2026starts with an experimental design, statistical analyses can be done to identify which input variables are the most important (highest contribution to the variance of the output) and \u2018main effect plots\u2019 can be created to visualize input-output relationships (see Booker, 1998; Jones et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 13
                            }
                        ],
                        "text": "Furthermore, Booker et.al. (1999) have shown how response surfaces can be used to accelerate a derivative-free method of local optimization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 720,
                                "start": 0
                            }
                        ],
                        "text": "Booker et al. found that the reduction in function evaluations can be substantial. While these local methods are exciting developments, we should bear in mind that the methods can only guarantee convergence to a critical point. It is entirely possible that this point will merely be a saddle point or \u2018flat spot\u2019 of the function. This is illustrated in Figure 9 for yet another possible true function (True Function #4). In this case, the response surface is minimized at a sampled point and the gradient of the function is zero at this point. Thus, all of the previous methods would have declared success. Yet we are not even at a local minimum, far less a global one. A well known theorem by Torn and Zilinskas (1987) tells us that, in order to converge to the global optimum for a general continuous function, the sequence of iterates must be dense."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Examples of surrogate modeling of computer simulations. Presented at the ISSMO/NASA First Internet Conference on Approximations and Fast Reanalysis in Engineering Optimization, June"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1815604"
                        ],
                        "name": "M. Schonlau",
                        "slug": "M.-Schonlau",
                        "structuredName": {
                            "firstName": "Matthias",
                            "lastName": "Schonlau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schonlau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145228938"
                        ],
                        "name": "W. Welch",
                        "slug": "W.-Welch",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Welch",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Welch"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118096690"
                        ],
                        "name": "Donald R. Jones",
                        "slug": "Donald-R.-Jones",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Jones",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Donald R. Jones"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 73,
                                "start": 51
                            }
                        ],
                        "text": "In the literature, this method has been pursued by Schonlau et al. (1997), and by Sasena (2000)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 51
                            }
                        ],
                        "text": "In the literature, this method has been pursued by Schonlau et al. (1997), and by Sasena (2000). The performance of this method on Test Function #4 is explored in Figure 20."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 103
                            }
                        ],
                        "text": "Some progress has already been made in handling constraints for the expected improvement approach (see Schonlau et al. (1997) and Booker et al. (1999))."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 58806889,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aa2555e746b4d157a52d9bc4c63c9b386a720016",
            "isKey": false,
            "numCitedBy": 388,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Global-versus-local-search-in-constrained-of-models-Schonlau-Welch",
            "title": {
                "fragments": [],
                "text": "Global versus local search in constrained optimization of computer models"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3085659"
                        ],
                        "name": "H. Kushner",
                        "slug": "H.-Kushner",
                        "structuredName": {
                            "firstName": "Harold",
                            "lastName": "Kushner",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kushner"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 62599010,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "64c0650c3c559e540ad7fb73c4deadf340da474b",
            "isKey": false,
            "numCitedBy": 802,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A-New-Method-of-Locating-the-Maximum-Point-of-an-in-Kushner",
            "title": {
                "fragments": [],
                "text": "A New Method of Locating the Maximum Point of an Arbitrary Multipeak Curve in the Presence of Noise"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1963
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "79328060"
                        ],
                        "name": "P. C. Gehlen",
                        "slug": "P.-C.-Gehlen",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Gehlen",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. C. Gehlen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51407298"
                        ],
                        "name": "J. Beeler",
                        "slug": "J.-Beeler",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Beeler",
                            "middleNames": [
                                "R.",
                                "Jr."
                            ],
                            "suffix": "Jr"
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Beeler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "17144167"
                        ],
                        "name": "R. Jaffee",
                        "slug": "R.-Jaffee",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Jaffee",
                            "middleNames": [
                                "Isaac"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Jaffee"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 17804862,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "81484eb11e4457d56c474e5e995dbf858c539b4e",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Computer-experiments.-Gehlen-Beeler",
            "title": {
                "fragments": [],
                "text": "Computer experiments."
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 1972
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 291,
                                "start": 269
                            }
                        ],
                        "text": "\u2026is near a sampled point), we should nevertheless sample in a small neighborhood of the tentative solution to force the gradient of the surface to agree with the gradient of the true function (kriging can also be directly adapted to utilize derivative information; see Koehler and Owen, 1996)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 245,
                                "start": 223
                            }
                        ],
                        "text": "Since this means we have tentatively converged, we continue by sampling a little to the left and right of this point, since this will cause the gradient of the surface to match that of the function Having done this, further iterations quickly converge to the local minimum."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Computer experiments Design and Analysis of Experiments"
            },
            "venue": {
                "fragments": [],
                "text": "Handbook of Statistics"
            },
            "year": 1996
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Available on CD-ROM from the AIAA at http"
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the 8 th AIAA / USAF / NASA / ISMMO Symposium of Multidisciplinary Analysis & Optimization"
            },
            "year": 2000
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Key words: global optimization, response surface, kriging, splines"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 262
                            }
                        ],
                        "text": "\u2026starts with an experimental design, statistical analyses can be done to identify which input variables are the most important (highest contribution to the variance of the output) and \u2018main effect plots\u2019 can be created to visualize input-output relationships (see Booker, 1998; Jones et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Examples of surrogate modeling of computer simulations Presented at the ISSMO/NASA First Internet Conference on Approximations and Fast Reanalysis in Engineering Optimization"
            },
            "venue": {
                "fragments": [],
                "text": "Examples of surrogate modeling of computer simulations Presented at the ISSMO/NASA First Internet Conference on Approximations and Fast Reanalysis in Engineering Optimization"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 132
                            }
                        ],
                        "text": "The basis function shown for kriging is only one possibility, but it is a popular choice that appeared in an influential article by Sacks et al. (1989)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 5
                            }
                        ],
                        "text": "In this basis function, the parameters \u03b8 and p are assumed to satisfy \u03b8 0 and 0 p 2."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Design and analysis of computer experiments (with discussion)"
            },
            "venue": {
                "fragments": [],
                "text": "Statistical Science"
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 274,
                                "start": 262
                            }
                        ],
                        "text": "\u2026starts with an experimental design, statistical analyses can be done to identify which input variables are the most important (highest contribution to the variance of the output) and \u2018main effect plots\u2019 can be created to visualize input-output relationships (see Booker, 1998; Jones et al., 1998)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Examples of surrogate modeling of computer simulations"
            },
            "venue": {
                "fragments": [],
                "text": "Presented at the ISSMO / NASA First Internet Conference on Approximations and Fast Reanalysis in Engineering Optimization , June 14 - 27 ,"
            },
            "year": 1998
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 91
                            }
                        ],
                        "text": "On the left of the figure we show the contours of the two-dimensional Branin test function Dixon and Szego, 1978."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The global optimisation problem: an introduction"
            },
            "venue": {
                "fragments": [],
                "text": "Towards Global Optimisation"
            },
            "year": 1978
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 9,
            "methodology": 14,
            "result": 1
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 25,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/A-Taxonomy-of-Global-Optimization-Methods-Based-on-Jones/a3db48fdc9aaf6921f269817ba4ed16b9b198394?sort=total-citations"
}