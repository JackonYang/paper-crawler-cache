{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2402716"
                        ],
                        "name": "Colin Raffel",
                        "slug": "Colin-Raffel",
                        "structuredName": {
                            "firstName": "Colin",
                            "lastName": "Raffel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Colin Raffel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846258"
                        ],
                        "name": "Noam M. Shazeer",
                        "slug": "Noam-M.-Shazeer",
                        "structuredName": {
                            "firstName": "Noam",
                            "lastName": "Shazeer",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noam M. Shazeer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145625142"
                        ],
                        "name": "Adam Roberts",
                        "slug": "Adam-Roberts",
                        "structuredName": {
                            "firstName": "Adam",
                            "lastName": "Roberts",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adam Roberts"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3844009"
                        ],
                        "name": "Katherine Lee",
                        "slug": "Katherine-Lee",
                        "structuredName": {
                            "firstName": "Katherine",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Katherine Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46617804"
                        ],
                        "name": "Sharan Narang",
                        "slug": "Sharan-Narang",
                        "structuredName": {
                            "firstName": "Sharan",
                            "lastName": "Narang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sharan Narang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380243217"
                        ],
                        "name": "Michael Matena",
                        "slug": "Michael-Matena",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Matena",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Matena"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2389316"
                        ],
                        "name": "Yanqi Zhou",
                        "slug": "Yanqi-Zhou",
                        "structuredName": {
                            "firstName": "Yanqi",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yanqi Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157338362"
                        ],
                        "name": "Wei Li",
                        "slug": "Wei-Li",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35025299"
                        ],
                        "name": "Peter J. Liu",
                        "slug": "Peter-J.-Liu",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Liu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter J. Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 204838007,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3cfb319689f06bf04c2e28399361f414ca32c4b3",
            "isKey": false,
            "numCitedBy": 3965,
            "numCiting": 139,
            "paperAbstract": {
                "fragments": [],
                "text": "Transfer learning, where a model is first pre-trained on a data-rich task before being fine-tuned on a downstream task, has emerged as a powerful technique in natural language processing (NLP). The effectiveness of transfer learning has given rise to a diversity of approaches, methodology, and practice. In this paper, we explore the landscape of transfer learning techniques for NLP by introducing a unified framework that converts every language problem into a text-to-text format. Our systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks. By combining the insights from our exploration with scale and our new \"Colossal Clean Crawled Corpus\", we achieve state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more. To facilitate future work on transfer learning for NLP, we release our dataset, pre-trained models, and code."
            },
            "slug": "Exploring-the-Limits-of-Transfer-Learning-with-a-Raffel-Shazeer",
            "title": {
                "fragments": [],
                "text": "Exploring the Limits of Transfer Learning with a Unified Text-to-Text Transformer"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This systematic study compares pre-training objectives, architectures, unlabeled datasets, transfer approaches, and other factors on dozens of language understanding tasks and achieves state-of-the-art results on many benchmarks covering summarization, question answering, text classification, and more."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49387725"
                        ],
                        "name": "Jeff Wu",
                        "slug": "Jeff-Wu",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48422824"
                        ],
                        "name": "Rewon Child",
                        "slug": "Rewon-Child",
                        "structuredName": {
                            "firstName": "Rewon",
                            "lastName": "Child",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rewon Child"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "150970919"
                        ],
                        "name": "D. Luan",
                        "slug": "D.-Luan",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Luan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Luan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2698777"
                        ],
                        "name": "Dario Amodei",
                        "slug": "Dario-Amodei",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Amodei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dario Amodei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 81
                            }
                        ],
                        "text": "For example, fewer FLOPs are needed when training BERT-style models versus GPT-2 [11] models with comparable model and data sizes, and training steps."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 160025533,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9405cc0d6169988371b2755e573cc28650d14dfe",
            "isKey": false,
            "numCitedBy": 6498,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Natural language processing tasks, such as question answering, machine translation, reading comprehension, and summarization, are typically approached with supervised learning on taskspecific datasets. We demonstrate that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText. When conditioned on a document plus questions, the answers generated by the language model reach 55 F1 on the CoQA dataset matching or exceeding the performance of 3 out of 4 baseline systems without using the 127,000+ training examples. The capacity of the language model is essential to the success of zero-shot task transfer and increasing it improves performance in a log-linear fashion across tasks. Our largest model, GPT-2, is a 1.5B parameter Transformer that achieves state of the art results on 7 out of 8 tested language modeling datasets in a zero-shot setting but still underfits WebText. Samples from the model reflect these improvements and contain coherent paragraphs of text. These findings suggest a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
            },
            "slug": "Language-Models-are-Unsupervised-Multitask-Learners-Radford-Wu",
            "title": {
                "fragments": [],
                "text": "Language Models are Unsupervised Multitask Learners"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is demonstrated that language models begin to learn these tasks without any explicit supervision when trained on a new dataset of millions of webpages called WebText, suggesting a promising path towards building language processing systems which learn to perform tasks from their naturally occurring demonstrations."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152754428"
                        ],
                        "name": "Yoav Levine",
                        "slug": "Yoav-Levine",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Levine",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoav Levine"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1412384990"
                        ],
                        "name": "Barak Lenz",
                        "slug": "Barak-Lenz",
                        "structuredName": {
                            "firstName": "Barak",
                            "lastName": "Lenz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barak Lenz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751526108"
                        ],
                        "name": "O. Dagan",
                        "slug": "O.-Dagan",
                        "structuredName": {
                            "firstName": "Or",
                            "lastName": "Dagan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Dagan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "73775461"
                        ],
                        "name": "Ori Ram",
                        "slug": "Ori-Ram",
                        "structuredName": {
                            "firstName": "Ori",
                            "lastName": "Ram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ori Ram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "102589705"
                        ],
                        "name": "Dan Padnos",
                        "slug": "Dan-Padnos",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Padnos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dan Padnos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3074811"
                        ],
                        "name": "Or Sharir",
                        "slug": "Or-Sharir",
                        "structuredName": {
                            "firstName": "Or",
                            "lastName": "Sharir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Or Sharir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1389955537"
                        ],
                        "name": "S. Shalev-Shwartz",
                        "slug": "S.-Shalev-Shwartz",
                        "structuredName": {
                            "firstName": "Shai",
                            "lastName": "Shalev-Shwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Shalev-Shwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3140335"
                        ],
                        "name": "A. Shashua",
                        "slug": "A.-Shashua",
                        "structuredName": {
                            "firstName": "Amnon",
                            "lastName": "Shashua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Shashua"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701353"
                        ],
                        "name": "Y. Shoham",
                        "slug": "Y.-Shoham",
                        "structuredName": {
                            "firstName": "Yoav",
                            "lastName": "Shoham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shoham"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 199668663,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "772717eb2e369cd68c11b7da7aa779450dced9d0",
            "isKey": false,
            "numCitedBy": 108,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "The ability to learn from large unlabeled corpora has allowed neural language models to advance the frontier in natural language understanding. However, existing self-supervision techniques operate at the word form level, which serves as a surrogate for the underlying semantic content. This paper proposes a method to employ weak-supervision directly at the word sense level. Our model, named SenseBERT, is pre-trained to predict not only the masked words but also their WordNet supersenses. Accordingly, we attain a lexical-semantic level language model, without the use of human annotation. SenseBERT achieves significantly improved lexical understanding, as we demonstrate by experimenting on SemEval Word Sense Disambiguation, and by attaining a state of the art result on the \u2018Word in Context\u2019 task."
            },
            "slug": "SenseBERT:-Driving-Some-Sense-into-BERT-Levine-Lenz",
            "title": {
                "fragments": [],
                "text": "SenseBERT: Driving Some Sense into BERT"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper proposes a method to employ weak-supervision directly at the word sense level, pre-trained to predict not only the masked words but also their WordNet supersenses, and achieves a lexical-semantic level language model, without the use of human annotation."
            },
            "venue": {
                "fragments": [],
                "text": "ACL"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "51918868"
                        ],
                        "name": "Victor Sanh",
                        "slug": "Victor-Sanh",
                        "structuredName": {
                            "firstName": "Victor",
                            "lastName": "Sanh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Victor Sanh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380459402"
                        ],
                        "name": "Lysandre Debut",
                        "slug": "Lysandre-Debut",
                        "structuredName": {
                            "firstName": "Lysandre",
                            "lastName": "Debut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lysandre Debut"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40811585"
                        ],
                        "name": "Julien Chaumond",
                        "slug": "Julien-Chaumond",
                        "structuredName": {
                            "firstName": "Julien",
                            "lastName": "Chaumond",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julien Chaumond"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50335211"
                        ],
                        "name": "Thomas Wolf",
                        "slug": "Thomas-Wolf",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Wolf",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Wolf"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 203626972,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a54b56af24bb4873ed0163b77df63b92bd018ddc",
            "isKey": false,
            "numCitedBy": 2217,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "As Transfer Learning from large-scale pre-trained models becomes more prevalent in Natural Language Processing (NLP), operating these large models in on-the-edge and/or under constrained computational training or inference budgets remains challenging. In this work, we propose a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can then be fine-tuned with good performances on a wide range of tasks like its larger counterparts. While most prior work investigated the use of distillation for building task-specific models, we leverage knowledge distillation during the pre-training phase and show that it is possible to reduce the size of a BERT model by 40%, while retaining 97% of its language understanding capabilities and being 60% faster. To leverage the inductive biases learned by larger models during pre-training, we introduce a triple loss combining language modeling, distillation and cosine-distance losses. Our smaller, faster and lighter model is cheaper to pre-train and we demonstrate its capabilities for on-device computations in a proof-of-concept experiment and a comparative on-device study."
            },
            "slug": "DistilBERT,-a-distilled-version-of-BERT:-smaller,-Sanh-Debut",
            "title": {
                "fragments": [],
                "text": "DistilBERT, a distilled version of BERT: smaller, faster, cheaper and lighter"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work proposes a method to pre-train a smaller general-purpose language representation model, called DistilBERT, which can be fine-tuned with good performances on a wide range of tasks like its larger counterparts, and introduces a triple loss combining language modeling, distillation and cosine-distance losses."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39706649"
                        ],
                        "name": "Xiaoqi Jiao",
                        "slug": "Xiaoqi-Jiao",
                        "structuredName": {
                            "firstName": "Xiaoqi",
                            "lastName": "Jiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoqi Jiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1384668226"
                        ],
                        "name": "Yichun Yin",
                        "slug": "Yichun-Yin",
                        "structuredName": {
                            "firstName": "Yichun",
                            "lastName": "Yin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichun Yin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50812138"
                        ],
                        "name": "Lifeng Shang",
                        "slug": "Lifeng-Shang",
                        "structuredName": {
                            "firstName": "Lifeng",
                            "lastName": "Shang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lifeng Shang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145820291"
                        ],
                        "name": "Xin Jiang",
                        "slug": "Xin-Jiang",
                        "structuredName": {
                            "firstName": "Xin",
                            "lastName": "Jiang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xin Jiang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117025507"
                        ],
                        "name": "Xiao Chen",
                        "slug": "Xiao-Chen",
                        "structuredName": {
                            "firstName": "Xiao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111818678"
                        ],
                        "name": "Linlin Li",
                        "slug": "Linlin-Li",
                        "structuredName": {
                            "firstName": "Linlin",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Linlin Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49451193"
                        ],
                        "name": "F. Wang",
                        "slug": "F.-Wang",
                        "structuredName": {
                            "firstName": "Fang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688015"
                        ],
                        "name": "Qun Liu",
                        "slug": "Qun-Liu",
                        "structuredName": {
                            "firstName": "Qun",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qun Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": ". These are quite related to the training costs, but deserve a separate discussion. In particular, the inference phase allows for post-training model optimizations, for example via model distillation [2,3]. This discussion is beyond the scope of this article. 4The following \ufb01gures are based on internal AI21 Labs data. They can be somewhat lower due to discounts, or using preemptible versions of the sys"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 202719327,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0cbf97173391b0430140117027edcaf1a37968c7",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "Language model pre-training, such as BERT, has significantly improved the performances of many natural language processing tasks. However, pre-trained language models are usually computationally expensive, so it is difficult to efficiently execute them on resource-restricted devices. To accelerate inference and reduce model size while maintaining accuracy, we first propose a novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models. By leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be effectively transferred to a small \u201cstudent\u201d TinyBERT. Then, we introduce a new two-stage learning framework for TinyBERT, which performs Transformer distillation at both the pre-training and task-specific learning stages. This framework ensures that TinyBERT can capture the general-domain as well as the task-specific knowledge in BERT. TinyBERT4 with 4 layers is empirically effective and achieves more than 96.8% the performance of its teacher BERT-Base on GLUE benchmark, while being 7.5x smaller and 9.4x faster on inference. TinyBERT4 is also significantly better than 4-layer state-of-the-art baselines on BERT distillation, with only ~28% parameters and ~31% inference time of them. Moreover, TinyBERT6 with 6 layers performs on-par with its teacher BERT-Base."
            },
            "slug": "TinyBERT:-Distilling-BERT-for-Natural-Language-Jiao-Yin",
            "title": {
                "fragments": [],
                "text": "TinyBERT: Distilling BERT for Natural Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel Transformer distillation method that is specially designed for knowledge distillation (KD) of the Transformer-based models is proposed and, by leveraging this new KD method, the plenty of knowledge encoded in a large \u201cteacher\u201d BERT can be effectively transferred to a small \u201cstudent\u201d TinyBERT."
            },
            "venue": {
                "fragments": [],
                "text": "FINDINGS"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2362534"
                        ],
                        "name": "Zhenzhong Lan",
                        "slug": "Zhenzhong-Lan",
                        "structuredName": {
                            "firstName": "Zhenzhong",
                            "lastName": "Lan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhenzhong Lan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46221498"
                        ],
                        "name": "Mingda Chen",
                        "slug": "Mingda-Chen",
                        "structuredName": {
                            "firstName": "Mingda",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingda Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7685850"
                        ],
                        "name": "Sebastian Goodman",
                        "slug": "Sebastian-Goodman",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Goodman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sebastian Goodman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700980"
                        ],
                        "name": "Kevin Gimpel",
                        "slug": "Kevin-Gimpel",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Gimpel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Gimpel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48267618"
                        ],
                        "name": "Piyush Sharma",
                        "slug": "Piyush-Sharma",
                        "structuredName": {
                            "firstName": "Piyush",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Piyush Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737285"
                        ],
                        "name": "Radu Soricut",
                        "slug": "Radu-Soricut",
                        "structuredName": {
                            "firstName": "Radu",
                            "lastName": "Soricut",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Radu Soricut"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 202888986,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7a064df1aeada7e69e5173f7d4c8606f4470365b",
            "isKey": false,
            "numCitedBy": 2817,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Increasing model size when pretraining natural language representations often results in improved performance on downstream tasks. However, at some point further model increases become harder due to GPU/TPU memory limitations and longer training times. To address these problems, we present two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT. Comprehensive empirical evidence shows that our proposed methods lead to models that scale much better compared to the original BERT. We also use a self-supervised loss that focuses on modeling inter-sentence coherence, and show it consistently helps downstream tasks with multi-sentence inputs. As a result, our best model establishes new state-of-the-art results on the GLUE, RACE, and \\squad benchmarks while having fewer parameters compared to BERT-large. The code and the pretrained models are available at this https URL."
            },
            "slug": "ALBERT:-A-Lite-BERT-for-Self-supervised-Learning-of-Lan-Chen",
            "title": {
                "fragments": [],
                "text": "ALBERT: A Lite BERT for Self-supervised Learning of Language Representations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents two parameter-reduction techniques to lower memory consumption and increase the training speed of BERT, and uses a self-supervised loss that focuses on modeling inter-sentence coherence."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34176020"
                        ],
                        "name": "Jesse Dodge",
                        "slug": "Jesse-Dodge",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Dodge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesse Dodge"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40895369"
                        ],
                        "name": "Suchin Gururangan",
                        "slug": "Suchin-Gururangan",
                        "structuredName": {
                            "firstName": "Suchin",
                            "lastName": "Gururangan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Suchin Gururangan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35540755"
                        ],
                        "name": "Dallas Card",
                        "slug": "Dallas-Card",
                        "structuredName": {
                            "firstName": "Dallas",
                            "lastName": "Card",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dallas Card"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "4671928"
                        ],
                        "name": "Roy Schwartz",
                        "slug": "Roy-Schwartz",
                        "structuredName": {
                            "firstName": "Roy",
                            "lastName": "Schwartz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Roy Schwartz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144365875"
                        ],
                        "name": "Noah A. Smith",
                        "slug": "Noah-A.-Smith",
                        "structuredName": {
                            "firstName": "Noah",
                            "lastName": "Smith",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Noah A. Smith"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 202235596,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0e4cd6bae6ac1017e7b1b9bd644375aee65b8372",
            "isKey": false,
            "numCitedBy": 159,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Research in natural language processing proceeds, in part, by demonstrating that new models achieve superior performance (e.g., accuracy) on held-out test data, compared to previous results. In this paper, we demonstrate that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best. We argue for reporting additional details, especially performance on validation data obtained during model development. We present a novel technique for doing so: expected validation performance of the best-found model as a function of computation budget (i.e., the number of hyperparameter search trials or the overall training time). Using our approach, we find multiple recent model comparisons where authors would have reached a different conclusion if they had used more (or less) computation. Our approach also allows us to estimate the amount of computation required to obtain a given accuracy; applying it to several recently published results yields massive variation across papers, from hours to weeks. We conclude with a set of best practices for reporting experimental results which allow for robust future comparisons, and provide code to allow researchers to use our technique."
            },
            "slug": "Show-Your-Work:-Improved-Reporting-of-Experimental-Dodge-Gururangan",
            "title": {
                "fragments": [],
                "text": "Show Your Work: Improved Reporting of Experimental Results"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "It is demonstrated that test-set performance scores alone are insufficient for drawing accurate conclusions about which model performs best, and a novel technique is presented: expected validation performance of the best-found model as a function of computation budget."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144358401"
                        ],
                        "name": "Kevin Clark",
                        "slug": "Kevin-Clark",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Clark",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Clark"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707242"
                        ],
                        "name": "Minh-Thang Luong",
                        "slug": "Minh-Thang-Luong",
                        "structuredName": {
                            "firstName": "Minh-Thang",
                            "lastName": "Luong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Minh-Thang Luong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144783904"
                        ],
                        "name": "Christopher D. Manning",
                        "slug": "Christopher-D.-Manning",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Manning",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher D. Manning"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 213152193,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "756810258e3419af76aff38c895c20343b0602d0",
            "isKey": false,
            "numCitedBy": 1248,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "While masked language modeling (MLM) pre-training methods such as BERT produce excellent results on downstream NLP tasks, they require large amounts of compute to be effective. These approaches corrupt the input by replacing some tokens with [MASK] and then train a model to reconstruct the original tokens. As an alternative, we propose a more sample-efficient pre-training task called replaced token detection. Instead of masking the input, our approach corrupts it by replacing some input tokens with plausible alternatives sampled from a small generator network. Then, instead of training a model that predicts the original identities of the corrupted tokens, we train a discriminative model that predicts whether each token in the corrupted input was replaced by a generator sample or not. Thorough experiments demonstrate this new pre-training task is more efficient than MLM because the model learns from all input tokens rather than just the small subset that was masked out. As a result, the contextual representations learned by our approach substantially outperform the ones learned by methods such as BERT and XLNet given the same model size, data, and compute. The gains are particularly strong for small models; for example, we train a model on one GPU for 4 days that outperforms GPT (trained using 30x more compute) on the GLUE natural language understanding benchmark. Our approach also works well at scale, where we match the performance of RoBERTa, the current state-of-the-art pre-trained transformer, while using less than 1/4 of the compute."
            },
            "slug": "ELECTRA:-Pre-training-Text-Encoders-as-Rather-Than-Clark-Luong",
            "title": {
                "fragments": [],
                "text": "ELECTRA: Pre-training Text Encoders as Discriminators Rather Than Generators"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The contextual representations learned by the proposed replaced token detection pre-training task substantially outperform the ones learned by methods such as BERT and XLNet given the same model size, data, and compute."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39172707"
                        ],
                        "name": "Jacob Devlin",
                        "slug": "Jacob-Devlin",
                        "structuredName": {
                            "firstName": "Jacob",
                            "lastName": "Devlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jacob Devlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1744179"
                        ],
                        "name": "Ming-Wei Chang",
                        "slug": "Ming-Wei-Chang",
                        "structuredName": {
                            "firstName": "Ming-Wei",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming-Wei Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2544107"
                        ],
                        "name": "Kenton Lee",
                        "slug": "Kenton-Lee",
                        "structuredName": {
                            "firstName": "Kenton",
                            "lastName": "Lee",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenton Lee"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3259253"
                        ],
                        "name": "Kristina Toutanova",
                        "slug": "Kristina-Toutanova",
                        "structuredName": {
                            "firstName": "Kristina",
                            "lastName": "Toutanova",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kristina Toutanova"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 13
                            }
                        ],
                        "text": "Similarly, ALBERT [16] achieves better accuracy with fewer parameters by factorizing the embedding matrix and weight sharing across layers."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 99
                            }
                        ],
                        "text": "More quantitatively, here are current ballpark list-price costs of training differently sized BERT [4] models on the Wikipedia and Book corpora (15 GB)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 54,
                                "start": 50
                            }
                        ],
                        "text": "For example, fewer FLOPs are needed when training BERT-style models versus GPT-2 [11] models with comparable model and data sizes, and training steps."
                    },
                    "intents": []
                }
            ],
            "corpusId": 52967399,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df2b0e26d0599ce3e70df8a9da02e51594e0e992",
            "isKey": false,
            "numCitedBy": 35053,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a new language representation model called BERT, which stands for Bidirectional Encoder Representations from Transformers. Unlike recent language representation models (Peters et al., 2018a; Radford et al., 2018), BERT is designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers. As a result, the pre-trained BERT model can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks, such as question answering and language inference, without substantial task-specific architecture modifications. BERT is conceptually simple and empirically powerful. It obtains new state-of-the-art results on eleven natural language processing tasks, including pushing the GLUE score to 80.5 (7.7 point absolute improvement), MultiNLI accuracy to 86.7% (4.6% absolute improvement), SQuAD v1.1 question answering Test F1 to 93.2 (1.5 point absolute improvement) and SQuAD v2.0 Test F1 to 83.1 (5.1 point absolute improvement)."
            },
            "slug": "BERT:-Pre-training-of-Deep-Bidirectional-for-Devlin-Chang",
            "title": {
                "fragments": [],
                "text": "BERT: Pre-training of Deep Bidirectional Transformers for Language Understanding"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A new language representation model, BERT, designed to pre-train deep bidirectional representations from unlabeled text by jointly conditioning on both left and right context in all layers, which can be fine-tuned with just one additional output layer to create state-of-the-art models for a wide range of tasks."
            },
            "venue": {
                "fragments": [],
                "text": "NAACL"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152724169"
                        ],
                        "name": "J. Kaplan",
                        "slug": "J.-Kaplan",
                        "structuredName": {
                            "firstName": "Jared",
                            "lastName": "Kaplan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kaplan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52238703"
                        ],
                        "name": "Sam McCandlish",
                        "slug": "Sam-McCandlish",
                        "structuredName": {
                            "firstName": "Sam",
                            "lastName": "McCandlish",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sam McCandlish"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "103143311"
                        ],
                        "name": "T. Henighan",
                        "slug": "T.-Henighan",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Henighan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Henighan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31035595"
                        ],
                        "name": "Tom B. Brown",
                        "slug": "Tom-B.-Brown",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Brown",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tom B. Brown"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1490681878"
                        ],
                        "name": "Benjamin Chess",
                        "slug": "Benjamin-Chess",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Chess",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin Chess"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48422824"
                        ],
                        "name": "Rewon Child",
                        "slug": "Rewon-Child",
                        "structuredName": {
                            "firstName": "Rewon",
                            "lastName": "Child",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rewon Child"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145565184"
                        ],
                        "name": "Scott Gray",
                        "slug": "Scott-Gray",
                        "structuredName": {
                            "firstName": "Scott",
                            "lastName": "Gray",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Scott Gray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38909097"
                        ],
                        "name": "Alec Radford",
                        "slug": "Alec-Radford",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Radford",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Radford"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49387725"
                        ],
                        "name": "Jeff Wu",
                        "slug": "Jeff-Wu",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeff Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2698777"
                        ],
                        "name": "Dario Amodei",
                        "slug": "Dario-Amodei",
                        "structuredName": {
                            "firstName": "Dario",
                            "lastName": "Amodei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dario Amodei"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 210861095,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6c561d02500b2596a230b341a8eb8b921ca5bf2",
            "isKey": false,
            "numCitedBy": 506,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "We study empirical scaling laws for language model performance on the cross-entropy loss. The loss scales as a power-law with model size, dataset size, and the amount of compute used for training, with some trends spanning more than seven orders of magnitude. Other architectural details such as network width or depth have minimal effects within a wide range. Simple equations govern the dependence of overfitting on model/dataset size and the dependence of training speed on model size. These relationships allow us to determine the optimal allocation of a fixed compute budget. Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence."
            },
            "slug": "Scaling-Laws-for-Neural-Language-Models-Kaplan-McCandlish",
            "title": {
                "fragments": [],
                "text": "Scaling Laws for Neural Language Models"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Larger models are significantly more sample-efficient, such that optimally compute-efficient training involves training very large models on a relatively modest amount of data and stopping significantly before convergence."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2276422"
                        ],
                        "name": "Zhuohan Li",
                        "slug": "Zhuohan-Li",
                        "structuredName": {
                            "firstName": "Zhuohan",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhuohan Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145217343"
                        ],
                        "name": "Eric Wallace",
                        "slug": "Eric-Wallace",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Wallace",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Wallace"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2191455"
                        ],
                        "name": "Sheng Shen",
                        "slug": "Sheng-Shen",
                        "structuredName": {
                            "firstName": "Sheng",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sheng Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48085802"
                        ],
                        "name": "Kevin Lin",
                        "slug": "Kevin-Lin",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732330"
                        ],
                        "name": "K. Keutzer",
                        "slug": "K.-Keutzer",
                        "structuredName": {
                            "firstName": "Kurt",
                            "lastName": "Keutzer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Keutzer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38666915"
                        ],
                        "name": "D. Klein",
                        "slug": "D.-Klein",
                        "structuredName": {
                            "firstName": "Dan",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144307989"
                        ],
                        "name": "Joseph Gonzalez",
                        "slug": "Joseph-Gonzalez",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Gonzalez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Joseph Gonzalez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 211532277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2356781b8a98bf94e6fc73798c6cb65ac35e5f97",
            "isKey": false,
            "numCitedBy": 110,
            "numCiting": 79,
            "paperAbstract": {
                "fragments": [],
                "text": "Since hardware resources are limited, the objective of training deep learning models is typically to maximize accuracy subject to the time and memory constraints of training and inference. We study the impact of model size in this setting, focusing on Transformer models for NLP tasks that are limited by compute: self-supervised pretraining and high-resource machine translation. We first show that even though smaller Transformer models execute faster per iteration, wider and deeper models converge in significantly fewer steps. Moreover, this acceleration in convergence typically outpaces the additional computational overhead of using larger models. Therefore, the most compute-efficient training strategy is to counterintuitively train extremely large models but stop after a small number of iterations. \nThis leads to an apparent trade-off between the training efficiency of large Transformer models and the inference efficiency of small Transformer models. However, we show that large models are more robust to compression techniques such as quantization and pruning than small models. Consequently, one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models."
            },
            "slug": "Train-Large,-Then-Compress:-Rethinking-Model-Size-Li-Wallace",
            "title": {
                "fragments": [],
                "text": "Train Large, Then Compress: Rethinking Model Size for Efficient Training and Inference of Transformers"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "It is shown that large models are more robust to compression techniques such as quantization and pruning than small models, and one can get the best of both worlds: heavily compressed, large models achieve higher accuracy than lightly compressed, small models."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143808231"
                        ],
                        "name": "Nikita Kitaev",
                        "slug": "Nikita-Kitaev",
                        "structuredName": {
                            "firstName": "Nikita",
                            "lastName": "Kitaev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nikita Kitaev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6639036"
                        ],
                        "name": "Anselm Levskaya",
                        "slug": "Anselm-Levskaya",
                        "structuredName": {
                            "firstName": "Anselm",
                            "lastName": "Levskaya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anselm Levskaya"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 209315300,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "055fd6a9f7293269f1b22c1470e63bd02d8d9500",
            "isKey": false,
            "numCitedBy": 760,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Large Transformer models routinely achieve state-of-the-art results on a number of tasks but training these models can be prohibitively costly, especially on long sequences. We introduce two techniques to improve the efficiency of Transformers. For one, we replace dot-product attention by one that uses locality-sensitive hashing, changing its complexity from O($L^2$) to O($L\\log L$), where $L$ is the length of the sequence. Furthermore, we use reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of $N$ times, where $N$ is the number of layers. The resulting model, the Reformer, performs on par with Transformer models while being much more memory-efficient and much faster on long sequences."
            },
            "slug": "Reformer:-The-Efficient-Transformer-Kitaev-Kaiser",
            "title": {
                "fragments": [],
                "text": "Reformer: The Efficient Transformer"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work replaces dot-product attention by one that uses locality-sensitive hashing and uses reversible residual layers instead of the standard residuals, which allows storing activations only once in the training process instead of several times, making the model much more memory-efficient and much faster on long sequences."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380165568"
                        ],
                        "name": "Jonathan S. Rosenfeld",
                        "slug": "Jonathan-S.-Rosenfeld",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Rosenfeld",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan S. Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32928116"
                        ],
                        "name": "Amir Rosenfeld",
                        "slug": "Amir-Rosenfeld",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Rosenfeld",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amir Rosenfeld"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2083259"
                        ],
                        "name": "Yonatan Belinkov",
                        "slug": "Yonatan-Belinkov",
                        "structuredName": {
                            "firstName": "Yonatan",
                            "lastName": "Belinkov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonatan Belinkov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2613669"
                        ],
                        "name": "N. Shavit",
                        "slug": "N.-Shavit",
                        "structuredName": {
                            "firstName": "Nir",
                            "lastName": "Shavit",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Shavit"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 203592013,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d28c18a3c2a0afdc0a8634d18345af8d36e1f948",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "The dependency of the generalization error of neural networks on model and dataset size is of critical importance both in practice and for understanding the theory of neural networks. Nevertheless, the functional form of this dependency remains elusive. In this work, we present a functional form which approximates well the generalization error in practice. Capitalizing on the successful concept of model scaling (e.g., width, depth), we are able to simultaneously construct such a form and specify the exact models which can attain it across model/data scales. Our construction follows insights obtained from observations conducted over a range of model/data scales, in various model types and datasets, in vision and language tasks. We show that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data."
            },
            "slug": "A-Constructive-Prediction-of-the-Generalization-Rosenfeld-Rosenfeld",
            "title": {
                "fragments": [],
                "text": "A Constructive Prediction of the Generalization Error Across Scales"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work presents a functional form which approximates well the generalization error in practice, and shows that the form both fits the observations well across scales, and provides accurate predictions from small- to large-scale models and data."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2113243762"
                        ],
                        "name": "Hugo Touvron",
                        "slug": "Hugo-Touvron",
                        "structuredName": {
                            "firstName": "Hugo",
                            "lastName": "Touvron",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hugo Touvron"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3271933"
                        ],
                        "name": "M. Douze",
                        "slug": "M.-Douze",
                        "structuredName": {
                            "firstName": "Matthijs",
                            "lastName": "Douze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Douze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681054"
                        ],
                        "name": "H. J\u00e9gou",
                        "slug": "H.-J\u00e9gou",
                        "structuredName": {
                            "firstName": "Herv\u00e9",
                            "lastName": "J\u00e9gou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. J\u00e9gou"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 189928444,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c0aaee2337e5af680e5dca1bfc349a737dfec573",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "Data-augmentation is key to the training of neural networks for image classification. This paper first shows that existing augmentations induce a significant discrepancy between the typical size of the objects seen by the classifier at train and test time. We experimentally validate that, for a target test resolution, using a lower train resolution offers better classification at test time. \nWe then propose a simple yet effective and efficient strategy to optimize the classifier performance when the train and test resolutions differ. It involves only a computationally cheap fine-tuning of the network at the test resolution. This enables training strong classifiers using small training images. For instance, we obtain 77.1% top-1 accuracy on ImageNet with a ResNet-50 trained on 128x128 images, and 79.8% with one trained on 224x224 image. In addition, if we use extra training data we get 82.5% with the ResNet-50 train with 224x224 images. \nConversely, when training a ResNeXt-101 32x48d pre-trained in weakly-supervised fashion on 940 million public images at resolution 224x224 and further optimizing for test resolution 320x320, we obtain a test top-1 accuracy of 86.4% (top-5: 98.0%) (single-crop). To the best of our knowledge this is the highest ImageNet single-crop, top-1 and top-5 accuracy to date."
            },
            "slug": "Fixing-the-train-test-resolution-discrepancy-Touvron-Vedaldi",
            "title": {
                "fragments": [],
                "text": "Fixing the train-test resolution discrepancy"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is experimentally validated that, for a target test resolution, using a lower train resolution offers better classification at test time, and a simple yet effective and efficient strategy to optimize the classifier performance when the train and test resolutions differ is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "NeurIPS"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707642"
                        ],
                        "name": "D. Geman",
                        "slug": "D.-Geman",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3194361"
                        ],
                        "name": "S. Geman",
                        "slug": "S.-Geman",
                        "structuredName": {
                            "firstName": "Stuart",
                            "lastName": "Geman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Geman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9588317"
                        ],
                        "name": "Neil Hallonquist",
                        "slug": "Neil-Hallonquist",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Hallonquist",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil Hallonquist"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721284"
                        ],
                        "name": "L. Younes",
                        "slug": "L.-Younes",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Younes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Younes"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 8687210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "050da5d159fb0dd96143948e1cffeb3dec814673",
            "isKey": false,
            "numCitedBy": 246,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "Significance In computer vision, as in other fields of artificial intelligence, the methods of evaluation largely define the scientific effort. Most current evaluations measure detection accuracy, emphasizing the classification of regions according to objects from a predefined library. But detection is not the same as understanding. We present here a different evaluation system, in which a query engine prepares a written test (\u201cvisual Turing test\u201d) that uses binary questions to probe a system\u2019s ability to identify attributes and relationships in addition to recognizing objects. Today, computer vision systems are tested by their accuracy in detecting and localizing instances of objects. As an alternative, and motivated by the ability of humans to provide far richer descriptions and even tell a story about an image, we construct a \u201cvisual Turing test\u201d: an operator-assisted device that produces a stochastic sequence of binary questions from a given test image. The query engine proposes a question; the operator either provides the correct answer or rejects the question as ambiguous; the engine proposes the next question (\u201cjust-in-time truthing\u201d). The test is then administered to the computer-vision system, one question at a time. After the system\u2019s answer is recorded, the system is provided the correct answer and the next question. Parsing is trivial and deterministic; the system being tested requires no natural language processing. The query engine employs statistical constraints, learned from a training set, to produce questions with essentially unpredictable answers\u2014the answer to a question, given the history of questions and their correct answers, is nearly equally likely to be positive or negative. In this sense, the test is only about vision. The system is designed to produce streams of questions that follow natural story lines, from the instantiation of a unique object, through an exploration of its properties, and on to its relationships with other uniquely instantiated objects."
            },
            "slug": "Visual-Turing-test-for-computer-vision-systems-Geman-Geman",
            "title": {
                "fragments": [],
                "text": "Visual Turing test for computer vision systems"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents a different evaluation system, in which a query engine prepares a written test that uses binary questions to probe a system\u2019s ability to identify attributes and relationships in addition to recognizing objects."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings of the National Academy of Sciences"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "104185819"
                        ],
                        "name": "Neil Genzlinger",
                        "slug": "Neil-Genzlinger",
                        "structuredName": {
                            "firstName": "Neil",
                            "lastName": "Genzlinger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Neil Genzlinger"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 116
                            }
                        ],
                        "text": "In particular, the inference phase allows for post-training model optimizations, for example via model distillation [2, 3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 127667495,
            "fieldsOfStudy": [
                "Geography"
            ],
            "id": "5a554c8d22d47ac499aeb7fb0532ca9be65e5a2e",
            "isKey": false,
            "numCitedBy": 11471,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "A.-and-Q-Genzlinger",
            "title": {
                "fragments": [],
                "text": "A. and Q"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2006
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 20
                            }
                        ],
                        "text": ", Etchemendy and Li [6]) are not as sanguine."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "National research cloud: Ensuring the continuation of american innovation"
            },
            "venue": {
                "fragments": [],
                "text": "https: //hai.stanford.edu/news/national- research- cloud- ensuring- continuation- americaninnovation, Accessed: 2020-04-12"
            },
            "year": 2020
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and H"
            },
            "venue": {
                "fragments": [],
                "text": "Jegou, \u201cFixing the train-test resolution discrepancy,\u201d in Advances in Neural Information Processing Systems 32, Curran Associates, Inc."
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and N"
            },
            "venue": {
                "fragments": [],
                "text": "Shavit, \u201cA constructive prediction of the generalization error across scales,\u201d in International Conference on Learning Representations"
            },
            "year": 2020
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "New research from TSO Logic shows aws costs get lower every year"
            },
            "venue": {
                "fragments": [],
                "text": "https://aws.amazon. com/blogs/apn/new-research-from-tso-logic-shows-aws-costs-get-lower-every-year/, Accessed: 2020-04-12"
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Y"
            },
            "venue": {
                "fragments": [],
                "text": "Shoham, \u201cSense- BERT: Driving some sense into BERT,\u201d in Proceedings of the 2010 Conference of the Association for Computational Linguistics (ACL)"
            },
            "year": 2020
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and A"
            },
            "venue": {
                "fragments": [],
                "text": "Levskaya, \u201cReformer: The efficient transformer,\u201d in International Conference on Learning Representations"
            },
            "year": 2020
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 131
                            }
                        ],
                        "text": "Other training schemes can introduce additional factors that dictate cost; for example, the adversarial training scheme of ELECTRA [12] uses an additional \u201cgenerator\u201d model during training."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and C"
            },
            "venue": {
                "fragments": [],
                "text": "D. Manning, \u201cELECTRA: Pre-training text encoders as discriminators rather than generators,\u201d in International Conference on Learning Representations"
            },
            "year": 2020
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Train large"
            },
            "venue": {
                "fragments": [],
                "text": "then compress: Rethinking model size for efficient training and inference of transformers"
            },
            "year": 2020
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 128
                            }
                        ],
                        "text": "For example, based on information released by Google, we estimate that, at list-price, training the 11Bparameter variant5 of T5 [5] cost well above $1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and P"
            },
            "venue": {
                "fragments": [],
                "text": "J. Liu, \u201cExploring the limits of transfer learning with a unified text-to-text transformer,\u201d ArXiv e-prints"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "NLP\u2019s ImageNet moment has arrived"
            },
            "venue": {
                "fragments": [],
                "text": "https://thegradient.pub/nlp-imagenet/, Accessed: 2020-04-12"
            },
            "year": 2018
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and L"
            },
            "venue": {
                "fragments": [],
                "text": "Younes, \u201cVisual Turing test for computer vision systems,\u201d Proceedings of the National Academy of Sciences, vol. 112, no. 12, pp. 3618\u20133623"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and R"
            },
            "venue": {
                "fragments": [],
                "text": "Soricut, \u201cALBERT: A lite BERT for selfsupervised learning of language representations,\u201d in International Conference on Learning Representations"
            },
            "year": 2020
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 116
                            }
                        ],
                        "text": "In particular, the inference phase allows for post-training model optimizations, for example via model distillation [2, 3]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "DistilBERT"
            },
            "venue": {
                "fragments": [],
                "text": "a distilled version of BERT: Smaller, faster, cheaper and lighter,\u201d in NeurIPS EMC2 Workshop"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 36,
                                "start": 33
                            }
                        ],
                        "text": "7 It is worth noting the work of [7], which analyzes the impact of various variables, including model size and amount of compute, on performance, as measured by perplexity."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and D"
            },
            "venue": {
                "fragments": [],
                "text": "Amodei, Scaling laws for neural language models"
            },
            "year": 2020
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 6
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 30,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/The-Cost-of-Training-NLP-Models:-A-Concise-Overview-Sharir-Peleg/5290d7921f0266c8b50b79fc8a0b7d22868f4f60?sort=total-citations"
}