{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31056113"
                        ],
                        "name": "Albert Gural",
                        "slug": "Albert-Gural",
                        "structuredName": {
                            "firstName": "Albert",
                            "lastName": "Gural",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Albert Gural"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698147"
                        ],
                        "name": "B. Murmann",
                        "slug": "B.-Murmann",
                        "structuredName": {
                            "firstName": "Boris",
                            "lastName": "Murmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Murmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "In a complimentary line of work, Gural and Murmann [25] propose memory-optimal direct convolutions (MODC)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 187
                            }
                        ],
                        "text": "We compare against previous SOTA works: Bonsai [36], ProtoNN [24], Gradient Boosted Decision Tree Ensemble Pruning [12], kNN, radial basis function support vector machine (SVM), and MODC [25]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "A recent related work, MODC [25], is considerably slower than SpArSe, at 684 ms for MNIST on the Arduino Uno."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 174800118,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8260c36f2eb17eda929e80b65fce9bfcbdbeaa46",
            "isKey": true,
            "numCitedBy": 19,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "In the age of Internet of Things (IoT), embedded devices ranging from ARM Cortex M0s with hundreds of KB of RAM to Arduinos with 2KB RAM are expected to perform increasingly sophisticated classification tasks, such as voice and gesture recognition, activity tracking, and biometric security. While convolutional neural networks (CNNs), together with spectrogram preprocessing, are a natural solution to many of these classification tasks, storage of the network\u2019s activations often exceeds the hard memory constraints of embedded platforms. This paper presents memoryoptimal direct convolutions as a way to push classification accuracy as high as possible given strict hardware memory constraints at the expense of extra compute. We therefore explore the opposite end of the compute-memory trade-off curve from standard approaches that minimize latency. We validate the memory-optimal CNN technique with an Arduino implementation of the 10-class MNIST classification task, fitting the network specification, weights, and activations entirely within 2KB SRAM and achieving a state-of-theart classification accuracy for small-scale embedded systems of 99.15%."
            },
            "slug": "Memory-Optimal-Direct-Convolutions-for-Maximizing-Gural-Murmann",
            "title": {
                "fragments": [],
                "text": "Memory-Optimal Direct Convolutions for Maximizing Classification Accuracy in Embedded Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper validates the memory-optimal CNN technique with an Arduino implementation of the 10-class MNIST classification task, fitting the network specification, weights, and activations entirely within 2KB SRAM and achieving a state-of-theart classification accuracy for small-scale embedded systems of 99.15%."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3313708"
                        ],
                        "name": "P. Whatmough",
                        "slug": "P.-Whatmough",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Whatmough",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Whatmough"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143946900"
                        ],
                        "name": "Chuteng Zhou",
                        "slug": "Chuteng-Zhou",
                        "structuredName": {
                            "firstName": "Chuteng",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chuteng Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145318539"
                        ],
                        "name": "Patrick Hansen",
                        "slug": "Patrick-Hansen",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Hansen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrick Hansen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388016804"
                        ],
                        "name": "S. K. Venkataramanaiah",
                        "slug": "S.-K.-Venkataramanaiah",
                        "structuredName": {
                            "firstName": "Shreyas",
                            "lastName": "Venkataramanaiah",
                            "middleNames": [
                                "Kolala"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. K. Venkataramanaiah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706798"
                        ],
                        "name": "Jae-sun Seo",
                        "slug": "Jae-sun-Seo",
                        "structuredName": {
                            "firstName": "Jae-sun",
                            "lastName": "Seo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jae-sun Seo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39045061"
                        ],
                        "name": "Matthew Mattina",
                        "slug": "Matthew-Mattina",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Mattina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Mattina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 67855411,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e6dd204b1639af77eb53ab42bc9d95a3c3358022",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The computational demands of computer vision tasks based on state-of-the-art Convolutional Neural Network (CNN) image classification far exceed the energy budgets of mobile devices. This paper proposes FixyNN, which consists of a fixed-weight feature extractor that generates ubiquitous CNN features, and a conventional programmable CNN accelerator which processes a dataset-specific CNN. Image classification models for FixyNN are trained end-to-end via transfer learning, with the common feature extractor representing the transfered part, and the programmable part being learnt on the target dataset. Experimental results demonstrate FixyNN hardware can achieve very high energy efficiencies up to 26.6 TOPS/W ($4.81 \\times$ better than iso-area programmable accelerator). Over a suite of six datasets we trained models via transfer learning with an accuracy loss of $<1\\%$ resulting in up to 11.2 TOPS/W - nearly $2 \\times$ more efficient than a conventional programmable CNN accelerator of the same area."
            },
            "slug": "FixyNN:-Efficient-Hardware-for-Mobile-Computer-via-Whatmough-Zhou",
            "title": {
                "fragments": [],
                "text": "FixyNN: Efficient Hardware for Mobile Computer Vision via Transfer Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "Experimental results demonstrate FixyNN hardware can achieve very high energy efficiencies up to 26.6 TOPS/W ($4.81 \\times better than iso-area programmable accelerator) and over a suite of six datasets the authors trained models via transfer learning with an accuracy loss of <1\\% resulting in up to 11.2 TOPs/W - nearly $2 more efficient than a conventional programmable CNN accelerator of the same area."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1950815"
                        ],
                        "name": "Tien-Ju Yang",
                        "slug": "Tien-Ju-Yang",
                        "structuredName": {
                            "firstName": "Tien-Ju",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tien-Ju Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50579876"
                        ],
                        "name": "Yu-hsin Chen",
                        "slug": "Yu-hsin-Chen",
                        "structuredName": {
                            "firstName": "Yu-hsin",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-hsin Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691305"
                        ],
                        "name": "V. Sze",
                        "slug": "V.-Sze",
                        "structuredName": {
                            "firstName": "Vivienne",
                            "lastName": "Sze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sze"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "Although mobile phones are more constrained than general-purpose CPUs and GPUs, they still have many orders of magnitude more memory capacity and compute performance than MCUs (Table 1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 130
                            }
                        ],
                        "text": "In contrast to this work, the majority of preceding research on compute/memory efficient CNN inference has targeted CPUs and GPUs [33, 20, 63, 64, 50, 58, 53]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Experiments were run on four NVIDIA RTX 2080 GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "The broad proliferation of MCUs relative to desktop GPUs and CPUs stems from the fact that they are orders of magnitude cheaper (\u223c 600\u00d7) and less power hungry (\u223c 250, 000\u00d7)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "The best public market estimates suggest that around 50 billion MCU chips will ship in 2019 [2], which far eclipses other computer chips like graphics processing units (GPUs), whose shipments totalled roughly 100 million units in 2018 [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2779809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3ac1df952ffb63abb4231a4410f6f8375ccdfe79",
            "isKey": true,
            "numCitedBy": 478,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep convolutional neural networks (CNNs) are indispensable to state-of-the-art computer vision algorithms. However, they are still rarely deployed on battery-powered mobile devices, such as smartphones and wearable gadgets, where vision algorithms can enable many revolutionary real-world applications. The key limiting factor is the high energy consumption of CNN processing due to its high computational complexity. While there are many previous efforts that try to reduce the CNN model size or the amount of computation, we find that they do not necessarily result in lower energy consumption. Therefore, these targets do not serve as a good metric for energy cost estimation. To close the gap between CNN design and energy consumption optimization, we propose an energy-aware pruning algorithm for CNNs that directly uses the energy consumption of a CNN to guide the pruning process. The energy estimation methodology uses parameters extrapolated from actual hardware measurements. The proposed layer-by-layer pruning algorithm also prunes more aggressively than previously proposed pruning methods by minimizing the error in the output feature maps instead of the filter weights. For each layer, the weights are first pruned and then locally fine-tuned with a closed-form least-square solution to quickly restore the accuracy. After all layers are pruned, the entire network is globally fine-tuned using back-propagation. With the proposed pruning method, the energy consumption of AlexNet and GoogLeNet is reduced by 3.7x and 1.6x, respectively, with less than 1% top-5 accuracy loss. We also show that reducing the number of target classes in AlexNet greatly decreases the number of weights, but has a limited impact on energy consumption."
            },
            "slug": "Designing-Energy-Efficient-Convolutional-Neural-Yang-Chen",
            "title": {
                "fragments": [],
                "text": "Designing Energy-Efficient Convolutional Neural Networks Using Energy-Aware Pruning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes an energy-aware pruning algorithm for CNNs that directly uses the energy consumption of a CNN to guide the pruning process, and shows that reducing the number of target classes in AlexNet greatly decreases thenumber of weights, but has a limited impact on energy consumption."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2026843558"
                        ],
                        "name": "Ashish Kumar",
                        "slug": "Ashish-Kumar",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Kumar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2601821"
                        ],
                        "name": "Saurabh Goyal",
                        "slug": "Saurabh-Goyal",
                        "structuredName": {
                            "firstName": "Saurabh",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saurabh Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145859952"
                        ],
                        "name": "M. Varma",
                        "slug": "M.-Varma",
                        "structuredName": {
                            "firstName": "Manik",
                            "lastName": "Varma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Varma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 197
                            }
                        ],
                        "text": "The reliance of (4)-(6) on the l0 norm is motivated by our use of pruning to minimize the number of non-zeros in both \u03c9 and {xl} L l=1, which is also the compression mechanism used in related work [41, 32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 165
                            }
                        ],
                        "text": "The severe memory constraints for inference on MCUs have pushed research away from CNNs and toward simpler classifiers based on decision trees and nearest neighbors [41, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[41], we let the output layer use features at multiple scales by optionally routing the output of each block to the output layer through a fully connected (FC) layer (see Fig."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 152
                            }
                        ],
                        "text": "To the best of our knowledge, there are currently no CNN architectures or training procedures that produce CNNs satisfying these MCU memory constraints [41, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 154
                            }
                        ],
                        "text": "2 Models optimized for total memory footprint Next, we demonstrate that SpArSe resolves C1-C2 by finding CNNs that consume less device memory than Bonsai [41]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 22
                            }
                        ],
                        "text": "To match the setup in [41], we also report on binary versions of these datasets, meaning that the classes are split into two groups and re-labeled."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 80
                            }
                        ],
                        "text": "This has led many to conclude that CNNs should be abandoned on constrained MCUs [41, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 51,
                                "start": 47
                            }
                        ],
                        "text": "We compare against previous SOTA works: Bonsai [41], ProtoNN [32], Gradient Boosted Decision Tree Ensemble Pruning [22], kNN, and radial basis function support vector machine (SVM)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 168
                            }
                        ],
                        "text": "We use SpArSe to uncover SOTA models on four datasets, in terms of accuracy and model size, outperforming both pruning of popular architectures and MCU-specific models [41, 32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[41] propose Bonsai, a pruned shallow decision tree with non-axis aligned decision boundaries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4503192,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "db3e4b11a569fe17908417351678525f6304e7b7",
            "isKey": true,
            "numCitedBy": 167,
            "numCiting": 57,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper develops a novel tree-based algorithm, called Bonsai, for efficient prediction on IoT devices - such as those based on the Arduino Uno board having an 8 bit ATmega328P microcontroller operating at 16 MHz with no native floating point support, 2 KB RAM and 32 KB read-only flash. Bonsai maintains prediction accuracy while minimizing model size and prediction costs by: (a) developing a tree model which learns a single, shallow, sparse tree with powerful nodes; (b) sparsely projecting all data into a low-dimensional space in which the tree is learnt; and (c) jointly learning all tree and projection parameters. Experimental results on multiple benchmark datasets demonstrate that Bonsai can make predictions in milliseconds even on slow microcontrollers, can fit in KB of memory, has lower battery consumption than all other algorithms while achieving prediction accuracies that can be as much as 30% higher than state-of-the-art methods for resource-efficient machine learning. Bonsai is also shown to generalize to other resource constrained settings beyond IoT by generating significantly better search results as compared to Bing's L3 ranker when the model size is restricted to 300 bytes. Bonsai's code can be downloaded from (BonsaiCode)."
            },
            "slug": "Resource-efficient-Machine-Learning-in-2-KB-RAM-for-Kumar-Goyal",
            "title": {
                "fragments": [],
                "text": "Resource-efficient Machine Learning in 2 KB RAM for the Internet of Things"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "Bonsai can make predictions in milliseconds even on slow microcontrollers, can fit in KB of memory, has lower battery consumption than all other algorithms, and achieves prediction accuracies that can be as much as 30% higher than state-of-the-art methods for resource-efficient machine learning."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143840275"
                        ],
                        "name": "Song Han",
                        "slug": "Song-Han",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49543200"
                        ],
                        "name": "Xingyu Liu",
                        "slug": "Xingyu-Liu",
                        "structuredName": {
                            "firstName": "Xingyu",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xingyu Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3123774"
                        ],
                        "name": "Huizi Mao",
                        "slug": "Huizi-Mao",
                        "structuredName": {
                            "firstName": "Huizi",
                            "lastName": "Mao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huizi Mao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1560397753"
                        ],
                        "name": "Jing Pu",
                        "slug": "Jing-Pu",
                        "structuredName": {
                            "firstName": "Jing",
                            "lastName": "Pu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jing Pu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9182159"
                        ],
                        "name": "A. Pedram",
                        "slug": "A.-Pedram",
                        "structuredName": {
                            "firstName": "Ardavan",
                            "lastName": "Pedram",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pedram"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144764327"
                        ],
                        "name": "M. Horowitz",
                        "slug": "M.-Horowitz",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Horowitz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Horowitz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "80724002"
                        ],
                        "name": "W. Dally",
                        "slug": "W.-Dally",
                        "structuredName": {
                            "firstName": "William",
                            "lastName": "Dally",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Dally"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "Although mobile phones are more constrained than general-purpose CPUs and GPUs, they still have many orders of magnitude more memory capacity and compute performance than MCUs (Table 1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 130
                            }
                        ],
                        "text": "In contrast to this work, the majority of preceding research on compute/memory efficient CNN inference has targeted CPUs and GPUs [33, 20, 63, 64, 50, 58, 53]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Experiments were run on four NVIDIA RTX 2080 GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "The broad proliferation of MCUs relative to desktop GPUs and CPUs stems from the fact that they are orders of magnitude cheaper (\u223c 600\u00d7) and less power hungry (\u223c 250, 000\u00d7)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "The best public market estimates suggest that around 50 billion MCU chips will ship in 2019 [2], which far eclipses other computer chips like graphics processing units (GPUs), whose shipments totalled roughly 100 million units in 2018 [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1663491,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2e2b189f668cf2c06ebc44dc9b166648256cf457",
            "isKey": true,
            "numCitedBy": 1816,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "State-of-the-art deep neural networks (DNNs) have hundreds of millions of connections and are both computationally and memory intensive, making them difficult to deploy on embedded systems with limited hardware resources and power budgets. While custom hardware helps the computation, fetching weights from DRAM is two orders of magnitude more expensive than ALU operations, and dominates the required power. Previously proposed 'Deep Compression' makes it possible to fit large DNNs (AlexNet and VGGNet) fully in on-chip SRAM. This compression is achieved by pruning the redundant connections and having multiple connections share the same weight. We propose an energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing. Going from DRAM to SRAM gives EIE 120x energy saving, Exploiting sparsity saves 10x, Weight sharing gives 8x, Skipping zero activations from ReLU saves another 3x. Evaluated on nine DNN benchmarks, EIE is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression. EIE has a processing power of 102 GOPS working directly on a compressed network, corresponding to 3 TOPS on an uncompressed network, and processes FC layers of AlexNet at 1.88x104 frames/sec with a power dissipation of only 600mW. It is 24,000x and 3,400x more energy efficient than a CPU and GPU respectively. Compared with DaDianNao, EIE has 2.9x, 19x and 3x better throughput, energy efficiency and area efficiency."
            },
            "slug": "EIE:-Efficient-Inference-Engine-on-Compressed-Deep-Han-Liu",
            "title": {
                "fragments": [],
                "text": "EIE: Efficient Inference Engine on Compressed Deep Neural Network"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "An energy efficient inference engine (EIE) that performs inference on this compressed network model and accelerates the resulting sparse matrix-vector multiplication with weight sharing and is 189x and 13x faster when compared to CPU and GPU implementations of the same DNN without compression."
            },
            "venue": {
                "fragments": [],
                "text": "2016 ACM/IEEE 43rd Annual International Symposium on Computer Architecture (ISCA)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "120805419"
                        ],
                        "name": "Mingxing Tan",
                        "slug": "Mingxing-Tan",
                        "structuredName": {
                            "firstName": "Mingxing",
                            "lastName": "Tan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingxing Tan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Bo Chen",
                        "slug": "Bo-Chen",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34320634"
                        ],
                        "name": "Ruoming Pang",
                        "slug": "Ruoming-Pang",
                        "structuredName": {
                            "firstName": "Ruoming",
                            "lastName": "Pang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruoming Pang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053781980"
                        ],
                        "name": "Vijay Vasudevan",
                        "slug": "Vijay-Vasudevan",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Vasudevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vijay Vasudevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "Although mobile phones are more constrained than general-purpose CPUs and GPUs, they still have many orders of magnitude more memory capacity and compute performance than MCUs (Table 1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 130
                            }
                        ],
                        "text": "In contrast to this work, the majority of preceding research on compute/memory efficient CNN inference has targeted CPUs and GPUs [33, 20, 63, 64, 50, 58, 53]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Experiments were run on four NVIDIA RTX 2080 GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "The broad proliferation of MCUs relative to desktop GPUs and CPUs stems from the fact that they are orders of magnitude cheaper (\u223c 600\u00d7) and less power hungry (\u223c 250, 000\u00d7)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "The best public market estimates suggest that around 50 billion MCU chips will ship in 2019 [2], which far eclipses other computer chips like graphics processing units (GPUs), whose shipments totalled roughly 100 million units in 2018 [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 51891697,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "693c97ecedb0a84539b7162c95e89fa3cd84ca73",
            "isKey": true,
            "numCitedBy": 1613,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Designing convolutional neural networks (CNN) for mobile devices is challenging because mobile models need to be small and fast, yet still accurate. Although significant efforts have been dedicated to design and improve mobile CNNs on all dimensions, it is very difficult to manually balance these trade-offs when there are so many architectural possibilities to consider. In this paper, we propose an automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency. Unlike previous work, where latency is considered via another, often inaccurate proxy (e.g., FLOPS), our approach directly measures real-world inference latency by executing the model on mobile phones. To further strike the right balance between flexibility and search space size, we propose a novel factorized hierarchical search space that encourages layer diversity throughout the network. Experimental results show that our approach consistently outperforms state-of-the-art mobile CNN models across multiple vision tasks. On the ImageNet classification task, our MnasNet achieves 75.2% top-1 accuracy with 78ms latency on a Pixel phone, which is 1.8\u00d7 faster than MobileNetV2 with 0.5% higher accuracy and 2.3\u00d7 faster than NASNet with 1.2% higher accuracy. Our MnasNet also achieves better mAP quality than MobileNets for COCO object detection. Code is at https://github.com/tensorflow/tpu/tree/master/models/official/mnasnet."
            },
            "slug": "MnasNet:-Platform-Aware-Neural-Architecture-Search-Tan-Chen",
            "title": {
                "fragments": [],
                "text": "MnasNet: Platform-Aware Neural Architecture Search for Mobile"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "An automated mobile neural architecture search (MNAS) approach, which explicitly incorporate model latency into the main objective so that the search can identify a model that achieves a good trade-off between accuracy and latency."
            },
            "venue": {
                "fragments": [],
                "text": "2019 IEEE/CVF Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145834074"
                        ],
                        "name": "Han Cai",
                        "slug": "Han-Cai",
                        "structuredName": {
                            "firstName": "Han",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Han Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20515689"
                        ],
                        "name": "Ligeng Zhu",
                        "slug": "Ligeng-Zhu",
                        "structuredName": {
                            "firstName": "Ligeng",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ligeng Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143840275"
                        ],
                        "name": "Song Han",
                        "slug": "Song-Han",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Han"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 109
                            }
                        ],
                        "text": "This MOBO yields better coverage of the Pareto frontier than the deterministic scalarization methods used in [20, 56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "Although mobile phones are more constrained than general-purpose CPUs and GPUs, they still have many orders of magnitude more memory capacity and compute performance than MCUs (Table 1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 130
                            }
                        ],
                        "text": "In contrast to this work, the majority of preceding research on compute/memory efficient CNN inference has targeted CPUs and GPUs [33, 20, 63, 64, 50, 58, 53]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Experiments were run on four NVIDIA RTX 2080 GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "The broad proliferation of MCUs relative to desktop GPUs and CPUs stems from the fact that they are orders of magnitude cheaper (\u223c 600\u00d7) and less power hungry (\u223c 250, 000\u00d7)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "The best public market estimates suggest that around 50 billion MCU chips will ship in 2019 [2], which far eclipses other computer chips like graphics processing units (GPUs), whose shipments totalled roughly 100 million units in 2018 [4]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 101
                            }
                        ],
                        "text": "Algorithms for identifying performant CNN architectures have received significant attention recently [66, 23, 20, 45, 31, 24, 44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 85
                            }
                        ],
                        "text": "has been leveraged to achieve even more efficient networks on mobile phone platforms [20, 56]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 54438210,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dc8b789446416383bfafe9b1c504c4a2b17e68d1",
            "isKey": true,
            "numCitedBy": 1117,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural architecture search (NAS) has a great impact by automatically designing effective neural network architectures. However, the prohibitive computational demand of conventional NAS algorithms (e.g. $10^4$ GPU hours) makes it difficult to \\emph{directly} search the architectures on large-scale tasks (e.g. ImageNet). Differentiable NAS can reduce the cost of GPU hours via a continuous representation of network architecture but suffers from the high GPU memory consumption issue (grow linearly w.r.t. candidate set size). As a result, they need to utilize~\\emph{proxy} tasks, such as training on a smaller dataset, or learning with only a few blocks, or training just for a few epochs. These architectures optimized on proxy tasks are not guaranteed to be optimal on the target task. In this paper, we present \\emph{ProxylessNAS} that can \\emph{directly} learn the architectures for large-scale target tasks and target hardware platforms. We address the high memory consumption issue of differentiable NAS and reduce the computational cost (GPU hours and GPU memory) to the same level of regular training while still allowing a large candidate set. Experiments on CIFAR-10 and ImageNet demonstrate the effectiveness of directness and specialization. On CIFAR-10, our model achieves 2.08\\% test error with only 5.7M parameters, better than the previous state-of-the-art architecture AmoebaNet-B, while using 6$\\times$ fewer parameters. On ImageNet, our model achieves 3.1\\% better top-1 accuracy than MobileNetV2, while being 1.2$\\times$ faster with measured GPU latency. We also apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design."
            },
            "slug": "ProxylessNAS:-Direct-Neural-Architecture-Search-on-Cai-Zhu",
            "title": {
                "fragments": [],
                "text": "ProxylessNAS: Direct Neural Architecture Search on Target Task and Hardware"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "ProxylessNAS is presented, which can directly learn the architectures for large-scale target tasks and target hardware platforms and apply ProxylessNAS to specialize neural architectures for hardware with direct hardware metrics (e.g. latency) and provide insights for efficient CNN architecture design."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2193705"
                        ],
                        "name": "Dibakar Gope",
                        "slug": "Dibakar-Gope",
                        "structuredName": {
                            "firstName": "Dibakar",
                            "lastName": "Gope",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dibakar Gope"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31613624"
                        ],
                        "name": "Ganesh S. Dasika",
                        "slug": "Ganesh-S.-Dasika",
                        "structuredName": {
                            "firstName": "Ganesh",
                            "lastName": "Dasika",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ganesh S. Dasika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39045061"
                        ],
                        "name": "Matthew Mattina",
                        "slug": "Matthew-Mattina",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Mattina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Mattina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 101
                            }
                        ],
                        "text": "Advances include depth-wise separable layers [50], deployment-centric pruning [62, 45], quantization [58, 21], and matrix decomposition techniques [55]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 67877066,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6d564022f763f7a6c8ced7e09e63a551fb3e5666",
            "isKey": false,
            "numCitedBy": 19,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "Machine learning-based applications are increasingly prevalent in IoT devices. The power and storage constraints of these devices make it particularly challenging to run modern neural networks, limiting the number of new applications that can be deployed on an IoT system. A number of compression techniques have been proposed, each with its own trade-offs. We propose a hybrid network which combines the strengths of current neural- and tree-based learning techniques in conjunction with ternary quantization, and show a detailed analysis of the associated model design space. Using this hybrid model we obtained a 11.1% reduction in the number of computations, a 52.2% reduction in the model size, and a 30.6% reduction in the overall memory footprint over a state-of-the-art keyword-spotting neural network, with negligible loss in accuracy."
            },
            "slug": "Ternary-Hybrid-Neural-Tree-Networks-for-Highly-IoT-Gope-Dasika",
            "title": {
                "fragments": [],
                "text": "Ternary Hybrid Neural-Tree Networks for Highly Constrained IoT Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes a hybrid network which combines the strengths of current neural- and tree-based learning techniques in conjunction with ternary quantization, and shows a detailed analysis of the associated model design space."
            },
            "venue": {
                "fragments": [],
                "text": "MLSys"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1905484"
                        ],
                        "name": "Dimitrios Stamoulis",
                        "slug": "Dimitrios-Stamoulis",
                        "structuredName": {
                            "firstName": "Dimitrios",
                            "lastName": "Stamoulis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dimitrios Stamoulis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3306838"
                        ],
                        "name": "Ruizhou Ding",
                        "slug": "Ruizhou-Ding",
                        "structuredName": {
                            "firstName": "Ruizhou",
                            "lastName": "Ding",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ruizhou Ding"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1399962864"
                        ],
                        "name": "Di Wang",
                        "slug": "Di-Wang",
                        "structuredName": {
                            "firstName": "Di",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Di Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48209560"
                        ],
                        "name": "D. Lymberopoulos",
                        "slug": "D.-Lymberopoulos",
                        "structuredName": {
                            "firstName": "Dimitrios",
                            "lastName": "Lymberopoulos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Lymberopoulos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2381107"
                        ],
                        "name": "B. Priyantha",
                        "slug": "B.-Priyantha",
                        "structuredName": {
                            "firstName": "Bodhi",
                            "lastName": "Priyantha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Priyantha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49721997"
                        ],
                        "name": "J. Liu",
                        "slug": "J.-Liu",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704073"
                        ],
                        "name": "Diana Marculescu",
                        "slug": "Diana-Marculescu",
                        "structuredName": {
                            "firstName": "Diana",
                            "lastName": "Marculescu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diana Marculescu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[56] is less a NAS algorithm and more of a structured pruning approach, given that the only allowed architectures are reductions of MobileNetV2."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 93,
                                "start": 85
                            }
                        ],
                        "text": "has been leveraged to achieve even more efficient networks on mobile phone platforms [20, 56]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[56], the authors optimize the kernel size and number of feature maps of the MBConv layers in a MobileNetV2 backbone [53] by expressing each of the layer choices as a pruned version of a superkernel."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 109
                            }
                        ],
                        "text": "This MOBO yields better coverage of the Pareto frontier than the deterministic scalarization methods used in [20, 56]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 102352775,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "18ca023bbb1a24873140b5440479e74c7f90b684",
            "isKey": true,
            "numCitedBy": 192,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Can we automatically design a Convolutional Network (ConvNet) with the highest image classification accuracy under the runtime constraint of a mobile device? Neural architecture search (NAS) has revolutionized the design of hardware-efficient ConvNets by automating this process. However, the NAS problem remains challenging due to the combinatorially large design space, causing a significant searching time (at least 200 GPU-hours). To alleviate this complexity, we propose Single-Path NAS, a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4 hours. Our contributions are as follows: 1. Single-path search space: Compared to previous differentiable NAS methods, Single-Path NAS uses one single-path over-parameterized ConvNet to encode all architectural decisions with shared convolutional kernel parameters, hence drastically decreasing the number of trainable parameters and the search cost down to few epochs. 2. Hardware-efficient ImageNet classification: Single-Path NAS achieves 74.96% top-1 accuracy on ImageNet with 79ms latency on a Pixel 1 phone, which is state-of-the-art accuracy compared to NAS methods with similar constraints (<80ms). 3. NAS efficiency: Single-Path NAS search cost is only 8 epochs (30 TPU-hours), which is up to 5,000x faster compared to prior work. 4. Reproducibility: Unlike all recent mobile-efficient NAS methods which only release pretrained models, we open-source our entire codebase at: this https URL."
            },
            "slug": "Single-Path-NAS:-Designing-Hardware-Efficient-in-4-Stamoulis-Ding",
            "title": {
                "fragments": [],
                "text": "Single-Path NAS: Designing Hardware-Efficient ConvNets in less than 4 Hours"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work proposes Single-Path NAS, a novel differentiable NAS method for designing hardware-efficient ConvNets in less than 4 hours, and uses one single-path over-parameterized ConvNet to encode all architectural decisions with shared convolutional kernel parameters, hence drastically decreasing the number of trainable parameters and the search cost down to few epochs."
            },
            "venue": {
                "fragments": [],
                "text": "ECML/PKDD"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119045079"
                        ],
                        "name": "Kuan Wang",
                        "slug": "Kuan-Wang",
                        "structuredName": {
                            "firstName": "Kuan",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kuan Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47781592"
                        ],
                        "name": "Zhijian Liu",
                        "slug": "Zhijian-Liu",
                        "structuredName": {
                            "firstName": "Zhijian",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhijian Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49417466"
                        ],
                        "name": "Yujun Lin",
                        "slug": "Yujun-Lin",
                        "structuredName": {
                            "firstName": "Yujun",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yujun Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46698300"
                        ],
                        "name": "Ji Lin",
                        "slug": "Ji-Lin",
                        "structuredName": {
                            "firstName": "Ji",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143840275"
                        ],
                        "name": "Song Han",
                        "slug": "Song-Han",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Han"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "Advances include depth-wise separable layers [54], deploymentcentric pruning [64, 50], and quantization techniques [61]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 53746082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df71a17df5350b0dbf8e5e084ae56a65cee9aaf8",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Model quantization is a widely used technique to compress and accelerate deep neural network (DNN) inference. Emergent DNN hardware accelerators begin to support flexible bitwidth (1-8 bits) to further improve the computation efficiency, which raises a great challenge to find the optimal bitwidth for each layer: it requires domain experts to explore the vast design space trading off among accuracy, latency, energy, and model size, which is both time-consuming and sub-optimal. Conventional quantization algorithm ignores the different hardware architectures and quantizes all the layers in a uniform way. In this paper, we introduce the Hardware-Aware Automated Quantization (HAQ) framework which leverages the reinforcement learning to automatically determine the quantization policy, and we take the hardware accelerator's feedback in the design loop. Rather than relying on proxy signals such as FLOPs and model size, we employ a hardware simulator to generate direct feedback signals to the RL agent. Compared with conventional methods, our framework is fully automated and can specialize the quantization policy for different neural network architectures and hardware architectures. Our framework effectively reduced the latency by 1.4-1.95x and the energy consumption by 1.9x with negligible loss of accuracy compared with the fixed bitwidth (8 bits) quantization. Our framework reveals that the optimal policies on different hardware architectures (i.e., edge and cloud architectures) under different resource constraints (i.e., latency, energy and model size) are drastically different. We interpreted the implication of different quantization policies, which offer insights for both neural network architecture design and hardware architecture design."
            },
            "slug": "HAQ:-Hardware-Aware-Automated-Quantization-Wang-Liu",
            "title": {
                "fragments": [],
                "text": "HAQ: Hardware-Aware Automated Quantization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper introduces the Hardware-Aware Automated Quantization (HAQ) framework, which leverages the reinforcement learning to automatically determine the quantization policy, and takes the hardware accelerator's feedback in the design loop to reduce the latency and energy consumption."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144126997"
                        ],
                        "name": "Chirag Gupta",
                        "slug": "Chirag-Gupta",
                        "structuredName": {
                            "firstName": "Chirag",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chirag Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2170091"
                        ],
                        "name": "Arun Sai Suggala",
                        "slug": "Arun-Sai-Suggala",
                        "structuredName": {
                            "firstName": "Arun",
                            "lastName": "Suggala",
                            "middleNames": [
                                "Sai"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arun Sai Suggala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47989752"
                        ],
                        "name": "Ankit Goyal",
                        "slug": "Ankit-Goyal",
                        "structuredName": {
                            "firstName": "Ankit",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ankit Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2492972"
                        ],
                        "name": "H. Simhadri",
                        "slug": "H.-Simhadri",
                        "structuredName": {
                            "firstName": "Harsha",
                            "lastName": "Simhadri",
                            "middleNames": [
                                "Vardhan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Simhadri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8005713"
                        ],
                        "name": "Bhargavi Paranjape",
                        "slug": "Bhargavi-Paranjape",
                        "structuredName": {
                            "firstName": "Bhargavi",
                            "lastName": "Paranjape",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bhargavi Paranjape"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118070496"
                        ],
                        "name": "A. Kumar",
                        "slug": "A.-Kumar",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Kumar",
                            "middleNames": [
                                "Jith",
                                "Sreejith"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Kumar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2601821"
                        ],
                        "name": "Saurabh Goyal",
                        "slug": "Saurabh-Goyal",
                        "structuredName": {
                            "firstName": "Saurabh",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saurabh Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360916"
                        ],
                        "name": "Raghavendra Udupa",
                        "slug": "Raghavendra-Udupa",
                        "structuredName": {
                            "firstName": "Raghavendra",
                            "lastName": "Udupa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Raghavendra Udupa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145859952"
                        ],
                        "name": "M. Varma",
                        "slug": "M.-Varma",
                        "structuredName": {
                            "firstName": "Manik",
                            "lastName": "Varma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Varma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48964143"
                        ],
                        "name": "Prateek Jain",
                        "slug": "Prateek-Jain",
                        "structuredName": {
                            "firstName": "Prateek",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Prateek Jain"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 197
                            }
                        ],
                        "text": "The reliance of (4)-(6) on the l0 norm is motivated by our use of pruning to minimize the number of non-zeros in both \u03c9 and {xl} L l=1, which is also the compression mechanism used in related work [41, 32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 165
                            }
                        ],
                        "text": "The severe memory constraints for inference on MCUs have pushed research away from CNNs and toward simpler classifiers based on decision trees and nearest neighbors [41, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 160,
                                "start": 152
                            }
                        ],
                        "text": "To the best of our knowledge, there are currently no CNN architectures or training procedures that produce CNNs satisfying these MCU memory constraints [41, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[32] by targeting the same MCUs, but using NAS to find CNNs which are at least as small and more accurate."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 88,
                                "start": 80
                            }
                        ],
                        "text": "This has led many to conclude that CNNs should be abandoned on constrained MCUs [41, 32]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "We compare against previous SOTA works: Bonsai [41], ProtoNN [32], Gradient Boosted Decision Tree Ensemble Pruning [22], kNN, and radial basis function support vector machine (SVM)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[32] propose a compressed k-nearest neighbors (kNN) approach (ProtoNN), where model size is reduced by projecting data into a low-dimensional space, maintaining a subset of prototypes to classify against, and pruning parameters."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 168
                            }
                        ],
                        "text": "We use SpArSe to uncover SOTA models on four datasets, in terms of accuracy and model size, outperforming both pruning of popular architectures and MCU-specific models [41, 32]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[32] and at the same time satisfy C1-C2, even for devices with just 2 KB of RAM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1285363,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e703cba8751757e008a73f7f1d2c727971576ec3",
            "isKey": true,
            "numCitedBy": 116,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Several real-world applications require real-time prediction on resource-scarce devices such as an Internet of Things (IoT) sensor. Such applications demand prediction models with small storage and computational complexity that do not compromise significantly on accuracy. In this work, we propose ProtoNN, a novel algorithm that addresses the problem of real-time and accurate prediction on resource-scarce devices. Pro-toNN is inspired by k-Nearest Neighbor (KNN) but has several orders lower storage and prediction complexity. ProtoNN models can be deployed even on devices with puny storage and computational power (e.g. an Arduino UNO with 2kB RAM) to get excellent prediction accuracy. ProtoNN derives its strength from three key ideas: a) learning a small number of prototypes to represent the entire training set, b) sparse low dimensional projection of data, c) joint discriminative learning of the projection and prototypes with explicit model size constraint. We conduct systematic empirical evaluation of ProtoNN on a variety of supervised learning tasks (binary, multi-class, multi-label classification) and show that it gives nearly state-of-the-art prediction accuracy on resource-scarce devices while consuming several orders lower storage, and using minimal working memory."
            },
            "slug": "ProtoNN:-Compressed-and-Accurate-kNN-for-Devices-Gupta-Suggala",
            "title": {
                "fragments": [],
                "text": "ProtoNN: Compressed and Accurate kNN for Resource-scarce Devices"
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39838894"
                        ],
                        "name": "Yihui He",
                        "slug": "Yihui-He",
                        "structuredName": {
                            "firstName": "Yihui",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihui He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46698300"
                        ],
                        "name": "Ji Lin",
                        "slug": "Ji-Lin",
                        "structuredName": {
                            "firstName": "Ji",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ji Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47781592"
                        ],
                        "name": "Zhijian Liu",
                        "slug": "Zhijian-Liu",
                        "structuredName": {
                            "firstName": "Zhijian",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhijian Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35446689"
                        ],
                        "name": "Hanrui Wang",
                        "slug": "Hanrui-Wang",
                        "structuredName": {
                            "firstName": "Hanrui",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanrui Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2040091191"
                        ],
                        "name": "Li-Jia Li",
                        "slug": "Li-Jia-Li",
                        "structuredName": {
                            "firstName": "Li-Jia",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Li-Jia Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143840275"
                        ],
                        "name": "Song Han",
                        "slug": "Song-Han",
                        "structuredName": {
                            "firstName": "Song",
                            "lastName": "Han",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Song Han"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 134
                            }
                        ],
                        "text": "Table 5 presents an ablation experiment on SpArSe with MNIST where we replaced the multiobjective optimizer with a product scalarizer [11, 28] and excluded pruning from the search [13]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 52048008,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1717255b6aea01fe956cef998abbc3c399b5d7cf",
            "isKey": false,
            "numCitedBy": 843,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Model compression is an effective technique to efficiently deploy neural network models on mobile devices which have limited computation resources and tight power budgets. Conventional model compression techniques rely on hand-crafted features and require domain experts to explore the large design space trading off among model size, speed, and accuracy, which is usually sub-optimal and time-consuming. In this paper, we propose AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality. We achieved state-of-the-art model compression results in a fully automated way without any human efforts. Under 4\\(\\times \\) FLOPs reduction, we achieved 2.7% better accuracy than the hand-crafted model compression method for VGG-16 on ImageNet. We applied this automated, push-the-button compression pipeline to MobileNet-V1 and achieved a speedup of 1.53\\(\\times \\) on the GPU (Titan Xp) and 1.95\\(\\times \\) on an Android phone (Google Pixel 1), with negligible loss of accuracy."
            },
            "slug": "AMC:-AutoML-for-Model-Compression-and-Acceleration-He-Lin",
            "title": {
                "fragments": [],
                "text": "AMC: AutoML for Model Compression and Acceleration on Mobile Devices"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper proposes AutoML for Model Compression (AMC) which leverages reinforcement learning to efficiently sample the design space and can improve the model compression quality and achieves state-of-the-art model compression results in a fully automated way without any human efforts."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50875121"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiangyu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2148927556"
                        ],
                        "name": "Xinyu Zhou",
                        "slug": "Xinyu-Zhou",
                        "structuredName": {
                            "firstName": "Xinyu",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xinyu Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3287035"
                        ],
                        "name": "Mengxiao Lin",
                        "slug": "Mengxiao-Lin",
                        "structuredName": {
                            "firstName": "Mengxiao",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mengxiao Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 92
                            }
                        ],
                        "text": "CNNs designed for resource constrained inference have been widely published in recent years [53, 35, 65], motivated by the goal of enabling inference on mobile phone platforms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 24982157,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9da734397acd7ff7c557960c62fb1b400b27bd89",
            "isKey": false,
            "numCitedBy": 3251,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce an extremely computation-efficient CNN architecture named ShuffleNet, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs). The new architecture utilizes two new operations, pointwise group convolution and channel shuffle, to greatly reduce computation cost while maintaining accuracy. Experiments on ImageNet classification and MS COCO object detection demonstrate the superior performance of ShuffleNet over other structures, e.g. lower top-1 error (absolute 7.8%) than recent MobileNet [12] on ImageNet classification task, under the computation budget of 40 MFLOPs. On an ARM-based mobile device, ShuffleNet achieves ~13\u00c3\u2014 actual speedup over AlexNet while maintaining comparable accuracy."
            },
            "slug": "ShuffleNet:-An-Extremely-Efficient-Convolutional-Zhang-Zhou",
            "title": {
                "fragments": [],
                "text": "ShuffleNet: An Extremely Efficient Convolutional Neural Network for Mobile Devices"
            },
            "tldr": {
                "abstractSimilarityScore": 79,
                "text": "An extremely computation-efficient CNN architecture named ShuffleNet is introduced, which is designed specially for mobile devices with very limited computing power (e.g., 10-150 MFLOPs), to greatly reduce computation cost while maintaining accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1950815"
                        ],
                        "name": "Tien-Ju Yang",
                        "slug": "Tien-Ju-Yang",
                        "structuredName": {
                            "firstName": "Tien-Ju",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tien-Ju Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144727050"
                        ],
                        "name": "Andrew G. Howard",
                        "slug": "Andrew-G.-Howard",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Howard",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew G. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Bo Chen",
                        "slug": "Bo-Chen",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115476987"
                        ],
                        "name": "Xiao Zhang",
                        "slug": "Xiao-Zhang",
                        "structuredName": {
                            "firstName": "Xiao",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2072476561"
                        ],
                        "name": "Alec Go",
                        "slug": "Alec-Go",
                        "structuredName": {
                            "firstName": "Alec",
                            "lastName": "Go",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alec Go"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1691305"
                        ],
                        "name": "V. Sze",
                        "slug": "V.-Sze",
                        "structuredName": {
                            "firstName": "Vivienne",
                            "lastName": "Sze",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Sze"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595180"
                        ],
                        "name": "Hartwig Adam",
                        "slug": "Hartwig-Adam",
                        "structuredName": {
                            "firstName": "Hartwig",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hartwig Adam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "Although mobile phones are more constrained than general-purpose CPUs and GPUs, they still have many orders of magnitude more memory capacity and compute performance than MCUs (Table 1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 77
                            }
                        ],
                        "text": "Advances include depth-wise separable layers [54], deploymentcentric pruning [64, 50], and quantization techniques [61]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 130
                            }
                        ],
                        "text": "In contrast to this work, the majority of preceding research on compute/memory efficient CNN inference has targeted CPUs and GPUs [33, 20, 63, 64, 50, 58, 53]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Experiments were run on four NVIDIA RTX 2080 GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "The broad proliferation of MCUs relative to desktop GPUs and CPUs stems from the fact that they are orders of magnitude cheaper (\u223c 600\u00d7) and less power hungry (\u223c 250, 000\u00d7)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "The best public market estimates suggest that around 50 billion MCU chips will ship in 2019 [2], which far eclipses other computer chips like graphics processing units (GPUs), whose shipments totalled roughly 100 million units in 2018 [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4746618,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d16b21f3e99171c86365679435f9f03766750639",
            "isKey": true,
            "numCitedBy": 324,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "This work proposes an algorithm, called NetAdapt, that automatically adapts a pre-trained deep neural network to a mobile platform given a resource budget. While many existing algorithms simplify networks based on the number of MACs or weights, optimizing those indirect metrics may not necessarily reduce the direct metrics, such as latency and energy consumption. To solve this problem, NetAdapt incorporates direct metrics into its adaptation algorithm. These direct metrics are evaluated using empirical measurements, so that detailed knowledge of the platform and toolchain is not required. NetAdapt automatically and progressively simplifies a pre-trained network until the resource budget is met while maximizing the accuracy. Experiment results show that NetAdapt achieves better accuracy versus latency trade-offs on both mobile CPU and mobile GPU, compared with the state-of-the-art automated network simplification algorithms. For image classification on the ImageNet dataset, NetAdapt achieves up to a 1.7$\\times$ speedup in measured inference latency with equal or higher accuracy on MobileNets (V1&V2)."
            },
            "slug": "NetAdapt:-Platform-Aware-Neural-Network-Adaptation-Yang-Howard",
            "title": {
                "fragments": [],
                "text": "NetAdapt: Platform-Aware Neural Network Adaptation for Mobile Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "An algorithm that automatically adapts a pre-trained deep neural network to a mobile platform given a resource budget while maximizing the accuracy, and achieves better accuracy versus latency trade-offs on both mobile CPU and mobile GPU, compared with the state-of-the-art automated network simplification algorithms."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388574431"
                        ],
                        "name": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato",
                        "slug": "Jos\u00e9-Miguel-Hern\u00e1ndez-Lobato",
                        "structuredName": {
                            "firstName": "Jos\u00e9 Miguel",
                            "lastName": "Hern\u00e1ndez-Lobato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jos\u00e9 Miguel Hern\u00e1ndez-Lobato"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 184,
                                "start": 176
                            }
                        ],
                        "text": "CNNs designed for resource constrained inference have been widely published in recent years [49, 30, 63], motivated by the goal of enabling inference on mobile phone platforms [60, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 21734422,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fe8fa446b71f7eeca2e6d658158be115bcecc4e3",
            "isKey": false,
            "numCitedBy": 18,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Software-based implementations of deep neural network predictions consume large amounts of energy, limiting their deployment in power-constrained environments. Hardware acceleration is a promising alternative. However, it is challenging to efficiently design accelerators that have both low prediction error and low energy consumption. Bayesian optimization can be used to accelerate the design problem. However, most of the existing techniques collect data in a coupled way by always evaluating the two objectives (energy and error) jointly at the same input, which is inefficient. Instead, in this work we consider a decoupled approach in which, at each iteration, we choose which objective to evaluate next and at which input. We show that considering decoupled evaluations produces better solutions when computational resources are limited. Our results also indicate that evaluating the prediction error is more important than evaluating the energy consumption."
            },
            "slug": "Designing-Neural-Network-Hardware-Accelerators-with-Hern\u00e1ndez-Lobato",
            "title": {
                "fragments": [],
                "text": "Designing Neural Network Hardware Accelerators with Decoupled Objective Evaluations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work considers a decoupled approach in which, at each iteration, the author chooses which objective to evaluate next and at which input, and shows that considering decoupling evaluations produces better solutions when computational resources are limited."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066558041"
                        ],
                        "name": "Trevor Gale",
                        "slug": "Trevor-Gale",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Gale",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Gale"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152585800"
                        ],
                        "name": "Erich Elsen",
                        "slug": "Erich-Elsen",
                        "structuredName": {
                            "firstName": "Erich",
                            "lastName": "Elsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Erich Elsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50237813"
                        ],
                        "name": "Sara Hooker",
                        "slug": "Sara-Hooker",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Hooker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sara Hooker"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 208
                            }
                        ],
                        "text": "The question then becomes, should one look for Ep directly or begin with a large edge-set E and prune it? There is conflicting evidence whether the same validation accuracy can be achieved by both approaches [27, 28, 46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 67855585,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "26384278cf5d575fc32cb92c303fb648fa0d5217",
            "isKey": false,
            "numCitedBy": 362,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "We rigorously evaluate three state-of-the-art techniques for inducing sparsity in deep neural networks on two large-scale learning tasks: Transformer trained on WMT 2014 English-to-German, and ResNet-50 trained on ImageNet. Across thousands of experiments, we demonstrate that complex techniques (Molchanov et al., 2017; Louizos et al., 2017b) shown to yield high compression rates on smaller datasets perform inconsistently, and that simple magnitude pruning approaches achieve comparable or better results. Additionally, we replicate the experiments performed by (Frankle & Carbin, 2018) and (Liu et al., 2018) at scale and show that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization. Together, these results highlight the need for large-scale benchmarks in the field of model compression. We open-source our code, top performing model checkpoints, and results of all hyperparameter configurations to establish rigorous baselines for future work on compression and sparsification."
            },
            "slug": "The-State-of-Sparsity-in-Deep-Neural-Networks-Gale-Elsen",
            "title": {
                "fragments": [],
                "text": "The State of Sparsity in Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that unstructured sparse architectures learned through pruning cannot be trained from scratch to the same test set performance as a model trained with joint sparsification and optimization, and the need for large-scale benchmarks in the field of model compression is highlighted."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 141
                            }
                        ],
                        "text": "In the context of supervised visual tasks, state-of-the-art (SOTA) ML models typically take the form of convolutional neural networks (CNNs) [40]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 195908774,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abd1c342495432171beb7ca8fd9551ef13cbd0ff",
            "isKey": false,
            "numCitedBy": 82046,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "We trained a large, deep convolutional neural network to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes. On the test data, we achieved top-1 and top-5 error rates of 37.5% and 17.0%, respectively, which is considerably better than the previous state-of-the-art. The neural network, which has 60 million parameters and 650,000 neurons, consists of five convolutional layers, some of which are followed by max-pooling layers, and three fully connected layers with a final 1000-way softmax. To make training faster, we used non-saturating neurons and a very efficient GPU implementation of the convolution operation. To reduce overfitting in the fully connected layers we employed a recently developed regularization method called \"dropout\" that proved to be very effective. We also entered a variant of this model in the ILSVRC-2012 competition and achieved a winning top-5 test error rate of 15.3%, compared to 26.2% achieved by the second-best entry."
            },
            "slug": "ImageNet-classification-with-deep-convolutional-Krizhevsky-Sutskever",
            "title": {
                "fragments": [],
                "text": "ImageNet classification with deep convolutional neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A large, deep convolutional neural network was trained to classify the 1.2 million high-resolution images in the ImageNet LSVRC-2010 contest into the 1000 different classes and employed a recently developed regularization method called \"dropout\" that proved to be very effective."
            },
            "venue": {
                "fragments": [],
                "text": "Commun. ACM"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368067"
                        ],
                        "name": "Barret Zoph",
                        "slug": "Barret-Zoph",
                        "structuredName": {
                            "firstName": "Barret",
                            "lastName": "Zoph",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barret Zoph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053781980"
                        ],
                        "name": "Vijay Vasudevan",
                        "slug": "Vijay-Vasudevan",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Vasudevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vijay Vasudevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1397917613"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc V.",
                            "lastName": "Le",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12227989,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d0611891b9e8a7c5731146097b6f201578f47b2f",
            "isKey": false,
            "numCitedBy": 3538,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Developing neural network image classification models often requires significant architecture engineering. In this paper, we study a method to learn the model architectures directly on the dataset of interest. As this approach is expensive when the dataset is large, we propose to search for an architectural building block on a small dataset and then transfer the block to a larger dataset. The key contribution of this work is the design of a new search space (which we call the \"NASNet search space\") which enables transferability. In our experiments, we search for the best convolutional layer (or \"cell\") on the CIFAR-10 dataset and then apply this cell to the ImageNet dataset by stacking together more copies of this cell, each with their own parameters to design a convolutional architecture, which we name a \"NASNet architecture\". We also introduce a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models. On CIFAR-10 itself, a NASNet found by our method achieves 2.4% error rate, which is state-of-the-art. Although the cell is not searched for directly on ImageNet, a NASNet constructed from the best cell achieves, among the published works, state-of-the-art accuracy of 82.7% top-1 and 96.2% top-5 on ImageNet. Our model is 1.2% better in top-1 accuracy than the best human-invented architectures while having 9 billion fewer FLOPS - a reduction of 28% in computational demand from the previous state-of-the-art model. When evaluated at different levels of computational cost, accuracies of NASNets exceed those of the state-of-the-art human-designed models. For instance, a small version of NASNet also achieves 74% top-1 accuracy, which is 3.1% better than equivalently-sized, state-of-the-art models for mobile platforms. Finally, the image features learned from image classification are generically useful and can be transferred to other computer vision problems. On the task of object detection, the learned features by NASNet used with the Faster-RCNN framework surpass state-of-the-art by 4.0% achieving 43.1% mAP on the COCO dataset."
            },
            "slug": "Learning-Transferable-Architectures-for-Scalable-Zoph-Vasudevan",
            "title": {
                "fragments": [],
                "text": "Learning Transferable Architectures for Scalable Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper proposes to search for an architectural building block on a small dataset and then transfer the block to a larger dataset and introduces a new regularization technique called ScheduledDropPath that significantly improves generalization in the NASNet models."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70296695"
                        ],
                        "name": "Urmish Thakker",
                        "slug": "Urmish-Thakker",
                        "structuredName": {
                            "firstName": "Urmish",
                            "lastName": "Thakker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Urmish Thakker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144097639"
                        ],
                        "name": "Igor Fedorov",
                        "slug": "Igor-Fedorov",
                        "structuredName": {
                            "firstName": "Igor",
                            "lastName": "Fedorov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Igor Fedorov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2089359"
                        ],
                        "name": "Jesse G. Beu",
                        "slug": "Jesse-G.-Beu",
                        "structuredName": {
                            "firstName": "Jesse",
                            "lastName": "Beu",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jesse G. Beu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2193705"
                        ],
                        "name": "Dibakar Gope",
                        "slug": "Dibakar-Gope",
                        "structuredName": {
                            "firstName": "Dibakar",
                            "lastName": "Gope",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dibakar Gope"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48622443"
                        ],
                        "name": "Chu Zhou",
                        "slug": "Chu-Zhou",
                        "structuredName": {
                            "firstName": "Chu",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chu Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "31613624"
                        ],
                        "name": "Ganesh S. Dasika",
                        "slug": "Ganesh-S.-Dasika",
                        "structuredName": {
                            "firstName": "Ganesh",
                            "lastName": "Dasika",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ganesh S. Dasika"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39045061"
                        ],
                        "name": "Matthew Mattina",
                        "slug": "Matthew-Mattina",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Mattina",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew Mattina"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 203836150,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "da42d411b689637cbf1be9334fd262c761d82789",
            "isKey": false,
            "numCitedBy": 10,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Neural Networks (RNN) can be difficult to deploy on resource constrained devices due to their size. As a result, there is a need for compression techniques that can significantly compress RNNs without negatively impacting task accuracy. This paper introduces a method to compress RNNs for resource constrained environments using Kronecker product (KP). KPs can compress RNN layers by 16\u201338 \u00d7 with minimal accuracy loss. We show that KP can beat the task accuracy achieved by other state-of-the-art compression techniques across 4 benchmarks spanning 3 different applications, while simultaneously improving inference run-time."
            },
            "slug": "Pushing-the-limits-of-RNN-Compression-Thakker-Fedorov",
            "title": {
                "fragments": [],
                "text": "Pushing the limits of RNN Compression"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "It is shown that KP can beat the task accuracy achieved by other state-of-the-art compression techniques across 4 benchmarks spanning 3 different applications, while simultaneously improving inference run-time."
            },
            "venue": {
                "fragments": [],
                "text": "2019 Fifth Workshop on Energy Efficient Machine Learning and Cognitive Computing - NeurIPS Edition (EMC2-NIPS)"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "4 Results We report results on a variety of datasets: MNIST (55e3, 5e3, 10e3) [43], CIFAR10 (45e3, 5e3, 10e3) [39], CUReT (3704, 500, 1408) [60], and Chars4k (3897, 500, 1886) [25], corresponding to classification problems with 10, 10, 61, and 62 classes, respectively, with the training/validation/test set sizes provided in parentheses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18268744,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5d90f06bb70a0a3dced62413346235c02b1aa086",
            "isKey": false,
            "numCitedBy": 17474,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Groups at MIT and NYU have collected a dataset of millions of tiny colour images from the web. It is, in principle, an excellent dataset for unsupervised training of deep generative models, but previous researchers who have tried this have found it dicult to learn a good set of lters from the images. We show how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex. Using a novel parallelization algorithm to distribute the work among multiple machines connected on a network, we show how training such a model can be done in reasonable time. A second problematic aspect of the tiny images dataset is that there are no reliable class labels which makes it hard to use for object recognition experiments. We created two sets of reliable labels. The CIFAR-10 set has 6000 examples of each of 10 classes and the CIFAR-100 set has 600 examples of each of 100 non-overlapping classes. Using these labels, we show that object recognition is signicantly improved by pre-training a layer of features on a large set of unlabeled tiny images."
            },
            "slug": "Learning-Multiple-Layers-of-Features-from-Tiny-Krizhevsky",
            "title": {
                "fragments": [],
                "text": "Learning Multiple Layers of Features from Tiny Images"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown how to train a multi-layer generative model that learns to extract meaningful features which resemble those found in the human visual cortex, using a novel parallelization algorithm to distribute the work among multiple machines connected on a network."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2368067"
                        ],
                        "name": "Barret Zoph",
                        "slug": "Barret-Zoph",
                        "structuredName": {
                            "firstName": "Barret",
                            "lastName": "Zoph",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Barret Zoph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 101
                            }
                        ],
                        "text": "Algorithms for identifying performant CNN architectures have received significant attention recently [66, 23, 20, 45, 31, 24, 44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 158
                            }
                        ],
                        "text": "5 Network morphism Evaluating each configuration \u03a9 from a random initialization is slow, as evidenced by early NAS works which required thousands of GPU days [66, 67]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12713052,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "67d968c7450878190e45ac7886746de867bf673d",
            "isKey": false,
            "numCitedBy": 3482,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural networks are powerful and flexible models that work well for many difficult learning tasks in image, speech and natural language understanding. Despite their success, neural networks are still hard to design. In this paper, we use a recurrent network to generate the model descriptions of neural networks and train this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set. On the CIFAR-10 dataset, our method, starting from scratch, can design a novel network architecture that rivals the best human-invented architecture in terms of test set accuracy. Our CIFAR-10 model achieves a test error rate of 3.65, which is 0.09 percent better and 1.05x faster than the previous state-of-the-art model that used a similar architectural scheme. On the Penn Treebank dataset, our model can compose a novel recurrent cell that outperforms the widely-used LSTM cell, and other state-of-the-art baselines. Our cell achieves a test set perplexity of 62.4 on the Penn Treebank, which is 3.6 perplexity better than the previous state-of-the-art model. The cell can also be transferred to the character language modeling task on PTB and achieves a state-of-the-art perplexity of 1.214."
            },
            "slug": "Neural-Architecture-Search-with-Reinforcement-Zoph-Le",
            "title": {
                "fragments": [],
                "text": "Neural Architecture Search with Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper uses a recurrent network to generate the model descriptions of neural networks and trains this RNN with reinforcement learning to maximize the expected accuracy of the generated architectures on a validation set."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144727050"
                        ],
                        "name": "Andrew G. Howard",
                        "slug": "Andrew-G.-Howard",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Howard",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew G. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2717876"
                        ],
                        "name": "Menglong Zhu",
                        "slug": "Menglong-Zhu",
                        "structuredName": {
                            "firstName": "Menglong",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Menglong Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Bo Chen",
                        "slug": "Bo-Chen",
                        "structuredName": {
                            "firstName": "Bo",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bo Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2741985"
                        ],
                        "name": "Dmitry Kalenichenko",
                        "slug": "Dmitry-Kalenichenko",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Kalenichenko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dmitry Kalenichenko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108301072"
                        ],
                        "name": "Weijun Wang",
                        "slug": "Weijun-Wang",
                        "structuredName": {
                            "firstName": "Weijun",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weijun Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47447630"
                        ],
                        "name": "Tobias Weyand",
                        "slug": "Tobias-Weyand",
                        "structuredName": {
                            "firstName": "Tobias",
                            "lastName": "Weyand",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tobias Weyand"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2612392"
                        ],
                        "name": "M. Andreetto",
                        "slug": "M.-Andreetto",
                        "structuredName": {
                            "firstName": "Marco",
                            "lastName": "Andreetto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Andreetto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2595180"
                        ],
                        "name": "Hartwig Adam",
                        "slug": "Hartwig-Adam",
                        "structuredName": {
                            "firstName": "Hartwig",
                            "lastName": "Adam",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hartwig Adam"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 92
                            }
                        ],
                        "text": "CNNs designed for resource constrained inference have been widely published in recent years [53, 35, 65], motivated by the goal of enabling inference on mobile phone platforms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12670695,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3647d6d0f151dc05626449ee09cc7bce55be497e",
            "isKey": false,
            "numCitedBy": 10323,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a class of efficient models called MobileNets for mobile and embedded vision applications. MobileNets are based on a streamlined architecture that uses depth-wise separable convolutions to build light weight deep neural networks. We introduce two simple global hyper-parameters that efficiently trade off between latency and accuracy. These hyper-parameters allow the model builder to choose the right sized model for their application based on the constraints of the problem. We present extensive experiments on resource and accuracy tradeoffs and show strong performance compared to other popular models on ImageNet classification. We then demonstrate the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization."
            },
            "slug": "MobileNets:-Efficient-Convolutional-Neural-Networks-Howard-Zhu",
            "title": {
                "fragments": [],
                "text": "MobileNets: Efficient Convolutional Neural Networks for Mobile Vision Applications"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work introduces two simple global hyper-parameters that efficiently trade off between latency and accuracy and demonstrates the effectiveness of MobileNets across a wide range of applications and use cases including object detection, finegrain classification, face attributes and large scale geo-localization."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39838894"
                        ],
                        "name": "Yihui He",
                        "slug": "Yihui-He",
                        "structuredName": {
                            "firstName": "Yihui",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yihui He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50875121"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiangyu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "We consider both unstructured and structured, or channel [34], pruning, where the difference is that the latter prunes away entire groups of weights corresponding to output feature maps for convolution layers and input neurons for FC layers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 20157893,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ee53c9480132fc0d09b1192226cb2c460462fd6d",
            "isKey": false,
            "numCitedBy": 1507,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we introduce a new channel pruning method to accelerate very deep convolutional neural networks. Given a trained CNN model, we propose an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction. We further generalize this algorithm to multi-layer and multi-branch cases. Our method reduces the accumulated error and enhance the compatibility with various architectures. Our pruned VGG-16 achieves the state-of-the-art results by 5\u00d7 speed-up along with only 0.3% increase of error. More importantly, our method is able to accelerate modern networks like ResNet, Xception and suffers only 1.4%, 1.0% accuracy loss under 2\u00d7 speedup respectively, which is significant."
            },
            "slug": "Channel-Pruning-for-Accelerating-Very-Deep-Neural-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Channel Pruning for Accelerating Very Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes an iterative two-step algorithm to effectively prune each layer, by a LASSO regression based channel selection and least square reconstruction, and generalizes this algorithm to multi-layer and multi-branch cases."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3450466"
                        ],
                        "name": "Zichao Guo",
                        "slug": "Zichao-Guo",
                        "structuredName": {
                            "firstName": "Zichao",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zichao Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50875121"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "Xiangyu",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047503068"
                        ],
                        "name": "Haoyuan Mu",
                        "slug": "Haoyuan-Mu",
                        "structuredName": {
                            "firstName": "Haoyuan",
                            "lastName": "Mu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haoyuan Mu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145577184"
                        ],
                        "name": "Wen Heng",
                        "slug": "Wen-Heng",
                        "structuredName": {
                            "firstName": "Wen",
                            "lastName": "Heng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wen Heng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109370860"
                        ],
                        "name": "Zechun Liu",
                        "slug": "Zechun-Liu",
                        "structuredName": {
                            "firstName": "Zechun",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zechun Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1732264"
                        ],
                        "name": "Yichen Wei",
                        "slug": "Yichen-Wei",
                        "structuredName": {
                            "firstName": "Yichen",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichen Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 101
                            }
                        ],
                        "text": "Algorithms for identifying performant CNN architectures have received significant attention recently [66, 23, 20, 45, 31, 24, 44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 90262841,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "79e523beb1e1411a241edde0464b07c2ebc231d1",
            "isKey": false,
            "numCitedBy": 464,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "We revisit the one-shot Neural Architecture Search (NAS) paradigm and analyze its advantages over existing NAS approaches. Existing one-shot method, however, is hard to train and not yet effective on large scale datasets like ImageNet. This work propose a Single Path One-Shot model to address the challenge in the training. Our central idea is to construct a simplified supernet, where all architectures are single paths so that weight co-adaption problem is alleviated. Training is performed by uniform path sampling. All architectures (and their weights) are trained fully and equally. \nComprehensive experiments verify that our approach is flexible and effective. It is easy to train and fast to search. It effortlessly supports complex search spaces (e.g., building blocks, channel, mixed-precision quantization) and different search constraints (e.g., FLOPs, latency). It is thus convenient to use for various needs. It achieves start-of-the-art performance on the large dataset ImageNet."
            },
            "slug": "Single-Path-One-Shot-Neural-Architecture-Search-Guo-Zhang",
            "title": {
                "fragments": [],
                "text": "Single Path One-Shot Neural Architecture Search with Uniform Sampling"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A Single Path One-Shot model is proposed to construct a simplified supernet, where all architectures are single paths so that weight co-adaption problem is alleviated."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2824500"
                        ],
                        "name": "Pavlo Molchanov",
                        "slug": "Pavlo-Molchanov",
                        "structuredName": {
                            "firstName": "Pavlo",
                            "lastName": "Molchanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Pavlo Molchanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2342481"
                        ],
                        "name": "Stephen Tyree",
                        "slug": "Stephen-Tyree",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "Tyree",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen Tyree"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2976930"
                        ],
                        "name": "Tero Karras",
                        "slug": "Tero-Karras",
                        "structuredName": {
                            "firstName": "Tero",
                            "lastName": "Karras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tero Karras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1761103"
                        ],
                        "name": "Timo Aila",
                        "slug": "Timo-Aila",
                        "structuredName": {
                            "firstName": "Timo",
                            "lastName": "Aila",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Timo Aila"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690538"
                        ],
                        "name": "J. Kautz",
                        "slug": "J.-Kautz",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Kautz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Kautz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "Although mobile phones are more constrained than general-purpose CPUs and GPUs, they still have many orders of magnitude more memory capacity and compute performance than MCUs (Table 1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 77
                            }
                        ],
                        "text": "Advances include depth-wise separable layers [54], deploymentcentric pruning [64, 50], and quantization techniques [61]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 130
                            }
                        ],
                        "text": "In contrast to this work, the majority of preceding research on compute/memory efficient CNN inference has targeted CPUs and GPUs [33, 20, 63, 64, 50, 58, 53]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Experiments were run on four NVIDIA RTX 2080 GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "The broad proliferation of MCUs relative to desktop GPUs and CPUs stems from the fact that they are orders of magnitude cheaper (\u223c 600\u00d7) and less power hungry (\u223c 250, 000\u00d7)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "The best public market estimates suggest that around 50 billion MCU chips will ship in 2019 [2], which far eclipses other computer chips like graphics processing units (GPUs), whose shipments totalled roughly 100 million units in 2018 [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 16167970,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "026ecf916023e13191331a354271b7f9b86e50a1",
            "isKey": true,
            "numCitedBy": 347,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new framework for pruning convolutional kernels in neural networks to enable efficient inference, focusing on transfer learning where large and potentially unwieldy pretrained networks are adapted to specialized tasks. We interleave greedy criteria-based pruning with fine-tuning by backpropagation\u2014a computationally efficient procedure that maintains good generalization in the pruned network. We propose a new criterion based on an efficient first-order Taylor expansion to approximate the absolute change in training cost induced by pruning a network component. After normalization, the proposed criterion scales appropriately across all layers of a deep CNN, eliminating the need for per-layer sensitivity analysis. The proposed criterion demonstrates superior performance compared to other criteria, such as the norm of kernel weights or average feature map activation."
            },
            "slug": "Pruning-Convolutional-Neural-Networks-for-Resource-Molchanov-Tyree",
            "title": {
                "fragments": [],
                "text": "Pruning Convolutional Neural Networks for Resource Efficient Transfer Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A new criterion based on an efficient first-order Taylor expansion to approximate the absolute change in training cost induced by pruning a network component is proposed, demonstrating superior performance compared to other criteria, such as the norm of kernel weights or average feature map activation."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109168016"
                        ],
                        "name": "Zhuang Liu",
                        "slug": "Zhuang-Liu",
                        "structuredName": {
                            "firstName": "Zhuang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhuang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2984183"
                        ],
                        "name": "Mingjie Sun",
                        "slug": "Mingjie-Sun",
                        "structuredName": {
                            "firstName": "Mingjie",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mingjie Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1822702"
                        ],
                        "name": "Tinghui Zhou",
                        "slug": "Tinghui-Zhou",
                        "structuredName": {
                            "firstName": "Tinghui",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tinghui Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143983679"
                        ],
                        "name": "Gao Huang",
                        "slug": "Gao-Huang",
                        "structuredName": {
                            "firstName": "Gao",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gao Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753210"
                        ],
                        "name": "Trevor Darrell",
                        "slug": "Trevor-Darrell",
                        "structuredName": {
                            "firstName": "Trevor",
                            "lastName": "Darrell",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Trevor Darrell"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 208
                            }
                        ],
                        "text": "The question then becomes, should one look for Ep directly or begin with a large edge-set E and prune it? There is conflicting evidence whether the same validation accuracy can be achieved by both approaches [27, 28, 46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52978527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cdb25e4df6913bb94edcd1174d00baf2d21c9a6d",
            "isKey": false,
            "numCitedBy": 783,
            "numCiting": 83,
            "paperAbstract": {
                "fragments": [],
                "text": "Network pruning is widely used for reducing the heavy inference cost of deep models in low-resource settings. A typical pruning algorithm is a three-stage pipeline, i.e., training (a large model), pruning and fine-tuning. During pruning, according to a certain criterion, redundant weights are pruned and important weights are kept to best preserve the accuracy. In this work, we make several surprising observations which contradict common beliefs. For all state-of-the-art structured pruning algorithms we examined, fine-tuning a pruned model only gives comparable or worse performance than training that model with randomly initialized weights. For pruning algorithms which assume a predefined target network architecture, one can get rid of the full pipeline and directly train the target network from scratch. Our observations are consistent for multiple network architectures, datasets, and tasks, which imply that: 1) training a large, over-parameterized model is often not necessary to obtain an efficient final model, 2) learned \"important\" weights of the large model are typically not useful for the small pruned model, 3) the pruned architecture itself, rather than a set of inherited \"important\" weights, is more crucial to the efficiency in the final model, which suggests that in some cases pruning can be useful as an architecture search paradigm. Our results suggest the need for more careful baseline evaluations in future research on structured pruning methods. We also compare with the \"Lottery Ticket Hypothesis\" (Frankle & Carbin 2019), and find that with optimal learning rate, the \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization."
            },
            "slug": "Rethinking-the-Value-of-Network-Pruning-Liu-Sun",
            "title": {
                "fragments": [],
                "text": "Rethinking the Value of Network Pruning"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is found that with optimal learning rate, the \"winning ticket\" initialization as used in Frankle & Carbin (2019) does not bring improvement over random initialization, and the need for more careful baseline evaluations in future research on structured pruning methods is suggested."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2154062"
                        ],
                        "name": "S. Falkner",
                        "slug": "S.-Falkner",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Falkner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Falkner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145227684"
                        ],
                        "name": "Aaron Klein",
                        "slug": "Aaron-Klein",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Klein",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron Klein"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 74
                            }
                        ],
                        "text": "We adopt the combination of model-based and entirely random sampling from [26] to increase search space coverage."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 49571505,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "93436a26d744e0417e21df10abdfce2cc74b1e58",
            "isKey": false,
            "numCitedBy": 571,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern deep learning methods are very sensitive to many hyperparameters, and, due to the long training times of state-of-the-art models, vanilla Bayesian hyperparameter optimization is typically computationally infeasible. On the other hand, bandit-based configuration evaluation approaches based on random search lack guidance and do not converge to the best configurations as quickly. Here, we propose to combine the benefits of both Bayesian optimization and bandit-based methods, in order to achieve the best of both worlds: strong anytime performance and fast convergence to optimal configurations. We propose a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian neural networks, deep reinforcement learning, and convolutional neural networks. Our method is robust and versatile, while at the same time being conceptually simple and easy to implement."
            },
            "slug": "BOHB:-Robust-and-Efficient-Hyperparameter-at-Scale-Falkner-Klein",
            "title": {
                "fragments": [],
                "text": "BOHB: Robust and Efficient Hyperparameter Optimization at Scale"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a new practical state-of-the-art hyperparameter optimization method, which consistently outperforms both Bayesian optimization and Hyperband on a wide range of problem types, including high-dimensional toy functions, support vector machines, feed-forward neural networks, Bayesian Neural networks, deep reinforcement learning, and convolutional neural networks."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1400347470"
                        ],
                        "name": "M. A. Carreira-Perpi\u00f1\u00e1n",
                        "slug": "M.-A.-Carreira-Perpi\u00f1\u00e1n",
                        "structuredName": {
                            "firstName": "Miguel",
                            "lastName": "Carreira-Perpi\u00f1\u00e1n",
                            "middleNames": [
                                "\u00c1."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. A. Carreira-Perpi\u00f1\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "21154310"
                        ],
                        "name": "Yerlan Idelbayev",
                        "slug": "Yerlan-Idelbayev",
                        "structuredName": {
                            "firstName": "Yerlan",
                            "lastName": "Idelbayev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yerlan Idelbayev"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning can be expressed as\n\u03c9\u0304 = argmin ( \u2211\nG\u2208G 1[\u2016\u03c9G\u20162>0])\u2264s\nL ({\u03b1, \u03d1, \u03c9}) (8)\nwhere L (\u00b7) denotes the loss function for the appropriate task, e.g. cross-entropy for classification, G denotes the set of disjoint groups covering the indices of each entry in \u03c9, \u03c9G denotes a particular group of weights, and 1 [\u00b7] denotes the indicator function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning can be considered a form of NAS, where \u03c9\u0304 represents a sub-network of {\u03b1, \u03d1, \u03c9} given by {{V,Ep} , \u03d1, \u03c9}, and Ep \u2286 E contains only the edges for which \u03c9\u0304 is non-zero [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning is a procedure for zeroing out network parameters \u03c9 and can be seen as a way to generate a new set of parameters \u03c9\u0304 that have lower \u2016\u03c9\u0304\u20160."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 33
                            }
                        ],
                        "text": "3 Neural network pruning Pruning [42, 49, 21, 47] is essential to MCU deployment using SpArSe, as it heavily reduces the model size (4) and working memory (5)/(6) without significantly impacting classification accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 107
                            }
                        ],
                        "text": "We compare against previous SOTA works: Bonsai [41], ProtoNN [32], Gradient Boosted Decision Tree Ensemble Pruning [22], kNN, and radial basis function support vector machine (SVM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning enables SpArSe to quickly evaluate many sub-networks of a given network, thereby expanding the scope of the overall search."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning [42, 49, 21, 47] is essential to MCU deployment using SpArSe, as it heavily reduces the model size (4) and working memory (5)/(6) without significantly impacting classification accuracy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 52834711,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d719009bade1c245ac6e2fa9e4cd74eddd4f34b4",
            "isKey": true,
            "numCitedBy": 133,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Pruning a neural net consists of removing weights without degrading its performance. This is an old problem of renewed interest because of the need to compress ever larger nets so they can run in mobile devices. Pruning has been traditionally done by ranking or penalizing weights according to some criterion (such as magnitude), removing low-ranked weights and retraining the remaining ones. We formulate pruning as an optimization problem of finding the weights that minimize the loss while satisfying a pruning cost condition. We give a generic algorithm to solve this which alternates \"learning\" steps that optimize a regularized, data-dependent loss and \"compression\" steps that mark weights for pruning in a data-independent way. Magnitude thresholding arises naturally in the compression step, but unlike existing magnitude pruning approaches, our algorithm explores subsets of weights rather than committing irrevocably to a specific subset from the beginning. It is also able to learn automatically the best number of weights to prune in each layer of the net without incurring an exponentially costly model selection. Using a single pruning-level user parameter, we achieve state-of-the-art pruning in LeNet and ResNets of various sizes."
            },
            "slug": "\"Learning-Compression\"-Algorithms-for-Neural-Net-Carreira-Perpi\u00f1\u00e1n-Idelbayev",
            "title": {
                "fragments": [],
                "text": "\"Learning-Compression\" Algorithms for Neural Net Pruning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work forms pruning as an optimization problem of finding the weights that minimize the loss while satisfying a pruning cost condition, and gives a generic algorithm to solve this which alternates \"learning\" steps that optimize a regularized, data-dependent loss and \"compression\" Steps that mark weights for pruning in a data-independent way."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2391802"
                        ],
                        "name": "Hanxiao Liu",
                        "slug": "Hanxiao-Liu",
                        "structuredName": {
                            "firstName": "Hanxiao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanxiao Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34838386"
                        ],
                        "name": "K. Simonyan",
                        "slug": "K.-Simonyan",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Simonyan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Simonyan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35729970"
                        ],
                        "name": "Yiming Yang",
                        "slug": "Yiming-Yang",
                        "structuredName": {
                            "firstName": "Yiming",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yiming Yang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 101
                            }
                        ],
                        "text": "Algorithms for identifying performant CNN architectures have received significant attention recently [66, 23, 20, 45, 31, 24, 44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 49411844,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1f457e31b611da727f9aef76c283a18157dfa83",
            "isKey": false,
            "numCitedBy": 2237,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper addresses the scalability challenge of architecture search by formulating the task in a differentiable manner. Unlike conventional approaches of applying evolution or reinforcement learning over a discrete and non-differentiable search space, our method is based on the continuous relaxation of the architecture representation, allowing efficient search of the architecture using gradient descent. Extensive experiments on CIFAR-10, ImageNet, Penn Treebank and WikiText-2 show that our algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques. Our implementation has been made publicly available to facilitate further research on efficient architecture search algorithms."
            },
            "slug": "DARTS:-Differentiable-Architecture-Search-Liu-Simonyan",
            "title": {
                "fragments": [],
                "text": "DARTS: Differentiable Architecture Search"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The proposed algorithm excels in discovering high-performance convolutional architectures for image classification and recurrent architectures for language modeling, while being orders of magnitude faster than state-of-the-art non-differentiable techniques."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2501244"
                        ],
                        "name": "T. Elsken",
                        "slug": "T.-Elsken",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Elsken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Elsken"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2708564"
                        ],
                        "name": "J. H. Metzen",
                        "slug": "J.-H.-Metzen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Metzen",
                            "middleNames": [
                                "Hendrik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Metzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 101
                            }
                        ],
                        "text": "Algorithms for identifying performant CNN architectures have received significant attention recently [66, 23, 20, 45, 31, 24, 44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 52016139,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "114a32bc872f160b58f503aca13f887556b5006e",
            "isKey": false,
            "numCitedBy": 1283,
            "numCiting": 116,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy."
            },
            "slug": "Neural-Architecture-Search:-A-Survey-Elsken-Metzen",
            "title": {
                "fragments": [],
                "text": "Neural Architecture Search: A Survey"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An overview of existing work in this field of research is provided and neural architecture search methods are categorized according to three dimensions: search space, search strategy, and performance estimation strategy."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144882893"
                        ],
                        "name": "M. Sandler",
                        "slug": "M.-Sandler",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Sandler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Sandler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144727050"
                        ],
                        "name": "Andrew G. Howard",
                        "slug": "Andrew-G.-Howard",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Howard",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew G. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2717876"
                        ],
                        "name": "Menglong Zhu",
                        "slug": "Menglong-Zhu",
                        "structuredName": {
                            "firstName": "Menglong",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Menglong Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422677"
                        ],
                        "name": "A. Zhmoginov",
                        "slug": "A.-Zhmoginov",
                        "structuredName": {
                            "firstName": "Andrey",
                            "lastName": "Zhmoginov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Zhmoginov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34192119"
                        ],
                        "name": "Liang-Chieh Chen",
                        "slug": "Liang-Chieh-Chen",
                        "structuredName": {
                            "firstName": "Liang-Chieh",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liang-Chieh Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 104,
                                "start": 92
                            }
                        ],
                        "text": "CNNs designed for resource constrained inference have been widely published in recent years [53, 35, 65], motivated by the goal of enabling inference on mobile phone platforms."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 65
                            }
                        ],
                        "text": "Although mobile phones are more constrained than general-purpose CPUs and GPUs, they still have many orders of magnitude more memory capacity and compute performance than MCUs (Table 1)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 130
                            }
                        ],
                        "text": "In contrast to this work, the majority of preceding research on compute/memory efficient CNN inference has targeted CPUs and GPUs [33, 20, 63, 64, 50, 58, 53]."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Experiments were run on four NVIDIA RTX 2080 GPUs."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 117
                            }
                        ],
                        "text": "[56], the authors optimize the kernel size and number of feature maps of the MBConv layers in a MobileNetV2 backbone [53] by expressing each of the layer choices as a pruned version of a superkernel."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 56,
                                "start": 52
                            }
                        ],
                        "text": "The broad proliferation of MCUs relative to desktop GPUs and CPUs stems from the fact that they are orders of magnitude cheaper (\u223c 600\u00d7) and less power hungry (\u223c 250, 000\u00d7)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 169
                            }
                        ],
                        "text": "The best public market estimates suggest that around 50 billion MCU chips will ship in 2019 [2], which far eclipses other computer chips like graphics processing units (GPUs), whose shipments totalled roughly 100 million units in 2018 [4]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4555207,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dd9cfe7124c734f5a6fc90227d541d3dbcd72ba4",
            "isKey": true,
            "numCitedBy": 7406,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we describe a new mobile architecture, MobileNetV2, that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes. We also describe efficient ways of applying these mobile models to object detection in a novel framework we call SSDLite. Additionally, we demonstrate how to build mobile semantic segmentation models through a reduced form of DeepLabv3 which we call Mobile DeepLabv3. is based on an inverted residual structure where the shortcut connections are between the thin bottleneck layers. The intermediate expansion layer uses lightweight depthwise convolutions to filter features as a source of non-linearity. Additionally, we find that it is important to remove non-linearities in the narrow layers in order to maintain representational power. We demonstrate that this improves performance and provide an intuition that led to this design. Finally, our approach allows decoupling of the input/output domains from the expressiveness of the transformation, which provides a convenient framework for further analysis. We measure our performance on ImageNet [1] classification, COCO object detection [2], VOC image segmentation [3]. We evaluate the trade-offs between accuracy, and number of operations measured by multiply-adds (MAdd), as well as actual latency, and the number of parameters."
            },
            "slug": "MobileNetV2:-Inverted-Residuals-and-Linear-Sandler-Howard",
            "title": {
                "fragments": [],
                "text": "MobileNetV2: Inverted Residuals and Linear Bottlenecks"
            },
            "tldr": {
                "abstractSimilarityScore": 75,
                "text": "A new mobile architecture, MobileNetV2, is described that improves the state of the art performance of mobile models on multiple tasks and benchmarks as well as across a spectrum of different model sizes and allows decoupling of the input/output domains from the expressiveness of the transformation."
            },
            "venue": {
                "fragments": [],
                "text": "2018 IEEE/CVF Conference on Computer Vision and Pattern Recognition"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735300"
                        ],
                        "name": "S. Haykin",
                        "slug": "S.-Haykin",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Haykin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Haykin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713858"
                        ],
                        "name": "B. Kosko",
                        "slug": "B.-Kosko",
                        "structuredName": {
                            "firstName": "Bart",
                            "lastName": "Kosko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Kosko"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 193,
                                "start": 189
                            }
                        ],
                        "text": "To illustrate the challenge of deploying CNNs on MCUs, consider the seemingly simple task of deploying the well-known LeNet CNN on an Arduino Uno [1] to perform MNIST character recognition [43]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "4 Results We report results on a variety of datasets: MNIST (55e3, 5e3, 10e3) [43], CIFAR10 (45e3, 5e3, 10e3) [39], CUReT (3704, 500, 1408) [60], and Chars4k (3897, 500, 1886) [25], corresponding to classification problems with 10, 10, 61, and 62 classes, respectively, with the training/validation/test set sizes provided in parentheses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64294544,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f42b865e20e61a954239f421b42007236e671f19",
            "isKey": false,
            "numCitedBy": 3561,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer Neural Networks trained with the backpropagation algorithm constitute the best example of a successful Gradient-Based Learning technique. Given an appropriate network architecture, Gradient-Based Learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation, recognition, and language modeling. A new learning paradigm, called Graph Transformer Networks (GTN), allows such multi-module systems to be trained globally using Gradient-Based methods so as to minimize an overall performance measure. Two systems for on-line handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of Graph Transformer Networks. A Graph Transformer Network for reading bank check is also described. It uses Convolutional Neural Network character recognizers combined with global training techniques to provides record accuracy on business and personal checks. It is deployed commercially and reads several million checks per day."
            },
            "slug": "GradientBased-Learning-Applied-to-Document-Haykin-Kosko",
            "title": {
                "fragments": [],
                "text": "GradientBased Learning Applied to Document Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Various methods applied to handwritten character recognition are reviewed and compared and Convolutional Neural Networks, that are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2501244"
                        ],
                        "name": "T. Elsken",
                        "slug": "T.-Elsken",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Elsken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Elsken"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2708564"
                        ],
                        "name": "J. H. Metzen",
                        "slug": "J.-H.-Metzen",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Metzen",
                            "middleNames": [
                                "Hendrik"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. H. Metzen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[23], SpArSe uses a form of weight sharing called network morphism [62] to search over architectures without training each one from scratch."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "Search time can be reduced by constraining each proposal to be a morph of a reference \u03a9 \u2208 { \u03a9 n\u22121 j=0 [23]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 2
                            }
                        ],
                        "text": ", [23]), is not sufficient to identify networks that minimize RAM usage."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[23] can be seen as a special case of SpArSe."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 218
                            }
                        ],
                        "text": "The results show that including pruning as part of the optimization yields roughly a 80x reduction in number of parameters, indicating that the formulation of SpArSe is better suited to designing tiny CNNs compared to [23]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 130
                            }
                        ],
                        "text": "Importantly, previous NAS approaches have focused on searching for Ep directly by using |E| as one of the optimization objectives [23]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 101
                            }
                        ],
                        "text": "Algorithms for identifying performant CNN architectures have received significant attention recently [66, 23, 20, 45, 31, 24, 44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 4,
                                "start": 0
                            }
                        ],
                        "text": "[23] seek compact architectures by using the number of network edges as one of the objectives in the search, potential gains from weight sparsity are ignored, which can be significant (Section 4."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 160011572,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f959413cf1914ab5d9046c72cbfccd46485cb016",
            "isKey": true,
            "numCitedBy": 90,
            "numCiting": 81,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep Learning has enabled remarkable progress over the last years on a variety of tasks, such as image recognition, speech recognition, and machine translation. One crucial aspect for this progress are novel neural architectures. Currently employed architectures have mostly been developed manually by human experts, which is a time-consuming and error-prone process. Because of this, there is growing interest in automated neural architecture search methods. We provide an overview of existing work in this field of research and categorize them according to three dimensions: search space, search strategy, and performance estimation strategy."
            },
            "slug": "Neural-Architecture-Search-Elsken-Metzen",
            "title": {
                "fragments": [],
                "text": "Neural Architecture Search"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An overview of existing work in this field of research is provided and neural architecture search methods are categorized according to three dimensions: search space, search strategy, and performance estimation strategy."
            },
            "venue": {
                "fragments": [],
                "text": "Automated Machine Learning"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068206635"
                        ],
                        "name": "Tao Wei",
                        "slug": "Tao-Wei",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Wei",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Wei"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1906061249"
                        ],
                        "name": "Changhu Wang",
                        "slug": "Changhu-Wang",
                        "structuredName": {
                            "firstName": "Changhu",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Changhu Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145459057"
                        ],
                        "name": "Y. Rui",
                        "slug": "Y.-Rui",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Rui",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Rui"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145780208"
                        ],
                        "name": "C. Chen",
                        "slug": "C.-Chen",
                        "structuredName": {
                            "firstName": "Chang",
                            "lastName": "Chen",
                            "middleNames": [
                                "Wen"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 67
                            }
                        ],
                        "text": "[23], SpArSe uses a form of weight sharing called network morphism [62] to search over architectures without training each one from scratch."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 395154,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "02f1c79f1a09d9501cd39fc7b527cd9e13dff7ad",
            "isKey": false,
            "numCitedBy": 120,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved. We define this as network morphism in this research. After morphing a parent network, the child network is expected to inherit the knowledge from its parent network and also has the potential to continue growing into a more powerful one with much shortened training time. The first requirement for this network morphism is its ability to handle diverse morphing types of networks, including changes of depth, width, kernel size, and even subnet. To meet this requirement, we first introduce the network morphism equations, and then develop novel morphing algorithms for all these morphing types for both classic and convolutional neural networks. The second requirement is its ability to deal with non-linearity in a network. We propose a family of parametric-activation functions to facilitate the morphing of any continuous nonlinear activation neurons. Experimental results on benchmark datasets and typical neural networks demonstrate the effectiveness of the proposed network morphism scheme."
            },
            "slug": "Network-Morphism-Wei-Wang",
            "title": {
                "fragments": [],
                "text": "Network Morphism"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "A systematic study on how to morph a well-trained neural network to a new one so that its network function can be completely preserved, and proposes a family of parametric-activation functions to facilitate the morphing of any continuous nonlinear activation neurons."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "52184096"
                        ],
                        "name": "L. Bottou",
                        "slug": "L.-Bottou",
                        "structuredName": {
                            "firstName": "L\u00e9on",
                            "lastName": "Bottou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Bottou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1721248"
                        ],
                        "name": "P. Haffner",
                        "slug": "P.-Haffner",
                        "structuredName": {
                            "firstName": "Patrick",
                            "lastName": "Haffner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Haffner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "To illustrate the challenge of deploying CNNs on MCUs, consider the seemingly simple task of deploying the well-known LeNet CNN on an Arduino Uno [1] to perform MNIST character recognition [43]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "4 Results We report results on a variety of datasets: MNIST (55e3, 5e3, 10e3) [43], CIFAR10 (45e3, 5e3, 10e3) [39], CUReT (3704, 500, 1408) [60], and Chars4k (3897, 500, 1886) [25], corresponding to classification problems with 10, 10, 61, and 62 classes, respectively, with the training/validation/test set sizes provided in parentheses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14542261,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "162d958ff885f1462aeda91cd72582323fd6a1f4",
            "isKey": false,
            "numCitedBy": 35623,
            "numCiting": 246,
            "paperAbstract": {
                "fragments": [],
                "text": "Multilayer neural networks trained with the back-propagation algorithm constitute the best example of a successful gradient based learning technique. Given an appropriate network architecture, gradient-based learning algorithms can be used to synthesize a complex decision surface that can classify high-dimensional patterns, such as handwritten characters, with minimal preprocessing. This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task. Convolutional neural networks, which are specifically designed to deal with the variability of 2D shapes, are shown to outperform all other techniques. Real-life document recognition systems are composed of multiple modules including field extraction, segmentation recognition, and language modeling. A new learning paradigm, called graph transformer networks (GTN), allows such multimodule systems to be trained globally using gradient-based methods so as to minimize an overall performance measure. Two systems for online handwriting recognition are described. Experiments demonstrate the advantage of global training, and the flexibility of graph transformer networks. A graph transformer network for reading a bank cheque is also described. It uses convolutional neural network character recognizers combined with global training techniques to provide record accuracy on business and personal cheques. It is deployed commercially and reads several million cheques per day."
            },
            "slug": "Gradient-based-learning-applied-to-document-LeCun-Bottou",
            "title": {
                "fragments": [],
                "text": "Gradient-based learning applied to document recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper reviews various methods applied to handwritten character recognition and compares them on a standard handwritten digit recognition task, and Convolutional neural networks are shown to outperform all other techniques."
            },
            "venue": {
                "fragments": [],
                "text": "Proc. IEEE"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075783"
                        ],
                        "name": "Christos Louizos",
                        "slug": "Christos-Louizos",
                        "structuredName": {
                            "firstName": "Christos",
                            "lastName": "Louizos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christos Louizos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39677614"
                        ],
                        "name": "Karen Ullrich",
                        "slug": "Karen-Ullrich",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Ullrich",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Karen Ullrich"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning can be expressed as\n\u03c9\u0304 = argmin ( \u2211\nG\u2208G 1[\u2016\u03c9G\u20162>0])\u2264s\nL ({\u03b1, \u03d1, \u03c9}) (8)\nwhere L (\u00b7) denotes the loss function for the appropriate task, e.g. cross-entropy for classification, G denotes the set of disjoint groups covering the indices of each entry in \u03c9, \u03c9G denotes a particular group of weights, and 1 [\u00b7] denotes the indicator function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning can be considered a form of NAS, where \u03c9\u0304 represents a sub-network of {\u03b1, \u03d1, \u03c9} given by {{V,Ep} , \u03d1, \u03c9}, and Ep \u2286 E contains only the edges for which \u03c9\u0304 is non-zero [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning is a procedure for zeroing out network parameters \u03c9 and can be seen as a way to generate a new set of parameters \u03c9\u0304 that have lower \u2016\u03c9\u0304\u20160."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 76
                            }
                        ],
                        "text": "We use Sparse Variational Dropout (SpVD) [49] and Bayesian Compression (BC) [47] to realize unstructured and structured pruning, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 33
                            }
                        ],
                        "text": "3 Neural network pruning Pruning [42, 49, 21, 47] is essential to MCU deployment using SpArSe, as it heavily reduces the model size (4) and working memory (5)/(6) without significantly impacting classification accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 107
                            }
                        ],
                        "text": "We compare against previous SOTA works: Bonsai [41], ProtoNN [32], Gradient Boosted Decision Tree Ensemble Pruning [22], kNN, and radial basis function support vector machine (SVM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning enables SpArSe to quickly evaluate many sub-networks of a given network, thereby expanding the scope of the overall search."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning [42, 49, 21, 47] is essential to MCU deployment using SpArSe, as it heavily reduces the model size (4) and working memory (5)/(6) without significantly impacting classification accuracy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9328854,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9302e07c4951559ad9a538295029881a171faeec",
            "isKey": true,
            "numCitedBy": 372,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "Compression and computational efficiency in deep learning have become a problem of great significance. In this work, we argue that the most principled and effective way to attack this problem is by adopting a Bayesian point of view, where through sparsity inducing priors we prune large parts of the network. We introduce two novelties in this paper: 1) we use hierarchical priors to prune nodes instead of individual weights, and 2) we use the posterior uncertainties to determine the optimal fixed point precision to encode the weights. Both factors significantly contribute to achieving the state of the art in terms of compression rates, while still staying competitive with methods designed to optimize for speed or energy efficiency."
            },
            "slug": "Bayesian-Compression-for-Deep-Learning-Louizos-Ullrich",
            "title": {
                "fragments": [],
                "text": "Bayesian Compression for Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work argues that the most principled and effective way to attack the problem of compression and computational efficiency in deep learning is by adopting a Bayesian point of view, where through sparsity inducing priors the authors prune large parts of the network."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "25581960"
                        ],
                        "name": "Jonathan Frankle",
                        "slug": "Jonathan-Frankle",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Frankle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Frankle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701041"
                        ],
                        "name": "Michael Carbin",
                        "slug": "Michael-Carbin",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Carbin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Carbin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 210
                            }
                        ],
                        "text": "3 What SpArSe reveals about pruning Pruning can be considered a form of NAS, where \u03c9\u0304 represents a sub-network of {\u03b1, \u03b8, \u03c9} given by {{V,Ep} , \u03b8, \u03c9}, and Ep \u2286 E contains only the edges for which \u03c9\u0304 is non-zero [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 220,
                                "start": 208
                            }
                        ],
                        "text": "The question then becomes, should one look for Ep directly or begin with a large edge-set E and prune it? There is conflicting evidence whether the same validation accuracy can be achieved by both approaches [27, 28, 46]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 91
                            }
                        ],
                        "text": "The key is that pruning provides a mechanism for uncovering such high performing subgraphs [27]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 53388625,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "21937ecd9d66567184b83eca3d3e09eb4e6fbd60",
            "isKey": false,
            "numCitedBy": 1469,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural network pruning techniques can reduce the parameter counts of trained networks by over 90%, decreasing storage requirements and improving computational performance of inference without compromising accuracy. However, contemporary experience is that the sparse architectures produced by pruning are difficult to train from the start, which would similarly improve training performance. \nWe find that a standard pruning technique naturally uncovers subnetworks whose initializations made them capable of training effectively. Based on these results, we articulate the \"lottery ticket hypothesis:\" dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations. The winning tickets we find have won the initialization lottery: their connections have initial weights that make training particularly effective. \nWe present an algorithm to identify winning tickets and a series of experiments that support the lottery ticket hypothesis and the importance of these fortuitous initializations. We consistently find winning tickets that are less than 10-20% of the size of several fully-connected and convolutional feed-forward architectures for MNIST and CIFAR10. Above this size, the winning tickets that we find learn faster than the original network and reach higher test accuracy."
            },
            "slug": "The-Lottery-Ticket-Hypothesis:-Finding-Sparse,-Frankle-Carbin",
            "title": {
                "fragments": [],
                "text": "The Lottery Ticket Hypothesis: Finding Sparse, Trainable Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This work finds that dense, randomly-initialized, feed-forward networks contain subnetworks (\"winning tickets\") that - when trained in isolation - reach test accuracy comparable to the original network in a similar number of iterations, and articulate the \"lottery ticket hypothesis\"."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8796171"
                        ],
                        "name": "Dmitry Molchanov",
                        "slug": "Dmitry-Molchanov",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Molchanov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dmitry Molchanov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8806826"
                        ],
                        "name": "Arsenii Ashukha",
                        "slug": "Arsenii-Ashukha",
                        "structuredName": {
                            "firstName": "Arsenii",
                            "lastName": "Ashukha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arsenii Ashukha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2492721"
                        ],
                        "name": "D. Vetrov",
                        "slug": "D.-Vetrov",
                        "structuredName": {
                            "firstName": "Dmitry",
                            "lastName": "Vetrov",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Vetrov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning can be expressed as\n\u03c9\u0304 = argmin ( \u2211\nG\u2208G 1[\u2016\u03c9G\u20162>0])\u2264s\nL ({\u03b1, \u03d1, \u03c9}) (8)\nwhere L (\u00b7) denotes the loss function for the appropriate task, e.g. cross-entropy for classification, G denotes the set of disjoint groups covering the indices of each entry in \u03c9, \u03c9G denotes a particular group of weights, and 1 [\u00b7] denotes the indicator function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning can be considered a form of NAS, where \u03c9\u0304 represents a sub-network of {\u03b1, \u03d1, \u03c9} given by {{V,Ep} , \u03d1, \u03c9}, and Ep \u2286 E contains only the edges for which \u03c9\u0304 is non-zero [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning is a procedure for zeroing out network parameters \u03c9 and can be seen as a way to generate a new set of parameters \u03c9\u0304 that have lower \u2016\u03c9\u0304\u20160."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 41
                            }
                        ],
                        "text": "We use Sparse Variational Dropout (SpVD) [49] and Bayesian Compression (BC) [47] to realize unstructured and structured pruning, respectively."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 33
                            }
                        ],
                        "text": "3 Neural network pruning Pruning [42, 49, 21, 47] is essential to MCU deployment using SpArSe, as it heavily reduces the model size (4) and working memory (5)/(6) without significantly impacting classification accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 107
                            }
                        ],
                        "text": "We compare against previous SOTA works: Bonsai [41], ProtoNN [32], Gradient Boosted Decision Tree Ensemble Pruning [22], kNN, and radial basis function support vector machine (SVM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning enables SpArSe to quickly evaluate many sub-networks of a given network, thereby expanding the scope of the overall search."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning [42, 49, 21, 47] is essential to MCU deployment using SpArSe, as it heavily reduces the model size (4) and working memory (5)/(6) without significantly impacting classification accuracy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 18201582,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34cc3ceae5c3f7c8acbb89f2bff63f9d452b00d5",
            "isKey": true,
            "numCitedBy": 601,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore a recently proposed Variational Dropout technique that provided an elegant Bayesian interpretation to Gaussian Dropout. We extend Variational Dropout to the case when dropout rates are unbounded, propose a way to reduce the variance of the gradient estimator and report first experimental results with individual dropout rates per weight. Interestingly, it leads to extremely sparse solutions both in fully-connected and convolutional layers. This effect is similar to automatic relevance determination effect in empirical Bayes but has a number of advantages. We reduce the number of parameters up to 280 times on LeNet architectures and up to 68 times on VGG-like networks with a negligible decrease of accuracy."
            },
            "slug": "Variational-Dropout-Sparsifies-Deep-Neural-Networks-Molchanov-Ashukha",
            "title": {
                "fragments": [],
                "text": "Variational Dropout Sparsifies Deep Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "Variational Dropout is extended to the case when dropout rates are unbounded, a way to reduce the variance of the gradient estimator is proposed and first experimental results with individual drop out rates per weight are reported."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1690967"
                        ],
                        "name": "A. Blum",
                        "slug": "A.-Blum",
                        "structuredName": {
                            "firstName": "Avrim",
                            "lastName": "Blum",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Blum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3033269"
                        ],
                        "name": "Nika Haghtalab",
                        "slug": "Nika-Haghtalab",
                        "structuredName": {
                            "firstName": "Nika",
                            "lastName": "Haghtalab",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nika Haghtalab"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689184"
                        ],
                        "name": "Ariel D. Procaccia",
                        "slug": "Ariel-D.-Procaccia",
                        "structuredName": {
                            "firstName": "Ariel",
                            "lastName": "Procaccia",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ariel D. Procaccia"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 46343823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f0ddb2bc6e5464d992ddbcdfdc7e894150fc81f2",
            "isKey": false,
            "numCitedBy": 856,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore an as yet unexploited opportunity for drastically improving the efficiency of stochastic gradient variational Bayes (SGVB) with global model parameters. Regular SGVB estimators rely on sampling of parameters once per minibatch of data, and have variance that is constant w.r.t. the minibatch size. The efficiency of such estimators can be drastically improved upon by translating uncertainty about global parameters into local noise that is independent across datapoints in the minibatch. Such reparameterizations with local noise can be trivially parallelized and have variance that is inversely proportional to the minibatch size, generally leading to much faster convergence.We find an important connection with regularization by dropout: the original Gaussian dropout objective corresponds to SGVB with local noise, a scale-invariant prior and proportionally fixed posterior variance. Our method allows inference of more flexibly parameterized posteriors; specifically, we propose \\emph{variational dropout}, a generalization of Gaussian dropout, but with a more flexibly parameterized posterior, often leading to better generalization. The method is demonstrated through several experiments."
            },
            "slug": "Variational-Dropout-and-the-Local-Trick-Blum-Haghtalab",
            "title": {
                "fragments": [],
                "text": "Variational Dropout and the Local Reparameterization Trick"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The Variational dropout method is proposed, a generalization of Gaussian dropout, but with a more flexibly parameterized posterior, often leading to better generalization in stochastic gradient variational Bayes."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1402816540"
                        ],
                        "name": "C. S\u00f8nderby",
                        "slug": "C.-S\u00f8nderby",
                        "structuredName": {
                            "firstName": "Casper",
                            "lastName": "S\u00f8nderby",
                            "middleNames": [
                                "Kaae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. S\u00f8nderby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2785022"
                        ],
                        "name": "T. Raiko",
                        "slug": "T.-Raiko",
                        "structuredName": {
                            "firstName": "Tapani",
                            "lastName": "Raiko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Raiko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7435149"
                        ],
                        "name": "Lars Maal\u00f8e",
                        "slug": "Lars-Maal\u00f8e",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Maal\u00f8e",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lars Maal\u00f8e"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388358166"
                        ],
                        "name": "S\u00f8ren Kaae S\u00f8nderby",
                        "slug": "S\u00f8ren-Kaae-S\u00f8nderby",
                        "structuredName": {
                            "firstName": "S\u00f8ren",
                            "lastName": "S\u00f8nderby",
                            "middleNames": [
                                "Kaae"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S\u00f8ren Kaae S\u00f8nderby"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724252"
                        ],
                        "name": "O. Winther",
                        "slug": "O.-Winther",
                        "structuredName": {
                            "firstName": "Ole",
                            "lastName": "Winther",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Winther"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 217840524,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2ad303a88ba4fe57a4a3618aa42be90335481acc",
            "isKey": false,
            "numCitedBy": 113,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "Variational autoencoders are a powerful framework for unsupervised learning. However, previous work has been restricted to shallow models with one or two layers of fully factorized stochastic latent variables, limiting the flexibility of the latent representation. We propose three advances in training algorithms of variational autoencoders, for the first time allowing to train deep models of up to five stochastic layers, (1) using a structure similar to the Ladder network as the inference model, (2) warm-up period to support stochastic units staying active in early training, and (3) use of batch normalization. Using these improvements we show state-of-the-art log-likelihood results for generative modeling on several benchmark datasets."
            },
            "slug": "How-to-Train-Deep-Variational-Autoencoders-and-S\u00f8nderby-Raiko",
            "title": {
                "fragments": [],
                "text": "How to Train Deep Variational Autoencoders and Probabilistic Ladder Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work proposes three advances in training algorithms of variational autoencoders, for the first time allowing to train deep models of up to five stochastic layers, using a structure similar to the Ladder network as the inference model and shows state-of-the-art log-likelihood results for generative modeling on several benchmark datasets."
            },
            "venue": {
                "fragments": [],
                "text": "ICML 2016"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144228782"
                        ],
                        "name": "T. D. Campos",
                        "slug": "T.-D.-Campos",
                        "structuredName": {
                            "firstName": "Te\u00f3filo",
                            "lastName": "Campos",
                            "middleNames": [
                                "Em\u00eddio",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. D. Campos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6516879"
                        ],
                        "name": "Bodla Rakesh Babu",
                        "slug": "Bodla-Rakesh-Babu",
                        "structuredName": {
                            "firstName": "Bodla",
                            "lastName": "Babu",
                            "middleNames": [
                                "Rakesh"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bodla Rakesh Babu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145859952"
                        ],
                        "name": "M. Varma",
                        "slug": "M.-Varma",
                        "structuredName": {
                            "firstName": "Manik",
                            "lastName": "Varma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Varma"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 180,
                                "start": 176
                            }
                        ],
                        "text": "4 Results We report results on a variety of datasets: MNIST (55e3, 5e3, 10e3) [43], CIFAR10 (45e3, 5e3, 10e3) [39], CUReT (3704, 500, 1408) [60], and Chars4k (3897, 500, 1886) [25], corresponding to classification problems with 10, 10, 61, and 62 classes, respectively, with the training/validation/test set sizes provided in parentheses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4826173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbbd5fdc09349bbfdee7aa7365a9d37716852b32",
            "isKey": false,
            "numCitedBy": 514,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper tackles the problem of recognizing characters in images of natural scenes. In particular, we focus on recognizing characters in situations that would traditionally not be handled well by OCR techniques. We present an annotated database of images containing English and Kannada characters. The database comprises of images of street scenes taken in Bangalore, India using a standard camera. The problem is addressed in an object cateogorization framework based on a bag-of-visual-words representation. We assess the performance of various features based on nearest neighbour and SVM classification. It is demonstrated that the performance of the proposed method, using as few as 15 training images, can be far superior to that of commercial OCR systems. Furthermore, the method can benefit from synthetically generated training data obviating the need for expensive data collection and annotation."
            },
            "slug": "Character-Recognition-in-Natural-Images-Campos-Babu",
            "title": {
                "fragments": [],
                "text": "Character Recognition in Natural Images"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is demonstrated that the performance of the proposed method can be far superior to that of commercial OCR systems, and can benefit from synthetically generated training data obviating the need for expensive data collection and annotation."
            },
            "venue": {
                "fragments": [],
                "text": "VISAPP"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715321"
                        ],
                        "name": "J. Gubbi",
                        "slug": "J.-Gubbi",
                        "structuredName": {
                            "firstName": "Jayavardhana",
                            "lastName": "Gubbi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gubbi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1709598"
                        ],
                        "name": "R. Buyya",
                        "slug": "R.-Buyya",
                        "structuredName": {
                            "firstName": "Rajkumar",
                            "lastName": "Buyya",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Buyya"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1731133"
                        ],
                        "name": "S. Marusic",
                        "slug": "S.-Marusic",
                        "structuredName": {
                            "firstName": "Slaven",
                            "lastName": "Marusic",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Marusic"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145389998"
                        ],
                        "name": "M. Palaniswami",
                        "slug": "M.-Palaniswami",
                        "structuredName": {
                            "firstName": "Marimuthu",
                            "lastName": "Palaniswami",
                            "middleNames": [
                                "Swami"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Palaniswami"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 210
                            }
                        ],
                        "text": "In recent years, MCUs have been used to inject intelligence and connectivity into everything from industrial monitoring sensors to consumer devices, a trend commonly referred to as the Internet of Things (IoT) [19, 30, 48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 204982032,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "72c4d8b64a9959ea45677ca1955d3491ef0f1c62",
            "isKey": false,
            "numCitedBy": 9204,
            "numCiting": 111,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Internet-of-Things-(IoT):-A-vision,-architectural-Gubbi-Buyya",
            "title": {
                "fragments": [],
                "text": "Internet of Things (IoT): A vision, architectural elements, and future directions"
            },
            "venue": {
                "fragments": [],
                "text": "Future Gener. Comput. Syst."
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3472959"
                        ],
                        "name": "C. Rasmussen",
                        "slug": "C.-Rasmussen",
                        "structuredName": {
                            "firstName": "Carl",
                            "lastName": "Rasmussen",
                            "middleNames": [
                                "Edward"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Rasmussen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145715698"
                        ],
                        "name": "Christopher K. I. Williams",
                        "slug": "Christopher-K.-I.-Williams",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Williams",
                            "middleNames": [
                                "K.",
                                "I."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Christopher K. I. Williams"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 72
                            }
                        ],
                        "text": "Since fk(\u03a9) is unknown in practice, it is modeled by a Gaussian process [52] with a kernel \u03ba (\u00b7, \u00b7) that supports the types of variables included in \u03a9, i."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1430472,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "82266f6103bade9005ec555ed06ba20b5210ff22",
            "isKey": false,
            "numCitedBy": 18076,
            "numCiting": 231,
            "paperAbstract": {
                "fragments": [],
                "text": "Gaussian processes (GPs) provide a principled, practical, probabilistic approach to learning in kernel machines. GPs have received growing attention in the machine learning community over the past decade. The book provides a long-needed, systematic and unified treatment of theoretical and practical aspects of GPs in machine learning. The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics. The book deals with the supervised learning problem for both regression and classification, and includes detailed algorithms. A wide variety of covariance (kernel) functions are presented and their properties discussed. Model selection is discussed both from a Bayesian and classical perspective. Many connections to other well-known techniques from machine learning and statistics are discussed, including support vector machines, neural networks, splines, regularization networks, relevance vector machines and others. Theoretical issues including learning curves and the PAC-Bayesian framework are treated, and several approximation methods for learning with large datasets are discussed. The book contains illustrative examples and exercises. Code and datasets can be obtained on the web. Appendices provide mathematical background and a discussion of Gaussian Markov processes."
            },
            "slug": "Gaussian-Processes-for-Machine-Learning-Rasmussen-Williams",
            "title": {
                "fragments": [],
                "text": "Gaussian Processes for Machine Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The treatment is comprehensive and self-contained, targeted at researchers and students in machine learning and applied statistics, and deals with the supervised learning problem for both regression and classification."
            },
            "venue": {
                "fragments": [],
                "text": "Adaptive computation and machine learning"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 216078090,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5f5dc5b9a2ba710937e2c413b37b053cd673df02",
            "isKey": false,
            "numCitedBy": 17049,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract: How can we perform efficient inference and learning in directed probabilistic models, in the presence of continuous latent variables with intractable posterior distributions, and large datasets? We introduce a stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case. Our contributions is two-fold. First, we show that a reparameterization of the variational lower bound yields a lower bound estimator that can be straightforwardly optimized using standard stochastic gradient methods. Second, we show that for i.i.d. datasets with continuous latent variables per datapoint, posterior inference can be made especially efficient by fitting an approximate inference model (also called a recognition model) to the intractable posterior using the proposed lower bound estimator. Theoretical advantages are reflected in experimental results."
            },
            "slug": "Auto-Encoding-Variational-Bayes-Kingma-Welling",
            "title": {
                "fragments": [],
                "text": "Auto-Encoding Variational Bayes"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "A stochastic variational inference and learning algorithm that scales to large datasets and, under some mild differentiability conditions, even works in the intractable case is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398645148"
                        ],
                        "name": "E.C. Garrido-Merch\u00e1n",
                        "slug": "E.C.-Garrido-Merch\u00e1n",
                        "structuredName": {
                            "firstName": "E.C.",
                            "lastName": "Garrido-Merch\u00e1n",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E.C. Garrido-Merch\u00e1n"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388574404"
                        ],
                        "name": "D. Hern\u00e1ndez-Lobato",
                        "slug": "D.-Hern\u00e1ndez-Lobato",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Hern\u00e1ndez-Lobato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Hern\u00e1ndez-Lobato"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 75
                            }
                        ],
                        "text": ", real-valued, discrete, categorical, and hierarchically related variables [57, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 13690194,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "fb31ffcd6a5b85441de3c900809cf431bf2e19cf",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Dealing-with-Categorical-and-Integer-valued-in-with-Garrido-Merch\u00e1n-Hern\u00e1ndez-Lobato",
            "title": {
                "fragments": [],
                "text": "Dealing with Categorical and Integer-valued Variables in Bayesian Optimization with Gaussian Processes"
            },
            "venue": {
                "fragments": [],
                "text": "Neurocomputing"
            },
            "year": 2020
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720529"
                        ],
                        "name": "L. Atzori",
                        "slug": "L.-Atzori",
                        "structuredName": {
                            "firstName": "Luigi",
                            "lastName": "Atzori",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Atzori"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720772"
                        ],
                        "name": "A. Iera",
                        "slug": "A.-Iera",
                        "structuredName": {
                            "firstName": "Antonio",
                            "lastName": "Iera",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Iera"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727358"
                        ],
                        "name": "G. Morabito",
                        "slug": "G.-Morabito",
                        "structuredName": {
                            "firstName": "Giacomo",
                            "lastName": "Morabito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Morabito"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 210
                            }
                        ],
                        "text": "In recent years, MCUs have been used to inject intelligence and connectivity into everything from industrial monitoring sensors to consumer devices, a trend commonly referred to as the Internet of Things (IoT) [19, 30, 48]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2630520,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e72d544641ffea256a60678f4cd2ee707d3ee0bd",
            "isKey": false,
            "numCitedBy": 8034,
            "numCiting": 104,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Internet-of-Things:-A-survey-Atzori-Iera",
            "title": {
                "fragments": [],
                "text": "The Internet of Things: A survey"
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Networks"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1759839"
                        ],
                        "name": "S. Solla",
                        "slug": "S.-Solla",
                        "structuredName": {
                            "firstName": "Sara",
                            "lastName": "Solla",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Solla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning can be expressed as\n\u03c9\u0304 = argmin ( \u2211\nG\u2208G 1[\u2016\u03c9G\u20162>0])\u2264s\nL ({\u03b1, \u03d1, \u03c9}) (8)\nwhere L (\u00b7) denotes the loss function for the appropriate task, e.g. cross-entropy for classification, G denotes the set of disjoint groups covering the indices of each entry in \u03c9, \u03c9G denotes a particular group of weights, and 1 [\u00b7] denotes the indicator function."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning can be considered a form of NAS, where \u03c9\u0304 represents a sub-network of {\u03b1, \u03d1, \u03c9} given by {{V,Ep} , \u03d1, \u03c9}, and Ep \u2286 E contains only the edges for which \u03c9\u0304 is non-zero [27]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning is a procedure for zeroing out network parameters \u03c9 and can be seen as a way to generate a new set of parameters \u03c9\u0304 that have lower \u2016\u03c9\u0304\u20160."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 33
                            }
                        ],
                        "text": "3 Neural network pruning Pruning [42, 49, 21, 47] is essential to MCU deployment using SpArSe, as it heavily reduces the model size (4) and working memory (5)/(6) without significantly impacting classification accuracy."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 107
                            }
                        ],
                        "text": "We compare against previous SOTA works: Bonsai [41], ProtoNN [32], Gradient Boosted Decision Tree Ensemble Pruning [22], kNN, and radial basis function support vector machine (SVM)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning enables SpArSe to quickly evaluate many sub-networks of a given network, thereby expanding the scope of the overall search."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 7,
                                "start": 0
                            }
                        ],
                        "text": "Pruning [42, 49, 21, 47] is essential to MCU deployment using SpArSe, as it heavily reduces the model size (4) and working memory (5)/(6) without significantly impacting classification accuracy."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7785881,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e7297db245c3feb1897720b173a59fe7e36babb7",
            "isKey": true,
            "numCitedBy": 3518,
            "numCiting": 31,
            "paperAbstract": {
                "fragments": [],
                "text": "We have used information-theoretic ideas to derive a class of practical and nearly optimal schemes for adapting the size of a neural network. By removing unimportant weights from a network, several improvements can be expected: better generalization, fewer training examples required, and improved speed of learning and/or classification. The basic idea is to use second-derivative information to make a tradeoff between network complexity and training set error. Experiments confirm the usefulness of the methods on a real-world application."
            },
            "slug": "Optimal-Brain-Damage-LeCun-Denker",
            "title": {
                "fragments": [],
                "text": "Optimal Brain Damage"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "A class of practical and nearly optimal schemes for adapting the size of a neural network by using second-derivative information to make a tradeoff between network complexity and training set error is derived."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145859952"
                        ],
                        "name": "M. Varma",
                        "slug": "M.-Varma",
                        "structuredName": {
                            "firstName": "Manik",
                            "lastName": "Varma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Varma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688869"
                        ],
                        "name": "Andrew Zisserman",
                        "slug": "Andrew-Zisserman",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Zisserman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andrew Zisserman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 140
                            }
                        ],
                        "text": "4 Results We report results on a variety of datasets: MNIST (55e3, 5e3, 10e3) [43], CIFAR10 (45e3, 5e3, 10e3) [39], CUReT (3704, 500, 1408) [60], and Chars4k (3897, 500, 1886) [25], corresponding to classification problems with 10, 10, 61, and 62 classes, respectively, with the training/validation/test set sizes provided in parentheses."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2313314,
            "fieldsOfStudy": [
                "Environmental Science"
            ],
            "id": "7e3c3fee11758b15b56d719cca819303eca9b54b",
            "isKey": false,
            "numCitedBy": 1004,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We investigate texture classification from single images obtained under unknown viewpoint and illumination. A statistical approach is developed where textures are modelled by the joint probability distribution of filter responses. This distribution is represented by the frequency histogram of filter response cluster centres (textons). Recognition proceeds from single, uncalibrated images and the novelty here is that rotationally invariant filters are used and the filter response space is low dimensional.Classification performance is compared with the filter banks and methods of Leung and Malik [IJCV, 2001], Schmid [CVPR, 2001] and Cula and Dana [IJCV, 2004] and it is demonstrated that superior performance is achieved here. Classification results are presented for all 61 materials in the Columbia-Utrecht texture database.We also discuss the effects of various parameters on our classification algorithm--such as the choice of filter bank and rotational invariance, the size of the texton dictionary as well as the number of training images used. Finally, we present a method of reliably measuring relative orientation co-occurrence statistics in a rotationally invariant manner, and discuss whether incorporating such information can enhance the classifier\u2019s performance."
            },
            "slug": "A-Statistical-Approach-to-Texture-Classification-Varma-Zisserman",
            "title": {
                "fragments": [],
                "text": "A Statistical Approach to Texture Classification from Single Images"
            },
            "venue": {
                "fragments": [],
                "text": "International Journal of Computer Vision"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1920895"
                        ],
                        "name": "Biswajit Paria",
                        "slug": "Biswajit-Paria",
                        "structuredName": {
                            "firstName": "Biswajit",
                            "lastName": "Paria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Biswajit Paria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887808"
                        ],
                        "name": "Kirthevasan Kandasamy",
                        "slug": "Kirthevasan-Kandasamy",
                        "structuredName": {
                            "firstName": "Kirthevasan",
                            "lastName": "Kandasamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kirthevasan Kandasamy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719347"
                        ],
                        "name": "B. P\u00f3czos",
                        "slug": "B.-P\u00f3czos",
                        "structuredName": {
                            "firstName": "Barnab\u00e1s",
                            "lastName": "P\u00f3czos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. P\u00f3czos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 53034523,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d9ede71b944460c1293294c9f64bd88a8cc400de",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real world applications can be framed as multi-objective optimization problems, where we wish to simultaneously optimize for multiple criteria. Bayesian optimization techniques for the multi-objective setting are pertinent when the evaluation of the functions in question are expensive. Traditional methods for multi-objective optimization, both Bayesian and otherwise, are aimed at recovering the Pareto front of these objectives. However, in certain cases a practitioner might desire to identify Pareto optimal points only in a subset of the Pareto front due to external considerations. In this work, we propose a strategy based on random scalarizations of the objectives that addresses this problem. Our approach is able to flexibly sample from desired regions of the Pareto front and, computationally, is considerably cheaper than most approaches for MOO. We also study a notion of regret in the multi-objective setting and show that our strategy achieves sublinear regret. We experiment with both synthetic and real-life problems, and demonstrate superior performance of our proposed algorithm in terms of the flexibility and regret."
            },
            "slug": "A-Flexible-Framework-for-Multi-Objective-Bayesian-Paria-Kandasamy",
            "title": {
                "fragments": [],
                "text": "A Flexible Framework for Multi-Objective Bayesian Optimization using Random Scalarizations"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes a strategy based on random scalarizations of the objectives that is able to flexibly sample from desired regions of the Pareto front and, computationally, is considerably cheaper than most approaches for MOO."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 2019
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754860"
                        ],
                        "name": "Kevin Swersky",
                        "slug": "Kevin-Swersky",
                        "structuredName": {
                            "firstName": "Kevin",
                            "lastName": "Swersky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kevin Swersky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704657"
                        ],
                        "name": "D. Duvenaud",
                        "slug": "D.-Duvenaud",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Duvenaud",
                            "middleNames": [
                                "Kristjanson"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Duvenaud"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144108062"
                        ],
                        "name": "Jasper Snoek",
                        "slug": "Jasper-Snoek",
                        "structuredName": {
                            "firstName": "Jasper",
                            "lastName": "Snoek",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jasper Snoek"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144661829"
                        ],
                        "name": "F. Hutter",
                        "slug": "F.-Hutter",
                        "structuredName": {
                            "firstName": "Frank",
                            "lastName": "Hutter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Hutter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144484861"
                        ],
                        "name": "Michael A. Osborne",
                        "slug": "Michael-A.-Osborne",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Osborne",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael A. Osborne"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 75
                            }
                        ],
                        "text": ", real-valued, discrete, categorical, and hierarchically related variables [57, 29]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6573057,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "61b0a3062378927334a6cc7cd3ebeebbb5f745a7",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In practical Bayesian optimization, we must often search over structures with differing numbers of parameters. For instance, we may wish to search over neural network architectures with an unknown number of layers. To relate performance data gathered for different architectures, we define a new kernel for conditional parameter spaces that explicitly includes information about which parameters are relevant in a given structure. We show that this kernel improves model quality and Bayesian optimization results over several simpler baseline kernels."
            },
            "slug": "Raiders-of-the-Lost-Architecture:-Kernels-for-in-Swersky-Duvenaud",
            "title": {
                "fragments": [],
                "text": "Raiders of the Lost Architecture: Kernels for Bayesian Optimization in Conditional Parameter Spaces"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A new kernel for conditional parameter spaces that explicitly includes information about which parameters are relevant in a given structure is defined and it is shown that this kernel improves model quality and Bayesian optimization results over several simpler baseline kernels."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1920895"
                        ],
                        "name": "Biswajit Paria",
                        "slug": "Biswajit-Paria",
                        "structuredName": {
                            "firstName": "Biswajit",
                            "lastName": "Paria",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Biswajit Paria"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1887808"
                        ],
                        "name": "Kirthevasan Kandasamy",
                        "slug": "Kirthevasan-Kandasamy",
                        "structuredName": {
                            "firstName": "Kirthevasan",
                            "lastName": "Kandasamy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kirthevasan Kandasamy"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1719347"
                        ],
                        "name": "B. P\u00f3czos",
                        "slug": "B.-P\u00f3czos",
                        "structuredName": {
                            "firstName": "Barnab\u00e1s",
                            "lastName": "P\u00f3czos",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. P\u00f3czos"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 62
                            }
                        ],
                        "text": "SpArSe uses a MOBO based on the idea of random scalarizations [51]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 125653598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f60ed69aba0f64822fb40236bb3b07b092e41cb9",
            "isKey": false,
            "numCitedBy": 20,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Many real world applications can be framed as multi-objective optimization problems, where we wish to simultaneously optimize for multiple criteria. Bayesian optimization techniques for the multi-objective setting are pertinent when the evaluation of the functions in question are expensive. Traditional methods for multi-objective optimization, both Bayesian and otherwise, are aimed at recovering the Pareto front of these objectives. However, we argue that recovering the entire Pareto front may not be aligned with our goals in practice. For example, while a practitioner might desire to identify Pareto optimal points, she may wish to focus only on a particular region of the Pareto front due to external considerations. In this work we propose an approach based on random scalarizations of the objectives. We demonstrate that our approach can focus its sampling on certain regions of the Pareto front while being flexible enough to sample from the entire Pareto front if required. Furthermore, our approach is less computationally demanding compared to other existing approaches. In this paper, we also analyse a notion of regret in the multi-objective setting and obtain sublinear regret bounds. We compare the proposed approach to other state-of-the-art approaches on both synthetic and real-life experiments. The results demonstrate superior performance of our proposed algorithm in terms of flexibility, scalability and regret."
            },
            "slug": "A-Flexible-Multi-Objective-Bayesian-Optimization-Paria-Kandasamy",
            "title": {
                "fragments": [],
                "text": "A Flexible Multi-Objective Bayesian Optimization Approach using Random Scalarizations"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes an approach based on random scalarizations of the objectives that can focus its sampling on certain regions of the Pareto front while being flexible enough to sample from the entire Pare to front if required, and is less computationally demanding compared to other existing approaches."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2175946"
                        ],
                        "name": "L. Sifre",
                        "slug": "L.-Sifre",
                        "structuredName": {
                            "firstName": "L.",
                            "lastName": "Sifre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Sifre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746242"
                        ],
                        "name": "S. Mallat",
                        "slug": "S.-Mallat",
                        "structuredName": {
                            "firstName": "St\u00e9phane",
                            "lastName": "Mallat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Mallat"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 49,
                                "start": 45
                            }
                        ],
                        "text": "Advances include depth-wise separable layers [54], deploymentcentric pruning [64, 50], and quantization techniques [61]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 196607,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc90c96263679a51021a63884b36f3e6ed8444a0",
            "isKey": false,
            "numCitedBy": 187,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "A rigid-motion scattering computes adaptive invariants along translations and rotations, with a deep convolutional network. Convolutions are calculated on the rigid-motion group, with wavelets defined on the translation and rotation variables. It preserves joint rotation and translation information, while providing global invariants at any desired scale. Texture classification is studied, through the characterization of stationary processes from a single realization. State-of-the-art results are obtained on multiple texture data bases, with important rotation and scaling variabilities."
            },
            "slug": "Rigid-Motion-Scattering-for-Texture-Classification-Sifre-Mallat",
            "title": {
                "fragments": [],
                "text": "Rigid-Motion Scattering for Texture Classification"
            },
            "tldr": {
                "abstractSimilarityScore": 73,
                "text": "A rigid-motion scattering computes adaptive invariants along translations and rotations, with a deep convolutional network, that preserves joint rotation and translation information, while providing global invariants at any desired scale."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "70345806"
                        ],
                        "name": "George Eastman House",
                        "slug": "George-Eastman-House",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "House",
                            "middleNames": [
                                "Eastman"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George Eastman House"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "117676260"
                        ],
                        "name": "Guildhall StreetCambridge",
                        "slug": "Guildhall-StreetCambridge",
                        "structuredName": {
                            "firstName": "Guildhall",
                            "lastName": "StreetCambridge",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Guildhall StreetCambridge"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 217295,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d68725804eadecf83d707d89e12c5132bf376187",
            "isKey": false,
            "numCitedBy": 4438,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Sparse-Bayesian-Learning-and-the-Relevance-Vector-House-StreetCambridge",
            "title": {
                "fragments": [],
                "text": "Sparse Bayesian Learning and the Relevance Vector Machine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pruning decision forests. Personal Communications"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "United Nations. Department of Economic and Social Affairs. Population Division. World population prospects: The 2010 revision"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Internet of things (iot): A vision, architectural elements, and future directions. Future generation computer systems"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "TensorFlow Quantization-Aware Training"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "We compare against previous SOTA works: Bonsai [41], ProtoNN [32], Gradient Boosted Decision Tree Ensemble Pruning [22], kNN, and radial basis function support vector machine (SVM)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Pruning decision forests"
            },
            "venue": {
                "fragments": [],
                "text": "Personal Communications,"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The shape of the MCU market"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Global shipments of discrete graphics processing units from"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The 'Internet of Things' is now"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Arm mbed-cli"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 101
                            }
                        ],
                        "text": "Algorithms for identifying performant CNN architectures have received significant attention recently [66, 23, 20, 45, 31, 24, 44]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Literature on Neural Architecture Search at AutoML.org at Freiburg"
            },
            "venue": {
                "fragments": [],
                "text": "URL /https://www.automl.org/automl/literature-on-neural-architecture-search/"
            },
            "year": 2019
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Kingma and Max Welling . Auto - encoding variational bayes"
            },
            "venue": {
                "fragments": [],
                "text": "The International Conference on Learning Representations"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Why the Future of Machine Learning is Tiny"
            },
            "venue": {
                "fragments": [],
                "text": ""
            }
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 30,
            "methodology": 18,
            "result": 7
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 65,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/SpArSe:-Sparse-Architecture-Search-for-CNNs-on-Fedorov-Adams/90460ac9de7dc3b83c3d249514e5a6ec4ba7c4d1?sort=total-citations"
}