{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145055042"
                        ],
                        "name": "David Krueger",
                        "slug": "David-Krueger",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Krueger",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Krueger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3422058"
                        ],
                        "name": "Tegan Maharaj",
                        "slug": "Tegan-Maharaj",
                        "structuredName": {
                            "firstName": "Tegan",
                            "lastName": "Maharaj",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tegan Maharaj"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064949280"
                        ],
                        "name": "J'anos Kram'ar",
                        "slug": "J'anos-Kram'ar",
                        "structuredName": {
                            "firstName": "J'anos",
                            "lastName": "Kram'ar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J'anos Kram'ar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145507036"
                        ],
                        "name": "M. Pezeshki",
                        "slug": "M.-Pezeshki",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Pezeshki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Pezeshki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482072"
                        ],
                        "name": "Nicolas Ballas",
                        "slug": "Nicolas-Ballas",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Ballas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Ballas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145604319"
                        ],
                        "name": "Nan Rosemary Ke",
                        "slug": "Nan-Rosemary-Ke",
                        "structuredName": {
                            "firstName": "Nan",
                            "lastName": "Ke",
                            "middleNames": [
                                "Rosemary"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nan Rosemary Ke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1996705"
                        ],
                        "name": "Anirudh Goyal",
                        "slug": "Anirudh-Goyal",
                        "structuredName": {
                            "firstName": "Anirudh",
                            "lastName": "Goyal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anirudh Goyal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777528"
                        ],
                        "name": "H. Larochelle",
                        "slug": "H.-Larochelle",
                        "structuredName": {
                            "firstName": "H.",
                            "lastName": "Larochelle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Larochelle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "98109738"
                        ],
                        "name": "Chris Pal",
                        "slug": "Chris-Pal",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Pal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chris Pal"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12200521,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9f0687bcd0a7d7fc91b8c5d36c003a38b8853105",
            "isKey": false,
            "numCitedBy": 268,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose zoneout, a novel method for regularizing RNNs. At each timestep, zoneout stochastically forces some hidden units to maintain their previous values. Like dropout, zoneout uses random noise to train a pseudo-ensemble, improving generalization. But by preserving instead of dropping hidden units, gradient information and state information are more readily propagated through time, as in feedforward stochastic depth networks. We perform an empirical investigation of various RNN regularizers, and find that zoneout gives significant performance improvements across tasks. We achieve competitive results with relatively simple models in character- and word-level language modelling on the Penn Treebank and Text8 datasets, and combining with recurrent batch normalization yields state-of-the-art results on permuted sequential MNIST."
            },
            "slug": "Zoneout:-Regularizing-RNNs-by-Randomly-Preserving-Krueger-Maharaj",
            "title": {
                "fragments": [],
                "text": "Zoneout: Regularizing RNNs by Randomly Preserving Hidden Activations"
            },
            "tldr": {
                "abstractSimilarityScore": 53,
                "text": "This work proposes zoneout, a novel method for regularizing RNNs that uses random noise to train a pseudo-ensemble, improving generalization and performs an empirical investigation of various RNN regularizers, and finds that zoneout gives significant performance improvements across tasks."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2047446108"
                        ],
                        "name": "Tomas Mikolov",
                        "slug": "Tomas-Mikolov",
                        "structuredName": {
                            "firstName": "Tomas",
                            "lastName": "Mikolov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tomas Mikolov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713801"
                        ],
                        "name": "Anoop Deoras",
                        "slug": "Anoop-Deoras",
                        "structuredName": {
                            "firstName": "Anoop",
                            "lastName": "Deoras",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Anoop Deoras"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784529"
                        ],
                        "name": "H. Le",
                        "slug": "H.-Le",
                        "structuredName": {
                            "firstName": "Hai",
                            "lastName": "Le",
                            "middleNames": [
                                "Son"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2675251"
                        ],
                        "name": "Stefan Kombrink",
                        "slug": "Stefan-Kombrink",
                        "structuredName": {
                            "firstName": "Stefan",
                            "lastName": "Kombrink",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stefan Kombrink"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1899242"
                        ],
                        "name": "J. Cernock\u00fd",
                        "slug": "J.-Cernock\u00fd",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Cernock\u00fd",
                            "middleNames": [
                                "Honza"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Cernock\u00fd"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 58
                            }
                        ],
                        "text": ", 1993) using the train/validation/test split outlined in (Mikolov et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 169
                            }
                        ],
                        "text": "The HyperLSTM model is evaluated on character level prediction task on the Penn Treebank corpus (Marcus et al., 1993) using the train/validation/test split outlined in (Mikolov et al., 2012)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 69,
                                "start": 47
                            }
                        ],
                        "text": "Model(1) Test Validation Param Count ME n-gram (Mikolov et al., 2012) 1."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 46542477,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1fd7fc06653723b05abe5f3d1de393ddcf6bdddb",
            "isKey": true,
            "numCitedBy": 181,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "We explore the performance of several types of language mode ls on the word-level and the character-level language modelin g tasks. This includes two recently proposed recurrent neural netwo rk architectures, a feedforward neural network model, a maximum ent ropy model and the usual smoothed n-gram models. We then propose a simple technique for learning sub-word level units from th e data, and show that it combines advantages of both character and wo rdlevel models. Finally, we show that neural network based lan gu ge models can be order of magnitude smaller than compressed n-g ram models, at the same level of performance when applied to a Bro dcast news RT04 speech recognition task. By using sub-word un its, the size can be reduced even more."
            },
            "slug": "SUBWORD-LANGUAGE-MODELING-WITH-NEURAL-NETWORKS-Mikolov-Sutskever",
            "title": {
                "fragments": [],
                "text": "SUBWORD LANGUAGE MODELING WITH NEURAL NETWORKS"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A simple technique for learning sub-word level units from data is proposed, and it is shown that neural network based models can be order of magnitude smaller than compressed n-g ram models, at the same level of performance when applied to a Bro dcast news RT04 speech recognition task."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145704247"
                        ],
                        "name": "James Martens",
                        "slug": "James-Martens",
                        "structuredName": {
                            "firstName": "James",
                            "lastName": "Martens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "James Martens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 77
                            }
                        ],
                        "text": "This element-wise operation also has similarities to the Multiplicative RNN (Sutskever et al., 2011) and Multiplicative Integration RNN (Wu et al., 2016) where it was demonstrated that the multiplication-operation encouraged better gradient flow."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 7
                            }
                        ],
                        "text": "The amount of memory required in the system described in Equation 5 will be Nz times the memory of a Basic RNN, which limits the number of hidden units we can use in many practical applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8843166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0e5dd8b206806372b3e20b9a2fbdbd0cf9ce1de",
            "isKey": false,
            "numCitedBy": 1267,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent Neural Networks (RNNs) are very powerful sequence models that do not enjoy widespread use because it is extremely difficult to train them properly. Fortunately, recent advances in Hessian-free optimization have been able to overcome the difficulties associated with training RNNs, making it possible to apply them successfully to challenging sequence problems. In this paper we demonstrate the power of RNNs trained with the new Hessian-Free optimizer (HF) by applying them to character-level language modeling tasks. The standard RNN architecture, while effective, is not ideally suited for such tasks, so we introduce a new RNN variant that uses multiplicative (or \"gated\") connections which allow the current input character to determine the transition matrix from one hidden state vector to the next. After training the multiplicative RNN with the HF optimizer for five days on 8 high-end Graphics Processing Units, we were able to surpass the performance of the best previous single method for character-level language modeling \u2013 a hierarchical non-parametric sequence model. To our knowledge this represents the largest recurrent neural network application to date."
            },
            "slug": "Generating-Text-with-Recurrent-Neural-Networks-Sutskever-Martens",
            "title": {
                "fragments": [],
                "text": "Generating Text with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The power of RNNs trained with the new Hessian-Free optimizer by applying them to character-level language modeling tasks is demonstrated, and a new RNN variant that uses multiplicative connections which allow the current input character to determine the transition matrix from one hidden state vector to the next is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360589"
                        ],
                        "name": "Julian G. Zilly",
                        "slug": "Julian-G.-Zilly",
                        "structuredName": {
                            "firstName": "Julian",
                            "lastName": "Zilly",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Julian G. Zilly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2100612"
                        ],
                        "name": "R. Srivastava",
                        "slug": "R.-Srivastava",
                        "structuredName": {
                            "firstName": "Rupesh",
                            "lastName": "Srivastava",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2865775"
                        ],
                        "name": "J. Koutn\u00edk",
                        "slug": "J.-Koutn\u00edk",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Koutn\u00edk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koutn\u00edk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1101453,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dba53e72c182e25e98e8f73a99d75ff69dda0c2",
            "isKey": false,
            "numCitedBy": 351,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "Many sequential processing tasks require complex nonlinear transition functions from one step to the next. However, recurrent neural networks with 'deep' transition functions remain difficult to train, even when using Long Short-Term Memory (LSTM) networks. We introduce a novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem that illuminates several modeling and optimization issues and improves our understanding of the LSTM cell. Based on this analysis we propose Recurrent Highway Networks, which extend the LSTM architecture to allow step-to-step transition depths larger than one. Several language modeling experiments demonstrate that the proposed architecture results in powerful and efficient models. On the Penn Treebank corpus, solely increasing the transition depth from 1 to 10 improves word-level perplexity from 90.6 to 65.4 using the same number of parameters. On the larger Wikipedia datasets for character prediction (text8 and enwik8), RHNs outperform all previous results and achieve an entropy of 1.27 bits per character."
            },
            "slug": "Recurrent-Highway-Networks-Zilly-Srivastava",
            "title": {
                "fragments": [],
                "text": "Recurrent Highway Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A novel theoretical analysis of recurrent networks based on Gersgorin's circle theorem is introduced that illuminates several modeling and optimization issues and improves the understanding of the LSTM cell."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3374063"
                        ],
                        "name": "Yuhuai Wu",
                        "slug": "Yuhuai-Wu",
                        "structuredName": {
                            "firstName": "Yuhuai",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuhuai Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35097114"
                        ],
                        "name": "Saizheng Zhang",
                        "slug": "Saizheng-Zhang",
                        "structuredName": {
                            "firstName": "Saizheng",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saizheng Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48379623"
                        ],
                        "name": "Y. Zhang",
                        "slug": "Y.-Zhang",
                        "structuredName": {
                            "firstName": "Y.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 18
                            }
                        ],
                        "text": "We also find HyperLSTM\u2019s samples to be a bit more coherent than the samples produced by Layer Norm LSTM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 116
                            }
                        ],
                        "text": "We benchmark the modified model on WMT\u201914 En\u2192Fr using the same test/validation set split described in the GNMT paper (Wu et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 38
                            }
                        ],
                        "text": "027 GNMT WPM-32K, ensemble of 8 LSTMs (Wu et al., 2016) 40."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 43
                            }
                        ],
                        "text": ", 2011) and Multiplicative Integration RNN (Wu et al., 2016) where it was demonstrated that the multiplication-operation encouraged better gradient flow."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 102
                            }
                        ],
                        "text": "We experiment with the Neural Machine Translation task using the same experimental setup outlined in (Wu et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 137
                            }
                        ],
                        "text": "This element-wise operation also has similarities to the Multiplicative RNN (Sutskever et al., 2011) and Multiplicative Integration RNN (Wu et al., 2016) where it was demonstrated that the multiplication-operation encouraged better gradient flow."
                    },
                    "intents": []
                }
            ],
            "corpusId": 7386147,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "136cf66392f1d6bf42da4cc070888996dc472b91",
            "isKey": true,
            "numCitedBy": 131,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce a general and simple structural design called Multiplicative Integration (MI) to improve recurrent neural networks (RNNs). MI changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters. The new structure can be easily embedded into many popular RNN models, including LSTMs and GRUs. We empirically analyze its learning behaviour and conduct evaluations on several tasks using different RNN models. Our experimental results demonstrate that Multiplicative Integration can provide a substantial performance boost over many of the existing RNN models."
            },
            "slug": "On-Multiplicative-Integration-with-Recurrent-Neural-Wu-Zhang",
            "title": {
                "fragments": [],
                "text": "On Multiplicative Integration with Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "This work introduces a general and simple structural design called Multiplicative Integration, which changes the way in which information from difference sources flows and is integrated in the computational building block of an RNN, while introducing almost no extra parameters."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2583391"
                        ],
                        "name": "Nal Kalchbrenner",
                        "slug": "Nal-Kalchbrenner",
                        "structuredName": {
                            "firstName": "Nal",
                            "lastName": "Kalchbrenner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nal Kalchbrenner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1841008"
                        ],
                        "name": "Ivo Danihelka",
                        "slug": "Ivo-Danihelka",
                        "structuredName": {
                            "firstName": "Ivo",
                            "lastName": "Danihelka",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ivo Danihelka"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7823468,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5b791cd374c7109693aaddee2c12d659ae4e3ec0",
            "isKey": false,
            "numCitedBy": 315,
            "numCiting": 52,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper introduces Grid Long Short-Term Memory, a network of LSTM cells arranged in a multidimensional grid that can be applied to vectors, sequences or higher dimensional data such as images. The network differs from existing deep LSTM architectures in that the cells are connected between network layers as well as along the spatiotemporal dimensions of the data. The network provides a unified way of using LSTM for both deep and sequential computation. We apply the model to algorithmic tasks such as 15-digit integer addition and sequence memorization, where it is able to significantly outperform the standard LSTM. We then give results for two empirical tasks. We find that 2D Grid LSTM achieves 1.47 bits per character on the Wikipedia character prediction benchmark, which is state-of-the-art among neural approaches. In addition, we use the Grid LSTM to define a novel two-dimensional translation model, the Reencoder, and show that it outperforms a phrase-based reference system on a Chinese-to-English translation task."
            },
            "slug": "Grid-Long-Short-Term-Memory-Kalchbrenner-Danihelka",
            "title": {
                "fragments": [],
                "text": "Grid Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Grid LSTM is used to define a novel two-dimensional translation model, the Reencoder, and it is shown that it outperforms a phrase-based reference system on a Chinese-to-English translation task."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2348758"
                        ],
                        "name": "Tim Cooijmans",
                        "slug": "Tim-Cooijmans",
                        "structuredName": {
                            "firstName": "Tim",
                            "lastName": "Cooijmans",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tim Cooijmans"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482072"
                        ],
                        "name": "Nicolas Ballas",
                        "slug": "Nicolas-Ballas",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Ballas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Ballas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40201308"
                        ],
                        "name": "C\u00e9sar Laurent",
                        "slug": "C\u00e9sar-Laurent",
                        "structuredName": {
                            "firstName": "C\u00e9sar",
                            "lastName": "Laurent",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C\u00e9sar Laurent"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1760871"
                        ],
                        "name": "Aaron C. Courville",
                        "slug": "Aaron-C.-Courville",
                        "structuredName": {
                            "firstName": "Aaron",
                            "lastName": "Courville",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aaron C. Courville"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 115,
                                "start": 93
                            }
                        ],
                        "text": "For the weight scaling vectors, we used a method described in Recurrent Batch Normalization (Cooijmans et al., 2016) where the scaling vectors are initialized to 0.1 rather than 1.0 and this has shown to help gradient flow."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 250,
                                "start": 228
                            }
                        ],
                        "text": "\u2026version of the setup of Equation 5:\nht = \u03c6 ( dh(zh) Whht\u22121 + dx(zx) Wxxt + b(zb) ) , where\ndh(zh) =Whzzh\ndx(zx) =Wxzzx b(zb) =Wbzzb + b0\n(8)\nThis formulation of the HyperRNN has some similarities to Recurrent Batch Normalization (Cooijmans et al., 2016) and Layer Normalization (Ba et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026already does a formidable job of generating time-varying parameters of a Mixture Gaussian distribution used to generate realistic handwriting samples, the ability to go one level deeper, and to dynamically generate the generative model is one of the key advantages of HyperRNN over a normal RNN."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 260,
                                "start": 231
                            }
                        ],
                        "text": "Below is the more memory efficient version of the setup of Equation 5:\nht = \u03c6 ( dh(zh) Whht\u22121 + dx(zx) Wxxt + b(zb) ) , where\ndh(zh) =Whzzh\ndx(zx) =Wxzzx b(zb) =Wbzzb + b0\n(8)\nThis formulation of the HyperRNN has some similarities to Recurrent Batch Normalization (Cooijmans et al., 2016) and Layer Normalization (Ba et al., 2016)."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1107124,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "952454718139dba3aafc6b3b67c4f514ac3964af",
            "isKey": true,
            "numCitedBy": 345,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a reparameterization of LSTM that brings the benefits of batch normalization to recurrent neural networks. Whereas previous works only apply batch normalization to the input-to-hidden transformation of RNNs, we demonstrate that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps. We evaluate our proposal on various sequential problems such as sequence classification, language modeling and question answering. Our empirical results show that our batch-normalized LSTM consistently leads to faster convergence and improved generalization."
            },
            "slug": "Recurrent-Batch-Normalization-Cooijmans-Ballas",
            "title": {
                "fragments": [],
                "text": "Recurrent Batch Normalization"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "It is demonstrated that it is both possible and beneficial to batch-normalize the hidden-to-hidden transition, thereby reducing internal covariate shift between time steps."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39713408"
                        ],
                        "name": "Mikael Henaff",
                        "slug": "Mikael-Henaff",
                        "structuredName": {
                            "firstName": "Mikael",
                            "lastName": "Henaff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mikael Henaff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3149531"
                        ],
                        "name": "Arthur D. Szlam",
                        "slug": "Arthur-D.-Szlam",
                        "structuredName": {
                            "firstName": "Arthur",
                            "lastName": "Szlam",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Arthur D. Szlam"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 181,
                                "start": 162
                            }
                        ],
                        "text": "Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al., 2016) with the same dropout probability."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 81
                            }
                        ],
                        "text": "The biases in Equation 10 are initialized to zero and Orthogonal Initialization (Henaff et al., 2016) is performed for all weights."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 43118,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4ca151307e3be93c6cd14ed403f6162892e7fbed",
            "isKey": false,
            "numCitedBy": 40,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Although RNNs have been shown to be powerful tools for processing sequential data, finding architectures or optimization strategies that allow them to model very long term dependencies is still an active area of research. In this work, we carefully analyze two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps. We explicitly construct RNN solutions to these problems, and using these constructions, illuminate both the problems themselves and the way in which RNNs store different types of information in their hidden states. These constructions furthermore explain the success of recent methods that specify unitary initializations or constraints on the transition matrices."
            },
            "slug": "Orthogonal-RNNs-and-Long-Memory-Tasks-Henaff-Szlam",
            "title": {
                "fragments": [],
                "text": "Orthogonal RNNs and Long-Memory Tasks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work carefully analyzes two synthetic datasets originally outlined in (Hochreiter and Schmidhuber, 1997) which are used to evaluate the ability of RNNs to store information over many time steps and explicitly construct RNN solutions to these problems."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1700431"
                        ],
                        "name": "Stanislau Semeniuta",
                        "slug": "Stanislau-Semeniuta",
                        "structuredName": {
                            "firstName": "Stanislau",
                            "lastName": "Semeniuta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stanislau Semeniuta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3091861"
                        ],
                        "name": "Aliaksei Severyn",
                        "slug": "Aliaksei-Severyn",
                        "structuredName": {
                            "firstName": "Aliaksei",
                            "lastName": "Severyn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aliaksei Severyn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1839848"
                        ],
                        "name": "E. Barth",
                        "slug": "E.-Barth",
                        "structuredName": {
                            "firstName": "Erhardt",
                            "lastName": "Barth",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Barth"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 124
                            }
                        ],
                        "text": "The only place we use dropout is in the single location in Equation 13, developed in Recurrent Dropout without Memory Loss (Semeniuta et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 94,
                                "start": 72
                            }
                        ],
                        "text": "We also find HyperLSTM\u2019s samples to be a bit more coherent than the samples produced by Layer Norm LSTM."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026already does a formidable job of generating time-varying parameters of a Mixture Gaussian distribution used to generate realistic handwriting samples, the ability to go one level deeper, and to dynamically generate the generative model is one of the key advantages of HyperRNN over a normal RNN."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the following experiments, we will benchmark the performance of static hypernetworks on image recognition with MNIST and CIFAR-10, and the performance of dynamic hypernetworks on language modelling with Penn Treebank and Hutter Prize Wikipedia (enwik8) datasets and handwriting generation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 147
                            }
                        ],
                        "text": "\u2026state update equations for the main LSTM will incorporate a single dropout (Hinton et al., 2012) gate, as developed in Recurrent Dropout without Memory Loss (Semeniuta et al., 2016), as we found this to help regularize the entire model during training:\nct = \u03c3(ft) ct\u22121 + \u03c3(it) DropOut(\u03c6(gt)) ht\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 679,
                                "start": 668
                            }
                        ],
                        "text": "For brevity, y is one of {i, g, f, o} to avoid writing four sets of identical equations:\nzyh =W y h\u0302h h\u0302t\u22121 + b y h\u0302h zyx =W y\nh\u0302x h\u0302t\u22121 + b\ny\nh\u0302x\nzyb =W y h\u0302b h\u0302t\u22121\n(11)\nAs in the memory efficient version of the HyperRNN, we will focus on the efficient version of the HyperLSTM, where we use weight scaling vectors d to modify the rows of the weight matrices:\nyt = LN ( dyh W y hht\u22121 + d y x W yxxt + by(zyb ) ) , where\ndyh(zh) =W y hzzh dyx(zx) =W y xzzx by(zyb ) =W y bzz y b + b y 0\n(12)\nIn our implementation, the cell and hidden state update equations for the main LSTM will incorporate a single dropout (Hinton et al., 2012) gate, as developed in Recurrent Dropout without Memory Loss (Semeniuta et al., 2016), as we found this to help regularize the entire model during training:\nct = \u03c3(ft) ct\u22121 + \u03c3(it) DropOut(\u03c6(gt)) ht = \u03c3(ot) \u03c6(LN(ct))\n(13)\n1In practice, all eight weight matrices are concatenated into one large matrix for computational efficiency."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 87
                            }
                        ],
                        "text": "Our implementation of Recurrent Dropout Basic LSTM cell reproduced similar results as (Semeniuta et al., 2016), where they have also experimented with different dropout settings."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1707814,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf76789618f5db929393c1187514ce6c3502c3cd",
            "isKey": true,
            "numCitedBy": 178,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a novel approach to recurrent neural network (RNN) regularization. Differently from the widely adopted dropout method, which is applied to forward connections of feedforward architectures or RNNs, we propose to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory. Our approach is as easy to implement and apply as the regular feed-forward dropout and we demonstrate its effectiveness for the most effective modern recurrent network \u2013 Long Short-Term Memory network. Our experiments on three NLP benchmarks show consistent improvements even when combined with conventional feed-forward dropout."
            },
            "slug": "Recurrent-Dropout-without-Memory-Loss-Semeniuta-Severyn",
            "title": {
                "fragments": [],
                "text": "Recurrent Dropout without Memory Loss"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper proposes to drop neurons directly in recurrent connections in a way that does not cause loss of long-term memory, and demonstrates its effectiveness for the most effective modern recurrent network \u2013 Long Short-Term Memory network."
            },
            "venue": {
                "fragments": [],
                "text": "COLING"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8270717"
                        ],
                        "name": "Junyoung Chung",
                        "slug": "Junyoung-Chung",
                        "structuredName": {
                            "firstName": "Junyoung",
                            "lastName": "Chung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyoung Chung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3103594"
                        ],
                        "name": "Sungjin Ahn",
                        "slug": "Sungjin-Ahn",
                        "structuredName": {
                            "firstName": "Sungjin",
                            "lastName": "Ahn",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sungjin Ahn"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1463401,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "65eee67dee969fdf8b44c87c560d66ad4d78e233",
            "isKey": false,
            "numCitedBy": 443,
            "numCiting": 91,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning both hierarchical and temporal representation has been among the long-standing challenges of recurrent neural networks. Multiscale recurrent neural networks have been considered as a promising approach to resolve this issue, yet there has been a lack of empirical evidence showing that this type of models can actually capture the temporal dependencies by discovering the latent hierarchical structure of the sequence. In this paper, we propose a novel multiscale approach, called the hierarchical multiscale recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism. We show some evidence that our proposed multiscale architecture can discover underlying hierarchical structure in the sequences without using explicit boundary information. We evaluate our proposed model on character-level language modelling and handwriting sequence modelling."
            },
            "slug": "Hierarchical-Multiscale-Recurrent-Neural-Networks-Chung-Ahn",
            "title": {
                "fragments": [],
                "text": "Hierarchical Multiscale Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "A novel multiscale approach, called the hierarchical multiscales recurrent neural networks, which can capture the latent hierarchical structure in the sequence by encoding the temporal dependencies with different timescales using a novel update mechanism is proposed."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1641711590"
                        ],
                        "name": "Xu Jia",
                        "slug": "Xu-Jia",
                        "structuredName": {
                            "firstName": "Xu",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xu Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3384995"
                        ],
                        "name": "Bert De Brabandere",
                        "slug": "Bert-De-Brabandere",
                        "structuredName": {
                            "firstName": "Bert",
                            "lastName": "Brabandere",
                            "middleNames": [
                                "De"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bert De Brabandere"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1704728"
                        ],
                        "name": "T. Tuytelaars",
                        "slug": "T.-Tuytelaars",
                        "structuredName": {
                            "firstName": "Tinne",
                            "lastName": "Tuytelaars",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Tuytelaars"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1681236"
                        ],
                        "name": "L. Gool",
                        "slug": "L.-Gool",
                        "structuredName": {
                            "firstName": "Luc",
                            "lastName": "Gool",
                            "middleNames": [
                                "Van"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Gool"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 207,
                                "start": 184
                            }
                        ],
                        "text": "\u2026with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are predicted by another network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2097418,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "aba48504f4f9563eafa44e0cfb22e1345d767c80",
            "isKey": false,
            "numCitedBy": 667,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "In a traditional convolutional layer, the learned filters stay fixed after training. In contrast, we introduce a new framework, the Dynamic Filter Network, where filters are generated dynamically conditioned on an input. We show that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters. A wide variety of filtering operation can be learned this way, including local spatial transformations, but also others like selective (de)blurring or adaptive feature extraction. Moreover, multiple such layers can be combined, e.g. in a recurrent architecture. We demonstrate the effectiveness of the dynamic filter network on the tasks of video and stereo prediction, and reach state-of-the-art performance on the moving MNIST dataset with a much smaller model. By visualizing the learned filters, we illustrate that the network has picked up flow information by only looking at unlabelled training data. This suggests that the network can be used to pretrain networks for various supervised tasks in an unsupervised way, like optical flow and depth estimation."
            },
            "slug": "Dynamic-Filter-Networks-Jia-Brabandere",
            "title": {
                "fragments": [],
                "text": "Dynamic Filter Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "The Dynamic Filter Network is introduced, where filters are generated dynamically conditioned on an input, and it is shown that this architecture is a powerful one, with increased flexibility thanks to its adaptive nature, yet without an excessive increase in the number of model parameters."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 73
                            }
                        ],
                        "text": "This is indeed the case in the residual network family of architectures (He et al., 2016a) that we will be experimenting with later is an example of such a design."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 94
                            }
                        ],
                        "text": "While residual networks can be be very deep, and in some experiments as deep as 1001 layers ((He et al., 2016b), it is important to understand whether some these layers share common properties and can be reduced effectively by introducing weight sharing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 36
                            }
                        ],
                        "text": "The residual network architectures (He et al., 2016a; Zagoruyko & Komodakis, 2016) are popular for image recognition tasks, as they can accommodate very deep networks while maintaining effective gradient flow across layers using skip connections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the following experiments, we will benchmark the performance of static hypernetworks on image recognition with MNIST and CIFAR-10, and the performance of dynamic hypernetworks on language modelling with Penn Treebank and Hutter Prize Wikipedia (enwik8) datasets and handwriting generation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 6447277,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "77f0a39b8e02686fd85b01971f8feb7f60971f80",
            "isKey": true,
            "numCitedBy": 6555,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep residual networks have emerged as a family of extremely deep architectures showing compelling accuracy and nice convergence behaviors. In this paper, we analyze the propagation formulations behind the residual building blocks, which suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation. A series of ablation experiments support the importance of these identity mappings. This motivates us to propose a new residual unit, which makes training easier and improves generalization. We report improved results using a 1001-layer ResNet on CIFAR-10 (4.62 % error) and CIFAR-100, and a 200-layer ResNet on ImageNet. Code is available at: https://github.com/KaimingHe/resnet-1k-layers."
            },
            "slug": "Identity-Mappings-in-Deep-Residual-Networks-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Identity Mappings in Deep Residual Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "The propagation formulations behind the residual building blocks suggest that the forward and backward signals can be directly propagated from one block to any other block, when using identity mappings as the skip connections and after-addition activation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8270717"
                        ],
                        "name": "Junyoung Chung",
                        "slug": "Junyoung-Chung",
                        "structuredName": {
                            "firstName": "Junyoung",
                            "lastName": "Chung",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Junyoung Chung"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1854385"
                        ],
                        "name": "\u00c7aglar G\u00fcl\u00e7ehre",
                        "slug": "\u00c7aglar-G\u00fcl\u00e7ehre",
                        "structuredName": {
                            "firstName": "\u00c7aglar",
                            "lastName": "G\u00fcl\u00e7ehre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "\u00c7aglar G\u00fcl\u00e7ehre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1979489"
                        ],
                        "name": "Kyunghyun Cho",
                        "slug": "Kyunghyun-Cho",
                        "structuredName": {
                            "firstName": "Kyunghyun",
                            "lastName": "Cho",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kyunghyun Cho"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026already does a formidable job of generating time-varying parameters of a Mixture Gaussian distribution used to generate realistic handwriting samples, the ability to go one level deeper, and to dynamically generate the generative model is one of the key advantages of HyperRNN over a normal RNN."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8577750,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d14c7e5f5cace4c925abc74c88baa474e9f31a28",
            "isKey": false,
            "numCitedBy": 654,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work, we propose a novel recurrent neural network (RNN) architecture. The proposed RNN, gated-feedback RNN (GF-RNN), extends the existing approach of stacking multiple recurrent layers by allowing and controlling signals flowing from upper recurrent layers to lower layers using a global gating unit for each pair of layers. The recurrent signals exchanged between layers are gated adaptively based on the previous hidden states and the current input. We evaluated the proposed GF-RNN with different types of recurrent units, such as tanh, long short-term memory and gated recurrent units, on the tasks of character-level language modeling and Python program evaluation. Our empirical evaluation of different RNN units, revealed that in both tasks, the GF-RNN outperforms the conventional approaches to build deep stacked RNNs. We suggest that the improvement arises because the GFRNN can adaptively assign different layers to different timescales and layer-to-layer interactions (including the top-down ones which are not usually present in a stacked RNN) by learning to gate these interactions."
            },
            "slug": "Gated-Feedback-Recurrent-Neural-Networks-Chung-G\u00fcl\u00e7ehre",
            "title": {
                "fragments": [],
                "text": "Gated Feedback Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "The empirical evaluation of different RNN units revealed that the proposed gated-feedback RNN outperforms the conventional approaches to build deep stacked RNNs in the tasks of character-level language modeling and Python program evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2069136633"
                        ],
                        "name": "Adriana Romero",
                        "slug": "Adriana-Romero",
                        "structuredName": {
                            "firstName": "Adriana",
                            "lastName": "Romero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Adriana Romero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2482072"
                        ],
                        "name": "Nicolas Ballas",
                        "slug": "Nicolas-Ballas",
                        "structuredName": {
                            "firstName": "Nicolas",
                            "lastName": "Ballas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nicolas Ballas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3127597"
                        ],
                        "name": "S. Kahou",
                        "slug": "S.-Kahou",
                        "structuredName": {
                            "firstName": "Samira",
                            "lastName": "Kahou",
                            "middleNames": [
                                "Ebrahimi"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Kahou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3186079"
                        ],
                        "name": "Antoine Chassang",
                        "slug": "Antoine-Chassang",
                        "structuredName": {
                            "firstName": "Antoine",
                            "lastName": "Chassang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Antoine Chassang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143706039"
                        ],
                        "name": "C. Gatta",
                        "slug": "C.-Gatta",
                        "structuredName": {
                            "firstName": "Carlo",
                            "lastName": "Gatta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Gatta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2723173,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd85a549add0c7c7def36aca29837efd24b24080",
            "isKey": false,
            "numCitedBy": 2037,
            "numCiting": 47,
            "paperAbstract": {
                "fragments": [],
                "text": "While depth tends to improve network performances, it also makes gradient-based training more difficult since deeper networks tend to be more non-linear. The recently proposed knowledge distillation approach is aimed at obtaining small and fast-to-execute models, and it has shown that a student network could imitate the soft output of a larger teacher network or ensemble of networks. In this paper, we extend this idea to allow the training of a student that is deeper and thinner than the teacher, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student. Because the student intermediate hidden layer will generally be smaller than the teacher's intermediate hidden layer, additional parameters are introduced to map the student hidden layer to the prediction of the teacher hidden layer. This allows one to train deeper students that can generalize better or run faster, a trade-off that is controlled by the chosen student capacity. For example, on CIFAR-10, a deep student network with almost 10.4 times less parameters outperforms a larger, state-of-the-art teacher network."
            },
            "slug": "FitNets:-Hints-for-Thin-Deep-Nets-Romero-Ballas",
            "title": {
                "fragments": [],
                "text": "FitNets: Hints for Thin Deep Nets"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper extends the idea of a student network that could imitate the soft output of a larger teacher network or ensemble of networks, using not only the outputs but also the intermediate representations learned by the teacher as hints to improve the training process and final performance of the student."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846883"
                        ],
                        "name": "Kenneth O. Stanley",
                        "slug": "Kenneth-O.-Stanley",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Stanley",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth O. Stanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1401079497"
                        ],
                        "name": "David B. D'Ambrosio",
                        "slug": "David-B.-D'Ambrosio",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "D'Ambrosio",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David B. D'Ambrosio"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3351507"
                        ],
                        "name": "J. Gauci",
                        "slug": "J.-Gauci",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Gauci",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Gauci"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 11
                            }
                        ],
                        "text": "HyperNEAT (Stanley et al., 2009) is an example of hypernetworks where the inputs are a set of virtual coordinates for each weight in the main network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 69
                            }
                        ],
                        "text": "An instance of this approach is the work on the HyperNEAT framework (Stanley et al., 2009)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 26390526,
            "fieldsOfStudy": [
                "Biology",
                "Computer Science"
            ],
            "id": "6e744cf0273a84b087e94191fd654210e8fec8e9",
            "isKey": false,
            "numCitedBy": 702,
            "numCiting": 72,
            "paperAbstract": {
                "fragments": [],
                "text": "Research in neuroevolutionthat is, evolving artificial neural networks (ANNs) through evolutionary algorithmsis inspired by the evolution of biological brains, which can contain trillions of connections. Yet while neuroevolution has produced successful results, the scale of natural brains remains far beyond reach. This article presents a method called hypercube-based NeuroEvolution of Augmenting Topologies (HyperNEAT) that aims to narrow this gap. HyperNEAT employs an indirect encoding called connective compositional pattern-producing networks (CPPNs) that can produce connectivity patterns with symmetries and repeating motifs by interpreting spatial patterns generated within a hypercube as connectivity patterns in a lower-dimensional space. This approach can exploit the geometry of the task by mapping its regularities onto the topology of the network, thereby shifting problem difficulty away from dimensionality to the underlying problem structure. Furthermore, connective CPPNs can represent the same connectivity pattern at any resolution, allowing ANNs to scale to new numbers of inputs and outputs without further evolution. HyperNEAT is demonstrated through visual discrimination and food-gathering tasks, including successful visual discrimination networks containing over eight million connections. The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution."
            },
            "slug": "A-Hypercube-Based-Encoding-for-Evolving-Large-Scale-Stanley-D'Ambrosio",
            "title": {
                "fragments": [],
                "text": "A Hypercube-Based Encoding for Evolving Large-Scale Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The main conclusion is that the ability to explore the space of regular connectivity patterns opens up a new class of complex high-dimensional tasks to neuroevolution."
            },
            "venue": {
                "fragments": [],
                "text": "Artificial Life"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 43
                            }
                        ],
                        "text": "Even before the work on HyperNEAT and DCT, Schmidhuber (1992; 1993) has suggested the concept of fast weights in which one network can produce context-dependent weight changes for a second network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16683347,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bc22e87a26d020215afe91c751e5bdaddd8e4922",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 22,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous algorithms for supervised sequence learning are based on dynamic recurrent networks. This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: The first net learns to produce context-dependent weight changes for the second net whose weights may vary very quickly. The method offers the potential for STM storage efficiency: A single weight (instead of a full-fledged unit) may be sufficient for storing temporal information. Various learning methods are derived. Two experiments with unknown time delays illustrate the approach. One experiment shows how the system can be used for adaptive temporary variable binding."
            },
            "slug": "Learning-to-Control-Fast-Weight-Memories:-An-to-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning to Control Fast-Weight Memories: An Alternative to Dynamic Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "This paper describes an alternative class of gradient-based systems consisting of two feedforward nets that learn to deal with temporal sequences using fast weights: the first net learns to produce context-dependent weight changes for the second net whose weights may vary very quickly."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 149
                            }
                        ],
                        "text": "We will train our model on the IAM online handwriting database (Liwicki & Bunke, 2005) and have our model predict pen strokes as per Section 4.2 of (Graves, 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026already does a formidable job of generating time-varying parameters of a Mixture Gaussian distribution used to generate realistic handwriting samples, the ability to go one level deeper, and to dynamically generate the generative model is one of the key advantages of HyperRNN over a normal RNN."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 29
                            }
                        ],
                        "text": "Now the question is whether we can also train a deep convolutional network, using a single hypernetwork generating a set of weights for each layer, on a dataset more challenging than MNIST."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 20
                            }
                        ],
                        "text": "Using the setup in (Graves, 2013), we use networks with 1000 units and train the network to predict the next character."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 65
                            }
                        ],
                        "text": "We compare our model to the basic LSTM cell, stacked LSTM cells (Graves, 2013), and LSTM with layer normalization applied."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 97
                            }
                        ],
                        "text": "The policy learned by the HyperLSTM cell not only per-\n1Our implementation, to replicate setup of (Graves, 2013)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the following experiments, we will benchmark the performance of static hypernetworks on image recognition with MNIST and CIFAR-10, and the performance of dynamic hypernetworks on language modelling with Penn Treebank and Hutter Prize Wikipedia (enwik8) datasets and handwriting generation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 40,
                                "start": 28
                            }
                        ],
                        "text": "Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al., 2016) with the same dropout probability."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 38
                            }
                        ],
                        "text": "We start by applying a hypernetwork to generate the filters for a convolutional network on MNIST."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 163,
                                "start": 151
                            }
                        ],
                        "text": "In this task, we note that data augmentation and applying recurrent dropout improved the performance of all models, compared to the original setup by (Graves, 2013)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 1697424,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89b1f4740ae37fd04f6ac007577bdd34621f0861",
            "isKey": true,
            "numCitedBy": 3176,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time. The approach is demonstrated for text (where the data are discrete) and online handwriting (where the data are real-valued). It is then extended to handwriting synthesis by allowing the network to condition its predictions on a text sequence. The resulting system is able to generate highly realistic cursive handwriting in a wide variety of styles."
            },
            "slug": "Generating-Sequences-With-Recurrent-Neural-Networks-Graves",
            "title": {
                "fragments": [],
                "text": "Generating Sequences With Recurrent Neural Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper shows how Long Short-term Memory recurrent neural networks can be used to generate complex sequences with long-range structure, simply by predicting one data point at a time."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2115913164"
                        ],
                        "name": "Min Lin",
                        "slug": "Min-Lin",
                        "structuredName": {
                            "firstName": "Min",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min Lin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35370244"
                        ],
                        "name": "Qiang Chen",
                        "slug": "Qiang-Chen",
                        "structuredName": {
                            "firstName": "Qiang",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qiang Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143653681"
                        ],
                        "name": "Shuicheng Yan",
                        "slug": "Shuicheng-Yan",
                        "structuredName": {
                            "firstName": "Shuicheng",
                            "lastName": "Yan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuicheng Yan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 16636683,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5e83ab70d0cbc003471e87ec306d27d9c80ecb16",
            "isKey": false,
            "numCitedBy": 4255,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a novel deep network structure called \"Network In Network\" (NIN) to enhance model discriminability for local patches within the receptive field. The conventional convolutional layer uses linear filters followed by a nonlinear activation function to scan the input. Instead, we build micro neural networks with more complex structures to abstract the data within the receptive field. We instantiate the micro neural network with a multilayer perceptron, which is a potent function approximator. The feature maps are obtained by sliding the micro networks over the input in a similar manner as CNN; they are then fed into the next layer. Deep NIN can be implemented by stacking mutiple of the above described structure. With enhanced local modeling via the micro network, we are able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers. We demonstrated the state-of-the-art classification performances with NIN on CIFAR-10 and CIFAR-100, and reasonable performances on SVHN and MNIST datasets."
            },
            "slug": "Network-In-Network-Lin-Chen",
            "title": {
                "fragments": [],
                "text": "Network In Network"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "With enhanced local modeling via the micro network, the proposed deep network structure NIN is able to utilize global average pooling over feature maps in the classification layer, which is easier to interpret and less prone to overfitting than traditional fully connected layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2134433"
                        ],
                        "name": "Sergey Zagoruyko",
                        "slug": "Sergey-Zagoruyko",
                        "structuredName": {
                            "firstName": "Sergey",
                            "lastName": "Zagoruyko",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergey Zagoruyko"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2505902"
                        ],
                        "name": "N. Komodakis",
                        "slug": "N.-Komodakis",
                        "structuredName": {
                            "firstName": "Nikos",
                            "lastName": "Komodakis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Komodakis"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 64
                            }
                        ],
                        "text": "Our experiment will use a version of the wide residual network (Zagoruyko & Komodakis, 2016), described in Table 1, a popular and simple variant of the family of residual network architectures, and we will focus configurations (N = 6,K = 1) and (N = 6,K = 2), referred to as WRN 40-1 and WRN 40-2\u2026"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 54
                            }
                        ],
                        "text": "The residual network architectures (He et al., 2016a; Zagoruyko & Komodakis, 2016) are popular for image recognition tasks, as they can accommodate very deep networks while maintaining effective gradient flow across layers using skip connections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the following experiments, we will benchmark the performance of static hypernetworks on image recognition with MNIST and CIFAR-10, and the performance of dynamic hypernetworks on language modelling with Penn Treebank and Hutter Prize Wikipedia (enwik8) datasets and handwriting generation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 68
                            }
                        ],
                        "text": "We obtained similar classification accuracy numbers as reported in (Zagoruyko & Komodakis, 2016) with our own implementation."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 15276198,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1c4e9156ca07705531e45960b7a919dc473abb51",
            "isKey": true,
            "numCitedBy": 4354,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep residual networks were shown to be able to scale up to thousands of layers and still have improving performance. However, each fraction of a percent of improved accuracy costs nearly doubling the number of layers, and so training very deep residual networks has a problem of diminishing feature reuse, which makes these networks very slow to train. To tackle these problems, in this paper we conduct a detailed experimental study on the architecture of ResNet blocks, based on which we propose a novel architecture where we decrease depth and increase width of residual networks. We call the resulting network structures wide residual networks (WRNs) and show that these are far superior over their commonly used thin and very deep counterparts. For example, we demonstrate that even a simple 16-layer-deep wide residual network outperforms in accuracy and efficiency all previous deep residual networks, including thousand-layer-deep networks, achieving new state-of-the-art results on CIFAR, SVHN, COCO, and significant improvements on ImageNet. Our code and models are available at this https URL"
            },
            "slug": "Wide-Residual-Networks-Zagoruyko-Komodakis",
            "title": {
                "fragments": [],
                "text": "Wide Residual Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper conducts a detailed experimental study on the architecture of ResNet blocks and proposes a novel architecture where the depth and width of residual networks are decreased and the resulting network structures are called wide residual networks (WRNs), which are far superior over their commonly used thin and very deep counterparts."
            },
            "venue": {
                "fragments": [],
                "text": "BMVC"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144792148"
                        ],
                        "name": "Wojciech M. Czarnecki",
                        "slug": "Wojciech-M.-Czarnecki",
                        "structuredName": {
                            "firstName": "Wojciech",
                            "lastName": "Czarnecki",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wojciech M. Czarnecki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217144"
                        ],
                        "name": "Simon Osindero",
                        "slug": "Simon-Osindero",
                        "structuredName": {
                            "firstName": "Simon",
                            "lastName": "Osindero",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Simon Osindero"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145824029"
                        ],
                        "name": "David Silver",
                        "slug": "David-Silver",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Silver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Silver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2645384"
                        ],
                        "name": "K. Kavukcuoglu",
                        "slug": "K.-Kavukcuoglu",
                        "structuredName": {
                            "firstName": "Koray",
                            "lastName": "Kavukcuoglu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Kavukcuoglu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 135,
                                "start": 84
                            }
                        ],
                        "text": "The concept of a network interacting with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 107,
                                "start": 85
                            }
                        ],
                        "text": "The concept of a network interacting with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5615093,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27760cc69be4ce445962ccb270a02d1944a9d4c9",
            "isKey": false,
            "numCitedBy": 252,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Training directed neural networks typically requires forward-propagating data through a computation graph, followed by backpropagating error signal, to produce weight updates. All layers, or more generally, modules, of the network are therefore locked, in the sense that they must wait for the remainder of the network to execute forwards and propagate error backwards before they can be updated. In this work we break this constraint by decoupling modules by introducing a model of the future computation of the network graph. These models predict what the result of the modelled subgraph will produce using only local information. In particular we focus on modelling error gradients: by using the modelled synthetic gradient in place of true backpropagated error gradients we decouple subgraphs, and can update them independently and asynchronously i.e. we realise decoupled neural interfaces. We show results for feed-forward models, where every layer is trained asynchronously, recurrent neural networks (RNNs) where predicting one's future gradient extends the time over which the RNN can effectively model, and also a hierarchical RNN system with ticking at different timescales. Finally, we demonstrate that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation."
            },
            "slug": "Decoupled-Neural-Interfaces-using-Synthetic-Jaderberg-Czarnecki",
            "title": {
                "fragments": [],
                "text": "Decoupled Neural Interfaces using Synthetic Gradients"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is demonstrated that in addition to predicting gradients, the same framework can be used to predict inputs, resulting in models which are decoupled in both the forward and backwards pass -- amounting to independent networks which co-learn such that they can be composed into a single functioning corporation."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49178343"
                        ],
                        "name": "Jie Zhou",
                        "slug": "Jie-Zhou",
                        "structuredName": {
                            "firstName": "Jie",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jie Zhou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112866139"
                        ],
                        "name": "Ying Cao",
                        "slug": "Ying-Cao",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ying Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108084524"
                        ],
                        "name": "Xuguang Wang",
                        "slug": "Xuguang-Wang",
                        "structuredName": {
                            "firstName": "Xuguang",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xuguang Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144326610"
                        ],
                        "name": "Peng Li",
                        "slug": "Peng-Li",
                        "structuredName": {
                            "firstName": "Peng",
                            "lastName": "Li",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peng Li"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145738410"
                        ],
                        "name": "W. Xu",
                        "slug": "W.-Xu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Xu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Xu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 49
                            }
                        ],
                        "text": "Model Test BLEU Log Perplexity Deep-Att + PosUnk (Zhou et al., 2016) 39."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8586038,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b60abe57bc195616063be10638c6437358c81d1e",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural machine translation (NMT) aims at solving machine translation (MT) problems using neural networks and has exhibited promising results in recent years. However, most of the existing NMT models are shallow and there is still a performance gap between a single NMT model and the best conventional MT system. In this work, we introduce a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers. Fast-forward connections play an essential role in propagating the gradients and building a deep topology of depth 16. On the WMT\u201914 English-to-French task, we achieve BLEU=37.7 with a single attention model, which outperforms the corresponding single shallow model by 6.2 BLEU points. This is the first time that a single NMT model achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points. We can still achieve BLEU=36.3 even without using an attention mechanism. After special handling of unknown words and model ensembling, we obtain the best score reported to date on this task with BLEU=40.4. Our models are also validated on the more difficult WMT\u201914 English-to-German task."
            },
            "slug": "Deep-Recurrent-Models-with-Fast-Forward-Connections-Zhou-Cao",
            "title": {
                "fragments": [],
                "text": "Deep Recurrent Models with Fast-Forward Connections for Neural Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work introduces a new type of linear connections, named fast-forward connections, based on deep Long Short-Term Memory (LSTM) networks, and an interleaved bi-directional architecture for stacking the LSTM layers, and achieves state-of-the-art performance and outperforms the best conventional model by 0.7 BLEU points."
            },
            "venue": {
                "fragments": [],
                "text": "TACL"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2100612"
                        ],
                        "name": "R. Srivastava",
                        "slug": "R.-Srivastava",
                        "structuredName": {
                            "firstName": "Rupesh",
                            "lastName": "Srivastava",
                            "middleNames": [
                                "Kumar"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3035541"
                        ],
                        "name": "Klaus Greff",
                        "slug": "Klaus-Greff",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Greff",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klaus Greff"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2722012,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b92aa7024b87f50737b372e5df31ef091ab54e62",
            "isKey": false,
            "numCitedBy": 1317,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Theoretical and empirical evidence indicates that the depth of neural networks is crucial for their success. However, training becomes more difficult as depth increases, and training of very deep networks remains an open problem. Here we introduce a new architecture designed to overcome this. Our so-called highway networks allow unimpeded information flow across many layers on information highways. They are inspired by Long Short-Term Memory recurrent networks and use adaptive gating units to regulate the information flow. Even with hundreds of layers, highway networks can be trained directly through simple gradient descent. This enables the study of extremely deep and efficient architectures."
            },
            "slug": "Training-Very-Deep-Networks-Srivastava-Greff",
            "title": {
                "fragments": [],
                "text": "Training Very Deep Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A new architecture designed to overcome the challenges of training very deep networks, inspired by Long Short-Term Memory recurrent networks, which allows unimpeded information flow across many layers on information highways."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2865775"
                        ],
                        "name": "J. Koutn\u00edk",
                        "slug": "J.-Koutn\u00edk",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Koutn\u00edk",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Koutn\u00edk"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 225,
                                "start": 205
                            }
                        ],
                        "text": "Closely related to our approach is a simplified variation of HyperNEAT, where the structure is fixed and the weights are evolved through Discrete Cosine Transform (DCT) is called Compressed Weight Search (Koutnik et al., 2010)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 63
                            }
                        ],
                        "text": "First, it can be argued that Discrete Cosine Transform used in Compressed Weight Search may be too simple and using the DCT prior may not be suitable for many problems."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 102
                            }
                        ],
                        "text": "In addition to end-to-end learning with gradient descent, our approach strikes a good balance between Compressed Weight Search and HyperNEAT in terms of model flexibility and training simplicity."
                    },
                    "intents": []
                }
            ],
            "corpusId": 6289791,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "490f43dcb4d875cf4170a0f454df9a29d4a1bd20",
            "isKey": true,
            "numCitedBy": 78,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose a new indirect encoding scheme for neural networks in which the weight matrices are represented in the frequency domain by sets Fourier coefficients. This scheme exploits spatial regularities in the matrix to reduce the dimensionality of the representation by ignoring high-frequency coefficients, as is done in lossy image compression. We compare the efficiency of searching in this \"compressed\" network space to searching in the space of directly encoded networks, using the CoSyNE neuroevolution algorithm on three benchmark problems: pole-balancing, ball throwing and octopus arm control. The results show that this encoding can dramatically reduce the search space dimensionality such that solutions can be found in significantly fewer evaluations"
            },
            "slug": "Evolving-neural-networks-in-compressed-weight-space-Koutn\u00edk-Gomez",
            "title": {
                "fragments": [],
                "text": "Evolving neural networks in compressed weight space"
            },
            "tldr": {
                "abstractSimilarityScore": 74,
                "text": "A new indirect encoding scheme for neural networks in which the weight matrices are represented in the frequency domain by sets Fourier coefficients, which can dramatically reduce the search space dimensionality such that solutions can be found in significantly fewer evaluations."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO '10"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143939165"
                        ],
                        "name": "Chrisantha Fernando",
                        "slug": "Chrisantha-Fernando",
                        "structuredName": {
                            "firstName": "Chrisantha",
                            "lastName": "Fernando",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chrisantha Fernando"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32689129"
                        ],
                        "name": "D. Banarse",
                        "slug": "D.-Banarse",
                        "structuredName": {
                            "firstName": "Dylan",
                            "lastName": "Banarse",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Banarse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47447264"
                        ],
                        "name": "Malcolm Reynolds",
                        "slug": "Malcolm-Reynolds",
                        "structuredName": {
                            "firstName": "Malcolm",
                            "lastName": "Reynolds",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Malcolm Reynolds"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143923544"
                        ],
                        "name": "F. Besse",
                        "slug": "F.-Besse",
                        "structuredName": {
                            "firstName": "Frederic",
                            "lastName": "Besse",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Besse"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144846367"
                        ],
                        "name": "D. Pfau",
                        "slug": "D.-Pfau",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pfau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pfau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3093886"
                        ],
                        "name": "Max Jaderberg",
                        "slug": "Max-Jaderberg",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Jaderberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Max Jaderberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1975889"
                        ],
                        "name": "Marc Lanctot",
                        "slug": "Marc-Lanctot",
                        "structuredName": {
                            "firstName": "Marc",
                            "lastName": "Lanctot",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc Lanctot"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 187
                            }
                        ],
                        "text": "Using this setting, hypernetworks can approximately re-\ncover the convolutional architecture without explicitly being told to do so, a similar result obtained by \u201cConvolution by Evolution\" (Fernando et al., 2016)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 178,
                                "start": 157
                            }
                        ],
                        "text": "Even more closely related to our approach are Differentiable Pattern Producing Networks (DPPNs), where the structure is evolved but the weights are learned (Fernando et al., 2016), and ACDC-Networks (Moczulski et al., 2015), where linear layers are compressed with DCT and the parameters are learned."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1117341,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "99901920de0ca5e129cc2b3fbdcdd257a105f16a",
            "isKey": false,
            "numCitedBy": 88,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "In this work we introduce a differentiable version of the Compositional Pattern Producing Network, called the DPPN. Unlike a standard CPPN, the topology of a DPPN is evolved but the weights are learned. A Lamarckian algorithm, that combines evolution and learning, produces DPPNs to reconstruct an image. Our main result is that DPPNs can be evolved/trained to compress the weights of a denoising autoencoder from 157684 to roughly 200 parameters, while achieving a reconstruction accuracy comparable to a fully connected network with more than two orders of magnitude more parameters. The regularization ability of the DPPN allows it to rediscover (approximate) convolutional network architectures embedded within a fully connected architecture. Such convolutional architectures are the current state of the art for many computer vision applications, so it is satisfying that DPPNs are capable of discovering this structure rather than having to build it in by design. DPPNs exhibit better generalization when tested on the Omniglot dataset after being trained on MNIST, than directly encoded fully connected autoencoders. DPPNs are therefore a new framework for integrating learning and evolution."
            },
            "slug": "Convolution-by-Evolution:-Differentiable-Pattern-Fernando-Banarse",
            "title": {
                "fragments": [],
                "text": "Convolution by Evolution: Differentiable Pattern Producing Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 60,
                "text": "This work introduces a differentiable version of the Compositional Pattern Producing Network, called the DPPN, which can be evolved/trained to compress the weights of a denoising autoencoder while achieving a reconstruction accuracy comparable to a fully connected network with more than two orders of magnitude more parameters."
            },
            "venue": {
                "fragments": [],
                "text": "GECCO"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1993475"
                        ],
                        "name": "K. Rocki",
                        "slug": "K.-Rocki",
                        "structuredName": {
                            "firstName": "Kamil",
                            "lastName": "Rocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rocki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026already does a formidable job of generating time-varying parameters of a Mixture Gaussian distribution used to generate realistic handwriting samples, the ability to go one level deeper, and to dynamically generate the generative model is one of the key advantages of HyperRNN over a normal RNN."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5063280,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ca9d174c70c9102d88f2707bc395d6a384e1de1d",
            "isKey": false,
            "numCitedBy": 11,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Recurrent neural nets are widely used for predicting temporal data. Their inherent deep feedforward structure allows learning complex sequential patterns. It is believed that top-down feedback might be an important missing ingredient which in theory could help disambiguate similar patterns depending on broader context. In this paper we introduce surprisal-driven recurrent networks, which take into account past error information when making new predictions. This is achieved by continuously monitoring the discrepancy between most recent predictions and the actual observations. Furthermore, we show that it outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task achieving 1.37 BPC on the test portion of the text."
            },
            "slug": "Surprisal-Driven-Feedback-in-Recurrent-Networks-Rocki",
            "title": {
                "fragments": [],
                "text": "Surprisal-Driven Feedback in Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This paper introduces surprisal-driven recurrent networks, which take into account past error information when making new predictions, which outperforms other stochastic and fully deterministic approaches on enwik8 character level prediction task."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143983679"
                        ],
                        "name": "Gao Huang",
                        "slug": "Gao-Huang",
                        "structuredName": {
                            "firstName": "Gao",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gao Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109168016"
                        ],
                        "name": "Zhuang Liu",
                        "slug": "Zhuang-Liu",
                        "structuredName": {
                            "firstName": "Zhuang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhuang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7446832"
                        ],
                        "name": "Kilian Q. Weinberger",
                        "slug": "Kilian-Q.-Weinberger",
                        "structuredName": {
                            "firstName": "Kilian",
                            "lastName": "Weinberger",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kilian Q. Weinberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the following experiments, we will benchmark the performance of static hypernetworks on image recognition with MNIST and CIFAR-10, and the performance of dynamic hypernetworks on language modelling with Penn Treebank and Hutter Prize Wikipedia (enwik8) datasets and handwriting generation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 68
                            }
                        ],
                        "text": "The original resnet and subsequent derivatives (Zhang et al., 2016; Huang et al., 2016a) achieved state-of-the-art image recognition performance on a variety of public datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 9433631,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5694e46284460a648fe29117cbc55f6c9be3fa3c",
            "isKey": false,
            "numCitedBy": 19204,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Recent work has shown that convolutional networks can be substantially deeper, more accurate, and efficient to train if they contain shorter connections between layers close to the input and those close to the output. In this paper, we embrace this observation and introduce the Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion. Whereas traditional convolutional networks with L layers have L connections&#x2014;one between each layer and its subsequent layer&#x2014;our network has L(L+1)/2 direct connections. For each layer, the feature-maps of all preceding layers are used as inputs, and its own feature-maps are used as inputs into all subsequent layers. DenseNets have several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters. We evaluate our proposed architecture on four highly competitive object recognition benchmark tasks (CIFAR-10, CIFAR-100, SVHN, and ImageNet). DenseNets obtain significant improvements over the state-of-the-art on most of them, whilst requiring less memory and computation to achieve high performance. Code and pre-trained models are available at https://github.com/liuzhuang13/DenseNet."
            },
            "slug": "Densely-Connected-Convolutional-Networks-Huang-Liu",
            "title": {
                "fragments": [],
                "text": "Densely Connected Convolutional Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The Dense Convolutional Network (DenseNet), which connects each layer to every other layer in a feed-forward fashion, and has several compelling advantages: they alleviate the vanishing-gradient problem, strengthen feature propagation, encourage feature reuse, and substantially reduce the number of parameters."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1993475"
                        ],
                        "name": "K. Rocki",
                        "slug": "K.-Rocki",
                        "structuredName": {
                            "firstName": "Kamil",
                            "lastName": "Rocki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Rocki"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1782515,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7dded890956b37df5ac4c42b8ffbc142725f2801",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "The following report introduces ideas augmenting standard Long Short Term Memory (LSTM) architecture with multiple memory cells per hidden unit in order to improve its generalization capabilities. It considers both deterministic and stochastic variants of memory operation. It is shown that the nondeterministic Array-LSTM approach improves state-of-the-art performance on character level text prediction achieving 1.402 BPC on enwik8 dataset. Furthermore, this report estabilishes baseline neural-based results of 1.12 BPC and 1.19 BPC for enwik9 and enwik10 datasets respectively."
            },
            "slug": "Recurrent-Memory-Array-Structures-Rocki",
            "title": {
                "fragments": [],
                "text": "Recurrent Memory Array Structures"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is shown that the nondeterministic Array-LSTM approach improves state-of-the-art performance on character level text prediction achieving 1.402 BPC on enwik8 dataset."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47968984"
                        ],
                        "name": "Ke Zhang",
                        "slug": "Ke-Zhang",
                        "structuredName": {
                            "firstName": "Ke",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ke Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46611171"
                        ],
                        "name": "Miao Sun",
                        "slug": "Miao-Sun",
                        "structuredName": {
                            "firstName": "Miao",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Miao Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3244463"
                        ],
                        "name": "T. Han",
                        "slug": "T.-Han",
                        "structuredName": {
                            "firstName": "Tony",
                            "lastName": "Han",
                            "middleNames": [
                                "X."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Han"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3451660"
                        ],
                        "name": "Xingfang Yuan",
                        "slug": "Xingfang-Yuan",
                        "structuredName": {
                            "firstName": "Xingfang",
                            "lastName": "Yuan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xingfang Yuan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46846042"
                        ],
                        "name": "Liru Guo",
                        "slug": "Liru-Guo",
                        "structuredName": {
                            "firstName": "Liru",
                            "lastName": "Guo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liru Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2149917454"
                        ],
                        "name": "Tao Liu",
                        "slug": "Tao-Liu",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Liu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the following experiments, we will benchmark the performance of static hypernetworks on image recognition with MNIST and CIFAR-10, and the performance of dynamic hypernetworks on language modelling with Penn Treebank and Hutter Prize Wikipedia (enwik8) datasets and handwriting generation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 48
                            }
                        ],
                        "text": "The original resnet and subsequent derivatives (Zhang et al., 2016; Huang et al., 2016a) achieved state-of-the-art image recognition performance on a variety of public datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206663466,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5543b8f38cdb1b6bc618f3e25c5f10e57ec01d15",
            "isKey": false,
            "numCitedBy": 175,
            "numCiting": 50,
            "paperAbstract": {
                "fragments": [],
                "text": "A residual networks family with hundreds or even thousands of layers dominates major image recognition tasks, but building a network by simply stacking residual blocks inevitably limits its optimization ability. This paper proposes a novel residual network architecture, residual networks of residual networks (RoR), to dig the optimization ability of residual networks. RoR substitutes optimizing residual mapping of residual mapping for optimizing original residual mapping. In particular, RoR adds levelwise shortcut connections upon original residual networks to promote the learning capability of residual networks. More importantly, RoR can be applied to various kinds of residual networks (ResNets, Pre-ResNets, and WRN) and significantly boost their performance. Our experiments demonstrate the effectiveness and versatility of RoR, where it achieves the best performance in all residual-network-like structures. Our RoR-3-WRN58-4 + SD models achieve new state-of-the-art results on CIFAR-10, CIFAR-100, and SVHN, with the test errors of 3.77%, 19.73%, and 1.59%, respectively. RoR-3 models also achieve state-of-the-art results compared with ResNets on the ImageNet data set."
            },
            "slug": "Residual-Networks-of-Residual-Networks:-Multilevel-Zhang-Sun",
            "title": {
                "fragments": [],
                "text": "Residual Networks of Residual Networks: Multilevel Residual Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "A novel residual network architecture, residual networks of residual networks (RoR) is proposed, to dig the optimization ability of residual Networks, where RoR substitutes optimizing residual mapping of residual mapping for optimizing original residual mapping."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Circuits and Systems for Video Technology"
            },
            "year": 2018
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 169,
                                "start": 150
                            }
                        ],
                        "text": "\u2026y xzzx by(zyb ) =W y bzz y b + b y 0\n(12)\nIn our implementation, the cell and hidden state update equations for the main LSTM will incorporate a single dropout (Hinton et al., 2012) gate, as developed in Recurrent Dropout without Memory Loss (Semeniuta et al., 2016), as we found this to help\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14832074,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1366de5bb112746a555e9c0cd00de3ad8628aea8",
            "isKey": false,
            "numCitedBy": 6242,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "When a large feedforward neural network is trained on a small training set, it typically performs poorly on held-out test data. This \"overfitting\" is greatly reduced by randomly omitting half of the feature detectors on each training case. This prevents complex co-adaptations in which a feature detector is only helpful in the context of several other specific feature detectors. Instead, each neuron learns to detect a feature that is generally helpful for producing the correct answer given the combinatorially large variety of internal contexts in which it must operate. Random \"dropout\" gives big improvements on many benchmark tasks and sets new records for speech and object recognition."
            },
            "slug": "Improving-neural-networks-by-preventing-of-feature-Hinton-Srivastava",
            "title": {
                "fragments": [],
                "text": "Improving neural networks by preventing co-adaptation of feature detectors"
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2466953"
                        ],
                        "name": "Saahil Ognawala",
                        "slug": "Saahil-Ognawala",
                        "structuredName": {
                            "firstName": "Saahil",
                            "lastName": "Ognawala",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saahil Ognawala"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145040409"
                        ],
                        "name": "Justin Bayer",
                        "slug": "Justin-Bayer",
                        "structuredName": {
                            "firstName": "Justin",
                            "lastName": "Bayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Justin Bayer"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 42
                            }
                        ],
                        "text": "Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al., 2016) with the same dropout probability."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5988748,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44a47c2bee3ea51c17bd8d16a64053c18856d427",
            "isKey": false,
            "numCitedBy": 3,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "Advancements in parallel processing have lead to a surge in multilayer perceptrons' (MLP) applications and deep learning in the past decades. Recurrent Neural Networks (RNNs) give additional representational power to feedforward MLPs by providing a way to treat sequential data. However, RNNs are hard to train using conventional error backpropagation methods because of the difficulty in relating inputs over many time-steps. Regularization approaches from MLP sphere, like dropout and noisy weight training, have been insufficiently applied and tested on simple RNNs. Moreover, solutions have been proposed to improve convergence in RNNs but not enough to improve the long term dependency remembering capabilities thereof. \nIn this study, we aim to empirically evaluate the remembering and generalization ability of RNNs on polyphonic musical datasets. The models are trained with injected noise, random dropout, norm-based regularizers and their respective performances compared to well-initialized plain RNNs and advanced regularization methods like fast-dropout. We conclude with evidence that training with noise does not improve performance as conjectured by a few works in RNN optimization before ours."
            },
            "slug": "Regularizing-Recurrent-Networks-On-Injected-Noise-Ognawala-Bayer",
            "title": {
                "fragments": [],
                "text": "Regularizing Recurrent Networks - On Injected Noise and Norm-based Methods"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Evidence is concluded that training with noise does not improve performance as conjectured by a few works in RNN optimization before the authors', and the remembering and generalization ability of RNNs on polyphonic musical datasets is evaluated."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 203,
                                "start": 200
                            }
                        ],
                        "text": "We perform experiments to investigate the behaviors of hypernetworks in a range of contexts and find that hypernetworks mix well with other techniques such as batch normalization and layer normalization."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 38,
                                "start": 8
                            }
                        ],
                        "text": "We find this to be a good trade off, as this formulation of converting W (z) into W (d(z)) decreases the amount of memory required by the dynamic hypernetwork."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 131
                            }
                        ],
                        "text": "Our main result is that hypernetworks can generate non-shared weights for LSTM that work better than the standard version of LSTM (Hochreiter & Schmidhuber, 1997)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 95,
                                "start": 64
                            }
                        ],
                        "text": "Our focus will be on the basic version of the LSTM architecture Hochreiter & Schmidhuber (1997), given by:\nit =W i hht\u22121 +W i xxt + b i gt =W g hht\u22121 +W g xxt + b g\nft =W f h ht\u22121 +W f x xt + b f ot =W o hht\u22121 +W o xxt + b o\nct = \u03c3(ft) ct\u22121 + \u03c3(it) \u03c6(gt) ht = \u03c3(ot) \u03c6(ct)\n(9)\nwhere W yh \u2208 RNh\u00d7Nh ,W\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 48
                            }
                        ],
                        "text": "The Long Short-Term Memory (LSTM) architecture (Hochreiter & Schmidhuber, 1997) is usually better than the Basic RNN at storing and retrieving information over longer time steps."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": true,
            "numCitedBy": 52390,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 67,
                                "start": 43
                            }
                        ],
                        "text": "Even before the work on HyperNEAT and DCT, Schmidhuber (1992; 1993) has suggested the concept of fast weights in which one network can produce context-dependent weight changes for a second network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 258,
                                "start": 241
                            }
                        ],
                        "text": "Small scale experiments were conducted to demonstrate fast weights for feed forward networks at the time, but perhaps due to the lack of modern computational tools, the recurrent network version was mentioned mainly as a thought experiment (Schmidhuber, 1993)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 61002624,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7ee98330fb5969839d88bcabdb44d03848dc9d35",
            "isKey": false,
            "numCitedBy": 73,
            "numCiting": 9,
            "paperAbstract": {
                "fragments": [],
                "text": "Weight modifications in traditional neural nets are computed by hard-wired algorithms. Without exception, all previous weight change algorithms have many specific limitations. Is it (in principle) possible to overcome limitations of hard-wired algorithms by allowing neural nets to run and improve their own weight change algorithms? This paper constructively demonstrates that the answer (in principle) is \u2018yes\u2019. I derive an initial gradientbased sequence learning algorithm for a \u2018self-referential\u2019 recurrent network that can \u2018speak\u2019 about its own weight matrix in terms of activations. It uses some of its input and output units for observing its own errors and for explicitly analyzing and modifying its own weight matrix, including those parts of the weight matrix responsible for analyzing and modifying the weight matrix. The result is the first \u2018introspective\u2019 neural net with explicit potential control over all of its own adaptive parameters. A disadvantage of the algorithm is its high computational complexity per time step which is independent of the sequence length and equals O(nconnlognconn), where riconn is the number of connections. Another disadvantage is the high number of local minima of the unusually complex error surface. The purpose of this paper, however, is not to come up with the most efficient \u2018introspective\u2019 or \u2018self-referential\u2019 weight change algorithm, but to show that such algorithms are possible at all."
            },
            "slug": "A-\u2018Self-Referential\u2019-Weight-Matrix-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "A \u2018Self-Referential\u2019 Weight Matrix"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "An initial gradientbased sequence learning algorithm is derived for a \u2018self-referential\u2019 recurrent network that can \u2018speak\u2019 about its own weight matrix in terms of activations and is the first \u2018introspective\u2019 neural net with explicit potential control over all of its own adaptive parameters."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1726807"
                        ],
                        "name": "Diederik P. Kingma",
                        "slug": "Diederik-P.-Kingma",
                        "structuredName": {
                            "firstName": "Diederik",
                            "lastName": "Kingma",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Diederik P. Kingma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2503659"
                        ],
                        "name": "Jimmy Ba",
                        "slug": "Jimmy-Ba",
                        "structuredName": {
                            "firstName": "Jimmy",
                            "lastName": "Ba",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jimmy Ba"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 205,
                                "start": 188
                            }
                        ],
                        "text": "We train the network with a 55000 / 5000 / 10000 split for the training, validation and test sets and use the 5000 validation samples for early stopping, and train the network using Adam (Kingma & Ba, 2015) with a learning rate of 0.001 on mini-batches of size 1000."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026already does a formidable job of generating time-varying parameters of a Mixture Gaussian distribution used to generate realistic handwriting samples, the ability to go one level deeper, and to dynamically generate the generative model is one of the key advantages of HyperRNN over a normal RNN."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6628106,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a6cb366736791bcccc5c8639de5a8f9636bf87e8",
            "isKey": true,
            "numCitedBy": 91740,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments. The method is straightforward to implement, is computationally efficient, has little memory requirements, is invariant to diagonal rescaling of the gradients, and is well suited for problems that are large in terms of data and/or parameters. The method is also appropriate for non-stationary objectives and problems with very noisy and/or sparse gradients. The hyper-parameters have intuitive interpretations and typically require little tuning. Some connections to related algorithms, on which Adam was inspired, are discussed. We also analyze the theoretical convergence properties of the algorithm and provide a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework. Empirical results demonstrate that Adam works well in practice and compares favorably to other stochastic optimization methods. Finally, we discuss AdaMax, a variant of Adam based on the infinity norm."
            },
            "slug": "Adam:-A-Method-for-Stochastic-Optimization-Kingma-Ba",
            "title": {
                "fragments": [],
                "text": "Adam: A Method for Stochastic Optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This work introduces Adam, an algorithm for first-order gradient-based optimization of stochastic objective functions, based on adaptive estimates of lower-order moments, and provides a regret bound on the convergence rate that is comparable to the best known results under the online convex optimization framework."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2206490"
                        ],
                        "name": "Marcin Andrychowicz",
                        "slug": "Marcin-Andrychowicz",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Andrychowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Andrychowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715051"
                        ],
                        "name": "Misha Denil",
                        "slug": "Misha-Denil",
                        "structuredName": {
                            "firstName": "Misha",
                            "lastName": "Denil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Misha Denil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2016840"
                        ],
                        "name": "Sergio Gomez Colmenarejo",
                        "slug": "Sergio-Gomez-Colmenarejo",
                        "structuredName": {
                            "firstName": "Sergio",
                            "lastName": "Colmenarejo",
                            "middleNames": [
                                "Gomez"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sergio Gomez Colmenarejo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3243579"
                        ],
                        "name": "Matthew W. Hoffman",
                        "slug": "Matthew-W.-Hoffman",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Hoffman",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew W. Hoffman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144846367"
                        ],
                        "name": "D. Pfau",
                        "slug": "D.-Pfau",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Pfau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Pfau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1725157"
                        ],
                        "name": "T. Schaul",
                        "slug": "T.-Schaul",
                        "structuredName": {
                            "firstName": "Tom",
                            "lastName": "Schaul",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Schaul"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 134,
                                "start": 109
                            }
                        ],
                        "text": "The concept of a network interacting with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2928017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "395dd01c0d24777c660cf195c4cfadcdf51fb7e8",
            "isKey": false,
            "numCitedBy": 1347,
            "numCiting": 48,
            "paperAbstract": {
                "fragments": [],
                "text": "The move from hand-designed features to learned features in machine learning has been wildly successful. In spite of this, optimization algorithms are still designed by hand. In this paper we show how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way. Our learned algorithms, implemented by LSTMs, outperform generic, hand-designed competitors on the tasks for which they are trained, and also generalize well to new tasks with similar structure. We demonstrate this on a number of tasks, including simple convex problems, training neural networks, and styling images with neural art."
            },
            "slug": "Learning-to-learn-by-gradient-descent-by-gradient-Andrychowicz-Denil",
            "title": {
                "fragments": [],
                "text": "Learning to learn by gradient descent by gradient descent"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper shows how the design of an optimization algorithm can be cast as a learning problem, allowing the algorithm to learn to exploit structure in the problems of interest in an automatic way."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143983679"
                        ],
                        "name": "Gao Huang",
                        "slug": "Gao-Huang",
                        "structuredName": {
                            "firstName": "Gao",
                            "lastName": "Huang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gao Huang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117103358"
                        ],
                        "name": "Yu Sun",
                        "slug": "Yu-Sun",
                        "structuredName": {
                            "firstName": "Yu",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu Sun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109168016"
                        ],
                        "name": "Zhuang Liu",
                        "slug": "Zhuang-Liu",
                        "structuredName": {
                            "firstName": "Zhuang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhuang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371029"
                        ],
                        "name": "Daniel Sedra",
                        "slug": "Daniel-Sedra",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Sedra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daniel Sedra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7446832"
                        ],
                        "name": "Kilian Q. Weinberger",
                        "slug": "Kilian-Q.-Weinberger",
                        "structuredName": {
                            "firstName": "Kilian",
                            "lastName": "Weinberger",
                            "middleNames": [
                                "Q."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kilian Q. Weinberger"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 87,
                                "start": 68
                            }
                        ],
                        "text": "The original resnet and subsequent derivatives (Zhang et al., 2016; Huang et al., 2016a) achieved state-of-the-art image recognition performance on a variety of public datasets."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6773885,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "51db1f3c8dfc7d4077da39c96bb90a6358128111",
            "isKey": false,
            "numCitedBy": 1458,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Very deep convolutional networks with hundreds of layers have led to significant reductions in error on competitive benchmarks. Although the unmatched expressiveness of the many layers can be highly desirable at test time, training very deep networks comes with its own set of challenges. The gradients can vanish, the forward flow often diminishes, and the training time can be painfully slow. To address these problems, we propose stochastic depth, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time. We start with very deep networks but during training, for each mini-batch, randomly drop a subset of layers and bypass them with the identity function. This simple approach complements the recent success of residual networks. It reduces training time substantially and improves the test error significantly on almost all data sets that we used for evaluation. With stochastic depth we can increase the depth of residual networks even beyond 1200 layers and still yield meaningful improvements in test error (4.91 % on CIFAR-10)."
            },
            "slug": "Deep-Networks-with-Stochastic-Depth-Huang-Sun",
            "title": {
                "fragments": [],
                "text": "Deep Networks with Stochastic Depth"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Stochastic depth is proposed, a training procedure that enables the seemingly contradictory setup to train short networks and use deep networks at test time and reduces training time substantially and improves the test error significantly on almost all data sets that were used for evaluation."
            },
            "venue": {
                "fragments": [],
                "text": "ECCV"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2271057"
                        ],
                        "name": "Luca Bertinetto",
                        "slug": "Luca-Bertinetto",
                        "structuredName": {
                            "firstName": "Luca",
                            "lastName": "Bertinetto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luca Bertinetto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143848064"
                        ],
                        "name": "Jo\u00e3o F. Henriques",
                        "slug": "Jo\u00e3o-F.-Henriques",
                        "structuredName": {
                            "firstName": "Jo\u00e3o",
                            "lastName": "Henriques",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jo\u00e3o F. Henriques"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1881617"
                        ],
                        "name": "Jack Valmadre",
                        "slug": "Jack-Valmadre",
                        "structuredName": {
                            "firstName": "Jack",
                            "lastName": "Valmadre",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jack Valmadre"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143635540"
                        ],
                        "name": "Philip H. S. Torr",
                        "slug": "Philip-H.-S.-Torr",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Torr",
                            "middleNames": [
                                "H.",
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip H. S. Torr"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1687524"
                        ],
                        "name": "A. Vedaldi",
                        "slug": "A.-Vedaldi",
                        "structuredName": {
                            "firstName": "Andrea",
                            "lastName": "Vedaldi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Vedaldi"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 179,
                                "start": 156
                            }
                        ],
                        "text": "\u2026with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are predicted by another network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2169827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "4423357dd21cc59662c6fabaf9839b15ef0fb8a8",
            "isKey": false,
            "numCitedBy": 348,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "One-shot learning is usually tackled by using generative models or discriminative embeddings. Discriminative methods based on deep learning, which are very effective in other learning scenarios, are ill-suited for one-shot learning as they need large amounts of training data. In this paper, we propose a method to learn the parameters of a deep model in one shot. We construct the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar. In this manner we obtain an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one-shot classification objective in a learning to learn formulation. In order to make the construction feasible, we propose a number of factorizations of the parameters of the pupil network. We demonstrate encouraging results by learning characters from single exemplars in Omniglot, and by tracking visual objects from a single initial exemplar in the Visual Object Tracking benchmark."
            },
            "slug": "Learning-feed-forward-one-shot-learners-Bertinetto-Henriques",
            "title": {
                "fragments": [],
                "text": "Learning feed-forward one-shot learners"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper constructs the learner as a second deep network, called a learnet, which predicts the parameters of a pupil network from a single exemplar, and obtains an efficient feed-forward one-shot learner, trained end-to-end by minimizing a one- shot classification objective in a learning to learn formulation."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1788202"
                        ],
                        "name": "M. Goudreau",
                        "slug": "M.-Goudreau",
                        "structuredName": {
                            "firstName": "Mark",
                            "lastName": "Goudreau",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Goudreau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145157784"
                        ],
                        "name": "C. Lee Giles",
                        "slug": "C.-Lee-Giles",
                        "structuredName": {
                            "firstName": "C.",
                            "lastName": "Giles",
                            "middleNames": [
                                "Lee"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Lee Giles"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1752242"
                        ],
                        "name": "S. Chakradhar",
                        "slug": "S.-Chakradhar",
                        "structuredName": {
                            "firstName": "Srimat",
                            "lastName": "Chakradhar",
                            "middleNames": [
                                "T."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chakradhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2158193072"
                        ],
                        "name": "Dong Chen",
                        "slug": "Dong-Chen",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Chen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 20077435,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "160ad1b01973986944a5f9a17924711ee1861552",
            "isKey": false,
            "numCitedBy": 80,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "We examine the representational capabilities of first-order and second-order single-layer recurrent neural networks (SLRNN's) with hard-limiting neurons. We show that a second-order SLRNN is strictly more powerful than a first-order SLRNN. However, if the first-order SLRNN is augmented with output layers of feedforward neurons, it can implement any finite-state recognizer, but only if state-splitting is employed. When a state is split, it is divided into two equivalent states. The judicious use of state-splitting allows for efficient implementation of finite-state recognizers using augmented first-order SLRNN's."
            },
            "slug": "First-order-versus-second-order-single-layer-neural-Goudreau-Giles",
            "title": {
                "fragments": [],
                "text": "First-order versus second-order single-layer recurrent neural networks"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The judicious use of state-splitting allows for efficient implementation of finite-state recognizers using augmented first-order SLRNN's."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3009779"
                        ],
                        "name": "Marcin Moczulski",
                        "slug": "Marcin-Moczulski",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Moczulski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Moczulski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715051"
                        ],
                        "name": "Misha Denil",
                        "slug": "Misha-Denil",
                        "structuredName": {
                            "firstName": "Misha",
                            "lastName": "Denil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Misha Denil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40231275"
                        ],
                        "name": "J. Appleyard",
                        "slug": "J.-Appleyard",
                        "structuredName": {
                            "firstName": "Jeremy",
                            "lastName": "Appleyard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Appleyard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 222,
                                "start": 200
                            }
                        ],
                        "text": "Even more closely related to our approach are Differentiable Pattern Producing Networks (DPPNs), where the structure is evolved but the weights are learned (Fernando et al., 2016), and ACDC-Networks (Moczulski et al., 2015), where linear layers are compressed with DCT and the parameters are learned."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1204679,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "54c3e878bf0ff2fdde16e439b5579ee99ee0d0d8",
            "isKey": false,
            "numCitedBy": 79,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "The linear layer is one of the most pervasive modules in deep learning representations. However, it requires $O(N^2)$ parameters and $O(N^2)$ operations. These costs can be prohibitive in mobile applications or prevent scaling in many domains. Here, we introduce a deep, differentiable, fully-connected neural network module composed of diagonal matrices of parameters, $\\mathbf{A}$ and $\\mathbf{D}$, and the discrete cosine transform $\\mathbf{C}$. The core module, structured as $\\mathbf{ACDC^{-1}}$, has $O(N)$ parameters and incurs $O(N log N )$ operations. We present theoretical results showing how deep cascades of ACDC layers approximate linear layers. ACDC is, however, a stand-alone module and can be used in combination with any other types of module. In our experiments, we show that it can indeed be successfully interleaved with ReLU modules in convolutional neural networks for image recognition. Our experiments also study critical factors in the training of these structured modules, including initialization and depth. Finally, this paper also provides a connection between structured linear transforms used in deep learning and the field of Fourier optics, illustrating how ACDC could in principle be implemented with lenses and diffractive elements."
            },
            "slug": "ACDC:-A-Structured-Efficient-Linear-Layer-Moczulski-Denil",
            "title": {
                "fragments": [],
                "text": "ACDC: A Structured Efficient Linear Layer"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A deep, differentiable, fully-connected neural network module composed of diagonal matrices of parameters, $\\mathbf{A}$ and $D}$, and the discrete cosine transform, illustrating how ACDC could in principle be implemented with lenses and diffractive elements."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715051"
                        ],
                        "name": "Misha Denil",
                        "slug": "Misha-Denil",
                        "structuredName": {
                            "firstName": "Misha",
                            "lastName": "Denil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Misha Denil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3355894"
                        ],
                        "name": "B. Shakibi",
                        "slug": "B.-Shakibi",
                        "structuredName": {
                            "firstName": "Babak",
                            "lastName": "Shakibi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Shakibi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46573521"
                        ],
                        "name": "Laurent Dinh",
                        "slug": "Laurent-Dinh",
                        "structuredName": {
                            "firstName": "Laurent",
                            "lastName": "Dinh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Laurent Dinh"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706809"
                        ],
                        "name": "Marc'Aurelio Ranzato",
                        "slug": "Marc'Aurelio-Ranzato",
                        "structuredName": {
                            "firstName": "Marc'Aurelio",
                            "lastName": "Ranzato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marc'Aurelio Ranzato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 24
                            }
                        ],
                        "text": ", 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are predicted by another network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "\u2026concept of a network interacting with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1639981,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e8650503ab80ad7299f0845b1843abf3a97f313a",
            "isKey": false,
            "numCitedBy": 1009,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "We demonstrate that there is significant redundancy in the parameterization of several deep learning models. Given only a few weight values for each feature it is possible to accurately predict the remaining values. Moreover, we show that not only can the parameter values be predicted, but many of them need not be learned at all. We train several different architectures by learning only a small number of weights and predicting the rest. In the best case we are able to predict more than 95% of the weights of a network without any drop in accuracy."
            },
            "slug": "Predicting-Parameters-in-Deep-Learning-Denil-Shakibi",
            "title": {
                "fragments": [],
                "text": "Predicting Parameters in Deep Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 72,
                "text": "It is demonstrated that there is significant redundancy in the parameterization of several deep learning models and not only can the parameter values be predicted, but many of them need not be learned at all."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 71
                            }
                        ],
                        "text": "A subsequent work demonstrated practical applications of fast weights (Gomez & Schmidhuber, 2005), where a generator network is learnt through evolution to solve an artificial control problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1086771,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9382c0ec9904ea93089f439479607f7fd0195505",
            "isKey": false,
            "numCitedBy": 38,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "In practice, almost all control systems in use today implement some form of linear control. However, there are many tasks for which conventional control engineering methods are not directly applicable because there is not enough information about how the system should be controlled (i.e. reinforcement learning problems). In this paper, we explore an approach to such problems that evolves fast-weight neural networks. These networks, although capable of implementing arbitrary non-linear mappings, can more easily exploit the piecewise linearity inherent in most systems, in order to produce simpler and more comprehensible controllers. The method is tested on 2D mobile robot version of the pole balancing task where the controller must learn to switch between two operating modes, one using a single pole and the other using a jointed pole version that has not before been solved."
            },
            "slug": "Evolving-Modular-Fast-Weight-Networks-for-Control-Gomez-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Evolving Modular Fast-Weight Networks for Control"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "An approach to problems that evolves fast-weight neural networks, although capable of implementing arbitrary non-linear mappings, can more easily exploit the piecewise linearity inherent in most systems, in order to produce simpler and more comprehensible controllers."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2057642721"
                        ],
                        "name": "Mart\u00edn Abadi",
                        "slug": "Mart\u00edn-Abadi",
                        "structuredName": {
                            "firstName": "Mart\u00edn",
                            "lastName": "Abadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mart\u00edn Abadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2078528337"
                        ],
                        "name": "Ashish Agarwal",
                        "slug": "Ashish-Agarwal",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Agarwal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ashish Agarwal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144758007"
                        ],
                        "name": "P. Barham",
                        "slug": "P.-Barham",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Barham",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Barham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2445241"
                        ],
                        "name": "E. Brevdo",
                        "slug": "E.-Brevdo",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Brevdo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Brevdo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545358"
                        ],
                        "name": "Z. Chen",
                        "slug": "Z.-Chen",
                        "structuredName": {
                            "firstName": "Z.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48738717"
                        ],
                        "name": "C. Citro",
                        "slug": "C.-Citro",
                        "structuredName": {
                            "firstName": "Craig",
                            "lastName": "Citro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Citro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "36347083"
                        ],
                        "name": "Andy Davis",
                        "slug": "Andy-Davis",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Davis",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Andy Davis"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145139947"
                        ],
                        "name": "Matthieu Devin",
                        "slug": "Matthieu-Devin",
                        "structuredName": {
                            "firstName": "Matthieu",
                            "lastName": "Devin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthieu Devin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780892"
                        ],
                        "name": "S. Ghemawat",
                        "slug": "S.-Ghemawat",
                        "structuredName": {
                            "firstName": "Sanjay",
                            "lastName": "Ghemawat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Ghemawat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153440022"
                        ],
                        "name": "Ian J. Goodfellow",
                        "slug": "Ian-J.-Goodfellow",
                        "structuredName": {
                            "firstName": "Ian",
                            "lastName": "Goodfellow",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ian J. Goodfellow"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064102917"
                        ],
                        "name": "A. Harp",
                        "slug": "A.-Harp",
                        "structuredName": {
                            "firstName": "Andrew",
                            "lastName": "Harp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Harp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2060655766"
                        ],
                        "name": "Geoffrey Irving",
                        "slug": "Geoffrey-Irving",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Irving",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey Irving"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2090818"
                        ],
                        "name": "M. Isard",
                        "slug": "M.-Isard",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Isard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Isard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39978391"
                        ],
                        "name": "Yangqing Jia",
                        "slug": "Yangqing-Jia",
                        "structuredName": {
                            "firstName": "Yangqing",
                            "lastName": "Jia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yangqing Jia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1944541"
                        ],
                        "name": "R. J\u00f3zefowicz",
                        "slug": "R.-J\u00f3zefowicz",
                        "structuredName": {
                            "firstName": "Rafal",
                            "lastName": "J\u00f3zefowicz",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. J\u00f3zefowicz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1942300"
                        ],
                        "name": "M. Kudlur",
                        "slug": "M.-Kudlur",
                        "structuredName": {
                            "firstName": "Manjunath",
                            "lastName": "Kudlur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Kudlur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3369421"
                        ],
                        "name": "J. Levenberg",
                        "slug": "J.-Levenberg",
                        "structuredName": {
                            "firstName": "Josh",
                            "lastName": "Levenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Levenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30415265"
                        ],
                        "name": "Dandelion Man\u00e9",
                        "slug": "Dandelion-Man\u00e9",
                        "structuredName": {
                            "firstName": "Dandelion",
                            "lastName": "Man\u00e9",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dandelion Man\u00e9"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3089272"
                        ],
                        "name": "R. Monga",
                        "slug": "R.-Monga",
                        "structuredName": {
                            "firstName": "Rajat",
                            "lastName": "Monga",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Monga"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144375552"
                        ],
                        "name": "Sherry Moore",
                        "slug": "Sherry-Moore",
                        "structuredName": {
                            "firstName": "Sherry",
                            "lastName": "Moore",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sherry Moore"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20154699"
                        ],
                        "name": "D. Murray",
                        "slug": "D.-Murray",
                        "structuredName": {
                            "firstName": "Derek",
                            "lastName": "Murray",
                            "middleNames": [
                                "Gordon"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Murray"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37232298"
                        ],
                        "name": "C. Olah",
                        "slug": "C.-Olah",
                        "structuredName": {
                            "firstName": "Christopher",
                            "lastName": "Olah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Olah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1789737"
                        ],
                        "name": "Jonathon Shlens",
                        "slug": "Jonathon-Shlens",
                        "structuredName": {
                            "firstName": "Jonathon",
                            "lastName": "Shlens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathon Shlens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32163737"
                        ],
                        "name": "Benoit Steiner",
                        "slug": "Benoit-Steiner",
                        "structuredName": {
                            "firstName": "Benoit",
                            "lastName": "Steiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benoit Steiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35210462"
                        ],
                        "name": "Kunal Talwar",
                        "slug": "Kunal-Talwar",
                        "structuredName": {
                            "firstName": "Kunal",
                            "lastName": "Talwar",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kunal Talwar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2080690"
                        ],
                        "name": "P. Tucker",
                        "slug": "P.-Tucker",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Tucker",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Tucker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2657155"
                        ],
                        "name": "Vincent Vanhoucke",
                        "slug": "Vincent-Vanhoucke",
                        "structuredName": {
                            "firstName": "Vincent",
                            "lastName": "Vanhoucke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vincent Vanhoucke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2053781980"
                        ],
                        "name": "Vijay Vasudevan",
                        "slug": "Vijay-Vasudevan",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Vasudevan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vijay Vasudevan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765169"
                        ],
                        "name": "F. Vi\u00e9gas",
                        "slug": "F.-Vi\u00e9gas",
                        "structuredName": {
                            "firstName": "Fernanda",
                            "lastName": "Vi\u00e9gas",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Vi\u00e9gas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47941411"
                        ],
                        "name": "P. Warden",
                        "slug": "P.-Warden",
                        "structuredName": {
                            "firstName": "Pete",
                            "lastName": "Warden",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Warden"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145233583"
                        ],
                        "name": "M. Wattenberg",
                        "slug": "M.-Wattenberg",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wattenberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wattenberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35078078"
                        ],
                        "name": "M. Wicke",
                        "slug": "M.-Wicke",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Wicke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Wicke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2117163698"
                        ],
                        "name": "Yuan Yu",
                        "slug": "Yuan-Yu",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2152198093"
                        ],
                        "name": "Xiaoqiang Zheng",
                        "slug": "Xiaoqiang-Zheng",
                        "structuredName": {
                            "firstName": "Xiaoqiang",
                            "lastName": "Zheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoqiang Zheng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5707386,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9c9d7247f8c51ec5a02b0d911d1d7b9e8160495d",
            "isKey": false,
            "numCitedBy": 9255,
            "numCiting": 70,
            "paperAbstract": {
                "fragments": [],
                "text": "TensorFlow is an interface for expressing machine learning algorithms, and an implementation for executing such algorithms. A computation expressed using TensorFlow can be executed with little or no change on a wide variety of heterogeneous systems, ranging from mobile devices such as phones and tablets up to large-scale distributed systems of hundreds of machines and thousands of computational devices such as GPU cards. The system is flexible and can be used to express a wide variety of algorithms, including training and inference algorithms for deep neural network models, and it has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields, including speech recognition, computer vision, robotics, information retrieval, natural language processing, geographic information extraction, and computational drug discovery. This paper describes the TensorFlow interface and an implementation of that interface that we have built at Google. The TensorFlow API and a reference implementation were released as an open-source package under the Apache 2.0 license in November, 2015 and are available at www.tensorflow.org."
            },
            "slug": "TensorFlow:-Large-Scale-Machine-Learning-on-Systems-Abadi-Agarwal",
            "title": {
                "fragments": [],
                "text": "TensorFlow: Large-Scale Machine Learning on Heterogeneous Distributed Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The TensorFlow interface and an implementation of that interface that is built at Google are described, which has been used for conducting research and for deploying machine learning systems into production across more than a dozen areas of computer science and other fields."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48607963"
                        ],
                        "name": "Yonghui Wu",
                        "slug": "Yonghui-Wu",
                        "structuredName": {
                            "firstName": "Yonghui",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yonghui Wu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144927151"
                        ],
                        "name": "M. Schuster",
                        "slug": "M.-Schuster",
                        "structuredName": {
                            "firstName": "Mike",
                            "lastName": "Schuster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Schuster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2545358"
                        ],
                        "name": "Z. Chen",
                        "slug": "Z.-Chen",
                        "structuredName": {
                            "firstName": "Z.",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2827616"
                        ],
                        "name": "Quoc V. Le",
                        "slug": "Quoc-V.-Le",
                        "structuredName": {
                            "firstName": "Quoc",
                            "lastName": "Le",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Quoc V. Le"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144739074"
                        ],
                        "name": "Mohammad Norouzi",
                        "slug": "Mohammad-Norouzi",
                        "structuredName": {
                            "firstName": "Mohammad",
                            "lastName": "Norouzi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mohammad Norouzi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3153147"
                        ],
                        "name": "Wolfgang Macherey",
                        "slug": "Wolfgang-Macherey",
                        "structuredName": {
                            "firstName": "Wolfgang",
                            "lastName": "Macherey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wolfgang Macherey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2048712"
                        ],
                        "name": "M. Krikun",
                        "slug": "M.-Krikun",
                        "structuredName": {
                            "firstName": "Maxim",
                            "lastName": "Krikun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Krikun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145144022"
                        ],
                        "name": "Yuan Cao",
                        "slug": "Yuan-Cao",
                        "structuredName": {
                            "firstName": "Yuan",
                            "lastName": "Cao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yuan Cao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145312180"
                        ],
                        "name": "Qin Gao",
                        "slug": "Qin-Gao",
                        "structuredName": {
                            "firstName": "Qin",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Qin Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "113439369"
                        ],
                        "name": "Klaus Macherey",
                        "slug": "Klaus-Macherey",
                        "structuredName": {
                            "firstName": "Klaus",
                            "lastName": "Macherey",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Klaus Macherey"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2367620"
                        ],
                        "name": "J. Klingner",
                        "slug": "J.-Klingner",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Klingner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Klingner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145825976"
                        ],
                        "name": "Apurva Shah",
                        "slug": "Apurva-Shah",
                        "structuredName": {
                            "firstName": "Apurva",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Apurva Shah"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145657834"
                        ],
                        "name": "Melvin Johnson",
                        "slug": "Melvin-Johnson",
                        "structuredName": {
                            "firstName": "Melvin",
                            "lastName": "Johnson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Melvin Johnson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109059862"
                        ],
                        "name": "Xiaobing Liu",
                        "slug": "Xiaobing-Liu",
                        "structuredName": {
                            "firstName": "Xiaobing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaobing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40527594"
                        ],
                        "name": "Lukasz Kaiser",
                        "slug": "Lukasz-Kaiser",
                        "structuredName": {
                            "firstName": "Lukasz",
                            "lastName": "Kaiser",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lukasz Kaiser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2776283"
                        ],
                        "name": "Stephan Gouws",
                        "slug": "Stephan-Gouws",
                        "structuredName": {
                            "firstName": "Stephan",
                            "lastName": "Gouws",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephan Gouws"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2739610"
                        ],
                        "name": "Y. Kato",
                        "slug": "Y.-Kato",
                        "structuredName": {
                            "firstName": "Yoshikiyo",
                            "lastName": "Kato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Kato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1765329"
                        ],
                        "name": "Taku Kudo",
                        "slug": "Taku-Kudo",
                        "structuredName": {
                            "firstName": "Taku",
                            "lastName": "Kudo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Taku Kudo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1754386"
                        ],
                        "name": "H. Kazawa",
                        "slug": "H.-Kazawa",
                        "structuredName": {
                            "firstName": "Hideto",
                            "lastName": "Kazawa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kazawa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144077726"
                        ],
                        "name": "K. Stevens",
                        "slug": "K.-Stevens",
                        "structuredName": {
                            "firstName": "Keith",
                            "lastName": "Stevens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Stevens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753079661"
                        ],
                        "name": "George Kurian",
                        "slug": "George-Kurian",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Kurian",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "George Kurian"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056800684"
                        ],
                        "name": "Nishant Patil",
                        "slug": "Nishant-Patil",
                        "structuredName": {
                            "firstName": "Nishant",
                            "lastName": "Patil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nishant Patil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49337181"
                        ],
                        "name": "Wei Wang",
                        "slug": "Wei-Wang",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39660914"
                        ],
                        "name": "C. Young",
                        "slug": "C.-Young",
                        "structuredName": {
                            "firstName": "Cliff",
                            "lastName": "Young",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Young"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2119125158"
                        ],
                        "name": "Jason R. Smith",
                        "slug": "Jason-R.-Smith",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Smith",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason R. Smith"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2909504"
                        ],
                        "name": "Jason Riesa",
                        "slug": "Jason-Riesa",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Riesa",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason Riesa"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "29951847"
                        ],
                        "name": "Alex Rudnick",
                        "slug": "Alex-Rudnick",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Rudnick",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Rudnick"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1689108"
                        ],
                        "name": "Oriol Vinyals",
                        "slug": "Oriol-Vinyals",
                        "structuredName": {
                            "firstName": "Oriol",
                            "lastName": "Vinyals",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oriol Vinyals"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48342565"
                        ],
                        "name": "Macduff Hughes",
                        "slug": "Macduff-Hughes",
                        "structuredName": {
                            "firstName": "Macduff",
                            "lastName": "Hughes",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Macduff Hughes"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49959210"
                        ],
                        "name": "J. Dean",
                        "slug": "J.-Dean",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Dean",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Dean"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 116
                            }
                        ],
                        "text": "We benchmark the modified model on WMT\u201914 En\u2192Fr using the same test/validation set split described in the GNMT paper (Wu et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026already does a formidable job of generating time-varying parameters of a Mixture Gaussian distribution used to generate realistic handwriting samples, the ability to go one level deeper, and to dynamically generate the generative model is one of the key advantages of HyperRNN over a normal RNN."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 146
                            }
                        ],
                        "text": "The residual network architectures (He et al., 2016a; Zagoruyko & Komodakis, 2016) are popular for image recognition tasks, as they can accommodate very deep networks while maintaining effective gradient flow across layers using skip connections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 102
                            }
                        ],
                        "text": "We experiment with the Neural Machine Translation task using the same experimental setup outlined in (Wu et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 39,
                                "start": 23
                            }
                        ],
                        "text": "One reason for this reduction in accuracy is because different layers of a deep network is trained to extract different levels of features, and require different kinds of filters to perform optimally."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 137
                            }
                        ],
                        "text": "This element-wise operation also has similarities to the Multiplicative RNN (Sutskever et al., 2011) and Multiplicative Integration RNN (Wu et al., 2016) where it was demonstrated that the multiplication-operation encouraged better gradient flow."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 67
                            }
                        ],
                        "text": "The amount of memory required in the system described in Equation 5 will be Nz times the memory of a Basic RNN, which limits the number of hidden units we can use in many practical applications."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 3603249,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "dbde7dfa6cae81df8ac19ef500c42db96c3d1edd",
            "isKey": false,
            "numCitedBy": 4712,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "Neural Machine Translation (NMT) is an end-to-end learning approach for automated translation, with the potential to overcome many of the weaknesses of conventional phrase-based translation systems. Unfortunately, NMT systems are known to be computationally expensive both in training and in translation inference. Also, most NMT systems have difficulty with rare words. These issues have hindered NMT's use in practical deployments and services, where both accuracy and speed are essential. In this work, we present GNMT, Google's Neural Machine Translation system, which attempts to address many of these issues. Our model consists of a deep LSTM network with 8 encoder and 8 decoder layers using attention and residual connections. To improve parallelism and therefore decrease training time, our attention mechanism connects the bottom layer of the decoder to the top layer of the encoder. To accelerate the final translation speed, we employ low-precision arithmetic during inference computations. To improve handling of rare words, we divide words into a limited set of common sub-word units (\"wordpieces\") for both input and output. This method provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delimited models, naturally handles translation of rare words, and ultimately improves the overall accuracy of the system. Our beam search technique employs a length-normalization procedure and uses a coverage penalty, which encourages generation of an output sentence that is most likely to cover all the words in the source sentence. On the WMT'14 English-to-French and English-to-German benchmarks, GNMT achieves competitive results to state-of-the-art. Using a human side-by-side evaluation on a set of isolated simple sentences, it reduces translation errors by an average of 60% compared to Google's phrase-based production system."
            },
            "slug": "Google's-Neural-Machine-Translation-System:-the-Gap-Wu-Schuster",
            "title": {
                "fragments": [],
                "text": "Google's Neural Machine Translation System: Bridging the Gap between Human and Machine Translation"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "GNMT, Google's Neural Machine Translation system, is presented, which attempts to address many of the weaknesses of conventional phrase-based translation systems and provides a good balance between the flexibility of \"character\"-delimited models and the efficiency of \"word\"-delicited models."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34917892"
                        ],
                        "name": "Djork-Arn\u00e9 Clevert",
                        "slug": "Djork-Arn\u00e9-Clevert",
                        "structuredName": {
                            "firstName": "Djork-Arn\u00e9",
                            "lastName": "Clevert",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Djork-Arn\u00e9 Clevert"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2465270"
                        ],
                        "name": "Thomas Unterthiner",
                        "slug": "Thomas-Unterthiner",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Unterthiner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Unterthiner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5273326,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f63e917638553414526a0cc8550de4ad2d83fe7a",
            "isKey": false,
            "numCitedBy": 3703,
            "numCiting": 74,
            "paperAbstract": {
                "fragments": [],
                "text": "We introduce the \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies. Like rectified linear units (ReLUs), leaky ReLUs (LReLUs) and parametrized ReLUs (PReLUs), ELUs alleviate the vanishing gradient problem via the identity for positive values. However, ELUs have improved learning characteristics compared to the units with other activation functions. In contrast to ReLUs, ELUs have negative values which allows them to push mean unit activations closer to zero like batch normalization but with lower computational complexity. Mean shifts toward zero speed up learning by bringing the normal gradient closer to the unit natural gradient because of a reduced bias shift effect. While LReLUs and PReLUs have negative values, too, they do not ensure a noise-robust deactivation state. ELUs saturate to a negative value with smaller inputs and thereby decrease the forward propagated variation and information. Therefore, ELUs code the degree of presence of particular phenomena in the input, while they do not quantitatively model the degree of their absence. In experiments, ELUs lead not only to faster learning, but also to significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers. On CIFAR-100 ELUs networks significantly outperform ReLU networks with batch normalization while batch normalization does not improve ELU networks. ELU networks are among the top 10 reported CIFAR-10 results and yield the best published result on CIFAR-100, without resorting to multi-view evaluation or model averaging. On ImageNet, ELU networks considerably speed up learning compared to a ReLU network with the same architecture, obtaining less than 10% classification error for a single crop, single model network."
            },
            "slug": "Fast-and-Accurate-Deep-Network-Learning-by-Linear-Clevert-Unterthiner",
            "title": {
                "fragments": [],
                "text": "Fast and Accurate Deep Network Learning by Exponential Linear Units (ELUs)"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "The \"exponential linear unit\" (ELU) which speeds up learning in deep neural networks and leads to higher classification accuracies and significantly better generalization performance than ReLUs and LReLUs on networks with more than 5 layers."
            },
            "venue": {
                "fragments": [],
                "text": "ICLR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8387085"
                        ],
                        "name": "Zichao Yang",
                        "slug": "Zichao-Yang",
                        "structuredName": {
                            "firstName": "Zichao",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zichao Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3009779"
                        ],
                        "name": "Marcin Moczulski",
                        "slug": "Marcin-Moczulski",
                        "structuredName": {
                            "firstName": "Marcin",
                            "lastName": "Moczulski",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Marcin Moczulski"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1715051"
                        ],
                        "name": "Misha Denil",
                        "slug": "Misha-Denil",
                        "structuredName": {
                            "firstName": "Misha",
                            "lastName": "Denil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Misha Denil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1737568"
                        ],
                        "name": "N. D. Freitas",
                        "slug": "N.-D.-Freitas",
                        "structuredName": {
                            "firstName": "Nando",
                            "lastName": "Freitas",
                            "middleNames": [
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. D. Freitas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46234526"
                        ],
                        "name": "Alex Smola",
                        "slug": "Alex-Smola",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Smola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alex Smola"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1779453"
                        ],
                        "name": "Le Song",
                        "slug": "Le-Song",
                        "structuredName": {
                            "firstName": "Le",
                            "lastName": "Song",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Le Song"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2124864085"
                        ],
                        "name": "Ziyu Wang",
                        "slug": "Ziyu-Wang",
                        "structuredName": {
                            "firstName": "Ziyu",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ziyu Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 166,
                                "start": 149
                            }
                        ],
                        "text": "\u2026interacting with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are predicted by another\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8132136,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "27a99c21a1324f087b2f144adc119f04137dfd87",
            "isKey": false,
            "numCitedBy": 232,
            "numCiting": 53,
            "paperAbstract": {
                "fragments": [],
                "text": "The fully-connected layers of deep convolutional neural networks typically contain over 90% of the network parameters. Reducing the number of parameters while preserving predictive performance is critically important for training big models in distributed systems and for deployment in embedded devices. In this paper, we introduce a novel Adaptive Fastfood transform to reparameterize the matrix-vector multiplication of fully connected layers. Reparameterizing a fully connected layer with d inputs and n outputs with the Adaptive Fastfood transform reduces the storage and computational costs costs from O(nd) to O(n) and O(n log d) respectively. Using the Adaptive Fastfood transform in convolutional networks results in what we call a deep fried convnet. These convnets are end-to-end trainable, and enable us to attain substantial reductions in the number of parameters without affecting prediction accuracy on the MNIST and ImageNet datasets."
            },
            "slug": "Deep-Fried-Convnets-Yang-Moczulski",
            "title": {
                "fragments": [],
                "text": "Deep Fried Convnets"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A novel Adaptive Fastfood transform is introduced to reparameterize the matrix-vector multiplication of fully connected layers of deep convolutional neural networks, resulting in what is called a deep fried convnet."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE International Conference on Computer Vision (ICCV)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 73
                            }
                        ],
                        "text": "This is indeed the case in the residual network family of architectures (He et al., 2016a) that we will be experimenting with later is an example of such a design."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 94
                            }
                        ],
                        "text": "While residual networks can be be very deep, and in some experiments as deep as 1001 layers ((He et al., 2016b), it is important to understand whether some these layers share common properties and can be reduced effectively by introducing weight sharing."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 36
                            }
                        ],
                        "text": "The residual network architectures (He et al., 2016a; Zagoruyko & Komodakis, 2016) are popular for image recognition tasks, as they can accommodate very deep networks while maintaining effective gradient flow across layers using skip connections."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": false,
            "numCitedBy": 97653,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1696384"
                        ],
                        "name": "S. Srihari",
                        "slug": "S.-Srihari",
                        "structuredName": {
                            "firstName": "Sargur",
                            "lastName": "Srihari",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Srihari"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "\u2026already does a formidable job of generating time-varying parameters of a Mixture Gaussian distribution used to generate realistic handwriting samples, the ability to go one level deeper, and to dynamically generate the generative model is one of the key advantages of HyperRNN over a normal RNN."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14616022,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "d4ca18249446328c86d9da295a21c679aea1ed77",
            "isKey": false,
            "numCitedBy": 709,
            "numCiting": 10,
            "paperAbstract": {
                "fragments": [],
                "text": "p(t | x) t x x x = 0.8 = 0.5 = 0.2 Figure 7: Plot of the conditional probability densities of the target data, for various values of x, obtained by taking vertical slices through the contours in Figure 6, for x = 0:2, x = 0:5 and x = 0:8. It is clear that the Mixture Density Network is able to capture correctly the multimodal nature of the target data density function at intermediate values of x. x Figure 8: Plot of the priors i (x) as a function of x for the 3 kernel functions from the same Mixture Density Network as was used to plot Figure 6. At both small and large values of x, where the conditional probability density of the target data is unimodal, only one of the kernels has a prior probability which diiers signiicantly from zero. At intermediate values of x, where the conditional density is tri-modal, the three kernels have comparable priors."
            },
            "slug": "Mixture-Density-Networks-Srihari",
            "title": {
                "fragments": [],
                "text": "Mixture Density Networks"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1734174"
                        ],
                        "name": "M. Marcus",
                        "slug": "M.-Marcus",
                        "structuredName": {
                            "firstName": "Mitchell",
                            "lastName": "Marcus",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Marcus"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2424234"
                        ],
                        "name": "Beatrice Santorini",
                        "slug": "Beatrice-Santorini",
                        "structuredName": {
                            "firstName": "Beatrice",
                            "lastName": "Santorini",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Beatrice Santorini"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2063206"
                        ],
                        "name": "Mary Ann Marcinkiewicz",
                        "slug": "Mary-Ann-Marcinkiewicz",
                        "structuredName": {
                            "firstName": "Mary",
                            "lastName": "Marcinkiewicz",
                            "middleNames": [
                                "Ann"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mary Ann Marcinkiewicz"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 252796,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b44fcbeea9415d400c5f5789d6b892b6f98daff",
            "isKey": false,
            "numCitedBy": 8207,
            "numCiting": 75,
            "paperAbstract": {
                "fragments": [],
                "text": "Abstract : As a result of this grant, the researchers have now published oil CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, with over 3 million words of that material assigned skeletal grammatical structure. This material now includes a fully hand-parsed version of the classic Brown corpus. About one half of the papers at the ACL Workshop on Using Large Text Corpora this past summer were based on the materials generated by this grant."
            },
            "slug": "Building-a-Large-Annotated-Corpus-of-English:-The-Marcus-Santorini",
            "title": {
                "fragments": [],
                "text": "Building a Large Annotated Corpus of English: The Penn Treebank"
            },
            "tldr": {
                "abstractSimilarityScore": 80,
                "text": "As a result of this grant, the researchers have now published on CDROM a corpus of over 4 million words of running text annotated with part-of- speech (POS) tags, which includes a fully hand-parsed version of the classic Brown corpus."
            },
            "venue": {
                "fragments": [],
                "text": "CL"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In the following experiments, we will benchmark the performance of static hypernetworks on image recognition with MNIST and CIFAR-10, and the performance of dynamic hypernetworks on language modelling with Penn Treebank and Hutter Prize Wikipedia (enwik8) datasets and handwriting generation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 64
                            }
                        ],
                        "text": "We will train our model on the IAM online handwriting database (Liwicki & Bunke, 2005) and have our model predict pen strokes as per Section 4.2 of (Graves, 2013)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1745101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9eb7daa88879f283ae05e359d6c502a320b833c9",
            "isKey": true,
            "numCitedBy": 182,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present IAM-OnDB - a new large online handwritten sentences database. It is publicly available and consists of text acquired via an electronic interface from a whiteboard. The database contains about 86 K word instances from an 11 K dictionary written by more than 200 writers. We also describe a recognizer for unconstrained English text that was trained and tested using this database. This recognizer is based on hidden Markov models (HMMs). In our experiments we show that by using larger training sets we can significantly increase the word recognition rate. This recognizer may serve as a benchmark reference for future research."
            },
            "slug": "IAM-OnDB-an-on-line-English-sentence-database-from-Liwicki-Bunke",
            "title": {
                "fragments": [],
                "text": "IAM-OnDB - an on-line English sentence database acquired from handwritten text on a whiteboard"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "IAM-OnDB is a new large online handwritten sentences database that consists of text acquired via an electronic interface from a whiteboard and a recognizer for unconstrained English text that was trained and tested using this database."
            },
            "venue": {
                "fragments": [],
                "text": "Eighth International Conference on Document Analysis and Recognition (ICDAR'05)"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688882"
                        ],
                        "name": "Yann LeCun",
                        "slug": "Yann-LeCun",
                        "structuredName": {
                            "firstName": "Yann",
                            "lastName": "LeCun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yann LeCun"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2219581"
                        ],
                        "name": "B. Boser",
                        "slug": "B.-Boser",
                        "structuredName": {
                            "firstName": "Bernhard",
                            "lastName": "Boser",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Boser"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747317"
                        ],
                        "name": "J. Denker",
                        "slug": "J.-Denker",
                        "structuredName": {
                            "firstName": "John",
                            "lastName": "Denker",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Denker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37274089"
                        ],
                        "name": "D. Henderson",
                        "slug": "D.-Henderson",
                        "structuredName": {
                            "firstName": "Donnie",
                            "lastName": "Henderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Henderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2799635"
                        ],
                        "name": "R. Howard",
                        "slug": "R.-Howard",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Howard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Howard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34859193"
                        ],
                        "name": "W. Hubbard",
                        "slug": "W.-Hubbard",
                        "structuredName": {
                            "firstName": "Wayne",
                            "lastName": "Hubbard",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Hubbard"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2041866"
                        ],
                        "name": "L. Jackel",
                        "slug": "L.-Jackel",
                        "structuredName": {
                            "firstName": "Lawrence",
                            "lastName": "Jackel",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Jackel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 128,
                                "start": 110
                            }
                        ],
                        "text": "On image classification with CIFAR-10, hypernetworks, when being used to generate weights for a deep convnet (LeCun et al., 1990), obtain respectable results compared to state-of-the-art models while having fewer learnable parameters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 113
                            }
                        ],
                        "text": "On a handwriting generation task with IAM handwriting dataset, Hypernetworks for LSTM achieves high quantitative and qualitative results."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2542741,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "86ab4cae682fbd49c5a5bedb630e5a40fa7529f6",
            "isKey": false,
            "numCitedBy": 2955,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "We present an application of back-propagation networks to handwritten digit recognition. Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task. The input of the network consists of normalized images of isolated digits. The method has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "slug": "Handwritten-Digit-Recognition-with-a-Network-LeCun-Boser",
            "title": {
                "fragments": [],
                "text": "Handwritten Digit Recognition with a Back-Propagation Network"
            },
            "tldr": {
                "abstractSimilarityScore": 62,
                "text": "Minimal preprocessing of the data was required, but architecture of the network was highly constrained and specifically designed for the task, and has 1% error rate and about a 9% reject rate on zipcode digits provided by the U.S. Postal Service."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "8068040"
                        ],
                        "name": "J. Xia",
                        "slug": "J.-Xia",
                        "structuredName": {
                            "firstName": "Jianlin",
                            "lastName": "Xia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Xia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144995007"
                        ],
                        "name": "S. Chandrasekaran",
                        "slug": "S.-Chandrasekaran",
                        "structuredName": {
                            "firstName": "Shivkumar",
                            "lastName": "Chandrasekaran",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Chandrasekaran"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145932526"
                        ],
                        "name": "M. Gu",
                        "slug": "M.-Gu",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Gu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "10685934"
                        ],
                        "name": "Xiaoye S. Li",
                        "slug": "Xiaoye-S.-Li",
                        "structuredName": {
                            "firstName": "Xiaoye",
                            "lastName": "Li",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaoye S. Li"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 109
                            }
                        ],
                        "text": "Our approach of constructing g(.) is similar to the hierarchically semiseparable matrix approach proposed by Xia et al. (2010)."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 11974447,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "7613796ad094e7e17efc789c35a9e3301ed712ce",
            "isKey": false,
            "numCitedBy": 279,
            "numCiting": 58,
            "paperAbstract": {
                "fragments": [],
                "text": "Semiseparable matrices and many other rank\u2010structured matrices have been widely used in developing new fast matrix algorithms. In this paper, we generalize the hierarchically semiseparable (HSS) matrix representations and propose some fast algorithms for HSS matrices. We represent HSS matrices in terms of general binary HSS trees and use simplified postordering notation for HSS forms. Fast HSS algorithms including new HSS structure generation and HSS form Cholesky factorization are developed. Moreover, we provide a new linear complexity explicit ULV factorization algorithm for symmetric positive definite HSS matrices with a low\u2010rank property. The corresponding factors can be used to solve the HSS systems also in linear complexity. Numerical examples demonstrate the efficiency of the algorithms. All these algorithms have nice data locality. They are useful in developing fast\u2010structured numerical methods for large discretized PDEs (such as elliptic equations), integral equations, eigenvalue problems, etc. Some applications are shown. Copyright \u00a9 2009 John Wiley & Sons, Ltd."
            },
            "slug": "Fast-algorithms-for-hierarchically-semiseparable-Xia-Chandrasekaran",
            "title": {
                "fragments": [],
                "text": "Fast algorithms for hierarchically semiseparable matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 51,
                "text": "This paper generalizes the hierarchically semiseparable (HSS) matrix representations and proposes some fast algorithms for HSS matrices that are useful in developing fast\u2010structured numerical methods for large discretized PDEs, integral equations, eigenvalue problems, etc."
            },
            "venue": {
                "fragments": [],
                "text": "Numer. Linear Algebra Appl."
            },
            "year": 2010
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 185,
                                "start": 170
                            }
                        ],
                        "text": "As the embeddings zh, zx and zb are of dimensions Nz , which is typically smaller than the hidden state size Nh\u0302 of the HyperRNN cell, a linear network is used to project the output of the HyperRNN cell into the embeddings in Equation 6."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 292,
                                "start": 277
                            }
                        ],
                        "text": "\u2026version of the setup of Equation 5:\nht = \u03c6 ( dh(zh) Whht\u22121 + dx(zx) Wxxt + b(zb) ) , where\ndh(zh) =Whzzh\ndx(zx) =Wxzzx b(zb) =Wbzzb + b0\n(8)\nThis formulation of the HyperRNN has some similarities to Recurrent Batch Normalization (Cooijmans et al., 2016) and Layer Normalization (Ba et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 308,
                                "start": 290
                            }
                        ],
                        "text": "Below is the more memory efficient version of the setup of Equation 5:\nht = \u03c6 ( dh(zh) Whht\u22121 + dx(zx) Wxxt + b(zb) ) , where\ndh(zh) =Whzzh\ndx(zx) =Wxzzx b(zb) =Wbzzb + b0\n(8)\nThis formulation of the HyperRNN has some similarities to Recurrent Batch Normalization (Cooijmans et al., 2016) and Layer Normalization (Ba et al., 2016)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "We want to know whether the HyperLSTM cell can learn a weight adjustment policy that can rival statistical moments-based normalization methods, hence Layer Normalization will be one of our baseline methods."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 30
                            }
                        ],
                        "text": "It is a normal LSTM cell with Layer Normalization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 208,
                                "start": 188
                            }
                        ],
                        "text": "\u2026with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are predicted by another network."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 130,
                                "start": 112
                            }
                        ],
                        "text": "We will therefore conduct experiments on two versions of HyperLSTM, one with and one without the application of Layer Normalization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 224,
                                "start": 220
                            }
                        ],
                        "text": "The concept of a network interacting with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are predicted by another network."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "While the LSTM model alone already does a formidable job of generating time-varying parameters of a Mixture Gaussian distribution used to generate realistic handwriting samples, the ability to go one level deeper, and to dynamically generate the generative model is one of the key advantages of\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 78
                            }
                        ],
                        "text": "As discussed earlier, we will also experiment with adding the option to use a Layer Normalization layer in the HyperLSTM."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Layer normalization. NIPS"
            },
            "venue": {
                "fragments": [],
                "text": "Layer normalization. NIPS"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Tinne Tuytelaars, and Luc Van Gool. Dynamic filter networks"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 42
                            }
                        ],
                        "text": "Unlike previous approaches (Graves, 2013; Ognawala & Bayer, 2014) of applying weight noise during training, we instead also apply dropout to the recurrent layer (Henaff et al., 2016) with the same dropout probability."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In the following experiments, we will benchmark the performance of static hypernetworks on image recognition with MNIST and CIFAR-10, and the performance of dynamic hypernetworks on language modelling with Penn Treebank and Hutter Prize Wikipedia (enwik8) datasets and handwriting generation."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Regularizing recurrent networks-on injected noise and normbased methods. arXiv preprint arXiv:1410"
            },
            "venue": {
                "fragments": [],
                "text": "Regularizing recurrent networks-on injected noise and normbased methods. arXiv preprint arXiv:1410"
            },
            "year": 2014
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The human knowledge compression contest. 2012. URL http://prize"
            },
            "venue": {
                "fragments": [],
                "text": "The human knowledge compression contest. 2012. URL http://prize"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deeplysupervised nets"
            },
            "venue": {
                "fragments": [],
                "text": "AISTATS"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 37,
                                "start": 19
                            }
                        ],
                        "text": "parameters in the model as a trade off."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Deep recurrent models with fastforward connections for neural machine translation. CoRR, abs/1606"
            },
            "venue": {
                "fragments": [],
                "text": "Deep recurrent models with fastforward connections for neural machine translation. CoRR, abs/1606"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Recurrent memory array structures. arXiv preprint arXiv:1607"
            },
            "venue": {
                "fragments": [],
                "text": "Recurrent memory array structures. arXiv preprint arXiv:1607"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 116
                            }
                        ],
                        "text": "We benchmark the modified model on WMT\u201914 En\u2192Fr using the same test/validation set split described in the GNMT paper (Wu et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 102
                            }
                        ],
                        "text": "We experiment with the Neural Machine Translation task using the same experimental setup outlined in (Wu et al., 2016)."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 152,
                                "start": 137
                            }
                        ],
                        "text": "This element-wise operation also has similarities to the Multiplicative RNN (Sutskever et al., 2011) and Multiplicative Integration RNN (Wu et al., 2016) where it was demonstrated that the multiplication-operation encouraged better gradient flow."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Yoshua Bengio, and Ruslan Salakhutdinov. On multiplicative integration with recurrent neural networks"
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 120,
                                "start": 108
                            }
                        ],
                        "text": "We train our model on the larger and more challenging Hutter Prize Wikipedia dataset, also known as enwik8 (Hutter, 2012) consisting of a sequence of 100M characters composed of 205 unique characters."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The human knowledge compression contest"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Publications in the Greek movie ''[[The Great Theory of"
            },
            "venue": {
                "fragments": [],
                "text": "Publications in the Greek movie ''[[The Great Theory of"
            }
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 168,
                                "start": 150
                            }
                        ],
                        "text": "\u2026concept of a network interacting with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 161,
                                "start": 143
                            }
                        ],
                        "text": "\u2026of a network interacting with another network is central to the work of (Jaderberg et al., 2016; Andrychowicz et al., 2016), and especially (Denil et al., 2013; Yang et al., 2015; Bertinetto et al., 2016; De Brabandere et al., 2016), where certain parameters in a convolutional network are\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Predicting Parameters in Deep Learning. In NIPS"
            },
            "venue": {
                "fragments": [],
                "text": "Predicting Parameters in Deep Learning. In NIPS"
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "URL http://prize"
            },
            "venue": {
                "fragments": [],
                "text": "hutter1.net/."
            },
            "year": 2012
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 34,
            "methodology": 23,
            "result": 4
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 62,
        "totalPages": 7
    },
    "page_url": "https://www.semanticscholar.org/paper/HyperNetworks-Ha-Dai/563783de03452683a9206e85fe6d661714436686?sort=total-citations"
}