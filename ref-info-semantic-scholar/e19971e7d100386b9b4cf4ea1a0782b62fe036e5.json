{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714004"
                        ],
                        "name": "A. Mnih",
                        "slug": "A.-Mnih",
                        "structuredName": {
                            "firstName": "Andriy",
                            "lastName": "Mnih",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Mnih"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "When the predictions of multiple PMF models are linearly combined with the predictions of multiple RBM models, recently introduced by [ 8 ], we achieve an error rate of 0.8861, that is nearly 7% better than the score of Netflix\u2019s own system."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7285098,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1626c940a64ad96a7ed53d7d6c0df63c6696956b",
            "isKey": false,
            "numCitedBy": 1880,
            "numCiting": 17,
            "paperAbstract": {
                "fragments": [],
                "text": "Most of the existing approaches to collaborative filtering cannot handle very large data sets. In this paper we show how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies. We present efficient learning and inference procedures for this class of models and demonstrate that RBM's can be successfully applied to the Netflix data set, containing over 100 million user/movie ratings. We also show that RBM's slightly outperform carefully-tuned SVD models. When the predictions of multiple RBM models and multiple SVD models are linearly combined, we achieve an error rate that is well over 6% better than the score of Netflix's own system."
            },
            "slug": "Restricted-Boltzmann-machines-for-collaborative-Salakhutdinov-Mnih",
            "title": {
                "fragments": [],
                "text": "Restricted Boltzmann machines for collaborative filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 61,
                "text": "This paper shows how a class of two-layer undirected graphical models, called Restricted Boltzmann Machines (RBM's), can be used to model tabular data, such as user's ratings of movies, and demonstrates that RBM's can be successfully applied to the Netflix data set."
            },
            "venue": {
                "fragments": [],
                "text": "ICML '07"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211659"
                        ],
                        "name": "Jason D. M. Rennie",
                        "slug": "Jason-D.-M.-Rennie",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Rennie",
                            "middleNames": [
                                "D.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason D. M. Rennie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 503367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6cd49dd5d26d1e8e33891f8e64ad3b5012e90ba6",
            "isKey": false,
            "numCitedBy": 1045,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Maximum Margin Matrix Factorization (MMMF) was recently suggested (Srebro et al., 2005) as a convex, infinite dimensional alternative to low-rank approximations and standard factor models. MMMF can be formulated as a semi-definite programming (SDP) and learned using standard SDP solvers. However, current SDP solvers can only handle MMMF problems on matrices of dimensionality up to a few hundred. Here, we investigate a direct gradient-based optimization method for MMMF and demonstrate it on large collaborative prediction problems. We compare against results obtained by Marlin (2004) and find that MMMF substantially outperforms all nine methods he tested."
            },
            "slug": "Fast-maximum-margin-matrix-factorization-for-Rennie-Srebro",
            "title": {
                "fragments": [],
                "text": "Fast maximum margin matrix factorization for collaborative prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "This work investigates a direct gradient-based optimization method for MMMF and finds that MMMf substantially outperforms all nine methods he tested and demonstrates it on large collaborative prediction problems."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2888023"
                        ],
                        "name": "D. Dueck",
                        "slug": "D.-Dueck",
                        "structuredName": {
                            "firstName": "Delbert",
                            "lastName": "Dueck",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Dueck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749650"
                        ],
                        "name": "B. Frey",
                        "slug": "B.-Frey",
                        "structuredName": {
                            "firstName": "Brendan",
                            "lastName": "Frey",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Frey"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 58,
                                "start": 51
                            }
                        ],
                        "text": "We also place zero-mean spherical Gaussian prio rs [1, 11] on user and movie feature vectors: p(U |\u03c3(2) U ) = N"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18330015,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5604264b7e470c87dbb19be713a1c9bce2852b5c",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "Many kinds of data can be viewed as consisting of a set of vectors, each of which is a noisy combination of a small number of noisy prototype vectors. Physically, these prototype vectors may correspond to different hidden variables that play a role in determining the measured data. For example, a gene\u2019s expression is inuenced by the presence of transcription factor proteins, and two genes may be activated by overlapping sets of transcription factors. Consequently, the activity of each gene can be explained by the activities of a small number of transcription factors. This task can be viewed as the problem of factorizing a data matrix, while taking into account hard constraints reecting structural knowledge of the problem and probabilistic relationships between variables that are induced by known uncertainties in the problem. We present soft-decision probabilistic sparse matrix factorization (PSMF) to better account for uncertainties due to varying levels of noise in the data, varying levels of noise in the prototypes used to explain the data, and uncertainty as to which hidden prototypes are selected to explain each expression vector."
            },
            "slug": "Probabilistic-Sparse-Matrix-Factorization-Dueck-Frey",
            "title": {
                "fragments": [],
                "text": "Probabilistic Sparse Matrix Factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work presents soft-decision probabilistic sparse matrix factorization (PSMF), a method for factorizing a data matrix while taking into account hard constraints reecting structural knowledge of the problem and Probabilistic relationships between variables that are induced by known uncertainties in the problem."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805742"
                        ],
                        "name": "Benjamin M Marlin",
                        "slug": "Benjamin-M-Marlin",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Marlin",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin M Marlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1804104"
                        ],
                        "name": "R. Zemel",
                        "slug": "R.-Zemel",
                        "structuredName": {
                            "firstName": "Richard",
                            "lastName": "Zemel",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Zemel"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 75
                            }
                        ],
                        "text": "A variety of probabilistic factor-based models has been pro posed recently [2, 3, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15346618,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cbb985cba78d7c42986381fa4a3fa92ba8092063",
            "isKey": false,
            "numCitedBy": 83,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a class of causal, discrete latent variable models called Multiple Multiplicative Factor models (MMFs). A data vector is represented in the latent space as a vector of factors that have discrete, non-negative expression levels. Each factor proposes a distribution over the data vector. The distinguishing feature of MMFs is that they combine the factors' proposed distributions multiplicatively, taking into account factor expression levels. The product formulation of MMFs allow factors to specialize to a subset of the items, while the causal generative semantics mean MMFs can readily accommodate missing data. This makes MMFs distinct from both directed models with mixture semantics and undirected product models. In this paper we present empirical results from the collaborative filtering domain showing that a binary/multinomial MMF model matches the performance of the best existing models while learning an interesting latent space description of the users."
            },
            "slug": "The-multiple-multiplicative-factor-model-for-Marlin-Zemel",
            "title": {
                "fragments": [],
                "text": "The multiple multiplicative factor model for collaborative filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "Empirical results from the collaborative filtering domain are presented showing that a binary/multinomial MMF model matches the performance of the best existing models while learning an interesting latent space description of the users."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1802785"
                        ],
                        "name": "S. Nowlan",
                        "slug": "S.-Nowlan",
                        "structuredName": {
                            "firstName": "Steven",
                            "lastName": "Nowlan",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Nowlan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 148
                            }
                        ],
                        "text": "Introducing priors for the hyperparameters and max imizing the log-posterior of the model over both parameters a nd hyperparameters as suggested in [6] allows model complexity to be controlled automatically bas ed on the training data."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 3,
                                "start": 0
                            }
                        ],
                        "text": "[6], originally applied to neural networks, can be used to de termine suitable values for the regularization parameters of a PMF model automatically without sig nificantly affecting the time needed to train the model."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5597033,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "de75e4e15e22d4376300e5c968e2db44be29ac9e",
            "isKey": false,
            "numCitedBy": 661,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "One way of simplifying neural networks so they generalize better is to add an extra term to the error function that will penalize complexity. Simple versions of this approach include penalizing the sum of the squares of the weights or penalizing the number of nonzero weights. We propose a more complicated penalty term in which the distribution of weight values is modeled as a mixture of multiple gaussians. A set of weights is simple if the weights have high probability density under the mixture model. This can be achieved by clustering the weights into subsets with the weights in each cluster having very similar values. Since we do not know the appropriate means or variances of the clusters in advance, we allow the parameters of the mixture model to adapt at the same time as the network learns. Simulations on two different problems demonstrate that this complexity term is more effective than previous complexity terms."
            },
            "slug": "Simplifying-Neural-Networks-by-Soft-Weight-Sharing-Nowlan-Hinton",
            "title": {
                "fragments": [],
                "text": "Simplifying Neural Networks by Soft Weight-Sharing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A more complicated penalty term is proposed in which the distribution of weight values is modeled as a mixture of multiple gaussians, which allows the parameters of the mixture model to adapt at the same time as the network learns."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1992
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1805742"
                        ],
                        "name": "Benjamin M Marlin",
                        "slug": "Benjamin-M-Marlin",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Marlin",
                            "middleNames": [
                                "M"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benjamin M Marlin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 75
                            }
                        ],
                        "text": "A variety of probabilistic factor-based models has been pro posed recently [2, 3, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 381243,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "992c958fe0f4bd148b6f4304e1b5e458b8575cb1",
            "isKey": false,
            "numCitedBy": 343,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we present a generative latent variable model for rating-based collaborative filtering called the User Rating Profile model (URP). The generative process which underlies URP is designed to produce complete user rating profiles, an assignment of one rating to each item for each user. Our model represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable. The rating for each item is generated by selecting a user attitude for the item, and then selecting a rating according to the preference pattern associated with that attitude. URP is related to several models including a multinomial mixture model, the aspect model [7], and LDA [1], but has clear advantages over each."
            },
            "slug": "Modeling-User-Rating-Profiles-For-Collaborative-Marlin",
            "title": {
                "fragments": [],
                "text": "Modeling User Rating Profiles For Collaborative Filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 66,
                "text": "A generative latent variable model for rating-based collaborative filtering called the User Rating Profile model (URP), which represents each user as a mixture of user attitudes, and the mixing proportions are distributed according to a Dirichlet random variable."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 12
                            }
                        ],
                        "text": "As shown by [9], this seemingly minor modification results in a difficult non-convex optimization problem whicannot be solved using standard SVD implementations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 5815325,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "755e566d3f10d057bc9e4908e4016ae6f7ca0753",
            "isKey": false,
            "numCitedBy": 787,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We study the common problem of approximating a target matrix with a matrix of lower rank. We provide a simple and efficient (EM) algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closed-form solution in general. We analyze, in addition, the nature of locally optimal solutions that arise in this context, demonstrate the utility of accommodating the weights in reconstructing the underlying low-rank representation, and extend the formulation to non-Gaussian noise models such as logistic models. Finally, we apply the methods developed to a collaborative filtering task."
            },
            "slug": "Weighted-Low-Rank-Approximations-Srebro-Jaakkola",
            "title": {
                "fragments": [],
                "text": "Weighted Low-Rank Approximations"
            },
            "tldr": {
                "abstractSimilarityScore": 57,
                "text": "This work provides a simple and efficient algorithm for solving weighted low-rank approximation problems, which, unlike their unweighted version, do not admit a closed-form solution in general."
            },
            "venue": {
                "fragments": [],
                "text": "ICML"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706280"
                        ],
                        "name": "Nathan Srebro",
                        "slug": "Nathan-Srebro",
                        "structuredName": {
                            "firstName": "Nathan",
                            "lastName": "Srebro",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nathan Srebro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35211659"
                        ],
                        "name": "Jason D. M. Rennie",
                        "slug": "Jason-D.-M.-Rennie",
                        "structuredName": {
                            "firstName": "Jason",
                            "lastName": "Rennie",
                            "middleNames": [
                                "D.",
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jason D. M. Rennie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "35132120"
                        ],
                        "name": "T. Jaakkola",
                        "slug": "T.-Jaakkola",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Jaakkola",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Jaakkola"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 27,
                                "start": 23
                            }
                        ],
                        "text": "the number of factors, [10] proposed penalizing the norms of U andV ."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 5048382,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cedf154c28178370d95510112413dc8cb48120a8",
            "isKey": false,
            "numCitedBy": 1128,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a novel approach to collaborative prediction, using low-norm instead of low-rank factorizations. The approach is inspired by, and has strong connections to, large-margin linear discrimination. We show how to learn low-norm factorizations by solving a semi-definite program, and discuss generalization error bounds for them."
            },
            "slug": "Maximum-Margin-Matrix-Factorization-Srebro-Rennie",
            "title": {
                "fragments": [],
                "text": "Maximum-Margin Matrix Factorization"
            },
            "tldr": {
                "abstractSimilarityScore": 86,
                "text": "A novel approach to collaborative prediction is presented, using low-norm instead of low-rank factorizations, inspired by, and has strong connections to, large-margin linear discrimination."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143936663"
                        ],
                        "name": "Thomas Hofmann",
                        "slug": "Thomas-Hofmann",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Hofmann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Hofmann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 74
                            }
                        ],
                        "text": "A variety of probabilistic factor-based models has been proposed recently [2, 3, 4]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 653762,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "355b86dafd852e4df905f6ad9402c7d03831d618",
            "isKey": false,
            "numCitedBy": 2692,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Probabilistic Latent Semantic Analysis is a novel statistical technique for the analysis of two-mode and co-occurrence data, which has applications in information retrieval and filtering, natural language processing, machine learning from text, and in related areas. Compared to standard Latent Semantic Analysis which stems from linear algebra and performs a Singular Value Decomposition of co-occurrence tables, the proposed method is based on a mixture decomposition derived from a latent class model. This results in a more principled approach which has a solid foundation in statistics. In order to avoid overfitting, we propose a widely applicable generalization of maximum likelihood model fitting by tempered EM. Our approach yields substantial and consistent improvements over Latent Semantic Analysis in a number of experiments."
            },
            "slug": "Probabilistic-Latent-Semantic-Analysis-Hofmann",
            "title": {
                "fragments": [],
                "text": "Probabilistic Latent Semantic Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This work proposes a widely applicable generalization of maximum likelihood model fitting by tempered EM, based on a mixture decomposition derived from a latent class model which results in a more principled approach which has a solid foundation in statistics."
            },
            "venue": {
                "fragments": [],
                "text": "UAI"
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678311"
                        ],
                        "name": "M. Welling",
                        "slug": "M.-Welling",
                        "structuredName": {
                            "firstName": "Max",
                            "lastName": "Welling",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Welling"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1398315116"
                        ],
                        "name": "M. Rosen-Zvi",
                        "slug": "M.-Rosen-Zvi",
                        "structuredName": {
                            "firstName": "Michal",
                            "lastName": "Rosen-Zvi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rosen-Zvi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 73
                            }
                        ],
                        "text": "The major drawback of such models is that exact inference is intractable [12], which means that potentially slow or inaccurate approximations are required for computing the posterior distribution over hidden factors in such models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2388827,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2184fb6d32bc46f252b940035029273563c4fc82",
            "isKey": false,
            "numCitedBy": 506,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Directed graphical models with one layer of observed random variables and one or more layers of hidden random variables have been the dominant modelling paradigm in many research fields. Although this approach has met with considerable success, the causal semantics of these models can make it difficult to infer the posterior distribution over the hidden variables. In this paper we propose an alternative two-layer model based on exponential family distributions and the semantics of undirected models. Inference in these \"exponential family harmoniums\" is fast while learning is performed by minimizing contrastive divergence. A member of this family is then studied as an alternative probabilistic model for latent semantic indexing. In experiments it is shown that they perform well on document retrieval tasks and provide an elegant solution to searching with keywords."
            },
            "slug": "Exponential-Family-Harmoniums-with-an-Application-Welling-Rosen-Zvi",
            "title": {
                "fragments": [],
                "text": "Exponential Family Harmoniums with an Application to Information Retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "An alternative two-layer model based on exponential family distributions and the semantics of undirected models is proposed, which performs well on document retrieval tasks and provides an elegant solution to searching with keywords."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2831141"
                        ],
                        "name": "Michael E. Tipping",
                        "slug": "Michael-E.-Tipping",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Tipping",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael E. Tipping"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1792884"
                        ],
                        "name": "Charles M. Bishop",
                        "slug": "Charles-M.-Bishop",
                        "structuredName": {
                            "firstName": "Charles",
                            "lastName": "Bishop",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Charles M. Bishop"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 25
                            }
                        ],
                        "text": "The major drawback of such models is that exact inference is intractable [12], which means that potentially slow or inaccurate approximations are required for computing the posterior distribution over hidden factors in such models."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 15538672,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "ff4b3bbb455c9cc561ddec097a869140b3c1303d",
            "isKey": false,
            "numCitedBy": 3438,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Principal component analysis (PCA) is a ubiquitous technique for data analysis and processing, but one which is not based on a probability model. We demonstrate how the principal axes of a set of observed data vectors may be determined through maximum likelihood estimation of parameters in a latent variable model that is closely related to factor analysis. We consider the properties of the associated likelihood function, giving an EM algorithm for estimating the principal subspace iteratively, and discuss, with illustrative examples, the advantages conveyed by this probabilistic approach to PCA."
            },
            "slug": "Probabilistic-Principal-Component-Analysis-Tipping-Bishop",
            "title": {
                "fragments": [],
                "text": "Probabilistic Principal Component Analysis"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 8,
            "methodology": 3
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 11,
        "totalPages": 2
    },
    "page_url": "https://www.semanticscholar.org/paper/Probabilistic-Matrix-Factorization-Salakhutdinov-Mnih/e19971e7d100386b9b4cf4ea1a0782b62fe036e5?sort=total-citations"
}