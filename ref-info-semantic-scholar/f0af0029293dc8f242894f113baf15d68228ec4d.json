{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7792071"
                        ],
                        "name": "Xiangnan He",
                        "slug": "Xiangnan-He",
                        "structuredName": {
                            "firstName": "Xiangnan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangnan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 94
                            }
                        ],
                        "text": "Note that our recent work of neural FM has proposed a Bilinear Interaction pooling operation [He and Chua, 2017], which can be seen as using a sum pooling over the pair-wise interaction layer."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 76,
                                "start": 59
                            }
                        ],
                        "text": "This work is orthogonal with our recent work on neural FM [He and Chua, 2017] that develops deep variants of FM for modelling high-order feature interactions, and it is the time that introduces the attention mechanism to factorization machines."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 151,
                                "start": 134
                            }
                        ],
                        "text": "When performing supervised learning on categorical predictor variables, it is important to account for the interactions between them [He and Chua, 2017; Cheng et al., 2016]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 290,
                                "start": 273
                            }
                        ],
                        "text": "\u2026that DeepCross is the deepest method (that stacks 10 layers above the embedding layer) among all compared methods, it provides evidence that deeper leaning is not always helpful, as deep networks can suffer from overfitting and are more difficult to optimize in practice [He and Chua, 2017]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 140
                            }
                        ],
                        "text": "Distinct from the continuous raw features found in images and audios, input features of the Web domain are mostly discrete and categorical [He and Chua, 2017]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 63
                            }
                        ],
                        "text": "Many variants to FM have been proposed, such as the neural FM [He and Chua, 2017] that deepens FM under the neural framework to learn high-order feature interactions, and the field-aware FM [Juan et al., 2016] that associates multiple embedding vectors for a feature to differentiate its interaction\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 302,
                                "start": 283
                            }
                        ],
                        "text": "Considering that DeepCross is the deepest method (that stacks 10 layers above the embedding layer) among all compared methods, it provides evidence that deeper leaning is not always helpful, as deep networks can suffer from overfitting and are more difficult to optimize in practice [He and Chua, 2017]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 81,
                                "start": 62
                            }
                        ],
                        "text": "Many variants to FM have been proposed, such as the neural FM [He and Chua, 2017] that deepens FM under the neural framework to learn high-order feature interactions, and the field-aware FM [Juan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2021204,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3d1f9a530e710fdd4e2313bda4c8a1f574e60ab6",
            "isKey": false,
            "numCitedBy": 854,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Many predictive tasks of web applications need to model categorical variables, such as user IDs and demographics like genders and occupations. To apply standard machine learning techniques, these categorical predictors are always converted to a set of binary features via one-hot encoding, making the resultant feature vector highly sparse. To learn from such sparse data effectively, it is crucial to account for the interactions between features. Factorization Machines (FMs) are a popular solution for efficiently using the second-order feature interactions. However, FM models feature interactions in a linear way, which can be insufficient for capturing the non-linear and complex inherent structure of real-world data. While deep neural networks have recently been applied to learn non-linear feature interactions in industry, such as the Wide&Deep by Google and DeepCross by Microsoft, the deep structure meanwhile makes them difficult to train. In this paper, we propose a novel model Neural Factorization Machine (NFM) for prediction under sparse settings. NFM seamlessly combines the linearity of FM in modelling second-order feature interactions and the non-linearity of neural network in modelling higher-order feature interactions. Conceptually, NFM is more expressive than FM since FM can be seen as a special case of NFM without hidden layers. Empirical results on two regression tasks show that with one hidden layer only, NFM significantly outperforms FM with a 7.3% relative improvement. Compared to the recent deep learning methods Wide&Deep and DeepCross, our NFM uses a shallower structure but offers better performance, being much easier to train and tune in practice."
            },
            "slug": "Neural-Factorization-Machines-for-Sparse-Predictive-He-Chua",
            "title": {
                "fragments": [],
                "text": "Neural Factorization Machines for Sparse Predictive Analytics"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "NFM seamlessly combines the linearity of FM in modelling second- order feature interactions and the non-linearity of neural network in modelling higher-order feature interactions, and is more expressive than FM since FM can be seen as a special case of NFM without hidden layers."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2061550"
                        ],
                        "name": "Heng-Tze Cheng",
                        "slug": "Heng-Tze-Cheng",
                        "structuredName": {
                            "firstName": "Heng-Tze",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Heng-Tze Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40338695"
                        ],
                        "name": "L. Koc",
                        "slug": "L.-Koc",
                        "structuredName": {
                            "firstName": "Levent",
                            "lastName": "Koc",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Koc"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2066076307"
                        ],
                        "name": "Jeremiah Harmsen",
                        "slug": "Jeremiah-Harmsen",
                        "structuredName": {
                            "firstName": "Jeremiah",
                            "lastName": "Harmsen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jeremiah Harmsen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3296031"
                        ],
                        "name": "Tal Shaked",
                        "slug": "Tal-Shaked",
                        "structuredName": {
                            "firstName": "Tal",
                            "lastName": "Shaked",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tal Shaked"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2073806959"
                        ],
                        "name": "Tushar Chandra",
                        "slug": "Tushar-Chandra",
                        "structuredName": {
                            "firstName": "Tushar",
                            "lastName": "Chandra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tushar Chandra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3312922"
                        ],
                        "name": "H. Aradhye",
                        "slug": "H.-Aradhye",
                        "structuredName": {
                            "firstName": "Hrishikesh",
                            "lastName": "Aradhye",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Aradhye"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064997996"
                        ],
                        "name": "Glen Anderson",
                        "slug": "Glen-Anderson",
                        "structuredName": {
                            "firstName": "Glen",
                            "lastName": "Anderson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Glen Anderson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32131713"
                        ],
                        "name": "G. Corrado",
                        "slug": "G.-Corrado",
                        "structuredName": {
                            "firstName": "Gregory",
                            "lastName": "Corrado",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Corrado"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2055400243"
                        ],
                        "name": "Wei Chai",
                        "slug": "Wei-Chai",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Chai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Chai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37413761"
                        ],
                        "name": "M. Ispir",
                        "slug": "M.-Ispir",
                        "structuredName": {
                            "firstName": "Mustafa",
                            "lastName": "Ispir",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Ispir"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1508890387"
                        ],
                        "name": "Rohan Anil",
                        "slug": "Rohan-Anil",
                        "structuredName": {
                            "firstName": "Rohan",
                            "lastName": "Anil",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rohan Anil"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50730596"
                        ],
                        "name": "Zakaria Haque",
                        "slug": "Zakaria-Haque",
                        "structuredName": {
                            "firstName": "Zakaria",
                            "lastName": "Haque",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zakaria Haque"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2217278"
                        ],
                        "name": "Lichan Hong",
                        "slug": "Lichan-Hong",
                        "structuredName": {
                            "firstName": "Lichan",
                            "lastName": "Hong",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lichan Hong"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "20048351"
                        ],
                        "name": "Vihan Jain",
                        "slug": "Vihan-Jain",
                        "structuredName": {
                            "firstName": "Vihan",
                            "lastName": "Jain",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Vihan Jain"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109059862"
                        ],
                        "name": "Xiaobing Liu",
                        "slug": "Xiaobing-Liu",
                        "structuredName": {
                            "firstName": "Xiaobing",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaobing Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068799083"
                        ],
                        "name": "Hemal Shah",
                        "slug": "Hemal-Shah",
                        "structuredName": {
                            "firstName": "Hemal",
                            "lastName": "Shah",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hemal Shah"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 13
                            }
                        ],
                        "text": "- Wide&Deep [Cheng et al., 2016]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 124
                            }
                        ],
                        "text": "However, the key problem with PR (and other similar cross feature-based solutions, such as the wide component of Wide&Deep [Cheng et al., 2016]) is that for sparse datasets where only a few cross features are observed, the parameters for unobserved cross features cannot be estimated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 15
                            }
                        ],
                        "text": "Specifically, [Cheng et al., 2016] proposes Wide&Deep for App recommendation, where the Deep component is a MLP on the concatenation of feature embedding vectors to learn feature interactions; and [Shan et al., 2016] proposes DeepCross for click-through rate prediction, which applies a deep\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 153
                            }
                        ],
                        "text": "When performing supervised learning on categorical predictor variables, it is important to account for the interactions between them [He and Chua, 2017; Cheng et al., 2016]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 3352400,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "657fbf29ea0b4904a3e98d1556f9acf38dddae5f",
            "isKey": true,
            "numCitedBy": 2152,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "Generalized linear models with nonlinear feature transformations are widely used for large-scale regression and classification problems with sparse inputs. Memorization of feature interactions through a wide set of cross-product feature transformations are effective and interpretable, while generalization requires more feature engineering effort. With less feature engineering, deep neural networks can generalize better to unseen feature combinations through low-dimensional dense embeddings learned for the sparse features. However, deep neural networks with embeddings can over-generalize and recommend less relevant items when the user-item interactions are sparse and high-rank. In this paper, we present Wide & Deep learning---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems. We productionized and evaluated the system on Google Play, a commercial mobile app store with over one billion active users and over one million apps. Online experiment results show that Wide & Deep significantly increased app acquisitions compared with wide-only and deep-only models. We have also open-sourced our implementation in TensorFlow."
            },
            "slug": "Wide-&-Deep-Learning-for-Recommender-Systems-Cheng-Koc",
            "title": {
                "fragments": [],
                "text": "Wide & Deep Learning for Recommender Systems"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "Wide & Deep learning is presented---jointly trained wide linear models and deep neural networks---to combine the benefits of memorization and generalization for recommender systems and is open-sourced in TensorFlow."
            },
            "venue": {
                "fragments": [],
                "text": "DLRS@RecSys"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2843982"
                        ],
                        "name": "Steffen Rendle",
                        "slug": "Steffen-Rendle",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Rendle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steffen Rendle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 72
                            }
                        ],
                        "text": "0.5 0.6 0.7 .8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nMovieLens\nAFM FM LibFM\n0.31\n0.32\n0.33 0.34 0.35 0.36\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31\n0.33\n0.35\n0.37 0.39 0.41\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nMovieLens\nAFM(train) AFM(test) FM(train) FM(test)\nFigure 5: Training and test error of each epoch\nthis, we perform some micro-level analysis by investigating the score of each feature interaction on MovieLens."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "Second, LibFM prevents overfitting via L2 regularization, while we employ dropout, which can be more effective due to the model averaging effect."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 97
                            }
                        ],
                        "text": "Figure 2 shows the validation error of AFM and FM w.r.t. different dropout ratios; the result of LibFM is also shown as a benchmark."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 41
                            }
                        ],
                        "text": "By specifying the input feature vector, [Rendle, 2012] shows that FM can subsume many specific factorization models such as MF, parallel factor analysis, and SVD++ [Koren, 2008]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 136
                            }
                        ],
                        "text": "To demonstrate\n0.44 0.45 0.46 0.47 0.48 0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M SE (v al id at io n )\nDropout Ratio\nMovieLens AFM FM LibFM 0.31 0.32 0.33 0.34 0.35 0.36 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31 0.32 0.33 0.34 0.35\n0 0.5 1 2 4 8 16\nR M SE (v al id at io n )\n\u03bb\nFrappe AFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31 0.33 0.35 0.37 0.39 0.41 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00 0.10 0.20 0.30 0.40 0.50 0.60\n0 20 40 60 80 100\nR M SE\nEpoch\nMovieLens AFM(train) AFM(test) FM(train) FM(test)\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0 0.1 0.2 0.3 0."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 7
                            }
                        ],
                        "text": "First, LibFM optimizes with the vanilla SGD, which adopts a fixed learning rate for all parameters; while we optimize FM with Adagrad, which adapts the learning rate for each parameter based on its frequency (i.e., smaller updates for frequent and larger updates for infrequent parameters)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 108
                            }
                        ],
                        "text": "We compare AFM with the following competitive methods that are designed for sparse data prediction: - LibFM [Rendle, 2012]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 8
                            }
                        ],
                        "text": "Besides LibFM, all methods are learned by the mini-batch Adagrad."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 214,
                                "start": 209
                            }
                        ],
                        "text": "Without special\n2grouplens.org/datasets/movielens/latest 3https://github.com/geffy/tffm\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nMovieLens\nAFM FM LibFM\n0.31\n0.32\n0.33\n0.34\n0.35\n0.36\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nFrappe\nAFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31\n0.33\n0.35\n0.37\n0.39\n0.41\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nFrappe\nAFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50\nR M\nSE\nEpoch\nMovieLens\nAFM(train)\nAFM(test)\nFM(train)\nFM(test)\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nMovieLens\nAFM FM LibFM\n0.31\n0.32\n0.33 0.34 0.35 0.36\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31\n0.33\n0.35\n0.37 0.39 0.41\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50\nR M\nSE\nEpoch\nMovieLens\nAFM(train) AFM(test) FM(train) FM(test)\nFigure 2: Validation error of AFM and FM w.r.t. different dropout ratios on the pair-wise interaction layer\nmention, the attention factor is also 256, same as the embedding size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 64,
                                "start": 59
                            }
                        ],
                        "text": "\u2022 Our implementation of FM offers a better performance than LibFM."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 230
                            }
                        ],
                        "text": "As can be seen from Figure 3, when \u03bb is set to a value larger than 0, AFM is improved (note that the result of\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nMovieLens\nAFM FM LibFM\n0.31\n0.32 0.33 0.34 0.35 0.36\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M SE (v al id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31\n0.33\n0.35 0.37 0.39 0.41\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M SE (v al id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50\nR M\nSE\nEpoch\nMovieLens\nAFM(train)\nAFM(test)\nFM(train)\nFM(test)\n0.44\n0.45 0.46 0.47 0.48 0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M SE (v al id at io n )\nDropout Ratio\nMovieLens AFM FM LibFM\n0.31 0.32 0.33 0.34 0.35 0.36 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32 0.33 0.34 0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id at io n )\n\u03bb\nFrappe AFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31 0.33 0.35 0.37 0.39 0.41 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50\nR M\nSE\nEpoch\nMovieLens\nAFM(train) AFM(test) FM(train) FM(test)\nFigure 3: Validation error of AFM w.r.t. different regularization strengths on the attention network\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nMovieLens\nAFM FM LibFM\n0.31\n0.32\n0.33\n0.34 0.35 0.36\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31\n0.33\n0.35\n0.37 0.39 0.41\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50\nR M\nSE\nEpoch\nMovieLens\nAFM(train) AFM(test) FM(train) FM(test)\n0.44\n0.45\n0.46 0.47 0.48 0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nMovieLens AFM FM LibFM\n0.31 0.32 0.33 0.34 0.35 0.36 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33 0.34 0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io n )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31 0.33 0.35 0.37 0.39 0.41 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30 0.40 0.50 0.60\n0 10 20 30 40 50\nR M SE\nEpoch\nMovieLens AFM(train) AFM(test) FM(train) FM(test)\nFigure 4: Validation error of AFM w.r.t. different attention factors\n\u03bb = 0 corresponds to the best performance obtained by AFM in Figure 2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 50,
                                "start": 45
                            }
                        ],
                        "text": "We carefully tuned the L2 regularization for LibFM and HOFM, and the dropout ratio for Wide&Deep and DeepCross."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 48
                            }
                        ],
                        "text": "Frappe MovieLens Method Param# RMSE Param# RMSE LibFM 1.38M 0.3385 23.24M 0.4735 HOFM 2.76M 0.3331 46.40M 0.4636 Wide&Deep 4.66M 0.3246 24.69M 0.4512 DeepCross 8.93M 0.3548 25.42M 0.5130 AFM 1.45M 0.3102 23.26M 0.4325\n\u2022 Lastly, DeepCross performs the worst, due to the severe problem of overfitting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 109
                            }
                        ],
                        "text": "We compare AFM with the following competitive methods that are designed for sparse data prediction:\n- LibFM [Rendle, 2012]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 29,
                                "start": 24
                            }
                        ],
                        "text": "\u2022 AFM outperforms FM and LibFM by a large margin."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 173,
                                "start": 168
                            }
                        ],
                        "text": "Even when dropout is not used and the overfitting issue does exist to a certain extent, AFM achieves a performance significantly better than the optimal performance of LibFM and FM (cf. the result of dropout ratio equals to 0)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 26
                            }
                        ],
                        "text": "Specifically, AFM betters LibFM with a 8.6% relative improvement by using less than 0.1M additional parameters; and AFM outperforms the second best method Wide&Deep with 4.3%, while using much fewer model parameters."
                    },
                    "intents": []
                }
            ],
            "corpusId": 5499886,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e50f4d3316d13841c287dcdf5479d7820d593571",
            "isKey": true,
            "numCitedBy": 1084,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Factorization approaches provide high accuracy in several important prediction problems, for example, recommender systems. However, applying factorization approaches to a new prediction problem is a nontrivial task and requires a lot of expert knowledge. Typically, a new model is developed, a learning algorithm is derived, and the approach has to be implemented.\n Factorization machines (FM) are a generic approach since they can mimic most factorization models just by feature engineering. This way, factorization machines combine the generality of feature engineering with the superiority of factorization models in estimating interactions between categorical variables of large domain. libFM is a software implementation for factorization machines that features stochastic gradient descent (SGD) and alternating least-squares (ALS) optimization, as well as Bayesian inference using Markov Chain Monto Carlo (MCMC). This article summarizes the recent research on factorization machines both in terms of modeling and learning, provides extensions for the ALS and MCMC algorithms, and describes the software tool libFM."
            },
            "slug": "Factorization-Machines-with-libFM-Rendle",
            "title": {
                "fragments": [],
                "text": "Factorization Machines with libFM"
            },
            "venue": {
                "fragments": [],
                "text": "TIST"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1801613"
                        ],
                        "name": "Chen-Kuang Cheng",
                        "slug": "Chen-Kuang-Cheng",
                        "structuredName": {
                            "firstName": "Chen-Kuang",
                            "lastName": "Cheng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen-Kuang Cheng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "48570355"
                        ],
                        "name": "Fen Xia",
                        "slug": "Fen-Xia",
                        "structuredName": {
                            "firstName": "Fen",
                            "lastName": "Xia",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fen Xia"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38144094"
                        ],
                        "name": "T. Zhang",
                        "slug": "T.-Zhang",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145310663"
                        ],
                        "name": "Irwin King",
                        "slug": "Irwin-King",
                        "structuredName": {
                            "firstName": "Irwin",
                            "lastName": "King",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Irwin King"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1785083"
                        ],
                        "name": "Michael R. Lyu",
                        "slug": "Michael-R.-Lyu",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Lyu",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael R. Lyu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "For interactions between selected features, GBFM sums them up with the same weight as FM does."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 9
                            }
                        ],
                        "text": "As such, GBFM is essentially a feature selection algorithm, which is fundamentally different with our AFM that can learn the importance of each feature interaction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 71,
                                "start": 53
                            }
                        ],
                        "text": "We are aware of a work similar to our proposal \u2014 GBFM [Cheng et al., 2014], which selects \u201cgood\u201d features with gradient boosting and models only the interactions between good features."
                    },
                    "intents": [
                        {
                            "id": "result"
                        }
                    ]
                }
            ],
            "corpusId": 3352281,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cd579e1e9cc350c3f7746e6ae6911a97e21ba27c",
            "isKey": true,
            "numCitedBy": 68,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Recommendation techniques have been well developed in the past decades. Most of them build models only based on user item rating matrix. However, in real world, there is plenty of auxiliary information available in recommendation systems. We can utilize these information as additional features to improve recommendation performance. We refer to recommendation with auxiliary information as context-aware recommendation. Context-aware Factorization Machines (FM) is one of the most successful context-aware recommendation models. FM models pairwise interactions between all features, in such way, a certain feature latent vector is shared to compute the factorized parameters it involved. In practice, there are tens of context features and not all the pairwise feature interactions are useful. Thus, one important challenge for context-aware recommendation is how to effectively select \"good\" interaction features. In this paper, we focus on solving this problem and propose a greedy interaction feature selection algorithm based on gradient boosting. Then we propose a novel Gradient Boosting Factorization Machine (GBFM) model to incorporate feature selection algorithm with Factorization Machines into a unified framework. The experimental results on both synthetic and real datasets demonstrate the efficiency and effectiveness of our algorithm compared to other state-of-the-art methods."
            },
            "slug": "Gradient-boosting-factorization-machines-Cheng-Xia",
            "title": {
                "fragments": [],
                "text": "Gradient boosting factorization machines"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A novel Gradient Boosting Factorization Machine (GBFM) model is proposed to incorporate feature selection algorithm with Factorization Machines into a unified framework and the efficiency and effectiveness of the algorithm compared to other state-of-the-art methods are demonstrated."
            },
            "venue": {
                "fragments": [],
                "text": "RecSys '14"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "27257992"
                        ],
                        "name": "Mathieu Blondel",
                        "slug": "Mathieu-Blondel",
                        "structuredName": {
                            "firstName": "Mathieu",
                            "lastName": "Blondel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mathieu Blondel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34433830"
                        ],
                        "name": "Akinori Fujino",
                        "slug": "Akinori-Fujino",
                        "structuredName": {
                            "firstName": "Akinori",
                            "lastName": "Fujino",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Akinori Fujino"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1735221"
                        ],
                        "name": "N. Ueda",
                        "slug": "N.-Ueda",
                        "structuredName": {
                            "firstName": "Naonori",
                            "lastName": "Ueda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Ueda"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1784290"
                        ],
                        "name": "Masakazu Ishihata",
                        "slug": "Masakazu-Ishihata",
                        "structuredName": {
                            "firstName": "Masakazu",
                            "lastName": "Ishihata",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Masakazu Ishihata"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 83,
                                "start": 63
                            }
                        ],
                        "text": "This is the TensorFlow implementation3 of the higher-order FM [Blondel et al., 2016]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2543489,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3f3b3dc86415ebda1043d2be55e75a29ffe2bd95",
            "isKey": false,
            "numCitedBy": 132,
            "numCiting": 26,
            "paperAbstract": {
                "fragments": [],
                "text": "Factorization machines (FMs) are a supervised learning approach that can use second-order feature combinations even when the data is very high-dimensional. Unfortunately, despite increasing interest in FMs, there exists to date no efficient training algorithm for higher-order FMs (HOFMs). In this paper, we present the first generic yet efficient algorithms for training arbitrary-order HOFMs. We also present new variants of HOFMs with shared parameters, which greatly reduce model size and prediction times while maintaining similar accuracy. We demonstrate the proposed approaches on four different link prediction tasks."
            },
            "slug": "Higher-Order-Factorization-Machines-Blondel-Fujino",
            "title": {
                "fragments": [],
                "text": "Higher-Order Factorization Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "The first generic yet efficient algorithms for training arbitrary-order higher-orderFactorization machines (HOFMs) are presented and new variants of HOFMs with shared parameters are presented, which greatly reduce model size and prediction times while maintaining similar accuracy."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2843982"
                        ],
                        "name": "Steffen Rendle",
                        "slug": "Steffen-Rendle",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Rendle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steffen Rendle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 17,
                                "start": 5
                            }
                        ],
                        "text": "FMs [Rendle, 2010] are mainly used for supervised learning under sparse settings; for example, in situations where categorical variables are converted to sparse feature vector via one-hot encoding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 100,
                                "start": 88
                            }
                        ],
                        "text": "To address the generalization issue of PR, factorization machines (FMs)1 were proposed [Rendle, 2010], which parameterize the weight of a cross feature as the inner product of the embedding vectors of the constituent features."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 153
                            }
                        ],
                        "text": "2 Factorization Machines As a general ML model for supervised learning, factorization machines were originally proposed for collaborative recommendation [Rendle, 2010; Rendle et al., 2011]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 19
                            }
                        ],
                        "text": "4 Related Work FMs [Rendle, 2010] are mainly used for supervised learning under sparse settings; for example, in situations where categorical variables are converted to sparse feature vector via one-hot encoding."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 129
                            }
                        ],
                        "text": "As a general ML model for supervised learning, factorization machines were originally proposed for collaborative recommendation [Rendle, 2010; Rendle et al., 2011]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17265929,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "df93596d4ed71d2863532c063c4c693711216abf",
            "isKey": true,
            "numCitedBy": 1819,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we introduce Factorization Machines (FM) which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models. Like SVMs, FMs are a general predictor working with any real valued feature vector. In contrast to SVMs, FMs model all interactions between variables using factorized parameters. Thus they are able to estimate interactions even in problems with huge sparsity (like recommender systems) where SVMs fail. We show that the model equation of FMs can be calculated in linear time and thus FMs can be optimized directly. So unlike nonlinear SVMs, a transformation in the dual form is not necessary and the model parameters can be estimated directly without the need of any support vector in the solution. We show the relationship to SVMs and the advantages of FMs for parameter estimation in sparse settings. On the other hand there are many different factorization models like matrix factorization, parallel factor analysis or specialized models like SVD++, PITF or FPMC. The drawback of these models is that they are not applicable for general prediction tasks but work only with special input data. Furthermore their model equations and optimization algorithms are derived individually for each task. We show that FMs can mimic these models just by specifying the input data (i.e. the feature vectors). This makes FMs easily applicable even for users without expert knowledge in factorization models."
            },
            "slug": "Factorization-Machines-Rendle",
            "title": {
                "fragments": [],
                "text": "Factorization Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "Factorization Machines (FM) are introduced which are a new model class that combines the advantages of Support Vector Machines (SVM) with factorization models and can mimic these models just by specifying the input data (i.e. the feature vectors)."
            },
            "venue": {
                "fragments": [],
                "text": "2010 IEEE International Conference on Data Mining"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39353098"
                        ],
                        "name": "Kaiming He",
                        "slug": "Kaiming-He",
                        "structuredName": {
                            "firstName": "Kaiming",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kaiming He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771551"
                        ],
                        "name": "X. Zhang",
                        "slug": "X.-Zhang",
                        "structuredName": {
                            "firstName": "X.",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3080683"
                        ],
                        "name": "Shaoqing Ren",
                        "slug": "Shaoqing-Ren",
                        "structuredName": {
                            "firstName": "Shaoqing",
                            "lastName": "Ren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shaoqing Ren"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [],
                        "name": "Jian Sun",
                        "slug": "Jian-Sun",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Sun",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Sun"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 60,
                                "start": 57
                            }
                        ],
                        "text": "For the attention network component which is a one-layer MLP, we apply L2 regularization on the weight matrix W to prevent the possible overfitting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 111,
                                "start": 108
                            }
                        ],
                        "text": "Specifically, [Cheng et al., 2016] proposes Wide&Deep for App recommendation, where the Deep component is a MLP on the concatenation of feature embedding vectors to learn feature interactions; and [Shan et al., 2016] proposes DeepCross for click-through rate prediction, which applies a deep residual MLP [He et al., 2016a] to learn cross features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 253
                            }
                        ],
                        "text": "\u2026for App recommendation, where the Deep component is a MLP on the concatenation of feature embedding vectors to learn feature interactions; and [Shan et al., 2016] proposes DeepCross for click-through rate prediction, which applies a deep residual MLP [He et al., 2016a] to learn cross features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 98
                            }
                        ],
                        "text": "The wide part is the same as the linear regression part of FM, and the deep part is a three-layer MLP with the layer size 1024, 512 and 256."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 96
                            }
                        ],
                        "text": "In contrast to matrix factorization (MF) that models the interaction between two entities only [He et al., 2016b], FM is designed to be a general machine learner for modelling the interactions between any number of entities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 96
                            }
                        ],
                        "text": ", 2016] proposes DeepCross for click-through rate prediction, which applies a deep residual MLP [He et al., 2016a] to learn cross features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 117,
                                "start": 114
                            }
                        ],
                        "text": "To address the generalization problem, we further parameterize the attention score with a multi-layer perceptron (MLP), which we call the attention network."
                    },
                    "intents": []
                }
            ],
            "corpusId": 206594692,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2c03df8b48bf3fa39054345bafabfeff15bfd11d",
            "isKey": true,
            "numCitedBy": 106571,
            "numCiting": 54,
            "paperAbstract": {
                "fragments": [],
                "text": "Deeper neural networks are more difficult to train. We present a residual learning framework to ease the training of networks that are substantially deeper than those used previously. We explicitly reformulate the layers as learning residual functions with reference to the layer inputs, instead of learning unreferenced functions. We provide comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth. On the ImageNet dataset we evaluate residual nets with a depth of up to 152 layers - 8\u00d7 deeper than VGG nets [40] but still having lower complexity. An ensemble of these residual nets achieves 3.57% error on the ImageNet test set. This result won the 1st place on the ILSVRC 2015 classification task. We also present analysis on CIFAR-10 with 100 and 1000 layers. The depth of representations is of central importance for many visual recognition tasks. Solely due to our extremely deep representations, we obtain a 28% relative improvement on the COCO object detection dataset. Deep residual nets are foundations of our submissions to ILSVRC & COCO 2015 competitions1, where we also won the 1st places on the tasks of ImageNet detection, ImageNet localization, COCO detection, and COCO segmentation."
            },
            "slug": "Deep-Residual-Learning-for-Image-Recognition-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Deep Residual Learning for Image Recognition"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "This work presents a residual learning framework to ease the training of networks that are substantially deeper than those used previously, and provides comprehensive empirical evidence showing that these residual networks are easier to optimize, and can gain accuracy from considerably increased depth."
            },
            "venue": {
                "fragments": [],
                "text": "2016 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7792071"
                        ],
                        "name": "Xiangnan He",
                        "slug": "Xiangnan-He",
                        "structuredName": {
                            "firstName": "Xiangnan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangnan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5462268"
                        ],
                        "name": "Hanwang Zhang",
                        "slug": "Hanwang-Zhang",
                        "structuredName": {
                            "firstName": "Hanwang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanwang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37596605"
                        ],
                        "name": "Min-Yen Kan",
                        "slug": "Min-Yen-Kan",
                        "structuredName": {
                            "firstName": "Min-Yen",
                            "lastName": "Kan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min-Yen Kan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 112,
                                "start": 96
                            }
                        ],
                        "text": "In contrast to matrix factorization (MF) that models the interaction between two entities only [He et al., 2016b], FM is designed to be a general machine learner for modelling the interactions between any number of entities."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 269,
                                "start": 253
                            }
                        ],
                        "text": "\u2026for App recommendation, where the Deep component is a MLP on the concatenation of feature embedding vectors to learn feature interactions; and [Shan et al., 2016] proposes DeepCross for click-through rate prediction, which applies a deep residual MLP [He et al., 2016a] to learn cross features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2896685,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ab443bd7e732374caabb5785b8d37bbfc724c845",
            "isKey": false,
            "numCitedBy": 750,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper contributes improvements on both the effectiveness and efficiency of Matrix Factorization (MF) methods for implicit feedback. We highlight two critical issues of existing works. First, due to the large space of unobserved feedback, most existing works resort to assign a uniform weight to the missing data to reduce computational complexity. However, such a uniform assumption is invalid in real-world settings. Second, most methods are also designed in an offline setting and fail to keep up with the dynamic nature of online data. We address the above two issues in learning MF models from implicit feedback. We first propose to weight the missing data based on item popularity, which is more effective and flexible than the uniform-weight assumption. However, such a non-uniform weighting poses efficiency challenge in learning the model. To address this, we specifically design a new learning algorithm based on the element-wise Alternating Least Squares (eALS) technique, for efficiently optimizing a MF model with variably-weighted missing data. We exploit this efficiency to then seamlessly devise an incremental update strategy that instantly refreshes a MF model given new feedback. Through comprehensive experiments on two public datasets in both offline and online protocols, we show that our implemented, open-source (https://github.com/hexiangnan/sigir16-eals) eALS consistently outperforms state-of-the-art implicit MF methods."
            },
            "slug": "Fast-Matrix-Factorization-for-Online-Recommendation-He-Zhang",
            "title": {
                "fragments": [],
                "text": "Fast Matrix Factorization for Online Recommendation with Implicit Feedback"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "A new learning algorithm based on the element-wise Alternating Least Squares (eALS) technique is designed, for efficiently optimizing a Matrix Factorization (MF) model with variably-weighted missing data and exploiting this efficiency to then seamlessly devise an incremental update strategy that instantly refreshes a MF model given new feedback."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145019134"
                        ],
                        "name": "Yu-Chin Juan",
                        "slug": "Yu-Chin-Juan",
                        "structuredName": {
                            "firstName": "Yu-Chin",
                            "lastName": "Juan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yu-Chin Juan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2056431469"
                        ],
                        "name": "Yong Zhuang",
                        "slug": "Yong-Zhuang",
                        "structuredName": {
                            "firstName": "Yong",
                            "lastName": "Zhuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yong Zhuang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40429042"
                        ],
                        "name": "Wei-Sheng Chin",
                        "slug": "Wei-Sheng-Chin",
                        "structuredName": {
                            "firstName": "Wei-Sheng",
                            "lastName": "Chin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei-Sheng Chin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711460"
                        ],
                        "name": "Chih-Jen Lin",
                        "slug": "Chih-Jen-Lin",
                        "structuredName": {
                            "firstName": "Chih-Jen",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chih-Jen Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 156,
                                "start": 139
                            }
                        ],
                        "text": "It has broad applications including recommendation systems [Bayer et al., 2017; Zhao et al., 2016], online advertising [Shan et al., 2016; Juan et al., 2016], and image recognition [Zhang et al., 2017; Wang et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 147
                            }
                        ],
                        "text": "\u2026as the neural FM [He and Chua, 2017] that deepens FM under the neural framework to learn high-order feature interactions, and the field-aware FM [Juan et al., 2016] that associates multiple embedding vectors for a feature to differentiate its interaction with other features of different fields."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 28
                            }
                        ],
                        "text": ", 2016], online advertising [Shan et al., 2016; Juan et al., 2016], and image recognition [Zhang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 190
                            }
                        ],
                        "text": "Many variants to FM have been proposed, such as the neural FM [He and Chua, 2017] that deepens FM under the neural framework to learn high-order feature interactions, and the field-aware FM [Juan et al., 2016] that associates multiple embedding vectors for a feature to differentiate its interaction with other features of different fields."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1472236,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a802eccf56ea69c65b14831335a484a69e5cd849",
            "isKey": true,
            "numCitedBy": 492,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Click-through rate (CTR) prediction plays an important role in computational advertising. Models based on degree-2 polynomial mappings and factorization machines (FMs) are widely used for this task. Recently, a variant of FMs, field-aware factorization machines (FFMs), outperforms existing models in some world-wide CTR-prediction competitions. Based on our experiences in winning two of them, in this paper we establish FFMs as an effective method for classifying large sparse data including those from CTR prediction. First, we propose efficient implementations for training FFMs. Then we comprehensively analyze FFMs and compare this approach with competing models. Experiments show that FFMs are very useful for certain classification problems. Finally, we have released a package of FFMs for public use."
            },
            "slug": "Field-aware-Factorization-Machines-for-CTR-Juan-Zhuang",
            "title": {
                "fragments": [],
                "text": "Field-aware Factorization Machines for CTR Prediction"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "This paper establishes FFMs as an effective method for classifying large sparse data including those from CTR prediction, and proposes efficient implementations for training FFMs and comprehensively analyze FFMs."
            },
            "venue": {
                "fragments": [],
                "text": "RecSys"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2897313"
                        ],
                        "name": "Nitish Srivastava",
                        "slug": "Nitish-Srivastava",
                        "structuredName": {
                            "firstName": "Nitish",
                            "lastName": "Srivastava",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nitish Srivastava"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695689"
                        ],
                        "name": "Geoffrey E. Hinton",
                        "slug": "Geoffrey-E.-Hinton",
                        "structuredName": {
                            "firstName": "Geoffrey",
                            "lastName": "Hinton",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Geoffrey E. Hinton"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2064160"
                        ],
                        "name": "A. Krizhevsky",
                        "slug": "A.-Krizhevsky",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Krizhevsky",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Krizhevsky"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701686"
                        ],
                        "name": "Ilya Sutskever",
                        "slug": "Ilya-Sutskever",
                        "structuredName": {
                            "firstName": "Ilya",
                            "lastName": "Sutskever",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ilya Sutskever"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145124475"
                        ],
                        "name": "R. Salakhutdinov",
                        "slug": "R.-Salakhutdinov",
                        "structuredName": {
                            "firstName": "Ruslan",
                            "lastName": "Salakhutdinov",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Salakhutdinov"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 256,
                                "start": 233
                            }
                        ],
                        "text": "Moreover, as dropout is disabled during testing and the whole network is used for prediction, dropout has another side effect of performing model averaging with smaller neural networks, which may potentially improve the performance [Srivastava et al., 2014]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 93
                            }
                        ],
                        "text": "The idea of dropout is randomly drop some neurons (along their connections) during training [Srivastava et al., 2014]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6844431,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "34f25a8704614163c4095b3ee2fc969b60de4698",
            "isKey": false,
            "numCitedBy": 29832,
            "numCiting": 39,
            "paperAbstract": {
                "fragments": [],
                "text": "Deep neural nets with a large number of parameters are very powerful machine learning systems. However, overfitting is a serious problem in such networks. Large networks are also slow to use, making it difficult to deal with overfitting by combining the predictions of many different large neural nets at test time. Dropout is a technique for addressing this problem. The key idea is to randomly drop units (along with their connections) from the neural network during training. This prevents units from co-adapting too much. During training, dropout samples from an exponential number of different \"thinned\" networks. At test time, it is easy to approximate the effect of averaging the predictions of all these thinned networks by simply using a single unthinned network that has smaller weights. This significantly reduces overfitting and gives major improvements over other regularization methods. We show that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
            },
            "slug": "Dropout:-a-simple-way-to-prevent-neural-networks-Srivastava-Hinton",
            "title": {
                "fragments": [],
                "text": "Dropout: a simple way to prevent neural networks from overfitting"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "It is shown that dropout improves the performance of neural networks on supervised learning tasks in vision, speech recognition, document classification and computational biology, obtaining state-of-the-art results on many benchmark data sets."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143891667"
                        ],
                        "name": "Long Chen",
                        "slug": "Long-Chen",
                        "structuredName": {
                            "firstName": "Long",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Long Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5462268"
                        ],
                        "name": "Hanwang Zhang",
                        "slug": "Hanwang-Zhang",
                        "structuredName": {
                            "firstName": "Hanwang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanwang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145974111"
                        ],
                        "name": "Jun Xiao",
                        "slug": "Jun-Xiao",
                        "structuredName": {
                            "firstName": "Jun",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jun Xiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143982887"
                        ],
                        "name": "Liqiang Nie",
                        "slug": "Liqiang-Nie",
                        "structuredName": {
                            "firstName": "Liqiang",
                            "lastName": "Nie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Liqiang Nie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2549731"
                        ],
                        "name": "Jian Shao",
                        "slug": "Jian-Shao",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Shao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Shao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2157221163"
                        ],
                        "name": "Wei Liu",
                        "slug": "Wei-Liu",
                        "structuredName": {
                            "firstName": "Wei",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wei Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 122
                            }
                        ],
                        "text": "We devise a novel model named AFM, which utilizes the recent advance in neural network modelling \u2014 the attention mechanism [Chen et al., 2017a; Chen et al., 2017b] \u2014 to enable feature interactions contribute differently to the prediction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 172
                            }
                        ],
                        "text": "Attention-based Pooling Layer Since the attention mechanism has been introduced to neural network modelling, it has been widely used in many tasks, such as recommendation [Chen et al., 2017a], information retrieval [Xiong et al., 2017], and computer vision [Chen et al., 2017b]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 206596371,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88513e738a95840de05a62f0e43d30a67b3c542e",
            "isKey": false,
            "numCitedBy": 1116,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual attention has been successfully applied in structural prediction tasks such as visual captioning and question answering. Existing visual attention models are generally spatial, i.e., the attention is modeled as spatial probabilities that re-weight the last conv-layer feature map of a CNN encoding an input image. However, we argue that such spatial attention does not necessarily conform to the attention mechanism &#x2014; a dynamic feature extractor that combines contextual fixations over time, as CNN features are naturally spatial, channel-wise and multi-layer. In this paper, we introduce a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN. In the task of image captioning, SCA-CNN dynamically modulates the sentence generation context in multi-layer feature maps, encoding where (i.e., attentive spatial locations at multiple layers) and what (i.e., attentive channels) the visual attention is. We evaluate the proposed SCA-CNN architecture on three benchmark image captioning datasets: Flickr8K, Flickr30K, and MSCOCO. It is consistently observed that SCA-CNN significantly outperforms state-of-the-art visual attention-based image captioning methods."
            },
            "slug": "SCA-CNN:-Spatial-and-Channel-Wise-Attention-in-for-Chen-Zhang",
            "title": {
                "fragments": [],
                "text": "SCA-CNN: Spatial and Channel-Wise Attention in Convolutional Networks for Image Captioning"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper introduces a novel convolutional neural network dubbed SCA-CNN that incorporates Spatial and Channel-wise Attentions in a CNN that significantly outperforms state-of-the-art visual attention-based image captioning methods."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "32689163"
                        ],
                        "name": "Immanuel Bayer",
                        "slug": "Immanuel-Bayer",
                        "structuredName": {
                            "firstName": "Immanuel",
                            "lastName": "Bayer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Immanuel Bayer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7792071"
                        ],
                        "name": "Xiangnan He",
                        "slug": "Xiangnan-He",
                        "structuredName": {
                            "firstName": "Xiangnan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangnan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3015985"
                        ],
                        "name": "Bhargav Kanagal",
                        "slug": "Bhargav-Kanagal",
                        "structuredName": {
                            "firstName": "Bhargav",
                            "lastName": "Kanagal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bhargav Kanagal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2843982"
                        ],
                        "name": "Steffen Rendle",
                        "slug": "Steffen-Rendle",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Rendle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steffen Rendle"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 78,
                                "start": 60
                            }
                        ],
                        "text": "It has broad applications including recommendation systems [Bayer et al., 2017; Zhao et al., 2016], online advertising [Shan et al., 2016; Juan et al., 2016], and image recognition [Zhang et al., 2017; Wang et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 59
                            }
                        ],
                        "text": "It has broad applications including recommendation systems [Bayer et al., 2017; Zhao et al., 2016], online advertising [Shan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 7953982,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "5720a5015ca67400fadd0ff6863519f4b030e731",
            "isKey": false,
            "numCitedBy": 179,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, interest in recommender research has shifted from explicit feedback towards implicit feedback data. A diversity of complex models has been proposed for a wide variety of applications. Despite this, learning from implicit feedback is still computationally challenging. So far, most work relies on stochastic gradient descent (SGD) solvers which are easy to derive, but in practice challenging to apply, especially for tasks with many items. For the simple matrix factorization model, an efficient coordinate descent (CD) solver has been previously proposed. However, efficient CD approaches have not been derived for more complex models. In this paper, we provide a new framework for deriving efficient CD algorithms for complex recommender models. We identify and introduce the property of k-separable models. We show that k-separability is a sufficient property to allow efficient optimization of implicit recommender problems with CD. We illustrate this framework on a variety of state-of-the-art models including factorization machines and Tucker decomposition. To summarize, our work provides the theory and building blocks to derive efficient implicit CD algorithms for complex recommender models."
            },
            "slug": "A-Generic-Coordinate-Descent-Framework-for-Learning-Bayer-He",
            "title": {
                "fragments": [],
                "text": "A Generic Coordinate Descent Framework for Learning from Implicit Feedback"
            },
            "tldr": {
                "abstractSimilarityScore": 38,
                "text": "It is shown that k-separability is a sufficient property to allow efficient optimization of implicit recommender problems with CD, and a new framework for deriving efficient CD algorithms for complex recommender models is provided."
            },
            "venue": {
                "fragments": [],
                "text": "WWW"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2146058658"
                        ],
                        "name": "Meng Wang",
                        "slug": "Meng-Wang",
                        "structuredName": {
                            "firstName": "Meng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Meng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50091230"
                        ],
                        "name": "Weijie Fu",
                        "slug": "Weijie-Fu",
                        "structuredName": {
                            "firstName": "Weijie",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weijie Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6175623"
                        ],
                        "name": "Shijie Hao",
                        "slug": "Shijie-Hao",
                        "structuredName": {
                            "firstName": "Shijie",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijie Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2661589"
                        ],
                        "name": "Hengchang Liu",
                        "slug": "Hengchang-Liu",
                        "structuredName": {
                            "firstName": "Hengchang",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hengchang Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145502658"
                        ],
                        "name": "Xindong Wu",
                        "slug": "Xindong-Wu",
                        "structuredName": {
                            "firstName": "Xindong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xindong Wu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 26
                            }
                        ],
                        "text": ", 2015] and data sampling [Wang et al., 2017b] techniques."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 121
                            }
                        ],
                        "text": "Owing to such generality, FM has been successfully applied to various applications, ranging from recommendation systems [Wang et al., 2017a; Chen et al., 2016] to natural language processing [Petroni et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 254,
                                "start": 236
                            }
                        ],
                        "text": "As AFM has a relatively high complexity quadratic to the number of non-zero features, we will consider improving its learning efficiency, for example by using learning to hash [Zhang et al., 2016b; Shen et al., 2015] and data sampling [Wang et al., 2017b] techniques."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15264694,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1d9b302a5a004e279b984f35d01190cb59658c50",
            "isKey": false,
            "numCitedBy": 93,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "Several models have been proposed to cope with the rapidly increasing size of data, such as Anchor Graph Regularization (AGR). The AGR approach significantly accelerates graph-based learning by exploring a set of anchors. However, when a dataset becomes much larger, AGR still faces a big graph which brings dramatically increasing computational costs. To overcome this issue, we propose a novel Hierarchical Anchor Graph Regularization (HAGR) approach by exploring multiple-layer anchors with a pyramid-style structure. In HAGR, the labels of datapoints are inferred from the coarsest anchors layer by layer in a coarse-to-fine manner. The label smoothness regularization is performed on all datapoints, and we demonstrate that the optimization process only involves a small-size reduced Laplacian matrix. We also introduce a fast approach to construct our hierarchical anchor graph based on an approximate nearest neighbor search technique. Experiments on million-scale datasets demonstrate the effectiveness and efficiency of the proposed HAGR approach over existing methods. Results show that the HAGR approach is even able to achieve a good performance within 3 minutes in an 8-million-example classification task."
            },
            "slug": "Learning-on-Big-Graph:-Label-Inference-and-with-Wang-Fu",
            "title": {
                "fragments": [],
                "text": "Learning on Big Graph: Label Inference and Regularization with Anchor Hierarchy"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This work proposes a novel Hierarchical Anchor Graph Regularization (HAGR) approach by exploring multiple-layer anchors with a pyramid-style structure and introduces a fast approach to construct the hierarchical anchor graph based on an approximate nearest neighbor search technique."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2843982"
                        ],
                        "name": "Steffen Rendle",
                        "slug": "Steffen-Rendle",
                        "structuredName": {
                            "firstName": "Steffen",
                            "lastName": "Rendle",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Steffen Rendle"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2382247"
                        ],
                        "name": "Zeno Gantner",
                        "slug": "Zeno-Gantner",
                        "structuredName": {
                            "firstName": "Zeno",
                            "lastName": "Gantner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zeno Gantner"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1825333"
                        ],
                        "name": "C. Freudenthaler",
                        "slug": "C.-Freudenthaler",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Freudenthaler",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Freudenthaler"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1388781075"
                        ],
                        "name": "L. Schmidt-Thieme",
                        "slug": "L.-Schmidt-Thieme",
                        "structuredName": {
                            "firstName": "Lars",
                            "lastName": "Schmidt-Thieme",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Schmidt-Thieme"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 153
                            }
                        ],
                        "text": "2 Factorization Machines As a general ML model for supervised learning, factorization machines were originally proposed for collaborative recommendation [Rendle, 2010; Rendle et al., 2011]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 68,
                                "start": 49
                            }
                        ],
                        "text": "It is shown that FM can suffer from overfitting [Rendle et al., 2011], so the L2 regularization is an essential ingredient to prevent overfitting for FM."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 143
                            }
                        ],
                        "text": "As a general ML model for supervised learning, factorization machines were originally proposed for collaborative recommendation [Rendle, 2010; Rendle et al., 2011]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207189080,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1683ffc189d16b616131c300f45af87602d211f7",
            "isKey": false,
            "numCitedBy": 517,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "The situation in which a choice is made is an important information for recommender systems. Context-aware recommenders take this information into account to make predictions. So far, the best performing method for context-aware rating prediction in terms of predictive accuracy is Multiverse Recommendation based on the Tucker tensor factorization model. However this method has two drawbacks: (1) its model complexity is exponential in the number of context variables and polynomial in the size of the factorization and (2) it only works for categorical context variables. On the other hand there is a large variety of fast but specialized recommender methods which lack the generality of context-aware methods. We propose to apply Factorization Machines (FMs) to model contextual information and to provide context-aware rating predictions. This approach results in fast context-aware recommendations because the model equation of FMs can be computed in linear time both in the number of context variables and the factorization size. For learning FMs, we develop an iterative optimization method that analytically finds the least-square solution for one parameter given the other ones. Finally, we show empirically that our approach outperforms Multiverse Recommendation in prediction quality and runtime."
            },
            "slug": "Fast-context-aware-recommendations-with-machines-Rendle-Gantner",
            "title": {
                "fragments": [],
                "text": "Fast context-aware recommendations with factorization machines"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This work proposes to apply Factorization Machines (FMs) to model contextual information and to provide context-aware rating predictions and shows empirically that this approach outperforms Multiverse Recommendation in prediction quality and runtime."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1701121"
                        ],
                        "name": "Y. Koren",
                        "slug": "Y.-Koren",
                        "structuredName": {
                            "firstName": "Yehuda",
                            "lastName": "Koren",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Koren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 165
                            }
                        ],
                        "text": "By specifying the input feature vector, [Rendle, 2012] shows that FM can subsume many specific factorization models such as MF, parallel factor analysis, and SVD++ [Koren, 2008]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 207168823,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "cf6f83fcce274606bf0264c59d1c78a30c9c9d18",
            "isKey": false,
            "numCitedBy": 3612,
            "numCiting": 29,
            "paperAbstract": {
                "fragments": [],
                "text": "Recommender systems provide users with personalized suggestions for products or services. These systems often rely on Collaborating Filtering (CF), where past transactions are analyzed in order to establish connections between users and products. The two more successful approaches to CF are latent factor models, which directly profile both users and products, and neighborhood models, which analyze similarities between products or users. In this work we introduce some innovations to both approaches. The factor and neighborhood models can now be smoothly merged, thereby building a more accurate combined model. Further accuracy improvements are achieved by extending the models to exploit both explicit and implicit feedback by the users. The methods are tested on the Netflix data. Results are better than those previously published on that dataset. In addition, we suggest a new evaluation metric, which highlights the differences among methods, based on their performance at a top-K recommendation task."
            },
            "slug": "Factorization-meets-the-neighborhood:-a-filtering-Koren",
            "title": {
                "fragments": [],
                "text": "Factorization meets the neighborhood: a multifaceted collaborative filtering model"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The factor and neighborhood models can now be smoothly merged, thereby building a more accurate combined model and a new evaluation metric is suggested, which highlights the differences among methods, based on their performance at a top-K recommendation task."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5462268"
                        ],
                        "name": "Hanwang Zhang",
                        "slug": "Hanwang-Zhang",
                        "structuredName": {
                            "firstName": "Hanwang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanwang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "26538630"
                        ],
                        "name": "Z. Kyaw",
                        "slug": "Z.-Kyaw",
                        "structuredName": {
                            "firstName": "Zawlin",
                            "lastName": "Kyaw",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Z. Kyaw"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9546964"
                        ],
                        "name": "Shih-Fu Chang",
                        "slug": "Shih-Fu-Chang",
                        "structuredName": {
                            "firstName": "Shih-Fu",
                            "lastName": "Chang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shih-Fu Chang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 182
                            }
                        ],
                        "text": "It has broad applications including recommendation systems [Bayer et al., 2017; Zhao et al., 2016], online advertising [Shan et al., 2016; Juan et al., 2016], and image recognition [Zhang et al., 2017; Wang et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18011736,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "996901b08a6b6f401146204f2db0d54aaf8749c8",
            "isKey": false,
            "numCitedBy": 418,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Visual relations, such as person ride bike and bike next to car, offer a comprehensive scene understanding of an image, and have already shown their great utility in connecting computer vision and natural language. However, due to the challenging combinatorial complexity of modeling subject-predicate-object relation triplets, very little work has been done to localize and predict visual relations. Inspired by the recent advances in relational representation learning of knowledge bases and convolutional object detection networks, we propose a Visual Translation Embedding network (VTransE) for visual relation detection. VTransE places objects in a low-dimensional relation space where a relation can be modeled as a simple vector translation, i.e., subject + predicate &#x2248; object. We propose a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass. To the best of our knowledge, VTransE is the first end-toend relation detection network. We demonstrate the effectiveness of VTransE over other state-of-the-art methods on two large-scale datasets: Visual Relationship and Visual Genome. Note that even though VTransE is a purely visual model, it is still competitive to the Lu&#x2019;s multi-modal model with language priors [27]."
            },
            "slug": "Visual-Translation-Embedding-Network-for-Visual-Zhang-Kyaw",
            "title": {
                "fragments": [],
                "text": "Visual Translation Embedding Network for Visual Relation Detection"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work proposes a novel feature extraction layer that enables object-relation knowledge transfer in a fully-convolutional fashion that supports training and inference in a single forward/backward pass, and proposes the first end-toend relation detection network."
            },
            "venue": {
                "fragments": [],
                "text": "2017 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145089705"
                        ],
                        "name": "Y. Shan",
                        "slug": "Y.-Shan",
                        "structuredName": {
                            "firstName": "Ying",
                            "lastName": "Shan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Shan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755821"
                        ],
                        "name": "T. R. Hoens",
                        "slug": "T.-R.-Hoens",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Hoens",
                            "middleNames": [
                                "Ryan"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. R. Hoens"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49097406"
                        ],
                        "name": "Jian Jiao",
                        "slug": "Jian-Jiao",
                        "structuredName": {
                            "firstName": "Jian",
                            "lastName": "Jiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jian Jiao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145341507"
                        ],
                        "name": "Haijing Wang",
                        "slug": "Haijing-Wang",
                        "structuredName": {
                            "firstName": "Haijing",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Haijing Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144580027"
                        ],
                        "name": "Dong Yu",
                        "slug": "Dong-Yu",
                        "structuredName": {
                            "firstName": "Dong",
                            "lastName": "Yu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dong Yu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143877014"
                        ],
                        "name": "J. C. Mao",
                        "slug": "J.-C.-Mao",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Mao",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. C. Mao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 120
                            }
                        ],
                        "text": "It has broad applications including recommendation systems [Bayer et al., 2017; Zhao et al., 2016], online advertising [Shan et al., 2016; Juan et al., 2016], and image recognition [Zhang et al., 2017; Wang et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 41,
                                "start": 22
                            }
                        ],
                        "text": ", 2016] and DeepCross [Shan et al., 2016] with a much simpler structure and fewer model parameters."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 96
                            }
                        ],
                        "text": "For prediction with such sparse data, it is crucial to model the interactions between features [Shan et al., 2016]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 226
                            }
                        ],
                        "text": "Specifically, [Cheng et al., 2016] proposes Wide&Deep for App recommendation, where the Deep component is a MLP on the concatenation of feature embedding vectors to learn feature interactions; and [Shan et al., 2016] proposes DeepCross for click-through rate prediction, which applies a deep residual MLP [He et al., 2016a] to learn cross features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 162,
                                "start": 145
                            }
                        ],
                        "text": "\u2026for App recommendation, where the Deep component is a MLP on the concatenation of feature embedding vectors to learn feature interactions; and [Shan et al., 2016] proposes DeepCross for click-through rate prediction, which applies a deep residual MLP [He et al., 2016a] to learn cross features."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 15
                            }
                        ],
                        "text": "For Wide&Deep, DeepCross and AFM, we find that pre-training their feature embeddings with FM leads to a lower RMSE than a random initialization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 218,
                                "start": 209
                            }
                        ],
                        "text": "By directly extending FM with the attention mechanism that learns the importance of each feature interaction, our AMF is more interpretable and empirically demonstrates superior performance over Wide&Deep and DeepCross."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 66,
                                "start": 28
                            }
                        ],
                        "text": ", 2016], online advertising [Shan et al., 2016; Juan et al., 2016], and image recognition [Zhang et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 189,
                                "start": 170
                            }
                        ],
                        "text": ", 2016] proposes Wide&Deep for App recommendation, where the Deep component is a MLP on the concatenation of feature embedding vectors to learn feature interactions; and [Shan et al., 2016] proposes DeepCross for click-through rate prediction, which applies a deep residual MLP [He et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 13
                            }
                        ],
                        "text": "- DeepCross [Shan et al., 2016]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 110,
                                "start": 101
                            }
                        ],
                        "text": "We carefully tuned the L2 regularization for LibFM and HOFM, and the dropout ratio for Wide&Deep and DeepCross."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 150
                            }
                        ],
                        "text": "Frappe MovieLens Method Param# RMSE Param# RMSE LibFM 1.38M 0.3385 23.24M 0.4735 HOFM 2.76M 0.3331 46.40M 0.4636 Wide&Deep 4.66M 0.3246 24.69M 0.4512 DeepCross 8.93M 0.3548 25.42M 0.5130 AFM 1.45M 0.3102 23.26M 0.4325\n\u2022 Lastly, DeepCross performs the worst, due to the severe problem of overfitting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 44
                            }
                        ],
                        "text": "We find that dropout does not work well for DeepCross, which might be caused by its use of batch normalization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 26,
                                "start": 17
                            }
                        ],
                        "text": "Considering that DeepCross is the deepest method (that stacks 10 layers above the embedding layer) among all compared methods, it provides evidence that deeper leaning is not always helpful, as deep networks can suffer from overfitting and are more difficult to optimize in practice [He and Chua, 2017]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 9704646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a83c778e918539941cba9dcaa6ec881b3ae7a29a",
            "isKey": true,
            "numCitedBy": 288,
            "numCiting": 25,
            "paperAbstract": {
                "fragments": [],
                "text": "Manually crafted combinatorial features have been the \"secret sauce\" behind many successful models. For web-scale applications, however, the variety and volume of features make these manually crafted features expensive to create, maintain, and deploy. This paper proposes the Deep Crossing model which is a deep neural network that automatically combines features to produce superior models. The input of Deep Crossing is a set of individual features that can be either dense or sparse. The important crossing features are discovered implicitly by the networks, which are comprised of an embedding and stacking layer, as well as a cascade of Residual Units. Deep Crossing is implemented with a modeling tool called the Computational Network Tool Kit (CNTK), powered by a multi-GPU platform. It was able to build, from scratch, two web-scale models for a major paid search engine, and achieve superior results with only a sub-set of the features used in the production models. This demonstrates the potential of using Deep Crossing as a general modeling paradigm to improve existing products, as well as to speed up the development of new models with a fraction of the investment in feature engineering and acquisition of deep domain knowledge."
            },
            "slug": "Deep-Crossing:-Web-Scale-Modeling-without-Manually-Shan-Hoens",
            "title": {
                "fragments": [],
                "text": "Deep Crossing: Web-Scale Modeling without Manually Crafted Combinatorial Features"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The Deep Crossing model is proposed which is a deep neural network that automatically combines features to produce superior models and was able to build, from scratch, two web-scale models for a major paid search engine, and achieve superior results with only a sub-set of the features used in the production models."
            },
            "venue": {
                "fragments": [],
                "text": "KDD"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40052301"
                        ],
                        "name": "Fabio Petroni",
                        "slug": "Fabio-Petroni",
                        "structuredName": {
                            "firstName": "Fabio",
                            "lastName": "Petroni",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fabio Petroni"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1875906"
                        ],
                        "name": "Luciano Del Corro",
                        "slug": "Luciano-Del-Corro",
                        "structuredName": {
                            "firstName": "Luciano",
                            "lastName": "Corro",
                            "middleNames": [
                                "Del"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Luciano Del Corro"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1777107"
                        ],
                        "name": "Rainer Gemulla",
                        "slug": "Rainer-Gemulla",
                        "structuredName": {
                            "firstName": "Rainer",
                            "lastName": "Gemulla",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rainer Gemulla"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 39
                            }
                        ],
                        "text": ", 2016] to natural language processing [Petroni et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 192
                            }
                        ],
                        "text": "Owing to such generality, FM has been successfully applied to various applications, ranging from recommendation systems [Wang et al., 2017a; Chen et al., 2016] to natural language processing [Petroni et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7236297,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b471a6c7e1d45c78e78ea7b06f6f798a905ae304",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "We propose CORE, a novel matrix factorization model that leverages contextual information for open relation extraction. Our model is based on factorization machines and integrates facts from various sources, such as knowledge bases or open information extractors, as well as the context in which these facts have been observed. We argue that integrating contextual information\u2014such as metadata about extraction sources, lexical context, or type information\u2014significantly improves prediction performance. Open information extractors, for example, may produce extractions that are unspecific or ambiguous when taken out of context. Our experimental study on a large real-world dataset indicates that CORE has significantly better prediction performance than state-ofthe-art approaches when contextual information is available."
            },
            "slug": "CORE:-Context-Aware-Open-Relation-Extraction-with-Petroni-Corro",
            "title": {
                "fragments": [],
                "text": "CORE: Context-Aware Open Relation Extraction with Factorization Machines"
            },
            "tldr": {
                "abstractSimilarityScore": 48,
                "text": "This work argues that integrating contextual information\u2014such as metadata about extraction sources, lexical context, or type information\u2014significantly improves prediction performance and proposes CORE, a novel matrix factorization model that leverages contextual information for open relation extraction."
            },
            "venue": {
                "fragments": [],
                "text": "EMNLP"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47122432"
                        ],
                        "name": "Zhou Zhao",
                        "slug": "Zhou-Zhao",
                        "structuredName": {
                            "firstName": "Zhou",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhou Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694235"
                        ],
                        "name": "Hanqing Lu",
                        "slug": "Hanqing-Lu",
                        "structuredName": {
                            "firstName": "Hanqing",
                            "lastName": "Lu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanqing Lu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724421"
                        ],
                        "name": "Deng Cai",
                        "slug": "Deng-Cai",
                        "structuredName": {
                            "firstName": "Deng",
                            "lastName": "Cai",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Deng Cai"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3945955"
                        ],
                        "name": "Xiaofei He",
                        "slug": "Xiaofei-He",
                        "structuredName": {
                            "firstName": "Xiaofei",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofei He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143749205"
                        ],
                        "name": "Yueting Zhuang",
                        "slug": "Yueting-Zhuang",
                        "structuredName": {
                            "firstName": "Yueting",
                            "lastName": "Zhuang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yueting Zhuang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 97,
                                "start": 80
                            }
                        ],
                        "text": "It has broad applications including recommendation systems [Bayer et al., 2017; Zhao et al., 2016], online advertising [Shan et al., 2016; Juan et al., 2016], and image recognition [Zhang et al., 2017; Wang et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 98,
                                "start": 59
                            }
                        ],
                        "text": "It has broad applications including recommendation systems [Bayer et al., 2017; Zhao et al., 2016], online advertising [Shan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 8370702,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "415b92442297702c1c684a7081ba176d1e7c0529",
            "isKey": false,
            "numCitedBy": 85,
            "numCiting": 61,
            "paperAbstract": {
                "fragments": [],
                "text": "A social recommendation system has attracted a lot of attention recently in the research communities of information retrieval, machine learning, and data mining. Traditional social recommendation algorithms are often based on batch machine learning methods which suffer from several critical limitations, e.g., extremely expensive model retraining cost whenever new user ratings arrive, unable to capture the change of user preferences over time. Therefore, it is important to make social recommendation system suitable for real-world online applications where data often arrives sequentially and user preferences may change dynamically and rapidly. In this paper, we present a new framework of online social recommendation from the viewpoint of online graph regularized user preference learning (OGRPL), which incorporates both collaborative user-item relationship as well as item content features into an unified preference learning process. We further develop an efficient iterative procedure, OGRPL-FW which utilizes the Frank-Wolfe algorithm, to solve the proposed online optimization problem. We conduct extensive experiments on several large-scale datasets, in which the encouraging results demonstrate that the proposed algorithms obtain significantly lower errors (in terms of both RMSE and MAE) than the state-of-the-art online recommendation methods when receiving the same amount of training data in the online learning process."
            },
            "slug": "User-Preference-Learning-for-Online-Social-Zhao-Lu",
            "title": {
                "fragments": [],
                "text": "User Preference Learning for Online Social Recommendation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a new framework of online social recommendation from the viewpoint of online graph regularized user preference learning (OGRPL), which incorporates both collaborative user-item relationship as well as item content features into an unified preference learning process and develops an efficient iterative procedure, OGRPL-FW, to solve the proposed online optimization problem."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6897666"
                        ],
                        "name": "Yang Yang",
                        "slug": "Yang-Yang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143962510"
                        ],
                        "name": "Zhengjun Zha",
                        "slug": "Zhengjun-Zha",
                        "structuredName": {
                            "firstName": "Zhengjun",
                            "lastName": "Zha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhengjun Zha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109218951"
                        ],
                        "name": "Yue Gao",
                        "slug": "Yue-Gao",
                        "structuredName": {
                            "firstName": "Yue",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yue Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1621039417"
                        ],
                        "name": "Xiaofeng Zhu",
                        "slug": "Xiaofeng-Zhu",
                        "structuredName": {
                            "firstName": "Xiaofeng",
                            "lastName": "Zhu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofeng Zhu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 51
                            }
                        ],
                        "text": ", 2015] and more semantic-rich multi-media content [Zhang et al., 2016a; Yang et al., 2014]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 228,
                                "start": 211
                            }
                        ],
                        "text": "Lastly, we will explore AFM on modelling other types of data for different applications, such as texts for question answering [Zhao et al., 2015] and more semantic-rich multi-media content [Zhang et al., 2016a; Yang et al., 2014]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11015076,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d4ab29decccb72ef944f9b2150106e595bf03345",
            "isKey": false,
            "numCitedBy": 128,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Semantic video indexing, also known as video annotation or video concept detection in literatures, has been attracting significant attention in recent years. Due to deficiency of labeled training videos, most of the existing approaches can hardly achieve satisfactory performance. In this paper, we propose a novel semantic video indexing approach, which exploits the abundant user-tagged Web images to help learn robust semantic video indexing classifiers. The following two major challenges are well studied: 1) noisy Web images with imprecise and/or incomplete tags; and 2) domain difference between images and videos. Specifically, we first apply a non-parametric approach to estimate the probabilities of images being correctly tagged as confidence scores. We then develop a robust transfer video indexing (RTVI) model to learn reliable classifiers from a limited number of training videos together with the abundance of user-tagged images. The RTVI model is equipped with a novel sample-specific robust loss function, which employs the confidence score of a Web image as prior knowledge to suppress the influence and control the contribution of this image in the learning process. Meanwhile, the RTVI model discovers an optimal kernel space, in which the mismatch between images and videos is minimized for tackling the domain difference problem. Besides, we devise an iterative algorithm to effectively optimize the proposed RTVI model and a theoretical analysis on the convergence of the proposed algorithm is provided as well. Extensive experiments on various real-world multimedia collections demonstrate the effectiveness of the proposed robust semantic video indexing approach."
            },
            "slug": "Exploiting-Web-Images-for-Semantic-Video-Indexing-Yang-Zha",
            "title": {
                "fragments": [],
                "text": "Exploiting Web Images for Semantic Video Indexing Via Robust Sample-Specific Loss"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "A robust transfer video indexing (RTVI) model is developed, equipped with a novel sample-specific robust loss function, which employs the confidence score of a Web image as prior knowledge to suppress the influence and control of this image in the learning process."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Multimedia"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7792071"
                        ],
                        "name": "Xiangnan He",
                        "slug": "Xiangnan-He",
                        "structuredName": {
                            "firstName": "Xiangnan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangnan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37596605"
                        ],
                        "name": "Min-Yen Kan",
                        "slug": "Min-Yen-Kan",
                        "structuredName": {
                            "firstName": "Min-Yen",
                            "lastName": "Kan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min-Yen Kan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2586395"
                        ],
                        "name": "Peichu Xie",
                        "slug": "Peichu-Xie",
                        "structuredName": {
                            "firstName": "Peichu",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peichu Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145228898"
                        ],
                        "name": "Xiao Chen",
                        "slug": "Xiao-Chen",
                        "structuredName": {
                            "firstName": "Xiao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiao Chen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 228
                            }
                        ],
                        "text": "Another promising direction is to develop FM variants for semisupervised and multi-view learning, for example by incorporating the widely used graph Laplacian [He et al., 2017a; Wang et al., 2016] and co-regularization designs [He et al., 2014; Yang et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2989432,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d7296b5b22b839836d8c226f049131eef1dc0d95",
            "isKey": false,
            "numCitedBy": 89,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering Web 2.0 items (i.e., web resources like videos, images) into semantic groups benefits many applications, such as organizing items, generating meaningful tags and improving web search. In this paper, we systematically investigate how user-generated comments can be used to improve the clustering of Web 2.0 items. In our preliminary study of Last.fm, we find that the two data sources extracted from user comments -- the textual comments and the commenting users -- provide complementary evidence to the items' intrinsic features. These sources have varying levels of quality, but we importantly we find that incorporating all three sources improves clustering. To accommodate such quality imbalance, we invoke multi-view clustering, in which each data source represents a view, aiming to best leverage the utility of different views. To combine multiple views under a principled framework, we propose CoNMF (Co-regularized Non-negative Matrix Factorization), which extends NMF for multi-view clustering by jointly factorizing the multiple matrices through co-regularization. Under our CoNMF framework, we devise two paradigms -- pair-wise CoNMF and cluster-wise CoNMF -- and propose iterative algorithms for their joint factorization. Experimental results on Last.fm and Yelp datasets demonstrate the effectiveness of our solution. In Last.fm, CoNMF betters k-means with a statistically significant F1 increase of 14%, while achieving comparable performance with the state-of-the-art multi-view clustering method CoSC (Co-regularized Spectral Clustering). On a Yelp dataset, CoNMF outperforms the best baseline CoSC with a statistically significant performance gain of 7%."
            },
            "slug": "Comment-based-multi-view-clustering-of-web-2.0-He-Kan",
            "title": {
                "fragments": [],
                "text": "Comment-based multi-view clustering of web 2.0 items"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This paper systematically investigates how user-generated comments can be used to improve the clustering of Web 2.0 items and proposes CoNMF (Co-regularized Non-negative Matrix Factorization), which extends NMF for multi-view clustering by jointly factorizing the multiple matrices through co-regularization."
            },
            "venue": {
                "fragments": [],
                "text": "WWW"
            },
            "year": 2014
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144618699"
                        ],
                        "name": "Fumin Shen",
                        "slug": "Fumin-Shen",
                        "structuredName": {
                            "firstName": "Fumin",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fumin Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780381"
                        ],
                        "name": "Chunhua Shen",
                        "slug": "Chunhua-Shen",
                        "structuredName": {
                            "firstName": "Chunhua",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chunhua Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46641573"
                        ],
                        "name": "W. Liu",
                        "slug": "W.-Liu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724393"
                        ],
                        "name": "H. Shen",
                        "slug": "H.-Shen",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Shen",
                            "middleNames": [
                                "Tao"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 176
                            }
                        ],
                        "text": "As AFM has a relatively high complexity quadratic to the number of non-zero features, we will consider improving its learning efficiency, for example by using learning to hash [Zhang et al., 2016b; Shen et al., 2015] and data sampling [Wang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 215,
                                "start": 198
                            }
                        ],
                        "text": "As AFM has a relatively high complexity quadratic to the number of non-zero features, we will consider improving its learning efficiency, for example by using learning to hash [Zhang et al., 2016b; Shen et al., 2015] and data sampling [Wang et al., 2017b] techniques."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11307479,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7c7899faf1512d2c7bbcb6fda5e881c79f8f135e",
            "isKey": false,
            "numCitedBy": 922,
            "numCiting": 45,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently, learning based hashing techniques have attracted broad research interests because they can support efficient storage and retrieval for high-dimensional data such as images, videos, documents, etc. However, a major difficulty of learning to hash lies in handling the discrete constraints imposed on the pursued hash codes, which typically makes hash optimizations very challenging (NP-hard in general). In this work, we propose a new supervised hashing framework, where the learning objective is to generate the optimal binary hash codes for linear classification. By introducing an auxiliary variable, we reformulate the objective such that it can be solved substantially efficiently by employing a regularization algorithm. One of the key steps in this algorithm is to solve a regularization sub-problem associated with the NP-hard binary optimization. We show that the sub-problem admits an analytical solution via cyclic coordinate descent. As such, a high-quality discrete solution can eventually be obtained in an efficient computing manner, therefore enabling to tackle massive datasets. We evaluate the proposed approach, dubbed Supervised Discrete Hashing (SDH), on four large image datasets and demonstrate its superiority to the state-of-the-art hashing methods in large-scale image retrieval."
            },
            "slug": "Supervised-Discrete-Hashing-Shen-Shen",
            "title": {
                "fragments": [],
                "text": "Supervised Discrete Hashing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This work proposes a new supervised hashing framework, where the learning objective is to generate the optimal binary hash codes for linear classification, and introduces an auxiliary variable to reformulate the objective such that it can be solved substantially efficiently by employing a regularization algorithm."
            },
            "venue": {
                "fragments": [],
                "text": "2015 IEEE Conference on Computer Vision and Pattern Recognition (CVPR)"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144799987"
                        ],
                        "name": "Tao Chen",
                        "slug": "Tao-Chen",
                        "structuredName": {
                            "firstName": "Tao",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tao Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7792071"
                        ],
                        "name": "Xiangnan He",
                        "slug": "Xiangnan-He",
                        "structuredName": {
                            "firstName": "Xiangnan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangnan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37596605"
                        ],
                        "name": "Min-Yen Kan",
                        "slug": "Min-Yen-Kan",
                        "structuredName": {
                            "firstName": "Min-Yen",
                            "lastName": "Kan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min-Yen Kan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 120
                            }
                        ],
                        "text": "Owing to such generality, FM has been successfully applied to various applications, ranging from recommendation systems [Wang et al., 2017a; Chen et al., 2016] to natural language processing [Petroni et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 158,
                                "start": 141
                            }
                        ],
                        "text": "Owing to such generality, FM has been successfully applied to various applications, ranging from recommendation systems [Wang et al., 2017a; Chen et al., 2016] to natural language processing [Petroni et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17224362,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "bdadef4d3265d1f1e3f92155ca1896df0f8619d4",
            "isKey": false,
            "numCitedBy": 68,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "While efforts have been made on bridging the semantic gap in image understanding, the in situ understanding of social media images is arguably more important but has had less progress. In this work, we enrich the representation of images in image tweets by considering their social context. We argue that in the microblog context, traditional image features, e.g., low-level SIFT or high-level detected objects, are far from adequate in interpreting the necessary semantics latent in image tweets. To bridge this gap, we move from the images' pixels to their context and propose a context-aware image bf tweet modelling (CITING) framework to mine and fuse contextual text to model such social media images' semantics. We start with tweet's intrinsic contexts, namely, 1) text within the image itself and 2) its accompanying text; and then we turn to the extrinsic contexts: 3) the external web page linked to by the tweet's embedded URL, and 4) the Web as a whole. These contexts can be leveraged to benefit many fundamental applications. To demonstrate the effectiveness our framework, we focus on the task of personalized image tweet recommendation, developing a feature-aware matrix factorization framework that encodes the contexts as a part of user interest modelling. Extensive experiments on a large Twitter dataset show that our proposed method significantly improves performance. Finally, to spur future studies, we have released both the code of our recommendation model and our image tweet dataset."
            },
            "slug": "Context-aware-Image-Tweet-Modelling-and-Chen-He",
            "title": {
                "fragments": [],
                "text": "Context-aware Image Tweet Modelling and Recommendation"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This work proposes a context-aware image bf tweet modelling (CITING) framework to mine and fuse contextual text to model such social media images' semantics and focuses on the task of personalized image tweet recommendation, developing a feature-aware matrix factorization framework that encodes the contexts as a part of user interest modelling."
            },
            "venue": {
                "fragments": [],
                "text": "ACM Multimedia"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "5462268"
                        ],
                        "name": "Hanwang Zhang",
                        "slug": "Hanwang-Zhang",
                        "structuredName": {
                            "firstName": "Hanwang",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Hanwang Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144618699"
                        ],
                        "name": "Fumin Shen",
                        "slug": "Fumin-Shen",
                        "structuredName": {
                            "firstName": "Fumin",
                            "lastName": "Shen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fumin Shen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46641573"
                        ],
                        "name": "W. Liu",
                        "slug": "W.-Liu",
                        "structuredName": {
                            "firstName": "W.",
                            "lastName": "Liu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "W. Liu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7792071"
                        ],
                        "name": "Xiangnan He",
                        "slug": "Xiangnan-He",
                        "structuredName": {
                            "firstName": "Xiangnan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangnan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3371599"
                        ],
                        "name": "Huanbo Luan",
                        "slug": "Huanbo-Luan",
                        "structuredName": {
                            "firstName": "Huanbo",
                            "lastName": "Luan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Huanbo Luan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144078686"
                        ],
                        "name": "Tat-Seng Chua",
                        "slug": "Tat-Seng-Chua",
                        "structuredName": {
                            "firstName": "Tat-Seng",
                            "lastName": "Chua",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Tat-Seng Chua"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 216,
                                "start": 176
                            }
                        ],
                        "text": "As AFM has a relatively high complexity quadratic to the number of non-zero features, we will consider improving its learning efficiency, for example by using learning to hash [Zhang et al., 2016b; Shen et al., 2015] and data sampling [Wang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 190
                            }
                        ],
                        "text": "Lastly, we will explore AFM on modelling other types of data for different applications, such as texts for question answering [Zhao et al., 2015] and more semantic-rich multi-media content [Zhang et al., 2016a; Yang et al., 2014]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 177
                            }
                        ],
                        "text": "As AFM has a relatively high complexity quadratic to the number of non-zero features, we will consider improving its learning efficiency, for example by using learning to hash [Zhang et al., 2016b; Shen et al., 2015] and data sampling [Wang et al., 2017b] techniques."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 13124023,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b526b9b39046d3661160256153a5b602166f316a",
            "isKey": false,
            "numCitedBy": 217,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the efficiency problem of Collaborative Filtering (CF) by hashing users and items as latent vectors in the form of binary codes, so that user-item affinity can be efficiently calculated in a Hamming space. However, existing hashing methods for CF employ binary code learning procedures that most suffer from the challenging discrete constraints. Hence, those methods generally adopt a two-stage learning scheme composed of relaxed optimization via discarding the discrete constraints, followed by binary quantization. We argue that such a scheme will result in a large quantization loss, which especially compromises the performance of large-scale CF that resorts to longer binary codes. In this paper, we propose a principled CF hashing framework called Discrete Collaborative Filtering (DCF), which directly tackles the challenging discrete optimization that should have been treated adequately in hashing. The formulation of DCF has two advantages: 1) the Hamming similarity induced loss that preserves the intrinsic user-item similarity, and 2) the balanced and uncorrelated code constraints that yield compact yet informative binary codes. We devise a computationally efficient algorithm with a rigorous convergence proof of DCF. Through extensive experiments on several real-world benchmarks, we show that DCF consistently outperforms state-of-the-art CF hashing techniques, e.g, though using only 8 bits, DCF is even significantly better than other methods using 128 bits."
            },
            "slug": "Discrete-Collaborative-Filtering-Zhang-Shen",
            "title": {
                "fragments": [],
                "text": "Discrete Collaborative Filtering"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper proposes a principled CF hashing framework called Discrete Collaborative Filtering (DCF), which directly tackles the challenging discrete optimization that should have been treated adequately in hashing, and devise a computationally efficient algorithm with a rigorous convergence proof of DCF."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144197768"
                        ],
                        "name": "Zhou Zhao",
                        "slug": "Zhou-Zhao",
                        "structuredName": {
                            "firstName": "Zhou",
                            "lastName": "Zhao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhou Zhao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108876951"
                        ],
                        "name": "Lijun Zhang",
                        "slug": "Lijun-Zhang",
                        "structuredName": {
                            "firstName": "Lijun",
                            "lastName": "Zhang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Lijun Zhang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3945955"
                        ],
                        "name": "Xiaofei He",
                        "slug": "Xiaofei-He",
                        "structuredName": {
                            "firstName": "Xiaofei",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiaofei He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1695492"
                        ],
                        "name": "Wilfred Ng",
                        "slug": "Wilfred-Ng",
                        "structuredName": {
                            "firstName": "Wilfred",
                            "lastName": "Ng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Wilfred Ng"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 145,
                                "start": 126
                            }
                        ],
                        "text": "Lastly, we will explore AFM on modelling other types of data for different applications, such as texts for question answering [Zhao et al., 2015] and more semantic-rich multi-media content [Zhang et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 144,
                                "start": 127
                            }
                        ],
                        "text": "Lastly, we will explore AFM on modelling other types of data for different applications, such as texts for question answering [Zhao et al., 2015] and more semantic-rich multi-media content [Zhang et al., 2016a; Yang et al., 2014]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15211029,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2141f99fcce1c5df886816b179adf9adfb3939ca",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 60,
            "paperAbstract": {
                "fragments": [],
                "text": "Expert finding for question answering is a challenging problem in community-based question answering (CQA) systems, arising in many real applications such as question routing and identification of best answers. In order to provide high-quality experts, many existing approaches learn the user model from their past question-answering activities in CQA systems. However, the past activities of users in most CQA systems are rather few, and thus the user model may not be well inferred in practice. In this paper, we consider the problem of expert finding from the viewpoint of missing value estimation. We then employ users' social networks for inferring user model, and thus improve the performance of expert finding in CQA systems. In addition, we develop a novel graph-regularized matrix completion algorithm for inferring the user model. We further develop two efficient iterative procedures, GRMC-EGM and GRMC-AGM, to solve the optimization problem. GRMC-EGM utilizes the Extended Gradient Method (EGM), while GRMC-AGM applies the Accelerated proximal Gradient search Method (AGM), for the optimization. We evaluate our methods on the well-known question answering system Quora, and the popular social network Twitter. Our empirical study shows the effectiveness of the proposed algorithms in comparison to the state-of-the-art expert finding algorithms."
            },
            "slug": "Expert-Finding-for-Question-Answering-via-Graph-Zhao-Zhang",
            "title": {
                "fragments": [],
                "text": "Expert Finding for Question Answering via Graph Regularized Matrix Completion"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper considers the problem of expert finding from the viewpoint of missing value estimation, and develops a novel graph-regularized matrix completion algorithm for inferring the user model and develops two efficient iterative procedures, GRMC-EGM andGRMC-AGM, to solve the optimization problem."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "7792071"
                        ],
                        "name": "Xiangnan He",
                        "slug": "Xiangnan-He",
                        "structuredName": {
                            "firstName": "Xiangnan",
                            "lastName": "He",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiangnan He"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "46572901"
                        ],
                        "name": "Ming Gao",
                        "slug": "Ming-Gao",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Gao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Gao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37596605"
                        ],
                        "name": "Min-Yen Kan",
                        "slug": "Min-Yen-Kan",
                        "structuredName": {
                            "firstName": "Min-Yen",
                            "lastName": "Kan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Min-Yen Kan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2506522"
                        ],
                        "name": "Dingxian Wang",
                        "slug": "Dingxian-Wang",
                        "structuredName": {
                            "firstName": "Dingxian",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Dingxian Wang"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 176,
                                "start": 160
                            }
                        ],
                        "text": "Another promising direction is to develop FM variants for semisupervised and multi-view learning, for example by incorporating the widely used graph Laplacian [He et al., 2017a; Wang et al., 2016] and co-regularization designs [He et al., 2014; Yang et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 73
                            }
                        ],
                        "text": "For binary classification or recommendation task with implicit feedback [He et al., 2017b], we can minimize the log loss."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 159
                            }
                        ],
                        "text": "Another promising direction is to develop FM variants for semisupervised and multi-view learning, for example by incorporating the widely used graph Laplacian [He et al., 2017a; Wang et al., 2016] and co-regularization designs [He et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 94809,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "728f470d67c5b2d6e3ea1fa13b3b3a23545b3f65",
            "isKey": false,
            "numCitedBy": 115,
            "numCiting": 43,
            "paperAbstract": {
                "fragments": [],
                "text": "The bipartite graph is a ubiquitous data structure that can model the relationship between two entity types: for instance, users and items, queries and webpages. In this paper, we study the problem of ranking vertices of a bipartite graph, based on the graph's link structure as well as prior information about vertices (which we term a query vector). We present a new solution, BiRank, which iteratively assigns scores to vertices and finally converges to a unique stationary ranking. In contrast to the traditional random walk-based methods, BiRank iterates towards optimizing a regularization function, which smooths the graph under the guidance of the query vector. Importantly, we establish how BiRank relates to the Bayesian methodology, enabling the future extension in a probabilistic way. To show the rationale and extendability of the ranking methodology, we further extend it to rank for the more generic n-partite graphs. BiRank's generic modeling of both the graph structure and vertex features enables it to model various ranking hypotheses flexibly. To illustrate its functionality, we apply the BiRank and TriRank (ranking for tripartite graphs) algorithms to two real-world applications: a general ranking scenario that predicts the future popularity of items, and a personalized ranking scenario that recommends items of interest to users. Extensive experiments on both synthetic and real-world datasets demonstrate BiRank's soundness (fast convergence), efficiency (linear in the number of graph edges), and effectiveness (achieving state-of-the-art in the two real-world tasks)."
            },
            "slug": "BiRank:-Towards-Ranking-on-Bipartite-Graphs-He-Gao",
            "title": {
                "fragments": [],
                "text": "BiRank: Towards Ranking on Bipartite Graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper presents a new solution, BiRank, which iteratively assigns scores to vertices and finally converges to a unique stationary ranking, and establishes how BiRank relates to the Bayesian methodology, enabling the future extension in a probabilistic way."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145192090"
                        ],
                        "name": "F. M. Harper",
                        "slug": "F.-M.-Harper",
                        "structuredName": {
                            "firstName": "F.",
                            "lastName": "Harper",
                            "middleNames": [
                                "Maxwell"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. M. Harper"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2478310"
                        ],
                        "name": "J. Konstan",
                        "slug": "J.-Konstan",
                        "structuredName": {
                            "firstName": "Joseph",
                            "lastName": "Konstan",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Konstan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 121,
                                "start": 97
                            }
                        ],
                        "text": "We perform experiments with two public datasets: Frappe [Baltrunas et al., 2015] and MovieLens2 [Harper and Konstan, 2015]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 16619709,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "276ebc620a8976026bd2d03582b9ecfa3738d43c",
            "isKey": false,
            "numCitedBy": 2909,
            "numCiting": 49,
            "paperAbstract": {
                "fragments": [],
                "text": "The MovieLens datasets are widely used in education, research, and industry. They are downloaded hundreds of thousands of times each year, reflecting their use in popular press programming books, traditional and online courses, and software. These datasets are a product of member activity in the MovieLens movie recommendation system, an active research platform that has hosted many experiments since its launch in 1997. This article documents the history of MovieLens and the MovieLens datasets. We include a discussion of lessons learned from running a long-standing, live research platform from the perspective of a research organization. We document best practices and limitations of using the MovieLens datasets in new research."
            },
            "slug": "The-MovieLens-Datasets:-History-and-Context-Harper-Konstan",
            "title": {
                "fragments": [],
                "text": "The MovieLens Datasets: History and Context"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "The history of MovieLens and the MovieLens datasets is documents, including a discussion of lessons learned from running a long-standing, live research platform from the perspective of a research organization, and best practices and limitations of using the Movie Lens datasets in new research are documented."
            },
            "venue": {
                "fragments": [],
                "text": "TIIS"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2666397"
                        ],
                        "name": "L. Baltrunas",
                        "slug": "L.-Baltrunas",
                        "structuredName": {
                            "firstName": "Linas",
                            "lastName": "Baltrunas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Baltrunas"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145639750"
                        ],
                        "name": "K. Church",
                        "slug": "K.-Church",
                        "structuredName": {
                            "firstName": "Karen",
                            "lastName": "Church",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Church"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1713164"
                        ],
                        "name": "Alexandros Karatzoglou",
                        "slug": "Alexandros-Karatzoglou",
                        "structuredName": {
                            "firstName": "Alexandros",
                            "lastName": "Karatzoglou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Alexandros Karatzoglou"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145709776"
                        ],
                        "name": "Nuria Oliver",
                        "slug": "Nuria-Oliver",
                        "structuredName": {
                            "firstName": "Nuria",
                            "lastName": "Oliver",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nuria Oliver"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 188,
                                "start": 182
                            }
                        ],
                        "text": "0.5 0.6 0.7 .8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nMovieLens\nAFM FM LibFM\n0.31\n0.32\n0.33 0.34 0.35 0.36\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31\n0.33\n0.35\n0.37 0.39 0.41\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nMovieLens\nAFM(train) AFM(test) FM(train) FM(test)\nFigure 5: Training and test error of each epoch\nthis, we perform some micro-level analysis by investigating the score of each feature interaction on MovieLens."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 41
                            }
                        ],
                        "text": "As such, the final experimental data for Frappe and MovieLens contain 288, 609 and 2, 006, 859 instances, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 252,
                                "start": 246
                            }
                        ],
                        "text": "To demonstrate\n0.44 0.45 0.46 0.47 0.48 0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M SE (v al id at io n )\nDropout Ratio\nMovieLens AFM FM LibFM 0.31 0.32 0.33 0.34 0.35 0.36 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31 0.32 0.33 0.34 0.35\n0 0.5 1 2 4 8 16\nR M SE (v al id at io n )\n\u03bb\nFrappe AFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31 0.33 0.35 0.37 0.39 0.41 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00 0.10 0.20 0.30 0.40 0.50 0.60\n0 20 40 60 80 100\nR M SE\nEpoch\nMovieLens AFM(train) AFM(test) FM(train) FM(test)\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0 0.1 0.2 0.3 0."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 79,
                                "start": 57
                            }
                        ],
                        "text": "We perform experiments with two public datasets: Frappe [Baltrunas et al., 2015] and MovieLens2 [Harper and Konstan, 2015]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 325,
                                "start": 319
                            }
                        ],
                        "text": "Without special\n2grouplens.org/datasets/movielens/latest 3https://github.com/geffy/tffm\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nMovieLens\nAFM FM LibFM\n0.31\n0.32\n0.33\n0.34\n0.35\n0.36\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nFrappe\nAFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31\n0.33\n0.35\n0.37\n0.39\n0.41\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nFrappe\nAFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50\nR M\nSE\nEpoch\nMovieLens\nAFM(train)\nAFM(test)\nFM(train)\nFM(test)\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nMovieLens\nAFM FM LibFM\n0.31\n0.32\n0.33 0.34 0.35 0.36\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31\n0.33\n0.35\n0.37 0.39 0.41\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50\nR M\nSE\nEpoch\nMovieLens\nAFM(train) AFM(test) FM(train) FM(test)\nFigure 2: Validation error of AFM and FM w.r.t. different dropout ratios on the pair-wise interaction layer\nmention, the attention factor is also 256, same as the embedding size."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 346,
                                "start": 340
                            }
                        ],
                        "text": "As can be seen from Figure 3, when \u03bb is set to a value larger than 0, AFM is improved (note that the result of\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nMovieLens\nAFM FM LibFM\n0.31\n0.32 0.33 0.34 0.35 0.36\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M SE (v al id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31\n0.33\n0.35 0.37 0.39 0.41\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M SE (v al id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50\nR M\nSE\nEpoch\nMovieLens\nAFM(train)\nAFM(test)\nFM(train)\nFM(test)\n0.44\n0.45 0.46 0.47 0.48 0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M SE (v al id at io n )\nDropout Ratio\nMovieLens AFM FM LibFM\n0.31 0.32 0.33 0.34 0.35 0.36 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32 0.33 0.34 0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id at io n )\n\u03bb\nFrappe AFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31 0.33 0.35 0.37 0.39 0.41 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50\nR M\nSE\nEpoch\nMovieLens\nAFM(train) AFM(test) FM(train) FM(test)\nFigure 3: Validation error of AFM w.r.t. different regularization strengths on the attention network\n0.44\n0.45\n0.46\n0.47\n0.48\n0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id\nat io\nn )\nDropout Ratio\nMovieLens\nAFM FM LibFM\n0.31\n0.32\n0.33\n0.34 0.35 0.36\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31\n0.33\n0.35\n0.37 0.39 0.41\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nFrappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50\nR M\nSE\nEpoch\nMovieLens\nAFM(train) AFM(test) FM(train) FM(test)\n0.44\n0.45\n0.46 0.47 0.48 0.49\n0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8\nR M\nSE (v\nal id at io n )\nDropout Ratio\nMovieLens AFM FM LibFM\n0.31 0.32 0.33 0.34 0.35 0.36 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 10 20 30 40 50 60 70 80 90 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.30\n0.31\n0.32\n0.33 0.34 0.35\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io n )\n\u03bb\nFrappe\nAFM FM LibFM\n0.30\n0.31\n0.32\n0.33\n0.34\n0.35\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nFrappe\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n0 0.5 1 2 4 8 16\nR M\nSE (v\nal id\nat io\nn )\n\u03bb\nMovieLens\nAFM FM LibFM\n0.43\n0.44\n0.45\n0.46\n0.47\n0.48\n1 4 8 16 32 64 128 256\nR M\nSE (v\nal id\nat io\nn )\nAttention Factors\nMovieLens\nAFM FM LibFM\n0.31 0.33 0.35 0.37 0.39 0.41 0 0.1 0.2 0.3 0.4 0.5 0.6 0.7 0.8 R M SE (v al id at io n ) Dropout Ratio Frappe AFM FM LibFM\n0.00\n0.10\n0.20\n0.30\n0.40\n0.50\n0.60\n0 20 40 60 80 100\nR M\nSE\nEpoch\nFrappe\nAFM(train) AFM(test) FM(train) FM(test)\n0.00\n0.10\n0.20\n0.30 0.40 0.50 0.60\n0 10 20 30 40 50\nR M SE\nEpoch\nMovieLens AFM(train) AFM(test) FM(train) FM(test)\nFigure 4: Validation error of AFM w.r.t. different attention factors\n\u03bb = 0 corresponds to the best performance obtained by AFM in Figure 2)."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 51
                            }
                        ],
                        "text": "Specifically, for AFM, the optimal dropout rati on Frappe and MovieLens is 0.2 and 0.5, respectively."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 9,
                                "start": 3
                            }
                        ],
                        "text": "On Frappe, both the training and test error of AFM are much lower than that of FM, indicating that AFM can better fit the data and lead to more accurate prediction."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 4
                            }
                        ],
                        "text": "The Frappe dataset has been used for context-aware recommendation, which contains 96, 203 app usage logs of users under different contexts."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 6,
                                "start": 0
                            }
                        ],
                        "text": "Frappe MovieLens Method Param# RMSE Param# RMSE LibFM 1.38M 0.3385 23.24M 0.4735 HOFM 2.76M 0.3331 46.40M 0.4636 Wide&Deep 4.66M 0.3246 24.69M 0.4512 DeepCross 8.93M 0.3548 25.42M 0.5130 AFM 1.45M 0.3102 23.26M 0.4325\n\u2022 Lastly, DeepCross performs the worst, due to the severe problem of overfitting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 25,
                                "start": 19
                            }
                        ],
                        "text": "The batch size for Frappe and MovieLens is set to 128 and 4096, respectively."
                    },
                    "intents": []
                }
            ],
            "corpusId": 1791006,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "00f4df1d155cc9bdf412e1d5d17c9193799e6e9f",
            "isKey": true,
            "numCitedBy": 67,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a real world deployment of a contextaware mobile app recommender system (RS) called Frapp e. Utilizing a hybrid-approach, we conducted a large-scale app market deployment with 1000 Android users combined with a small-scale local user study involving 33 users. The resulting usage logs and subjective feedback enabled us to gather key insights into (1) context-dependent app usage and (2) the perceptions and experiences of end-users while interacting with context-aware mobile app recommendations. While Frapp e performs very well based on usage-centric evaluation metrics insights from the small-scale study reveal some negative user experiences. Our results point to a number of actionable lessons learned specically related to designing, deploying and evaluating mobile context-aware RS in-thewild with real users."
            },
            "slug": "Frappe:-Understanding-the-Usage-and-Perception-of-Baltrunas-Church",
            "title": {
                "fragments": [],
                "text": "Frappe: Understanding the Usage and Perception of Mobile App Recommendations In-The-Wild"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "While Frapp e performs very well based on usage-centric evaluation metrics insights from the small-scale study reveal some negative user experiences, these results point to a number of actionable lessons learned specically related to designing, deploying and evaluating mobile context-aware RS in-thewild with real users."
            },
            "venue": {
                "fragments": [],
                "text": "ArXiv"
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2059358552"
                        ],
                        "name": "P. Cochat",
                        "slug": "P.-Cochat",
                        "structuredName": {
                            "firstName": "P.",
                            "lastName": "Cochat",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Cochat"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "13267685"
                        ],
                        "name": "L. Vaucoret",
                        "slug": "L.-Vaucoret",
                        "structuredName": {
                            "firstName": "L",
                            "lastName": "Vaucoret",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Vaucoret"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2097644863"
                        ],
                        "name": "J. Sarles",
                        "slug": "J.-Sarles",
                        "structuredName": {
                            "firstName": "J",
                            "lastName": "Sarles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Sarles"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 31,
                                "start": 13
                            }
                        ],
                        "text": "- Wide&Deep [Cheng et al., 2016]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 142,
                                "start": 124
                            }
                        ],
                        "text": "However, the key problem with PR (and other similar cross feature-based solutions, such as the wide component of Wide&Deep [Cheng et al., 2016]) is that for sparse datasets where only a few cross features are observed, the parameters for unobserved cross features cannot be estimated."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 53,
                                "start": 44
                            }
                        ],
                        "text": "Specifically, [Cheng et al., 2016] proposes Wide&Deep for App recommendation, where the Deep component is a MLP on the concatenation of feature embedding vectors to learn feature interactions; and [Shan et al., 2016] proposes DeepCross for click-through rate prediction, which applies a deep residual MLP [He et al., 2016a] to learn cross features."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 171,
                                "start": 153
                            }
                        ],
                        "text": "When performing supervised learning on categorical predictor variables, it is important to account for the interactions between them [He and Chua, 2017; Cheng et al., 2016]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 13,
                                "start": 4
                            }
                        ],
                        "text": "For Wide&Deep, DeepCross and AFM, we find that pre-training their feature embeddings with FM leads to a lower RMSE than a random initialization."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 204,
                                "start": 195
                            }
                        ],
                        "text": "By directly extending FM with the attention mechanism that learns the importance of each feature interaction, our AMF is more interpretable and empirically demonstrates superior performance over Wide&Deep and DeepCross."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 15
                            }
                        ],
                        "text": "Specifically, [Cheng et al., 2016] proposes Wide&Deep for App recommendation, where the Deep component is a MLP on the concatenation of feature embedding vectors to learn feature interactions; and [Shan et al., 2016] proposes DeepCross for click-through rate prediction, which applies a deep\u2026"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 96,
                                "start": 87
                            }
                        ],
                        "text": "We carefully tuned the L2 regularization for LibFM and HOFM, and the dropout ratio for Wide&Deep and DeepCross."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 113
                            }
                        ],
                        "text": "Frappe MovieLens Method Param# RMSE Param# RMSE LibFM 1.38M 0.3385 23.24M 0.4735 HOFM 2.76M 0.3331 46.40M 0.4636 Wide&Deep 4.66M 0.3246 24.69M 0.4512 DeepCross 8.93M 0.3548 25.42M 0.5130 AFM 1.45M 0.3102 23.26M 0.4325\n\u2022 Lastly, DeepCross performs the worst, due to the severe problem of overfitting."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 34,
                                "start": 14
                            }
                        ],
                        "text": "Specifically, [Cheng et al., 2016] proposes Wide&Deep for App recommendation, where the Deep component is a MLP on the concatenation of feature embedding vectors to learn feature interactions; and [Shan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 164,
                                "start": 155
                            }
                        ],
                        "text": "Specifically, AFM betters LibFM with a 8.6% relative improvement by using less than 0.1M additional parameters; and AFM outperforms the second best method Wide&Deep with 4.3%, while using much fewer model parameters."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 127,
                                "start": 107
                            }
                        ],
                        "text": "6% relative improvement, and consistently outperforms the state-of-the-art deep learning methods Wide&Deep [Cheng et al., 2016] and DeepCross [Shan et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11759366,
            "fieldsOfStudy": [
                "Medicine"
            ],
            "id": "bc6dff14a130c57a91d5a21339c23471faf1d46f",
            "isKey": true,
            "numCitedBy": 61372,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "disasters. Plenum, 2001. 11. Haley R, Thomas L, Hom J. Is there a Gulf War Syndrome? Searching for syndromes by factor analysis of symptoms. JAMA 1997;277:215\u201322. 12. Fukuda K, Nisenbaum R, Stewart G, et al. Chronic multi-symptom illness affecting Air Force veterans of the Gulf War. JAMA 1998;280:981\u20138. 13. Ismail K, Everitt B, Blatchley N, et al. Is there a Gulf War Syndrome? Lancet 1999;353:179\u201382. 14. Shapiro S, Lasarev M, McCauley L. Factor analysis of Gulf War illness: what does it add to our understanding of possible health effects of deployment. Am J Epidemiol 2002;156:578\u201385. 15. Doebbeling B, Clarke W, Watson D, et al. Is there a Persian Gulf War Syndrome? Evidence from a large population-based survey of veterans and nondeployed controls. Am J Med 2000;108:695\u2013704. 16. Knoke J, Smith T, Gray G, et al. Factor analysis of self reported symptoms: Does it identify a Gulf War Syndrome? Am J Epidemiol 2000;152:379\u201388. 17. Kang H, Mahan C, Lee K, et al. Evidence for a deployment-related Gulf War syndrome by factor analysis. Arch Environ Health 2002;57:61\u20138."
            },
            "slug": "Et-al-Cochat-Vaucoret",
            "title": {
                "fragments": [],
                "text": "Et al"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A large population-based survey of veterans and nondeployed controls found evidence of a deployment-related Gulf War syndrome by factor analysis in Air Force veterans and controls."
            },
            "venue": {
                "fragments": [],
                "text": "Archives de pediatrie : organe officiel de la Societe francaise de pediatrie"
            },
            "year": 2012
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 191,
                                "start": 171
                            }
                        ],
                        "text": "Attention-based Pooling Layer Since the attention mechanism has been introduced to neural network modelling, it has been widely used in many tasks, such as recommendation [Chen et al., 2017a], information retrieval [Xiong et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 150,
                                "start": 123
                            }
                        ],
                        "text": "We devise a novel model named AFM, which utilizes the recent advance in neural network modelling \u2014 the attention mechanism [Chen et al., 2017a; 2017b] \u2014 to enable feature interactions contribute differently to the prediction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 140,
                                "start": 122
                            }
                        ],
                        "text": "We devise a novel model named AFM, which utilizes the recent advance in neural network modelling \u2014 the attention mechanism [Chen et al., 2017a; Chen et al., 2017b] \u2014 to enable feature interactions contribute differently to the prediction."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 190,
                                "start": 172
                            }
                        ],
                        "text": "Attention-based Pooling Layer Since the attention mechanism has been introduced to neural network modelling, it has been widely used in many tasks, such as recommendation [Chen et al., 2017a], information retrieval [Xiong et al., 2017], and computer vision [Chen et al., 2017b]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Attentive collaborative filtering: Multimedia recommendation with feature- and item-level attention"
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR,"
            },
            "year": 2017
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47446553"
                        ],
                        "name": "Meng Wang",
                        "slug": "Meng-Wang",
                        "structuredName": {
                            "firstName": "Meng",
                            "lastName": "Wang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Meng Wang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "50091230"
                        ],
                        "name": "Weijie Fu",
                        "slug": "Weijie-Fu",
                        "structuredName": {
                            "firstName": "Weijie",
                            "lastName": "Fu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Weijie Fu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6175623"
                        ],
                        "name": "Shijie Hao",
                        "slug": "Shijie-Hao",
                        "structuredName": {
                            "firstName": "Shijie",
                            "lastName": "Hao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shijie Hao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143719920"
                        ],
                        "name": "D. Tao",
                        "slug": "D.-Tao",
                        "structuredName": {
                            "firstName": "Dacheng",
                            "lastName": "Tao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Tao"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2145502658"
                        ],
                        "name": "Xindong Wu",
                        "slug": "Xindong-Wu",
                        "structuredName": {
                            "firstName": "Xindong",
                            "lastName": "Wu",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xindong Wu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 195,
                                "start": 178
                            }
                        ],
                        "text": "Another promising direction is to develop FM variants for semisupervised and multi-view learning, for example by incorporating the widely used graph Laplacian [He et al., 2017a; Wang et al., 2016] and co-regularization designs [He et al., 2014; Yang et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 159
                            }
                        ],
                        "text": "Another promising direction is to develop FM variants for semisupervised and multi-view learning, for example by incorporating the widely used graph Laplacian [He et al., 2017a; Wang et al., 2016] and co-regularization designs [He et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 18856812,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a4be6fa575bf7325514acfd38a7d1c19e42b37a9",
            "isKey": false,
            "numCitedBy": 138,
            "numCiting": 69,
            "paperAbstract": {
                "fragments": [],
                "text": "Many graph-based semi-supervised learning methods for large datasets have been proposed to cope with the rapidly increasing size of data, such as Anchor Graph Regularization (AGR). This model builds a regularization framework by exploring the underlying structure of the whole dataset with both datapoints and anchors. Nevertheless, AGR still has limitations in its two components: (1) in anchor graph construction, the estimation of the local weights between each datapoint and its neighboring anchors could be biased and relatively slow; and (2) in anchor graph regularization, the adjacency matrix that estimates the relationship between datapoints, is not sufficiently effective. In this paper, we develop an Efficient Anchor Graph Regularization (EAGR) by tackling these issues. First, we propose a fast local anchor embedding method, which reformulates the optimization of local weights and obtains an analytical solution. We show that this method better reconstructs datapoints with anchors and speeds up the optimizing process. Second, we propose a new adjacency matrix among anchors by considering the commonly linked datapoints, which leads to a more effective normalized graph Laplacian over anchors. We show that, with the novel local weight estimation and normalized graph Laplacian, EAGR is able to achieve better classification accuracy with much less computational costs. Experimental results on several publicly available datasets demonstrate the effectiveness of our approach."
            },
            "slug": "Scalable-Semi-Supervised-Learning-by-Efficient-Wang-Fu",
            "title": {
                "fragments": [],
                "text": "Scalable Semi-Supervised Learning by Efficient Anchor Graph Regularization"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A fast local anchor embedding method, which reformulates the optimization of local weights and obtains an analytical solution, and a new adjacency matrix among anchors by considering the commonly linked datapoints, which leads to a more effective normalized graph Laplacian over anchors."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Knowledge and Data Engineering"
            },
            "year": 2016
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6897666"
                        ],
                        "name": "Yang Yang",
                        "slug": "Yang-Yang",
                        "structuredName": {
                            "firstName": "Yang",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yang Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1727419"
                        ],
                        "name": "Zhigang Ma",
                        "slug": "Zhigang-Ma",
                        "structuredName": {
                            "firstName": "Zhigang",
                            "lastName": "Ma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhigang Ma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2143684966"
                        ],
                        "name": "Yi Yang",
                        "slug": "Yi-Yang",
                        "structuredName": {
                            "firstName": "Yi",
                            "lastName": "Yang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yi Yang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144962210"
                        ],
                        "name": "F. Nie",
                        "slug": "F.-Nie",
                        "structuredName": {
                            "firstName": "Feiping",
                            "lastName": "Nie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Nie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1724393"
                        ],
                        "name": "H. Shen",
                        "slug": "H.-Shen",
                        "structuredName": {
                            "firstName": "Heng",
                            "lastName": "Shen",
                            "middleNames": [
                                "Tao"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Shen"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 262,
                                "start": 245
                            }
                        ],
                        "text": "Another promising direction is to develop FM variants for semisupervised and multi-view learning, for example by incorporating the widely used graph Laplacian [He et al., 2017a; Wang et al., 2016] and co-regularization designs [He et al., 2014; Yang et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 38
                            }
                        ],
                        "text": ", 2016] and co-regularization designs [He et al., 2014; Yang et al., 2015]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 4520179,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d454472b2e20c0df8b7fd794138766a59c7a41e3",
            "isKey": false,
            "numCitedBy": 147,
            "numCiting": 44,
            "paperAbstract": {
                "fragments": [],
                "text": "Clustering, as one of the most classical research problems in pattern recognition and data mining, has been widely explored and applied to various applications. Due to the rapid evolution of data on the Web, more emerging challenges have been posed on traditional clustering techniques: 1) correlations among related clustering tasks and/or within individual task are not well captured; 2) the problem of clustering out-of-sample data is seldom considered; and 3) the discriminative property of cluster label matrix is not well explored. In this paper, we propose a novel clustering model, namely multitask spectral clustering (MTSC), to cope with the above challenges. Specifically, two types of correlations are well considered: 1) intertask clustering correlation, which refers the relations among different clustering tasks and 2) intratask learning correlation, which enables the processes of learning cluster labels and learning mapping function to reinforce each other. We incorporate a novel l2,p -norm regularizer to control the coherence of all the tasks based on an assumption that related tasks should share a common low-dimensional representation. Moreover, for each individual task, an explicit mapping function is simultaneously learnt for predicting cluster labels by mapping features to the cluster label matrix. Meanwhile, we show that the learning process can naturally incorporate discriminative information to further improve clustering performance. We explore and discuss the relationships between our proposed model and several representative clustering techniques, including spectral clustering, k -means and discriminative k -means. Extensive experiments on various real-world datasets illustrate the advantage of the proposed MTSC model compared to state-of-the-art clustering approaches."
            },
            "slug": "Multitask-Spectral-Clustering-by-Exploring-Yang-Ma",
            "title": {
                "fragments": [],
                "text": "Multitask Spectral Clustering by Exploring Intertask Correlation"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel clustering model, namely multitask spectral clustering (MTSC), is proposed to cope with the above challenges and it is shown that the learning process can naturally incorporate discriminative information to further improve clustering performance."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Transactions on Cybernetics"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 159,
                                "start": 120
                            }
                        ],
                        "text": "Owing to such generality, FM has been successfully applied to various applications, ranging from recommendation systems [Wang et al., 2017a; Chen et al., 2016] to natural language processing [Petroni et al."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Xiangnan He"
            },
            "venue": {
                "fragments": [],
                "text": "Liqiang Nie and Tat-Seng Chua Item Silk Road: Recommending Items from Information Domains to Social Users SIGIR,"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 219,
                                "start": 202
                            }
                        ],
                        "text": "It has broad applications including recommendation systems [Bayer et al., 2017; Zhao et al., 2016], online advertising [Shan et al., 2016; Juan et al., 2016], and image recognition [Zhang et al., 2017; Wang et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Visual classification by l1-hypergraph modeling"
            },
            "venue": {
                "fragments": [],
                "text": "IEEE TKDE,"
            },
            "year": 2015
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 52,
                                "start": 32
                            }
                        ],
                        "text": ", 2017a], information retrieval [Xiong et al., 2017], and computer vision [Chen et al."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 234,
                                "start": 216
                            }
                        ],
                        "text": "Attention-based Pooling Layer Since the attention mechanism has been introduced to neural network modelling, it has been widely used in many tasks, such as recommendation [Chen et al., 2017a], information retrieval [Xiong et al., 2017], and computer vision [Chen et al., 2017b]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning to attend and to rank with word-entity duets"
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR,"
            },
            "year": 2017
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 91,
                                "start": 51
                            }
                        ],
                        "text": ", 2015] and more semantic-rich multi-media content [Zhang et al., 2016a; Yang et al., 2014]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 209,
                                "start": 190
                            }
                        ],
                        "text": "Lastly, we will explore AFM on modelling other types of data for different applications, such as texts for question answering [Zhao et al., 2015] and more semantic-rich multi-media content [Zhang et al., 2016a; Yang et al., 2014]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 196,
                                "start": 177
                            }
                        ],
                        "text": "As AFM has a relatively high complexity quadratic to the number of non-zero features, we will consider improving its learning efficiency, for example by using learning to hash [Zhang et al., 2016b; Shen et al., 2015] and data sampling [Wang et al., 2017b] techniques."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Learning from collective intelligence: Feature learning using social images and tags"
            },
            "venue": {
                "fragments": [],
                "text": "TMM,"
            },
            "year": 2016
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 243,
                                "start": 228
                            }
                        ],
                        "text": "Another promising direction is to develop FM variants for semisupervised and multi-view learning, for example by incorporating the widely used graph Laplacian [He et al., 2017a; Wang et al., 2016] and co-regularization designs [He et al., 2014; Yang et al., 2015]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 74,
                                "start": 38
                            }
                        ],
                        "text": ", 2016] and co-regularization designs [He et al., 2014; Yang et al., 2015]."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "and Xiao Chen"
            },
            "venue": {
                "fragments": [],
                "text": "Comment-based multi-view clustering of web 2.0 items. In WWW,"
            },
            "year": 2014
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 24,
            "methodology": 23,
            "result": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 37,
        "totalPages": 4
    },
    "page_url": "https://www.semanticscholar.org/paper/Attentional-Factorization-Machines:-Learning-the-of-Xiao-Ye/f0af0029293dc8f242894f113baf15d68228ec4d?sort=total-citations"
}