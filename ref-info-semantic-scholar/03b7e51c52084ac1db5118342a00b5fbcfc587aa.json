{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39263338"
                        ],
                        "name": "Longxin Lin",
                        "slug": "Longxin-Lin",
                        "structuredName": {
                            "firstName": "Longxin",
                            "lastName": "Lin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Longxin Lin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3248358,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "9cd8193a66cf53143cbba6ccb0c7b9c2ebf2452b",
            "isKey": false,
            "numCitedBy": 1012,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "To date, reinforcement learning has mostly been studied solving simple learning tasks. Reinforcement learning methods that have been studied so far typically converge slowly. The purpose of this work is thus two-fold: 1) to investigate the utility of reinforcement learning in solving much more complicated learning tasks than previously studied, and 2) to investigate methods that will speed up reinforcement learning.This paper compares eight reinforcement learning frameworks:adaptive heuristic critic (AHC) learning due to Sutton,Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning. The three extensions are experience replay, learning action models for planning, and teaching. The frameworks were investigated using connectionism as an approach to generalization. To evaluate the performance of different frameworks, a dynamic environment was used as a testbed. The environment is moderately complex and nondeterministic. This paper describes these frameworks and algorithms in detail and presents empirical evaluation of the frameworks."
            },
            "slug": "Self-improving-reactive-agents-based-on-learning,-Lin",
            "title": {
                "fragments": [],
                "text": "Self-improving reactive agents based on reinforcement learning, planning and teaching"
            },
            "tldr": {
                "abstractSimilarityScore": 49,
                "text": "This paper compares eight reinforcement learning frameworks: Adaptive heuristic critic (AHC) learning due to Sutton, Q-learning due to Watkins, and three extensions to both basic methods for speeding up learning and two extensions are experience replay, learning action models for planning, and teaching."
            },
            "venue": {
                "fragments": [],
                "text": "Machine Learning"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "12816769"
                        ],
                        "name": "Mitsuo Sato",
                        "slug": "Mitsuo-Sato",
                        "structuredName": {
                            "firstName": "Mitsuo",
                            "lastName": "Sato",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Mitsuo Sato"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144495773"
                        ],
                        "name": "K. Abe",
                        "slug": "K.-Abe",
                        "structuredName": {
                            "firstName": "Kenichi",
                            "lastName": "Abe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Abe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145609179"
                        ],
                        "name": "H. Takeda",
                        "slug": "H.-Takeda",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Takeda",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Takeda"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 206400834,
            "fieldsOfStudy": [
                "Mathematics"
            ],
            "id": "30f4838b8bcea650a29235c40b4e6fa4160fedb1",
            "isKey": false,
            "numCitedBy": 39,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "An efficient scheme is presented for a learning control problem of finite Markov chains with unknown dynamics, i.e. with unknown transition probabilities. The scheme is designed to optimize the asymptotic system performance and for easy application to models with relatively many states and decisions. In this scheme a control policy is determined each time through maximization of a simple performance criterion that explicitly incorporates a tradeoff between estimation of the unknown probabilities and control of the system. The policy determination can be easily performed even in the case of large-size models, since the maximizing operation can be greatly simplified by use of the policy-iteration method. It is proven that this scheme becomes epsilon -optimal as well as optimal by suitable choice of control parameter values in the sense that a relative frequency coefficient of making optimal decisions tends to the maximum. >"
            },
            "slug": "Learning-control-of-finite-Markov-chains-with-an-Sato-Abe",
            "title": {
                "fragments": [],
                "text": "Learning control of finite Markov chains with an explicit trade-off between estimation and control"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is proven that this scheme becomes epsilon -optimal as well as optimal by suitable choice of control parameter values in the sense that a relative frequency coefficient of making optimal decisions tends to the maximum."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Syst. Man Cybern."
            },
            "year": 1988
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 2,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/Q-learning-Watkins-Dayan/03b7e51c52084ac1db5118342a00b5fbcfc587aa?sort=total-citations"
}