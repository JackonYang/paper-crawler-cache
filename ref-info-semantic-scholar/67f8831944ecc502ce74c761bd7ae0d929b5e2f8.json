{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2784519"
                        ],
                        "name": "Peter Norvig",
                        "slug": "Peter-Norvig",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Norvig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Peter Norvig"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 16210282,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "71f6f2fb5fa836e99da7998bd9ea78fa2201a945",
            "isKey": false,
            "numCitedBy": 139,
            "numCiting": 32,
            "paperAbstract": {
                "fragments": [],
                "text": "The problem of deciding what is implied by a written text, of \u201creading between the lines\u201d is the problem of text inference. To extract proper inferences from a text requires a great deal of general knowledge on the part of the reader. Past approaches have often used a \u201cstrong method\u201d tuned to process a particular kind of knowledge structure (such as a script, or a plan). The alternative is a \u201cweak method\u201d which is applicable to a variety of knowledge structures. Just such a method is proposed here, one which recognizes six very general classes of inference. These classes are not dependent on individual knowledge structures, but instead rely on patterns of connectivity between concepts. Patterns are discovered, and inferences are suggested, by a process of marker passing between concepts."
            },
            "slug": "Marker-Passing-as-a-Weak-Method-for-Text-Norvig",
            "title": {
                "fragments": [],
                "text": "Marker Passing as a Weak Method for Text Inferencing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A method is proposed here, one which recognizes six very general classes of inference, which are not dependent on individual knowledge structures, but instead rely on patterns of connectivity between concepts."
            },
            "venue": {
                "fragments": [],
                "text": "Cogn. Sci."
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1749837"
                        ],
                        "name": "Eugene Charniak",
                        "slug": "Eugene-Charniak",
                        "structuredName": {
                            "firstName": "Eugene",
                            "lastName": "Charniak",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eugene Charniak"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 124142,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f7b27f149f4a946a7c04fb208c7d8dbf7fa8f34b",
            "isKey": false,
            "numCitedBy": 161,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe here the theory behind the language comprehension program Wimp. Wimp understands by first finding paths between the open-class words in a sentence using a marker passing, or spreading-activation, technique. This paper is primarily concerned with the \"meaning\" (or interpretation) of such paths. We argue that they are best thought of as backbones of proofs that the terms (words) at either end of the paths exist in the story and show how viewing paths in this way naturally leads to the kinds of inferences which are normally thought to characterize \"understanding.\" In a companion paper we show how this interpretation also accomplishes much of the work normally expected in the parsing of language (noun-phrase reference, word-sense disambiguation, etc) so we only briefly touch on this topic here. Wimp has been implemented and works on all of the examples herein."
            },
            "slug": "A-Neat-Theory-of-Marker-Passing-Charniak",
            "title": {
                "fragments": [],
                "text": "A Neat Theory of Marker Passing"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "It is argued that paths found between open-class words in a sentence are best thought of as backbones of proofs that the terms at either end of the paths exist in the story and shown how viewing paths in this way naturally leads to the kinds of inferences which are normally thought to characterize \"understanding\"."
            },
            "venue": {
                "fragments": [],
                "text": "AAAI"
            },
            "year": 1986
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1746656"
                        ],
                        "name": "E. Voorhees",
                        "slug": "E.-Voorhees",
                        "structuredName": {
                            "firstName": "Ellen",
                            "lastName": "Voorhees",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Voorhees"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 350665,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "59407446503d49a8cf5f5643b17502835b62f139",
            "isKey": false,
            "numCitedBy": 583,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes an automatic indexing procedure that uses the \u201cIS-A\u201d relations contained within WordNet and the set of nouns contained in a text to select a sense for each plysemous noun in the text. The result of the indexing procedure is a vector in which some of the terms represent word senses instead of word stems. Retrieval experiments comparing the effectivenss of these sense-based vectors vs. stem-based vectors show the stem-based vectors to be superior overall, although the sense-based vectors do improve the performance of some queries. The overall degradation is due in large part to the difficulty of disambiguating senses in short query statements. An analysis of these results suggests two conclusions: the IS-A links define a generalization/specialization hierarchy that is not sufficient to reliably select the correct sense of a noun from the set of fine sense distinctions in WordNet; and missing correct matches because of incorrect sense resolution has a much more deleterious effect on retrieval performance than does making spurious matches."
            },
            "slug": "Using-WordNet-to-disambiguate-word-senses-for-text-Voorhees",
            "title": {
                "fragments": [],
                "text": "Using WordNet to disambiguate word senses for text retrieval"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "The IS-A links define a generalization/specialization hierarchy that is not sufficient to reliably select the correct sense of a noun from the set of fine sense distinctions in WordNet; and missing correct matches because of incorrect sense resolution has a much more deleterious effect on retrieval performance than does making spurious matches."
            },
            "venue": {
                "fragments": [],
                "text": "SIGIR"
            },
            "year": 1993
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118077301"
                        ],
                        "name": "Jane Morris",
                        "slug": "Jane-Morris",
                        "structuredName": {
                            "firstName": "Jane",
                            "lastName": "Morris",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jane Morris"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145036961"
                        ],
                        "name": "Graeme Hirst",
                        "slug": "Graeme-Hirst",
                        "structuredName": {
                            "firstName": "Graeme",
                            "lastName": "Hirst",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Graeme Hirst"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10970495,
            "fieldsOfStudy": [
                "Linguistics",
                "Computer Science"
            ],
            "id": "ca40dc1300ab085406455894dd42fd02f9cc36f8",
            "isKey": false,
            "numCitedBy": 1092,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "In text, lexical cohesion is the result of chains of related words that contribute to the continuity of lexical meaning. These lexical chains are a direct result of units of text being \"about the same thing,\" and finding text structure involves finding units of text that are about the same thing. Hence, computing the chains is useful, since they will have a correspondence to the structure of the text. Determining the structure of text is an essential step in determining the deep meaning of the text. In this paper, a thesaurus is used as the major knowledge base for computing lexical chains. Correspondences between lexical chains and structural elements are shown to exist. Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure. The lexical chains also provide a semantic context for interpreting words, concepts, and sentences."
            },
            "slug": "Lexical-Cohesion-Computed-by-Thesaural-Relations-as-Morris-Hirst",
            "title": {
                "fragments": [],
                "text": "Lexical Cohesion Computed by Thesaural Relations as an Indicator of the Structure of Text"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "Since the lexical chains are computable, and exist in non-domain-specific text, they provide a valuable indicator of text structure, and provide a semantic context for interpreting words, concepts, and sentences."
            },
            "venue": {
                "fragments": [],
                "text": "Comput. Linguistics"
            },
            "year": 1991
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {},
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 4,
        "totalPages": 1
    },
    "page_url": "https://www.semanticscholar.org/paper/An-Electronic-Lexical-Database-Fellbaum/67f8831944ecc502ce74c761bd7ae0d929b5e2f8?sort=total-citations"
}