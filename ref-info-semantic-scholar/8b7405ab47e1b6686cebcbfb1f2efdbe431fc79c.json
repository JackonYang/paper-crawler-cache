{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10192330,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f828b401c86e0f8fddd8e77774e332dfd226cb05",
            "isKey": false,
            "numCitedBy": 587,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Previous work on learning regular languages from exemplary training sequences showed that long short-term memory (LSTM) outperforms traditional recurrent neural networks (RNNs). We demonstrate LSTMs superior performance on context-free language benchmarks for RNNs, and show that it works even better than previous hardwired or highly specialized architectures. To the best of our knowledge, LSTM variants are also the first RNNs to learn a simple context-sensitive language, namely a(n)b(n)c(n)."
            },
            "slug": "LSTM-recurrent-networks-learn-simple-context-free-Gers-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "LSTM recurrent networks learn simple context-free and context-sensitive languages"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "Long short-term memory (LSTM) variants are also the first RNNs to learn a simple context-sensitive language, namely a(n)b( n)c(n)."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Neural Networks"
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1780235"
                        ],
                        "name": "Fred Cummins",
                        "slug": "Fred-Cummins",
                        "structuredName": {
                            "firstName": "Fred",
                            "lastName": "Cummins",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Fred Cummins"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 11598600,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "11540131eae85b2e11d53df7f1360eeb6476e7f4",
            "isKey": false,
            "numCitedBy": 3338,
            "numCiting": 62,
            "paperAbstract": {
                "fragments": [],
                "text": "Long short-term memory (LSTM; Hochreiter & Schmidhuber, 1997) can solve numerous tasks not solvable by previous learning algorithms for recurrent neural networks (RNNs). We identify a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset. Without resets, the state may grow indefinitely and eventually cause the network to break down. Our remedy is a novel, adaptive forget gate that enables an LSTM cell to learn to reset itself at appropriate times, thus releasing internal resources. We review illustrative benchmark problems on which standard LSTM outperforms other RNN algorithms. All algorithms (including LSTM) fail to solve continual versions of these problems. LSTM with forget gates, however, easily solves them, and in an elegant way."
            },
            "slug": "Learning-to-Forget:-Continual-Prediction-with-LSTM-Gers-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Learning to Forget: Continual Prediction with LSTM"
            },
            "tldr": {
                "abstractSimilarityScore": 56,
                "text": "This work identifies a weakness of LSTM networks processing continual input streams that are not a priori segmented into subsequences with explicitly marked ends at which the network's internal state could be reset, and proposes a novel, adaptive forget gate that enables an LSTm cell to learn to reset itself at appropriate times, thus releasing internal resources."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3318461"
                        ],
                        "name": "M. Gagliolo",
                        "slug": "M.-Gagliolo",
                        "structuredName": {
                            "firstName": "Matteo",
                            "lastName": "Gagliolo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Gagliolo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "Cells developed using this method could also be incorporated into hybrid algorithms such as Evolino [14]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 11745761,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "75479012461814fd176556a56b32c2392462aef5",
            "isKey": false,
            "numCitedBy": 244,
            "numCiting": 95,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, gradient-based LSTM recurrent neural networks (RNNs) solved many previously RNN-unlearnable tasks. Sometimes, however, gradient information is of little use for training RNNs, due to numerous local minima. For such cases, we present a novel method: EVOlution of systems with LINear Outputs (Evolino). Evolino evolves weights to the nonlinear, hidden nodes of RNNs while computing optimal linear mappings from hidden state to output, using methods such as pseudo-inverse-based linear regression. If we instead use quadratic programming to maximize the margin, we obtain the first evolutionary recurrent support vector machines. We show that Evolino-based LSTM can solve tasks that Echo State nets (Jaeger, 2004a) cannot and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-based LSTM."
            },
            "slug": "Training-Recurrent-Networks-by-Evolino-Schmidhuber-Wierstra",
            "title": {
                "fragments": [],
                "text": "Training Recurrent Networks by Evolino"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "It is shown that Evolino-based LSTM can solve tasks that Echo State nets cannot and achieves higher accuracy in certain continuous function generation tasks than conventional gradient descent RNNs, including gradient-basedLSTM."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 1915014,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "44d2abe2175df8153f465f6c39b68b76a0d40ab9",
            "isKey": false,
            "numCitedBy": 52390,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "Learning to store information over extended time intervals by recurrent backpropagation takes a very long time, mostly because of insufficient, decaying error backflow. We briefly review Hochreiter's (1991) analysis of this problem, then address it by introducing a novel, efficient, gradient based method called long short-term memory (LSTM). Truncating the gradient where this does not do harm, LSTM can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units. Multiplicative gate units learn to open and close access to the constant error flow. LSTM is local in space and time; its computational complexity per time step and weight is O. 1. Our experiments with artificial data involve local, distributed, real-valued, and noisy pattern representations. In comparisons with real-time recurrent learning, back propagation through time, recurrent cascade correlation, Elman nets, and neural sequence chunking, LSTM leads to many more successful runs, and learns much faster. LSTM also solves complex, artificial long-time-lag tasks that have never been solved by previous recurrent network algorithms."
            },
            "slug": "Long-Short-Term-Memory-Hochreiter-Schmidhuber",
            "title": {
                "fragments": [],
                "text": "Long Short-Term Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A novel, efficient, gradient based method called long short-term memory (LSTM) is introduced, which can learn to bridge minimal time lags in excess of 1000 discrete-time steps by enforcing constant error flow through constant error carousels within special units."
            },
            "venue": {
                "fragments": [],
                "text": "Neural Computation"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2088368"
                        ],
                        "name": "F. Gers",
                        "slug": "F.-Gers",
                        "structuredName": {
                            "firstName": "Felix",
                            "lastName": "Gers",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gers"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1739396"
                        ],
                        "name": "N. Schraudolph",
                        "slug": "N.-Schraudolph",
                        "structuredName": {
                            "firstName": "Nicol",
                            "lastName": "Schraudolph",
                            "middleNames": [
                                "N."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Schraudolph"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 474078,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "047655e733a9eed9a500afd916efa566915b9110",
            "isKey": false,
            "numCitedBy": 1280,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "The temporal distance between events conveys information essential for numerous sequential tasks such as motor control and rhythm detection. While Hidden Markov Models tend to ignore this information, recurrent neural networks (RNNs) can in principle learn to make use of it. We focus on Long Short-Term Memory (LSTM) because it has been shown to outperform other RNNs on tasks involving long time lags. We find that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars. Without external resets or teacher forcing, our LSTM variant also learns to generate stable streams of precisely timed spikes and other highly nonlinear periodic patterns. This makes LSTM a promising approach for tasks that require the accurate measurement or generation of time intervals."
            },
            "slug": "Learning-Precise-Timing-with-LSTM-Recurrent-Gers-Schraudolph",
            "title": {
                "fragments": [],
                "text": "Learning Precise Timing with LSTM Recurrent Networks"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work finds that LSTM augmented by \"peephole connections\" from its internal cells to its multiplicative gates can learn the fine distinction between sequences of spikes spaced either 50 or 49 time steps apart without the help of any short training exemplars."
            },
            "venue": {
                "fragments": [],
                "text": "J. Mach. Learn. Res."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145155628"
                        ],
                        "name": "P. Rodr\u00edguez",
                        "slug": "P.-Rodr\u00edguez",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Rodr\u00edguez",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Rodr\u00edguez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716264"
                        ],
                        "name": "Janet Wiles",
                        "slug": "Janet-Wiles",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Wiles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Janet Wiles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9031409,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a34d3ccf53d20851f8154376e8b2faebcf99832f",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 7,
            "paperAbstract": {
                "fragments": [],
                "text": "Recently researchers have derived formal complexity analysis of analog computation in the setting of discrete-time dynamical systems. As an empirical constrast, training recurrent neural networks (RNNs) produces self-organized systems that are realizations of analog mechanisms. Previous work showed that a RNN can learn to process a simple context-free language (CFL) by counting. Herein, we extend that work to show that a RNN can learn a harder CFL, a simple palindrome, by organizing its resources into a symbol-sensitive counting solution, and we provide a dynamical systems analysis which demonstrates how the network: can not only count, but also copy and store counting information."
            },
            "slug": "Recurrent-Neural-Networks-Can-Learn-to-Implement-Rodr\u00edguez-Wiles",
            "title": {
                "fragments": [],
                "text": "Recurrent Neural Networks Can Learn to Implement Symbol-Sensitive Counting"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This work shows that a RNN can learn a harder CFL, a simple palindrome, by organizing its resources into a symbol-sensitive counting solution, and provides a dynamical systems analysis which demonstrates how the network can not only count, but also copy and store counting information."
            },
            "venue": {
                "fragments": [],
                "text": "NIPS"
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688276"
                        ],
                        "name": "Daan Wierstra",
                        "slug": "Daan-Wierstra",
                        "structuredName": {
                            "firstName": "Daan",
                            "lastName": "Wierstra",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Daan Wierstra"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30478864"
                        ],
                        "name": "A. F\u00f6rster",
                        "slug": "A.-F\u00f6rster",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "F\u00f6rster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. F\u00f6rster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145197867"
                        ],
                        "name": "Jan Peters",
                        "slug": "Jan-Peters",
                        "structuredName": {
                            "firstName": "Jan",
                            "lastName": "Peters",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jan Peters"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "recurrent policy gradient algorithm [18], a learning rate of 0."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14039355,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "92d009217b100882376ae5c90217da2e92471ad7",
            "isKey": false,
            "numCitedBy": 144,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents Recurrent Policy Gradients, a modelfree reinforcement learning (RL) method creating limited-memory stochastic policies for partially observable Markov decision problems (POMDPs) that require long-term memories of past observations. The approach involves approximating a policy gradient for a Recurrent Neural Network (RNN) by backpropagating return-weighted characteristic eligibilities through time. Using a \"Long Short-Term Memory\" architecture, we are able to outperform other RL methods on two important benchmark tasks. Furthermore, we show promising results on a complex car driving simulation task."
            },
            "slug": "Solving-Deep-Memory-POMDPs-with-Recurrent-Policy-Wierstra-F\u00f6rster",
            "title": {
                "fragments": [],
                "text": "Solving Deep Memory POMDPs with Recurrent Policy Gradients"
            },
            "tldr": {
                "abstractSimilarityScore": 92,
                "text": "Recurrent Policy Gradients, a modelfree reinforcement learning (RL) method creating limited-memory stochastic policies for partially observable Markov decision problems (POMDPs) that require long-term memories of past observations is presented."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2884373"
                        ],
                        "name": "J. Elman",
                        "slug": "J.-Elman",
                        "structuredName": {
                            "firstName": "Jeffrey",
                            "lastName": "Elman",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Elman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1716264"
                        ],
                        "name": "Janet Wiles",
                        "slug": "Janet-Wiles",
                        "structuredName": {
                            "firstName": "Janet",
                            "lastName": "Wiles",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Janet Wiles"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12382072,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0b7005984749cf5f2caa1072866b36e17713ab84",
            "isKey": false,
            "numCitedBy": 74,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "The broad context of this study is the investigation of the nature of computation in recurrent networks (RNs). The current study has two parts. The first is to show that a RN can solve a problem that we take to be of interest (a counting task), and the second is to use the solution as a platform for developing a more general understanding of RNs as computational mechanisms. We begin by presenting the empirical results of training RNs on the counting task. The task ( ) is the simplest possible grammar that requires a PDA or counter . A RN was trained to predict the deterministic elements in sequences of the form * where n =1 to 12. After training, it generalized to n =18. Contrary to our expectations, on analyzing the hidden unit dynamics, we fi nd no evidence of units acting like counters. Instead, we fi nd an oscillator . We then explore the possible range of behaviors of oscillators using iterated maps and in the second part of the paper we describe the use of iterated maps for understanding RN mechanisms in terms of \u201cactivation landscapes\u201d. Thi analysis leads to used an understanding of the behavior of network generated in the simulation study ."
            },
            "slug": "Learning-to-count-without-a-counter:-A-case-study-Elman-Wiles",
            "title": {
                "fragments": [],
                "text": "Learning to count without a counter: A case study of dynamics and activation landscapes in recurrent networks"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "The current study shows that a RN can solve a problem that the authors take to be of interest (a counting task), and uses the solution as a platform for developing a more general understanding of RNs as computational mechanisms."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1995
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1742807"
                        ],
                        "name": "H. Kitano",
                        "slug": "H.-Kitano",
                        "structuredName": {
                            "firstName": "Hiroaki",
                            "lastName": "Kitano",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Kitano"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 3109597,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "211855f1de279c452858177331860cbc326351ab",
            "isKey": false,
            "numCitedBy": 812,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "We present a new method of designing neural networks using the genetic algorithm. Recently there have been several reports claiming attempts to design neural networks using genetic algorithms were successful. However, these methods have a problem in scalability, i.e., the convergence characteristic degrades significantly as the size of the network increases. This is because these methods employ direct mapp ing of chromosomes into network connectivities. As an alternative approach, we propose a graph grammatical encoding that will encode graph generation grammar to the chromosome so that it generates more regular connectivity patterns with shorter chromosome length. Experimental results support that our new scheme provides magnitude of speedup in convergence of neural network design and exhibits desirable scaling property."
            },
            "slug": "Designing-Neural-Networks-Using-Genetic-Algorithms-Kitano",
            "title": {
                "fragments": [],
                "text": "Designing Neural Networks Using Genetic Algorithms with Graph Generation System"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "A graph grammatical encoding is proposed that will encode graph generation grammar to the chromosome so that it generates more regular connectivity patterns with shorter chromosome length."
            },
            "venue": {
                "fragments": [],
                "text": "Complex Syst."
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1846883"
                        ],
                        "name": "Kenneth O. Stanley",
                        "slug": "Kenneth-O.-Stanley",
                        "structuredName": {
                            "firstName": "Kenneth",
                            "lastName": "Stanley",
                            "middleNames": [
                                "O."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Kenneth O. Stanley"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 498161,
            "fieldsOfStudy": [
                "Biology"
            ],
            "id": "d03c916d49268d48fde3b76a68e64af7761835e7",
            "isKey": false,
            "numCitedBy": 2994,
            "numCiting": 90,
            "paperAbstract": {
                "fragments": [],
                "text": "An important question in neuroevolution is how to gain an advantage from evolving neural network topologies along with weights. We present a method, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task. We claim that the increased efficiency is due to (1) employing a principled method of crossover of different topologies, (2) protecting structural innovation using speciation, and (3) incrementally growing from minimal structure. We test this claim through a series of ablation studies that demonstrate that each component is necessary to the system as a whole and to each other. What results is signicantly faster learning. NEAT is also an important contribution to GAs because it shows how it is possible for evolution to both optimize and complexify solutions simultaneously, offering the possibility of evolving increasingly complex solutions over generations, and strengthening the analogy with biological evolution."
            },
            "slug": "Evolving-Neural-Networks-through-Augmenting-Stanley-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Evolving Neural Networks through Augmenting Topologies"
            },
            "tldr": {
                "abstractSimilarityScore": 55,
                "text": "A method is presented, NeuroEvolution of Augmenting Topologies (NEAT), which outperforms the best fixed-topology method on a challenging benchmark reinforcement learning task and shows how it is possible for evolution to both optimize and complexify solutions simultaneously."
            },
            "venue": {
                "fragments": [],
                "text": "Evolutionary Computation"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743758"
                        ],
                        "name": "M. Liwicki",
                        "slug": "M.-Liwicki",
                        "structuredName": {
                            "firstName": "Marcus",
                            "lastName": "Liwicki",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Liwicki"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1753223"
                        ],
                        "name": "A. Graves",
                        "slug": "A.-Graves",
                        "structuredName": {
                            "firstName": "Alex",
                            "lastName": "Graves",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Graves"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720945"
                        ],
                        "name": "H. Bunke",
                        "slug": "H.-Bunke",
                        "structuredName": {
                            "firstName": "Horst",
                            "lastName": "Bunke",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Bunke"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 5668166,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43fa5235e49fa6f16d047c999234d1b93df360b0",
            "isKey": false,
            "numCitedBy": 196,
            "numCiting": 21,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper we introduce a new connectionist approach to on-line handwriting recognition and address in particular the problem of recognizing handwritten whiteboard notes. The approach uses a bidirectional recurrent neural network with the long short-term memory architecture. We use a recently introduced objective function, known as Connectionist Temporal Classification (CTC), that directly trains the network to label unsegmented sequence data. Our new system achieves a word recognition rate of 74.0%, compared with 65.4% using a previously developed HMMbased recognition system."
            },
            "slug": "A-novel-approach-to-on-line-handwriting-recognition-Liwicki-Graves",
            "title": {
                "fragments": [],
                "text": "A novel approach to on-line handwriting recognition based on bidirectional long short-term memory networks"
            },
            "tldr": {
                "abstractSimilarityScore": 67,
                "text": "A new connectionist approach to on-line handwriting recognition and address in particular the problem of recognizing handwritten whiteboard notes using a recently introduced objective function, known as Connectionist Temporal Classification (CTC), that directly trains the network to label unsegmented sequence data."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145842938"
                        ],
                        "name": "Faustino J. Gomez",
                        "slug": "Faustino-J.-Gomez",
                        "structuredName": {
                            "firstName": "Faustino",
                            "lastName": "Gomez",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Faustino J. Gomez"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1686788"
                        ],
                        "name": "R. Miikkulainen",
                        "slug": "R.-Miikkulainen",
                        "structuredName": {
                            "firstName": "Risto",
                            "lastName": "Miikkulainen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Miikkulainen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 2209222,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "8939e9298eedb90aa4f040ab8e9f16872089a495",
            "isKey": false,
            "numCitedBy": 505,
            "numCiting": 42,
            "paperAbstract": {
                "fragments": [],
                "text": "Several researchers have demonstrated how complex action sequences can be learned through neuroevolution (i.e., evolving neural networks with genetic algorithms). However, complex general behavior such as evading predators or avoiding obstacles, which is not tied to specific environments, turns out to be very difficult to evolve. Often the system discovers mechanical strategies, such as moving back and forth, that help the agent cope but are not very effective, do not appear believable, and do not generalize to new environments. The problem is that a general strategy is too difficult for the evolution system to discover directly. This article proposes an approach wherein such complex general behavior is learned incrementally, by starting with simpler behavior and gradually making the task more challenging and general. The task transitions are implemented through successive stages of Delta coding (i.e., evolving modifications), which allows even converged populations to adapt to the new task. The method is tested in the stochastic, dynamic task of prey capture and is compared with direct evolution. The incremental approach evolves more effective and more general behavior and should also scale up to harder tasks."
            },
            "slug": "Incremental-Evolution-of-Complex-General-Behavior-Gomez-Miikkulainen",
            "title": {
                "fragments": [],
                "text": "Incremental Evolution of Complex General Behavior"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This article proposes an approach wherein complex general behavior is learned incrementally, by starting with simpler behavior and gradually making the task more challenging and general, which evolves more effective and more general behavior."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 1997
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1390067049"
                        ],
                        "name": "R. O\u2019Reilly",
                        "slug": "R.-O\u2019Reilly",
                        "structuredName": {
                            "firstName": "Randall",
                            "lastName": "O\u2019Reilly",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. O\u2019Reilly"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2723253"
                        ],
                        "name": "T. Braver",
                        "slug": "T.-Braver",
                        "structuredName": {
                            "firstName": "Todd",
                            "lastName": "Braver",
                            "middleNames": [
                                "Samuel"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Braver"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "153564781"
                        ],
                        "name": "Jonathan D. Cohen",
                        "slug": "Jonathan-D.-Cohen",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Cohen",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan D. Cohen"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 10523721,
            "fieldsOfStudy": [
                "Biology",
                "Psychology"
            ],
            "id": "357768b7206e67a7bb3549748668bf6187708cff",
            "isKey": false,
            "numCitedBy": 349,
            "numCiting": 107,
            "paperAbstract": {
                "fragments": [],
                "text": "FIVE CENTRAL FEATURES OF THE MODEL We define working memory as controlled processing involving active maintenance and/or rapid learning, where controlled processing is an emergent property of the dynamic interactions of multiple brain systems, but the prefrontal cortex (PFC) and hippocampus (HCMP) are especially influential owing to their specialized processing abilities and their privileged locations within the processing hierarchy (both the PFC and HCMP are well connected with a wide range of brain areas, allowing them to influence behavior at a global level). The specific features of our model include: (1) A PFC specialized for active maintenance of internal contextual information that is dynamically updated and self-regulated, allowing it to bias (control) ongoing processing according to maintained information (e.g., goals, instructions, partial products). (2) An HCMP specialized for rapid learning of arbitrary information, which can be recalled in the service of controlled processing, whereas the posterior perceptual and motor cortex (PMC) exhibits slow, long-term learning that can efficiently represent accumulated knowledge and skills. (3) Control that emerges from interacting systems (PFC, HCMP, and PMC). (4) Dimensions that define continua of specialization in different brain systems: for example, robust active maintenance, fast versus slow learning. (5) Integration of biological and computational principles. Working memory is an intuitively appealing theoretical construct \u2013 perhaps deceptively so."
            },
            "slug": "A-Biologically-Based-Computational-Model-of-Working-O\u2019Reilly-Braver",
            "title": {
                "fragments": [],
                "text": "A Biologically Based Computational Model of Working Memory"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "This model defines working memory as controlled processing involving active maintenance and/or rapid learning, where controlled processing is an emergent property of the dynamic interactions of multiple brain systems, but the prefrontal cortex and hippocampus are especially influential owing to their specialized processing abilities."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3011589"
                        ],
                        "name": "B. Bakker",
                        "slug": "B.-Bakker",
                        "structuredName": {
                            "firstName": "Bram",
                            "lastName": "Bakker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Bakker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2952153"
                        ],
                        "name": "F. Lin\u00e5ker",
                        "slug": "F.-Lin\u00e5ker",
                        "structuredName": {
                            "firstName": "Fredrik",
                            "lastName": "Lin\u00e5ker",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Lin\u00e5ker"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145341374"
                        ],
                        "name": "J. Schmidhuber",
                        "slug": "J.-Schmidhuber",
                        "structuredName": {
                            "firstName": "J\u00fcrgen",
                            "lastName": "Schmidhuber",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Schmidhuber"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [],
                        "text": "In a second configuration, evolution started out with a context-free language (ab, n \u2208 [1, 5]) and moved on to a multiobjective setting with one context-free and one context-sensitive language (abc and abc, (m,n) \u2208 [1, 4]\u00d7 [1, 4], t \u2208 [1, 5])."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [],
                        "text": "In order to validate the cells found, we performed validation tests on the deep memory T-Maze task as described in Bakker\u2019s work [1]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 8128197,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "43b6584e2b4d25e500e4a4ea4018b99fe10b62fd",
            "isKey": false,
            "numCitedBy": 24,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes how learning tasks in partially observable mobile robot domains can be solved by combining reinforcement learning with an unsupervised learning \"event extraction\" mechanism, called ARAVQ. ARAVQ transforms the robot's continuous, noisy, high-dimensional sensory input stream into a compact sequence of high-level events. The resulting hierarchical control system uses an LSTM recurrent neural network as the reinforcement learning component, which learns high-level actions in response to the history of high-level events. The high-level actions select low-level behaviors which take care of the real-time motor control. Illustrative experiments based on the Khepera mobile robot simulator are presented."
            },
            "slug": "Reinforcement-learning-in-partially-observable-Bakker-Lin\u00e5ker",
            "title": {
                "fragments": [],
                "text": "Reinforcement learning in partially observable mobile robot domains using unsupervised event extraction"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper describes how learning tasks in partially observable mobile robot domains can be solved by combining reinforcement learning with an unsupervised learning \"event extraction\" mechanism, called ARAVQ, which transforms the robot's continuous, noisy, high-dimensional sensory input stream into a compact sequence of high-level events."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE/RSJ International Conference on Intelligent Robots and Systems"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3308557"
                        ],
                        "name": "S. Hochreiter",
                        "slug": "S.-Hochreiter",
                        "structuredName": {
                            "firstName": "Sepp",
                            "lastName": "Hochreiter",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Hochreiter"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751762"
                        ],
                        "name": "Yoshua Bengio",
                        "slug": "Yoshua-Bengio",
                        "structuredName": {
                            "firstName": "Yoshua",
                            "lastName": "Bengio",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yoshua Bengio"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 17278462,
            "fieldsOfStudy": [
                "Chemistry"
            ],
            "id": "aed054834e2c696807cc8b227ac7a4197196e211",
            "isKey": false,
            "numCitedBy": 1588,
            "numCiting": 35,
            "paperAbstract": {
                "fragments": [],
                "text": "D3EGF(FIH)J KMLONPEGQSRPETN UCV.WYX(Z R.[ V R6\\M[ X N@]_^O\\`JaNcb V RcQ W d EGKeL(^(QgfhKeLOE?i)^(QSj ETNPfPQkRl[ V R)m\"[ X ^(KeLOEG^ npo qarpo m\"[ X ^(KeLOEG^tsAu EGNPb V ^ v wyx zlwO{(|(}<~O\u007fC}\u0081\u0080(\u0082(xp{a\u0083y\u0084.~A}\u0086\u0085\u0088\u0087_~ \u0089C\u008al\u00833\u0089#|<\u0080Az\u0086w#|l\u00806\u0087 \u008b(| \u008c JpfhL X\u008dV\u008f\u008e EG^O\u0090 QgJ \u0091 ETFOR\u0086\u0092\u0093] ^O\\\u0094J\u0095NPb V RcQ\u0097\u0096 X E)ETR \u00986EGKeLOETNcKMLOE\u009a\u0099 F\u0088\u009b ETN V RcQgJp^(^OE ZgZ E i ^(Qkj EGNPfhQSRO\u009b E \u009cOE2m1Jp^ RcNY\u009b E V\u0095Z sO\u009d\u009f\u009e! \u008d\u00a1 q.n sCD X KGKa\u00928\u009d\u00a2EG^ RPNhE\u00a4\u00a3 \u00a5\u00a6Q ZgZ E\u0095s m\u00a7J\u0095^ RPNO\u009b E V\u0095Z s( \u0308 X \u009b EG\u00a9#E\u0081Kas#\u009d V ^ V \u009c V s(H a \u009d\u00aba\u0095\u00ac3\u00ad \u00ae#|.\u0080Y \u0304y} xa\u00b0O\u007fC}l{\u008dx\u0093\u0087 \u0089 \u0083yxl\u0080Y~3{\u008d| \u0084 \u00b12\u0087Pz \u0084 \u009e V J Z J U N V fhKTJp^(Q \u0091 ETFOR\u0086\u0092 J\u0095\\ D vYf3RPEGb \u0301f V ^(\u009c\u00a7\u009d\u0088Jpb\u008fF X RPETN@D KTQ\u0097EG^(KTE i ^(QSjpEGNPfhQSR4v\u03bcJ\u0095\\ U\u00b6Z JaNPEG^(K\u00b7E jYQ V \u009c(Q \u0327D V ^ R V m V N3R V aOs#1 o \u00a1Ga r U Q\u0097NhE\u0081^OoTE1\u20444\u00bb,] R V\u0095Z vC1\u20442 3\u20444 \u0084 x \u00b1 x \u007f \u008b#\u00bf }\u00c0\u0087 \u00893\u0080t}l\u0082C}2\u0087P}<~ \u00act[ X NP\u0090\u0095E\u0081^\u00a7D KeL(b \u0301Qg\u009c(L X \u00a9yETN ] \u0091 DY]_\u00c1 \u009d\u0088J\u0095NPfhJ\u00c3\u00c2 Z j ETo\u0081Q V a\u0095 rpopo2\u00c4 X \u0090 V ^(J(sCD \u00c5)QSRPoTEGN ZgV ^(\u009c \u00c6 \u0089#|\u0095{3 \u0304\u008d|.\u0080(\u007fC}.\u008bC\u00bfY}p\u0084 \u0087Pz\u0086w"
            },
            "slug": "Gradient-Flow-in-Recurrent-Nets:-the-Difficulty-of-Hochreiter-Bengio",
            "title": {
                "fragments": [],
                "text": "Gradient Flow in Recurrent Nets: the Difficulty of Learning Long-Term Dependencies"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1766767"
                        ],
                        "name": "Shimon Whiteson",
                        "slug": "Shimon-Whiteson",
                        "structuredName": {
                            "firstName": "Shimon",
                            "lastName": "Whiteson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shimon Whiteson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39286677"
                        ],
                        "name": "Matthew E. Taylor",
                        "slug": "Matthew-E.-Taylor",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Taylor",
                            "middleNames": [
                                "E."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew E. Taylor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144848112"
                        ],
                        "name": "P. Stone",
                        "slug": "P.-Stone",
                        "structuredName": {
                            "firstName": "Peter",
                            "lastName": "Stone",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Stone"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 12808954,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "23cf29383d2c36a7b585e8f308d30909376a4268",
            "isKey": false,
            "numCitedBy": 59,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "To excel in challenging tasks, intelligent agents need sophisticated mechanisms for action selection: they need policies that dictate what action to take in each situation. Reinforcement learning (RL) algorithms are designed to learn such policies given only positive and negative rewards. Two contrasting approaches to RL that are currently in popular use are temporal difference (TD) methods, which learn value functions, and evolutionary methods, which optimize populations of candidate policies. Both approaches have had practical successes but few studies have directly compared them. Hence, there are no general guidelines describing their relative strengths and weaknesses. In addition, there has been little cross-collaboration, with few attempts to make them work together or to apply ideas from one to the other. In this article we aim to address these shortcomings via three empirical studies that compare these methods and investigate new ways of making them work together. First, we compare the two approaches in a benchmark task and identify variations of the task that isolate factors critical to the performance of each method. Second, we investigate ways to make evolutionary algorithms excel at on-line tasks by borrowing exploratory mechanisms traditionally used by TD methods. We present empirical results demonstrating a dramatic performance improvement. Third, we explore a novel way of making evolutionary and TD methods work together by using evolution to automatically discover good representations for TD function approximators. We present results demonstrating that this novel approach can outperform both TD and evolutionary methods alone."
            },
            "slug": "Empirical-Studies-in-Action-Selection-with-Learning-Whiteson-Taylor",
            "title": {
                "fragments": [],
                "text": "Empirical Studies in Action Selection with Reinforcement Learning"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "A novel way of making evolutionary and TD methods work together is explored by using evolution to automatically discover good representations for TD function approximators, and results are presented demonstrating that this novel approach can outperform both TD and evolutionary methods alone."
            },
            "venue": {
                "fragments": [],
                "text": "Adapt. Behav."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47055692"
                        ],
                        "name": "P. Werbos",
                        "slug": "P.-Werbos",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Werbos",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "P. Werbos"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 18470994,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "1a3d22599028a05669e884f3eaf19a342e190a87",
            "isKey": false,
            "numCitedBy": 4051,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "Backpropagation is now the most widely used tool in the field of artificial neural networks. At the core of backpropagation is a method for calculating derivatives exactly and efficiently in any large system made up of elementary subsystems or calculations which are represented by known, differentiable functions; thus, backpropagation has many applications which do not involve neural networks as such. This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis. Next, it presents the basic equations for backpropagation through time, and discusses applications to areas like pattern recognition involving dynamic systems, systems identification, and control. Finally, i t describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method. Pseudocode is provided to clarify the algorithms. The chain rule for ordered derivatives-the theorem which underlies backpropagation-is briefly discussed."
            },
            "slug": "Backpropagation-Through-Time:-What-It-Does-and-How-Werbos",
            "title": {
                "fragments": [],
                "text": "Backpropagation Through Time: What It Does and How to Do It"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "This paper first reviews basic backpropagation, a simple method which is now being widely used in areas like pattern recognition and fault diagnosis, and describes further extensions of this method, to deal with systems other than neural networks, systems involving simultaneous equations or true recurrent networks, and other practical issues which arise with this method."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1990
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2813459"
                        ],
                        "name": "Rowland R. Sillito",
                        "slug": "Rowland-R.-Sillito",
                        "structuredName": {
                            "firstName": "Rowland",
                            "lastName": "Sillito",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rowland R. Sillito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1843592"
                        ],
                        "name": "Robert B. Fisher",
                        "slug": "Robert-B.-Fisher",
                        "structuredName": {
                            "firstName": "Robert",
                            "lastName": "Fisher",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Robert B. Fisher"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 42134799,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1ee9478568f521f02bf9b4f40cddf48ff8a6f803",
            "isKey": false,
            "numCitedBy": 97,
            "numCiting": 8,
            "paperAbstract": {
                "fragments": [],
                "text": "An incremental one-class learning algorithm is proposed for the purpose of outlier detection. Outliers are identified by estimating - and thresholding - the probability distribution of the training data. In the early stages of training a non-parametric estimate of the training data distribution is obtained using kernel density estimation. Once the number of training examples reaches the maximum computationally feasible limit for kernel density estimation, we treat the kernel density estimate as a maximally-complex Gaussian mixture model, and keep the model complexity constant bymerging a pair of components for each newkernel added. This method is shown to outperform a current state-of-the-art incremental one-class learning algorithm (Incremental SVDD [5]) on a variety of datasets, while requiring only an upper limit on model complexity to be specified."
            },
            "slug": "Incremental-One-Class-Learning-with-Bounded-Sillito-Fisher",
            "title": {
                "fragments": [],
                "text": "Incremental One-Class Learning with Bounded Computational Complexity"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "This method is shown to outperform a current state-of-the-art incremental one-class learning algorithm (Incremental SVDD) on a variety of datasets, while requiring only an upper limit on model complexity to be specified."
            },
            "venue": {
                "fragments": [],
                "text": "ICANN"
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145080287"
                        ],
                        "name": "K. Deb",
                        "slug": "K.-Deb",
                        "structuredName": {
                            "firstName": "Kalyanmoy",
                            "lastName": "Deb",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Deb"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2112401245"
                        ],
                        "name": "S. Agrawal",
                        "slug": "S.-Agrawal",
                        "structuredName": {
                            "firstName": "Samir",
                            "lastName": "Agrawal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Agrawal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143761916"
                        ],
                        "name": "Amrit Pratap",
                        "slug": "Amrit-Pratap",
                        "structuredName": {
                            "firstName": "Amrit",
                            "lastName": "Pratap",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Amrit Pratap"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2939646"
                        ],
                        "name": "T. Meyarivan",
                        "slug": "T.-Meyarivan",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Meyarivan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Meyarivan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 9914171,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "6eddc19efa13f7e70301908d98e85a19d6f32a02",
            "isKey": false,
            "numCitedBy": 32712,
            "numCiting": 46,
            "paperAbstract": {
                "fragments": [],
                "text": "Multi-objective evolutionary algorithms (MOEAs) that use non-dominated sorting and sharing have been criticized mainly for: (1) their O(MN/sup 3/) computational complexity (where M is the number of objectives and N is the population size); (2) their non-elitism approach; and (3) the need to specify a sharing parameter. In this paper, we suggest a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties. Specifically, a fast non-dominated sorting approach with O(MN/sup 2/) computational complexity is presented. Also, a selection operator is presented that creates a mating pool by combining the parent and offspring populations and selecting the best N solutions (with respect to fitness and spread). Simulation results on difficult test problems show that NSGA-II is able, for most problems, to find a much better spread of solutions and better convergence near the true Pareto-optimal front compared to the Pareto-archived evolution strategy and the strength-Pareto evolutionary algorithm - two other elitist MOEAs that pay special attention to creating a diverse Pareto-optimal front. Moreover, we modify the definition of dominance in order to solve constrained multi-objective problems efficiently. Simulation results of the constrained NSGA-II on a number of test problems, including a five-objective, seven-constraint nonlinear problem, are compared with another constrained multi-objective optimizer, and the much better performance of NSGA-II is observed."
            },
            "slug": "A-fast-and-elitist-multiobjective-genetic-NSGA-II-Deb-Agrawal",
            "title": {
                "fragments": [],
                "text": "A fast and elitist multiobjective genetic algorithm: NSGA-II"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper suggests a non-dominated sorting-based MOEA, called NSGA-II (Non-dominated Sorting Genetic Algorithm II), which alleviates all of the above three difficulties, and modify the definition of dominance in order to solve constrained multi-objective problems efficiently."
            },
            "venue": {
                "fragments": [],
                "text": "IEEE Trans. Evol. Comput."
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47378595"
                        ],
                        "name": "G. Miller",
                        "slug": "G.-Miller",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Miller",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Miller"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 142281124,
            "fieldsOfStudy": [
                "Psychology"
            ],
            "id": "fd68c2e9e69822f2f4b12acaab6f9269a1a61d74",
            "isKey": false,
            "numCitedBy": 141,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "FORGETTING AND REMEMBERINGWhen remembering runs amok, past pain can disrupt someone's present. New drugs, psychotherapeutic approaches, and other strategies might temper traumatic memories."
            },
            "slug": "Learning-to-Forget-Miller",
            "title": {
                "fragments": [],
                "text": "Learning to Forget"
            },
            "venue": {
                "fragments": [],
                "text": "Science"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1694724"
                        ],
                        "name": "F. Gruau",
                        "slug": "F.-Gruau",
                        "structuredName": {
                            "firstName": "Fr\u00e9d\u00e9ric",
                            "lastName": "Gruau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Gruau"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 60749742,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ff33f09e9e211e1f1f4344beb9af1753c60cab4b",
            "isKey": false,
            "numCitedBy": 130,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Genetic-Synthesis-of-Modular-Neural-Networks-Gruau",
            "title": {
                "fragments": [],
                "text": "Genetic Synthesis of Modular Neural Networks"
            },
            "venue": {
                "fragments": [],
                "text": "ICGA"
            },
            "year": 1993
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "J. Schmidhuber. RNN overview"
            },
            "venue": {
                "fragments": [],
                "text": "J. Schmidhuber. RNN overview"
            },
            "year": 2004
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "LSTM networks have been shown to outperform other RNNs on numerous time series requiring the use of deep memory [13]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "http://www.idsia.ch/ \u0303juergen/rnn.html"
            },
            "venue": {
                "fragments": [],
                "text": "RNN overview,"
            },
            "year": 2004
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 3,
            "methodology": 2
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 23,
        "totalPages": 3
    },
    "page_url": "https://www.semanticscholar.org/paper/Evolving-Memory-Cell-Structures-for-Sequence-Bayer-Wierstra/8b7405ab47e1b6686cebcbfb1f2efdbe431fc79c?sort=total-citations"
}