{
    "links": [
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2109775603"
                        ],
                        "name": "Takeshi Matsumoto",
                        "slug": "Takeshi-Matsumoto",
                        "structuredName": {
                            "firstName": "Takeshi",
                            "lastName": "Matsumoto",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Takeshi Matsumoto"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145791377"
                        ],
                        "name": "H. Saito",
                        "slug": "H.-Saito",
                        "structuredName": {
                            "firstName": "Hiroshi",
                            "lastName": "Saito",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Saito"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144146557"
                        ],
                        "name": "M. Fujita",
                        "slug": "M.-Fujita",
                        "structuredName": {
                            "firstName": "Masahiro",
                            "lastName": "Fujita",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Fujita"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 20,
                                "start": 16
                            }
                        ],
                        "text": "The approach in [23] handles neither while loops nor pointers."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 121
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 10595082,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7065104eaf4978e72ad1dfa91ec6d04b6e4a94e7",
            "isKey": false,
            "numCitedBy": 30,
            "numCiting": 12,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we propose a formal equivalence checking method for source-to-source refinements in C programs for hardware behavioral descriptions. In the method, the textual differences between the two programs are identified at first to get hints where the equivalence must be checked. Then, the equivalence of differences is verified by symbolic simulation and validity checking techniques. If the equivalence is not established, our method incrementally extends statements to be verified based on dependency until the equivalence is proved. For the extensions, the method uses dependence graphs of the programs. Finally, through the experimental results, we show the method can efficiently perform equivalence checking"
            },
            "slug": "Equivalence-checking-of-C-programs-by-locally-on-Matsumoto-Saito",
            "title": {
                "fragments": [],
                "text": "Equivalence checking of C programs by locally performing symbolic simulation on dependence graphs"
            },
            "tldr": {
                "abstractSimilarityScore": 71,
                "text": "A formal equivalence checking method for source-to-source refinements in C programs for hardware behavioral descriptions and through the experimental results, it is shown the method can efficiently perform equivalence Checking."
            },
            "venue": {
                "fragments": [],
                "text": "7th International Symposium on Quality Electronic Design (ISQED'06)"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145781093"
                        ],
                        "name": "David A. Ramos",
                        "slug": "David-A.-Ramos",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ramos",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Ramos"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2373056"
                        ],
                        "name": "D. Engler",
                        "slug": "D.-Engler",
                        "structuredName": {
                            "firstName": "Dawson",
                            "lastName": "Engler",
                            "middleNames": [
                                "R."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Engler"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 219
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 12,
                                "start": 8
                            }
                        ],
                        "text": "UC-KLEE [30] performs a bounded model checking that checks equivalence for all inputs up to a certain size."
                    },
                    "intents": []
                }
            ],
            "corpusId": 30486598,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "120c819da02fcb312986ac492f723ef9ea3223b5",
            "isKey": false,
            "numCitedBy": 123,
            "numCiting": 24,
            "paperAbstract": {
                "fragments": [],
                "text": "Verifying code equivalence is useful in many situations, such as checking: yesterday's code against today's, different implementations of the same (standardized) interface, or an optimized routine against a reference implementation. We present a tool designed to easily check the equivalence of two arbitrary C functions. The tool provides guarantees far beyond those possible with testing, yet it often requires less work than writing even a single test case. It automatically synthesizes inputs to the routines and uses bit-accurate, sound symbolic execution to verify that they produce equivalent outputs on a finite number of paths, even for rich, nested data structures. We show that the approach works well, even on heavily-tested code, where it finds interesting errors and gets high statement coverage, often exhausting all feasible paths for a given input size. We also show how the simple trick of checking equivalence of identical code turns the verification tool chain against itself, finding errors in the underlying compiler and verification tool."
            },
            "slug": "Practical,-Low-Effort-Equivalence-Verification-of-Ramos-Engler",
            "title": {
                "fragments": [],
                "text": "Practical, Low-Effort Equivalence Verification of Real Code"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A tool designed to easily check the equivalence of two arbitrary C functions, which provides guarantees far beyond those possible with testing, yet it often requires less work than writing even a single test case."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145474353"
                        ],
                        "name": "Shuvendu K. Lahiri",
                        "slug": "Shuvendu-K.-Lahiri",
                        "structuredName": {
                            "firstName": "Shuvendu",
                            "lastName": "Lahiri",
                            "middleNames": [
                                "K."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shuvendu K. Lahiri"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3095589"
                        ],
                        "name": "C. Hawblitzel",
                        "slug": "C.-Hawblitzel",
                        "structuredName": {
                            "firstName": "Chris",
                            "lastName": "Hawblitzel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Hawblitzel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1706936"
                        ],
                        "name": "Ming Kawaguchi",
                        "slug": "Ming-Kawaguchi",
                        "structuredName": {
                            "firstName": "Ming",
                            "lastName": "Kawaguchi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ming Kawaguchi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1751228"
                        ],
                        "name": "H. Reb\u00ealo",
                        "slug": "H.-Reb\u00ealo",
                        "structuredName": {
                            "firstName": "Henrique",
                            "lastName": "Reb\u00ealo",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Reb\u00ealo"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 219
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 99,
                                "start": 95
                            }
                        ],
                        "text": "In another version of bounded model checking, differential symbolic execution [28] and SymDiff [20] bound the number of iterations of loops."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2470613,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "e0764b9142c211ec094543c447612dfe79da2662",
            "isKey": false,
            "numCitedBy": 188,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "In this paper, we describe SymDiff, a language-agnostic tool for equivalence checking and displaying semantic (behavioral) differences over imperative programs. The tool operates on an intermediate verification language Boogie, for which translations exist from various source languages such as C, C# and x86. We discuss the tool and the front-end interface to target various source languages. Finally, we provide a brief description of the front-end for C programs."
            },
            "slug": "SYMDIFF:-A-Language-Agnostic-Semantic-Diff-Tool-for-Lahiri-Hawblitzel",
            "title": {
                "fragments": [],
                "text": "SYMDIFF: A Language-Agnostic Semantic Diff Tool for Imperative Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "SymDiff is described, a language-agnostic tool for equivalence checking and displaying semantic (behavioral) differences over imperative programs that operates on an intermediate verification language Boogie."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111333766"
                        ],
                        "name": "Rahul Sharma",
                        "slug": "Rahul-Sharma",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144157872"
                        ],
                        "name": "Saurabh Gupta",
                        "slug": "Saurabh-Gupta",
                        "structuredName": {
                            "firstName": "Saurabh",
                            "lastName": "Gupta",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Saurabh Gupta"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790580"
                        ],
                        "name": "Bharath Hariharan",
                        "slug": "Bharath-Hariharan",
                        "structuredName": {
                            "firstName": "Bharath",
                            "lastName": "Hariharan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Bharath Hariharan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145419642"
                        ],
                        "name": "Percy Liang",
                        "slug": "Percy-Liang",
                        "structuredName": {
                            "firstName": "Percy",
                            "lastName": "Liang",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Percy Liang"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34894873"
                        ],
                        "name": "A. Nori",
                        "slug": "A.-Nori",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Nori",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Nori"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 137,
                                "start": 133
                            }
                        ],
                        "text": "The process may generate spurious equality relationships [26], however these can be systematically eliminated using a theorem prover [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 14,
                                "start": 10
                            }
                        ],
                        "text": "Note that [34] contains a completeness theorem, which is possible because it deals with a restricted language with no heap and has no specification to verify."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 231
                            }
                        ],
                        "text": "Our approach easily extends to generate non-linear equalities of a given degree d for invariants using ideas from invariant inference: We simply create a new feature for every monomial up to the degree d from the existing features [26, 34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 146,
                                "start": 142
                            }
                        ],
                        "text": "Our approach borrows a number of ideas from previous work on equivalence checking [6], translation validation [25], and software verification [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 23,
                                "start": 19
                            }
                        ],
                        "text": "In a previous work [34], we computed equality invariants for a single program using nullspace computations for a restricted language with assignments, branches, and loops."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 141,
                                "start": 137
                            }
                        ],
                        "text": "If proof obligation {E}C{Q} or {P}C{Q} fails then we can obtain a counter-example from the decision procedure and follow the approach of [34]: If the counter-example violates some equality of Q then we incorporate the data from the counter-example in the appropriate matrix."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 33,
                                "start": 29
                            }
                        ],
                        "text": "We generalize the results in [34], which uses nullspaces to compute invariants for a single program, to features:"
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 143,
                                "start": 139
                            }
                        ],
                        "text": "It can produce spurious equality relationships (for lack of sufficient data) but if an equality holds statically then it will be generated [34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2370176,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "bcafba343be287558b787a43b864d55842c000f8",
            "isKey": false,
            "numCitedBy": 124,
            "numCiting": 55,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a Guess-and-Check algorithm for computing algebraic equation invariants of the form \u2227ifi(x1,\u2026,xn)=0, where each fi is a polynomial over the variables x1,\u2026,xn of the program. The \"guess\" phase is data driven and derives a candidate invariant from data generated from concrete executions of the program. This candidate invariant is subsequently validated in a \"check\" phase by an off-the-shelf SMT solver. Iterating between the two phases leads to a sound algorithm. Moreover, we are able to prove a bound on the number of decision procedure queries which Guess-and-Check requires to obtain a sound invariant. We show how Guess-and-Check can be extended to generate arbitrary boolean combinations of linear equalities as invariants, which enables us to generate expressive invariants to be consumed by tools that cannot handle non-linear arithmetic. We have evaluated our technique on a number of benchmark programs from recent papers on invariant generation. Our results are encouraging --- we are able to efficiently compute algebraic invariants in all cases, with only a few tests."
            },
            "slug": "A-Data-Driven-Approach-for-Algebraic-Loop-Sharma-Gupta",
            "title": {
                "fragments": [],
                "text": "A Data Driven Approach for Algebraic Loop Invariants"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown how Guess-and-Check can be extended to generate arbitrary boolean combinations of linear equalities as invariants, which enables the technique to generate expressive invariants to be consumed by tools that cannot handle non-linear arithmetic."
            },
            "venue": {
                "fragments": [],
                "text": "ESOP"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2068439643"
                        ],
                        "name": "David W. Currie",
                        "slug": "David-W.-Currie",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Currie",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David W. Currie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741933"
                        ],
                        "name": "A. Hu",
                        "slug": "A.-Hu",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Hu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hu"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145092754"
                        ],
                        "name": "S. Rajan",
                        "slug": "S.-Rajan",
                        "structuredName": {
                            "firstName": "Sreeranga",
                            "lastName": "Rajan",
                            "middleNames": [
                                "P."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Rajan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 83
                            }
                        ],
                        "text": "Equivalence checking of low-level code has also been studied for embedded software [1, 6, 10, 11, 35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 121
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 85,
                                "start": 82
                            }
                        ],
                        "text": "Our approach borrows a number of ideas from previous work on equivalence checking [6], translation validation [25], and software verification [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2911896,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c1056238549215319ce651ef73da3d1f2d855bd5",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 13,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper describes a novel formal verification approach for equivalence checking of small, assembly-language routines for digital signal processors (DSP). By combining control-flow analysis, symbolic simulation, automatic decision procedures, and some domain-specific optimizations, we have built an automatic verification tool that compares structurally similar DSP assembly language routines. We tested our tool on code samples taken from a real application program and discovered several previously unknown bugs automatically. Runtime and memory requirements were reasonable on all examples. Our approach should generalize easily for multiple DSP architectures, eventually allowing comparison of code for two different DSPs (e.g., to verify a port from one DSP to another) and handling more complex DSPs (e.g. statically-scheduled, VLIW)."
            },
            "slug": "Automatic-formal-verification-of-dsp-software-Currie-Hu",
            "title": {
                "fragments": [],
                "text": "Automatic formal verification of dsp software"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "By combining control-flow analysis, symbolic simulation, automatic decision procedures, and some domain-specific optimizations, an automatic verification tool is built that compares structurally similar DSP assembly language routines."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 37th Design Automation Conference"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33548956"
                        ],
                        "name": "Yichen Xie",
                        "slug": "Yichen-Xie",
                        "structuredName": {
                            "firstName": "Yichen",
                            "lastName": "Xie",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yichen Xie"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 102,
                                "start": 98
                            }
                        ],
                        "text": "To illustrate just how important constraint simplification is, for the SAXPY benchmark, following [42] we perform slicing on VCs to eliminate constraints that are not relevant to the verification task."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14743527,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a0aebe3333b7b531e30bc7df69f20a9eb48feff1",
            "isKey": false,
            "numCitedBy": 197,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a software error-detection tool that exploits recent advances in boolean satisfiability (SAT) solvers. Our analysis is path sensitive, precise down to the bit level, and models pointers and heap data. Our approach is also highly scalable, which we achieve using two techniques. First, for each program function, several optimizations compress the size of the boolean formulas that model the control- and data-flow and the heap locations accessed by a function. Second, summaries in the spirit of type signatures are computed for each function, allowing inter-procedural analysis without a dramatic increase in the size of the boolean constraints to be solved.We demonstrate the effectiveness of our approach by constructing a lock interface inference and checking tool. In an interprocedural analysis of more than 23,000 lock related functions in the latest Linux kernel, the checker generated 300 warnings, of which 179 were unique locking errors, a false positive rate of only 40%."
            },
            "slug": "Scalable-error-detection-using-boolean-Xie-Aiken",
            "title": {
                "fragments": [],
                "text": "Scalable error detection using boolean satisfiability"
            },
            "tldr": {
                "abstractSimilarityScore": 88,
                "text": "A software error-detection tool that exploits recent advances in boolean satisfiability (SAT) solvers, and is path sensitive, precise down to the bit level, and models pointers and heap data."
            },
            "venue": {
                "fragments": [],
                "text": "POPL '05"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144996001"
                        ],
                        "name": "L. D. Moura",
                        "slug": "L.-D.-Moura",
                        "structuredName": {
                            "firstName": "Leonardo",
                            "lastName": "Moura",
                            "middleNames": [
                                "Mendon\u00e7a",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Moura"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3714351"
                        ],
                        "name": "N. Bj\u00f8rner",
                        "slug": "N.-Bj\u00f8rner",
                        "structuredName": {
                            "firstName": "Nikolaj",
                            "lastName": "Bj\u00f8rner",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Bj\u00f8rner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 15912959,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "3960dda299e0f8615a7db675b8e6905b375ecf8a",
            "isKey": false,
            "numCitedBy": 6280,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "Satisfiability Modulo Theories (SMT) problem is a decision problem for logical first order formulas with respect to combinations of background theories such as: arithmetic, bit-vectors, arrays, and uninterpreted functions. Z3 is a new and efficient SMT Solver freely available from Microsoft Research. It is used in various software verification and analysis applications."
            },
            "slug": "Z3:-An-Efficient-SMT-Solver-Moura-Bj\u00f8rner",
            "title": {
                "fragments": [],
                "text": "Z3: An Efficient SMT Solver"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "Z3 is a new and efficient SMT Solver freely available from Microsoft Research that is used in various software verification and analysis applications."
            },
            "venue": {
                "fragments": [],
                "text": "TACAS"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2280171"
                        ],
                        "name": "Xiushan Feng",
                        "slug": "Xiushan-Feng",
                        "structuredName": {
                            "firstName": "Xiushan",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiushan Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741933"
                        ],
                        "name": "A. Hu",
                        "slug": "A.-Hu",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Hu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 83
                            }
                        ],
                        "text": "Equivalence checking of low-level code has also been studied for embedded software [1, 6, 10, 11, 35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 121
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14503046,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f404efe0953ada483a9b7f14e9c082558d196ac",
            "isKey": false,
            "numCitedBy": 21,
            "numCiting": 34,
            "paperAbstract": {
                "fragments": [],
                "text": "Like hardware, embedded software faces stringent design constraints, undergoes extremely aggressive optimization, and therefore has a similar need for verifying the functional equivalence of two versions of a design, e.g., before and after an optimization. The concept of cutpoints was a breakthrough in the formal equivalence verification of combinational circuits and is the key enabling technology behind its successful commercialization. We introduce an analogous idea for formally verifying the equivalence of structurally similar, \"combinational\" software, i.e., software routines that compute a result and return/terminate, rather than executing indefinitely. We have implemented a proof-of-concept cutpoint approach in our prototype verification tool for the TI C6x family of VLIW DSPs, and our experiments show large improvements in runtime and memory usage."
            },
            "slug": "Cutpoints-for-formal-equivalence-verification-of-Feng-Hu",
            "title": {
                "fragments": [],
                "text": "Cutpoints for formal equivalence verification of embedded software"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work introduces an analogous idea for formally verifying the equivalence of structurally similar, \"combinational\" software, i.e., software routines that compute a result and return/terminate, rather than executing indefinitely."
            },
            "venue": {
                "fragments": [],
                "text": "EMSOFT"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2987608"
                        ],
                        "name": "Benny Godlin",
                        "slug": "Benny-Godlin",
                        "structuredName": {
                            "firstName": "Benny",
                            "lastName": "Godlin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Benny Godlin"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1747890"
                        ],
                        "name": "O. Strichman",
                        "slug": "O.-Strichman",
                        "structuredName": {
                            "firstName": "Ofer",
                            "lastName": "Strichman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "O. Strichman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "Regression verification [13] handles only partial equivalence: it does not deal with termination."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 16315423,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "33047cf60daf5f6146ad55b71bd8d6064ac6e73c",
            "isKey": false,
            "numCitedBy": 112,
            "numCiting": 15,
            "paperAbstract": {
                "fragments": [],
                "text": "Proving the equivalence of successive, closely related versions of a program has the potential of being easier in practice than functional verification, although both problems are undecidable. There are two main reasons for this claim: it circumvents the problem of specifying what the program should do, and in many cases it is computationally easier. We study theoretical and practical aspects of this problem, which we call regression verification."
            },
            "slug": "Regression-verification-Godlin-Strichman",
            "title": {
                "fragments": [],
                "text": "Regression verification"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "Proving the equivalence of successive, closely related versions of a program has the potential of being easier in practice than functional verification, although both problems are undecidable."
            },
            "venue": {
                "fragments": [],
                "text": "2009 46th ACM/IEEE Design Automation Conference"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144949450"
                        ],
                        "name": "K. Shashidhar",
                        "slug": "K.-Shashidhar",
                        "structuredName": {
                            "firstName": "K.",
                            "lastName": "Shashidhar",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Shashidhar"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743332"
                        ],
                        "name": "M. Bruynooghe",
                        "slug": "M.-Bruynooghe",
                        "structuredName": {
                            "firstName": "Maurice",
                            "lastName": "Bruynooghe",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Bruynooghe"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152374070"
                        ],
                        "name": "F. Catthoor",
                        "slug": "F.-Catthoor",
                        "structuredName": {
                            "firstName": "Francky",
                            "lastName": "Catthoor",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "F. Catthoor"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3272876"
                        ],
                        "name": "Gerda Janssens",
                        "slug": "Gerda-Janssens",
                        "structuredName": {
                            "firstName": "Gerda",
                            "lastName": "Janssens",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Gerda Janssens"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 83
                            }
                        ],
                        "text": "Equivalence checking of low-level code has also been studied for embedded software [1, 6, 10, 11, 35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 4485894,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "650ccf113c3324d08a232c6fe9d231b1f4508809",
            "isKey": false,
            "numCitedBy": 36,
            "numCiting": 20,
            "paperAbstract": {
                "fragments": [],
                "text": "Typically, a combination of manual and automated transformations is applied when algorithms for digital signal processing are adapted for energy and performance-efficient embedded systems. This poses severe verification problems. Verification becomes easier after converting the code into dynamic single-assignment form (DSA). This paper describes a method to prove equivalence between two programs in DSA where subscripts to array variables and loop bounds are (piecewise) affine expressions. For such programs, geometric modeling can be used and it can be shown, for groups of elements at once, that the outputs in both programs are the same function of the inputs."
            },
            "slug": "Verification-of-Source-Code-Transformations-by-Shashidhar-Bruynooghe",
            "title": {
                "fragments": [],
                "text": "Verification of Source Code Transformations by Program Equivalence Checking"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This paper describes a method to prove equivalence between two programs in DSA where subscripts to array variables and loop bounds are (piecewise) affine expressions."
            },
            "venue": {
                "fragments": [],
                "text": "CC"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703130"
                        ],
                        "name": "T. Reps",
                        "slug": "T.-Reps",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Reps",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Reps"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1702872"
                        ],
                        "name": "Shmuel Sagiv",
                        "slug": "Shmuel-Sagiv",
                        "structuredName": {
                            "firstName": "Shmuel",
                            "lastName": "Sagiv",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Shmuel Sagiv"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1771977"
                        ],
                        "name": "Greta Yorsh",
                        "slug": "Greta-Yorsh",
                        "structuredName": {
                            "firstName": "Greta",
                            "lastName": "Yorsh",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greta Yorsh"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 77,
                                "start": 69
                            }
                        ],
                        "text": "DDEC can also be combined with an abstract interpreter over binaries [31, 37]: these are capable of finding invariants over state elements of a single program (such as i \u2264 n)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 278343,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "1f75110858d4743e02a5c770a599d2ebc9f65383",
            "isKey": false,
            "numCitedBy": 150,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper shows how to achieve, under certain conditions, abstract-interpretation algorithms that enjoy the best possible precision for a given abstraction. The key idea is a simple process of successive approximation that makes repeated calls to a decision procedure, and obtains the best abstract value for a set of concrete stores that are represented symbolically, using a logical formula."
            },
            "slug": "Symbolic-Implementation-of-the-Best-Transformer-Reps-Sagiv",
            "title": {
                "fragments": [],
                "text": "Symbolic Implementation of the Best Transformer"
            },
            "tldr": {
                "abstractSimilarityScore": 100,
                "text": "This paper shows how to achieve, under certain conditions, abstract-interpretation algorithms that enjoy the best possible precision for a given abstraction."
            },
            "venue": {
                "fragments": [],
                "text": "VMCAI"
            },
            "year": 2004
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1755724"
                        ],
                        "name": "Jean-Baptiste Tristan",
                        "slug": "Jean-Baptiste-Tristan",
                        "structuredName": {
                            "firstName": "Jean-Baptiste",
                            "lastName": "Tristan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jean-Baptiste Tristan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3032062"
                        ],
                        "name": "Paul Govereau",
                        "slug": "Paul-Govereau",
                        "structuredName": {
                            "firstName": "Paul",
                            "lastName": "Govereau",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Paul Govereau"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143649399"
                        ],
                        "name": "J. G. Morrisett",
                        "slug": "J.-G.-Morrisett",
                        "structuredName": {
                            "firstName": "J.",
                            "lastName": "Morrisett",
                            "middleNames": [
                                "Gregory"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. G. Morrisett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 114
                            }
                        ],
                        "text": "Some of the most recent work on equivalence checking includes random interpretations [15] and equality saturation [36, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 966104,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b40c2883a0b9bef9be6c9fa56c9f8dd48e2c3909",
            "isKey": false,
            "numCitedBy": 95,
            "numCiting": 28,
            "paperAbstract": {
                "fragments": [],
                "text": "Translation validators are static analyzers that attempt to verify that program transformations preserve semantics. Normalizing translation validators do so by trying to match the value-graphs of an original function and its transformed counterpart. In this paper, we present the design of such a validator for LLVM's intra-procedural optimizations, a design that does not require any instrumentation of the optimizer, nor any rewriting of the source code to compile, and needs to run only once to validate a pipeline of optimizations. We present the results of our preliminary experiments on a set of benchmarks that include GCC, a perl interpreter, SQLite3, and other C programs."
            },
            "slug": "Evaluating-value-graph-translation-validation-for-Tristan-Govereau",
            "title": {
                "fragments": [],
                "text": "Evaluating value-graph translation validation for LLVM"
            },
            "tldr": {
                "abstractSimilarityScore": 45,
                "text": "The design of a translation validator for LLVM's intra-procedural optimizations is presented, a design that does not require any instrumentation of the optimizer, nor any rewriting of the source code to compile, and needs to run only once to validate a pipeline of optimizations."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI '11"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1714075"
                        ],
                        "name": "Isil Dillig",
                        "slug": "Isil-Dillig",
                        "structuredName": {
                            "firstName": "Isil",
                            "lastName": "Dillig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Isil Dillig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1711860"
                        ],
                        "name": "Thomas Dillig",
                        "slug": "Thomas-Dillig",
                        "structuredName": {
                            "firstName": "Thomas",
                            "lastName": "Dillig",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Thomas Dillig"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 212,
                                "start": 209
                            }
                        ],
                        "text": "The aim of our experiments is to describe the effectiveness of our core ideas and standard heuristics for constraint simplification, which we have not implemented, can be applied to achieve better performance [8]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2460101,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "0542d9db56023b5c2f0dbcb0a81cc317db6d0491",
            "isKey": false,
            "numCitedBy": 31,
            "numCiting": 41,
            "paperAbstract": {
                "fragments": [],
                "text": "Static analysis techniques that represent program states as formulas typically generate a large number of redundant formulas that are incrementally constructed from previous formulas. In addition to querying satisfiability and validity, analyses perform other operations on formulas, such as quantifier elimination, substitution, and instantiation, most of which are highly sensitive to formula size. Thus, the scalability of many static analysis techniques requires controlling the size of the generated formulas throughout the analysis. In this paper, we present a practical algorithm for reducing SMT formulas to a simplified form containing no redundant subparts. We present experimental evidence that on-line simplification of formulas dramatically improves scalability."
            },
            "slug": "Small-Formulas-for-Large-Programs:-On-Line-in-Dillig-Dillig",
            "title": {
                "fragments": [],
                "text": "Small Formulas for Large Programs: On-Line Constraint Simplification in Scalable Static Analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 46,
                "text": "This paper presents a practical algorithm for reducing SMT formulas to a simplified form containing no redundant subparts and presents experimental evidence that on-line simplification of formulas dramatically improves scalability."
            },
            "venue": {
                "fragments": [],
                "text": "SAS"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108851"
                        ],
                        "name": "Aditya V. Thakur",
                        "slug": "Aditya-V.-Thakur",
                        "structuredName": {
                            "firstName": "Aditya",
                            "lastName": "Thakur",
                            "middleNames": [
                                "V."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Aditya V. Thakur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703130"
                        ],
                        "name": "T. Reps",
                        "slug": "T.-Reps",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Reps",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Reps"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 7558873,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "42e6171a79b7a776c0a7180ea85c73b97a1732a4",
            "isKey": false,
            "numCitedBy": 53,
            "numCiting": 65,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper helps to bridge the gap between (i) the use of logic for specifying program semantics and performing program analysis, and (ii) abstract interpretation. Many operations needed by an abstract interpreter can be reduced to the problem of symbolic abstraction: the symbolic abstraction of a formula \u03d5 in logic, denoted by, is the most-precise value in abstract domain that over-approximates the meaning of \u03d5. We present a parametric framework that, given and, implements. The algorithm computes successively better over-approximations of. Because it approaches from \"above\", if it is taking too much time, a safe answer can be returned at any stage. \n \nMoreover, the framework is\"dual-use\": in addition to its applications in abstract interpretation, it provides a new way for an SMT (Satisfiability Modulo Theories) solver to perform unsatisfiability checking: given, the condition implies that \u03d5 is unsatisfiable."
            },
            "slug": "A-Method-for-Symbolic-Computation-of-Abstract-Thakur-Reps",
            "title": {
                "fragments": [],
                "text": "A Method for Symbolic Computation of Abstract Operations"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This paper presents a parametric framework that, given and, implements, provides a new way for an SMT (Satisfiability Modulo Theories) solver to perform unsatisfiability checking: given, the condition implies that \u03d5 is unsatisfiable."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144058482"
                        ],
                        "name": "R. Joshi",
                        "slug": "R.-Joshi",
                        "structuredName": {
                            "firstName": "Rajeev",
                            "lastName": "Joshi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Joshi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145802352"
                        ],
                        "name": "Greg Nelson",
                        "slug": "Greg-Nelson",
                        "structuredName": {
                            "firstName": "Greg",
                            "lastName": "Nelson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Greg Nelson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2118116274"
                        ],
                        "name": "Yunhong Zhou",
                        "slug": "Yunhong-Zhou",
                        "structuredName": {
                            "firstName": "Yunhong",
                            "lastName": "Zhou",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Yunhong Zhou"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 118
                            }
                        ],
                        "text": "In this section, we attempt to provide a detailed comparison between DDEC and techniques based on equality saturation [18, 36]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 110
                            }
                        ],
                        "text": "Superoptimizations and the related synthesis task have previously been limited to sequences of loop-free code [4, 16, 18, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 14405554,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "85e7266af5751f7e1c5e26b03e71e2f92af7b8c3",
            "isKey": false,
            "numCitedBy": 22,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "This article presents a design for the Denali-2 superoptimizer, which will generate minimum-instruction-length machine code for realistic machine architectures using automatic theorem-proving technology: specifically, using E-graph matching (a technique for pattern matching in the presence of equality information) and Boolean satisfiability solving.This article presents a precise definition of the underlying automatic programming problem solved by the Denali-2 superoptimizer. It sketches the E-graph matching phase and presents a detailed exposition and proof of soundness of the reduction of the automatic programming problem to the Boolean satisfiability problem."
            },
            "slug": "Denali:-A-practical-algorithm-for-generating-code-Joshi-Nelson",
            "title": {
                "fragments": [],
                "text": "Denali: A practical algorithm for generating optimal code"
            },
            "tldr": {
                "abstractSimilarityScore": 83,
                "text": "The Denali-2 superoptimizer will generate minimum-instruction-length machine code for realistic machine architectures using automatic theorem-proving technology: specifically, using E-graph matching and Boolean satisfiability solving."
            },
            "venue": {
                "fragments": [],
                "text": "TOPL"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145132131"
                        ],
                        "name": "D. Jackson",
                        "slug": "D.-Jackson",
                        "structuredName": {
                            "firstName": "Daniel",
                            "lastName": "Jackson",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Jackson"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3228348"
                        ],
                        "name": "David A. Ladd",
                        "slug": "David-A.-Ladd",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Ladd",
                            "middleNames": [
                                "A."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David A. Ladd"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 15606429,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "05529c6963c5cebbd5caeb10a72ca5beadd92ab8",
            "isKey": false,
            "numCitedBy": 201,
            "numCiting": 11,
            "paperAbstract": {
                "fragments": [],
                "text": "Describes a tool that takes two versions of a procedure and generates a report summarizing the semantic differences between them. Unlike existing tools based on comparison of program dependence graphs, our tool expresses its results in terms of the observable input-output behaviour of the procedure, rather than its syntactic structure. And because the analysis is truly semantic, it requires no prior matching of syntactic components, and generates fewer spurious differences, so that meaning-preserving transformations (such as renaming local variables) are correctly determined to have no visible effect. A preliminary experiment on modifications applied to the code of a large real-time system suggests that the approach is practical.<<ETX>>"
            },
            "slug": "Semantic-Diff:-a-tool-for-summarizing-the-effects-Jackson-Ladd",
            "title": {
                "fragments": [],
                "text": "Semantic Diff: a tool for summarizing the effects of modifications"
            },
            "tldr": {
                "abstractSimilarityScore": 63,
                "text": "A tool that takes two versions of a procedure and generates a report summarizing the semantic differences between them, which requires no prior matching of syntactic components, and generates fewer spurious differences, so that meaning-preserving transformations are correctly determined to have no visible effect."
            },
            "venue": {
                "fragments": [],
                "text": "Proceedings 1994 International Conference on Software Maintenance"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1720971"
                        ],
                        "name": "M. Rinard",
                        "slug": "M.-Rinard",
                        "structuredName": {
                            "firstName": "Martin",
                            "lastName": "Rinard",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Rinard"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 117
                            }
                        ],
                        "text": "However, for most interesting intra-procedural optimizations, simple equalities appear to be sufficiently expressive [25, 32, 36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 67293851,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "88a46614df275e4b612f0b5be84544af5a53395b",
            "isKey": false,
            "numCitedBy": 5,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": "This paper presents a new concept in compiler correctness: instead of proving that the compiler performs all of its transformations correctly, the compiler generates a proof that the transformed program correctly implements the input program. A simple proof checker can then verify that the program was compiled correctly. We call a compiler that produces such proofs a {\\em credible compiler}, because it produces verifiable evidence that it is operating correctly. Compiler optimizations usually consist of two steps --- an analysis step determines if it is legal to apply the optimization, and a transformation step applies the optimization to generate a transformed program that computes the same result as the original program. Our approach supports this two-step structure. It provides a logic that the compiler can use to prove that its program analysis results are correct, and a logic that the compiler can use to prove that the transformed program correctly simulates the original program. These logics are defined for a standard program representation, control flow graphs. This report defines these logics and proves that they are sound with respect to a standard operational semantics. It also presents detailed examples that demonstrate how a compiler can use the logics to prove the correctness of several standard optimizations. We believe that credible compilation has the potential to revolutionize the way compilers are built and used. Specifically, they will allow programmers to quickly determine if the compiler compiled their program correctly, help developers find and eliminate bugs in compiler passes, allow large groups of mutually untrusting people to collaborate productively on the same compiler, increase the speed with which compilers are developed and released, and make it possible to aggressively upgrade large, stable compiler systems without fear of inadvertantly introducing undetected errors."
            },
            "slug": "Credible-Compilers-Rinard",
            "title": {
                "fragments": [],
                "text": "Credible Compilers"
            },
            "tldr": {
                "abstractSimilarityScore": 98,
                "text": "This paper presents a new concept in compiler correctness: instead of proving that the compiler performs all of its transformations correctly, the compiler generates a proof that the transformed program correctly implements the input program, so that a simple proof checker can then verify that the program was compiled correctly."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1999
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1750460"
                        ],
                        "name": "S. Person",
                        "slug": "S.-Person",
                        "structuredName": {
                            "firstName": "Suzette",
                            "lastName": "Person",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Person"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145738070"
                        ],
                        "name": "Matthew B. Dwyer",
                        "slug": "Matthew-B.-Dwyer",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Dwyer",
                            "middleNames": [
                                "B."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew B. Dwyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143754832"
                        ],
                        "name": "S. Elbaum",
                        "slug": "S.-Elbaum",
                        "structuredName": {
                            "firstName": "Sebastian",
                            "lastName": "Elbaum",
                            "middleNames": [
                                "G."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Elbaum"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1723381"
                        ],
                        "name": "C. Pasareanu",
                        "slug": "C.-Pasareanu",
                        "structuredName": {
                            "firstName": "Corina",
                            "lastName": "Pasareanu",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Pasareanu"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 235,
                                "start": 219
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "In another version of bounded model checking, differential symbolic execution [28] and SymDiff [20] bound the number of iterations of loops."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 11107146,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "38a0e40c5b04ebe9154390ccde800dc6cebbd6cb",
            "isKey": false,
            "numCitedBy": 271,
            "numCiting": 71,
            "paperAbstract": {
                "fragments": [],
                "text": "Detecting and characterizing the effects of software changes is a fundamental component of software maintenance. Version differencing information can be used to perform version merging, infer change characteristics, produce program documentation, and guide program re-validation. Existing techniques for characterizing code changes, however, are imprecise leading to unnecessary maintenance efforts.\n In this paper, we introduce a novel extension and application of symbolic execution techniques that computes a precise behavioral characterization of a program change. This technique, which we call differential symbolic execution (DSE), exploits the fact that program versions are largely similar to reduce cost and improve the quality of analysis results. We define the foundational concepts of DSE, describe cost-effective tool support for DSE, and illustrate its potential benefit through an exploratory study that considers version histories of two Java code bases."
            },
            "slug": "Differential-symbolic-execution-Person-Dwyer",
            "title": {
                "fragments": [],
                "text": "Differential symbolic execution"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "A novel extension and application of symbolic execution techniques that computes a precise behavioral characterization of a program change that exploits the fact that program versions are largely similar to reduce cost and improve the quality of analysis results is introduced."
            },
            "venue": {
                "fragments": [],
                "text": "SIGSOFT '08/FSE-16"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741927"
                        ],
                        "name": "C. Wintersteiger",
                        "slug": "C.-Wintersteiger",
                        "structuredName": {
                            "firstName": "Christoph",
                            "lastName": "Wintersteiger",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Wintersteiger"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1728524"
                        ],
                        "name": "Y. Hamadi",
                        "slug": "Y.-Hamadi",
                        "structuredName": {
                            "firstName": "Youssef",
                            "lastName": "Hamadi",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Y. Hamadi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144996001"
                        ],
                        "name": "L. D. Moura",
                        "slug": "L.-D.-Moura",
                        "structuredName": {
                            "firstName": "Leonardo",
                            "lastName": "Moura",
                            "middleNames": [
                                "Mendon\u00e7a",
                                "de"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. D. Moura"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 35,
                                "start": 31
                            }
                        ],
                        "text": "Z3 is complete for this theory [41] and is able to soundly analyze the effect of x86 instructions on the machine state with bit-wise precision."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2910334,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f2622e8d4bcfa841271f329f935b7575a4929ef3",
            "isKey": false,
            "numCitedBy": 102,
            "numCiting": 37,
            "paperAbstract": {
                "fragments": [],
                "text": "In recent years, bit-precise reasoning has gained importance in hardware and software verification. Of renewed interest is the use of symbolic reasoning for synthesising loop invariants, ranking functions, or whole program fragments and hardware circuits. Solvers for the quantifier-free fragment of bit-vector logic exist and often rely on SAT solvers for efficiency. However, many techniques require quantifiers in bit-vector formulas to avoid an exponential blow-up during construction. Solvers for quantified formulas usually flatten the input to obtain a quantified Boolean formula, losing much of the word-level information in the formula. We present a new approach based on a set of effective word-level simplifications that are traditionally employed in automated theorem proving, heuristic quantifier instantiation methods used in SMT solvers, and model finding techniques based on skeletons/templates. Experimental results on two different types of benchmarks indicate that our method outperforms the traditional flattening approach by multiple orders of magnitude of runtime."
            },
            "slug": "Efficiently-solving-quantified-bit-vector-formulas-Wintersteiger-Hamadi",
            "title": {
                "fragments": [],
                "text": "Efficiently solving quantified bit-vector formulas"
            },
            "tldr": {
                "abstractSimilarityScore": 42,
                "text": "This work presents a new approach based on a set of effective word-level simplifications that are traditionally employed in automated theorem proving, heuristic quantifier instantiation methods used in SMT solvers, and model finding techniques based on skeletons/templates that outperforms the traditional flattening approach."
            },
            "venue": {
                "fragments": [],
                "text": "Formal Methods in Computer Aided Design"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1698938"
                        ],
                        "name": "A. Pnueli",
                        "slug": "A.-Pnueli",
                        "structuredName": {
                            "firstName": "Amir",
                            "lastName": "Pnueli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Pnueli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "119614910"
                        ],
                        "name": "M. Siegel",
                        "slug": "M.-Siegel",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Siegel",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Siegel"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774977"
                        ],
                        "name": "Eli Singerman",
                        "slug": "Eli-Singerman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Singerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eli Singerman"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 58
                            }
                        ],
                        "text": "Fractal symbolic analysis [24] and translation validation [14, 25, 29] are two techniques that reason about loops in general."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 429,
                                "start": 417
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14822655,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "d8b4164fef65ffc7082a3c95b0a706e5c3aa38f9",
            "isKey": false,
            "numCitedBy": 515,
            "numCiting": 30,
            "paperAbstract": {
                "fragments": [],
                "text": "We present the notion of translation validation as a new approach to the veriication of translators (compilers, code generators). Rather than proving in advance that the compiler always produces a target code which correctly implements the source code (compiler verii-cation), each individual translation (i.e. a run of the compiler) is followed by a validation phase which veriies that the target code produced on this run correctly implements the submitted source program. Several ingredients are necessary to set up the { fully automatic { translation validation process, among which are: 1. A common semantic framework for the representation of the source code and the generated target code. 2. A formalization of the notion of \"correct implementation\" as a re-nement relation. 3. A syntactic simulation-based proof method which allows to automatically verify that one model of the semantic framework, representing the produced target code, correctly implements another model which represents the source. These, and other ingredients are elaborated in this paper, in which we illustrate the new approach in a most challenging case. We consider a translation (compilation) from the synchronous multi-clock data-ow language Signal to asynchronous (sequential) C-code."
            },
            "slug": "Translation-Validation-Pnueli-Siegel",
            "title": {
                "fragments": [],
                "text": "Translation Validation"
            },
            "tldr": {
                "abstractSimilarityScore": 47,
                "text": "This paper considers a translation (compilation) from the synchronous multi-clock data-ow language Signal to asynchronous (sequential) C-code and presents the notion of translation validation as a new approach to the veriication of translators (compilers, code generators)."
            },
            "venue": {
                "fragments": [],
                "text": "TACAS"
            },
            "year": 1998
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144179390"
                        ],
                        "name": "B. Goldberg",
                        "slug": "B.-Goldberg",
                        "structuredName": {
                            "firstName": "Benjamin",
                            "lastName": "Goldberg",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "B. Goldberg"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688414"
                        ],
                        "name": "L. Zuck",
                        "slug": "L.-Zuck",
                        "structuredName": {
                            "firstName": "Lenore",
                            "lastName": "Zuck",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Zuck"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1680661"
                        ],
                        "name": "C. Barrett",
                        "slug": "C.-Barrett",
                        "structuredName": {
                            "firstName": "Clark",
                            "lastName": "Barrett",
                            "middleNames": [
                                "W."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "C. Barrett"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 58
                            }
                        ],
                        "text": "Fractal symbolic analysis [24] and translation validation [14, 25, 29] are two techniques that reason about loops in general."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 429,
                                "start": 417
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 14559811,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "97c1177a1702e97a30f860baf15023c1ce172549",
            "isKey": false,
            "numCitedBy": 42,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Into-the-Loops:-Practical-Issues-in-Translation-for-Goldberg-Zuck",
            "title": {
                "fragments": [],
                "text": "Into the Loops: Practical Issues in Translation Validation for Optimizing Compilers"
            },
            "venue": {
                "fragments": [],
                "text": "Electron. Notes Theor. Comput. Sci."
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2280171"
                        ],
                        "name": "Xiushan Feng",
                        "slug": "Xiushan-Feng",
                        "structuredName": {
                            "firstName": "Xiushan",
                            "lastName": "Feng",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Xiushan Feng"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1741933"
                        ],
                        "name": "A. Hu",
                        "slug": "A.-Hu",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Hu",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hu"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 101,
                                "start": 83
                            }
                        ],
                        "text": "Equivalence checking of low-level code has also been studied for embedded software [1, 6, 10, 11, 35]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 139,
                                "start": 121
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 15638480,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "c6e3137afaa8eacd1a16450f63f73f4ee9ff82fa",
            "isKey": false,
            "numCitedBy": 12,
            "numCiting": 16,
            "paperAbstract": {
                "fragments": [],
                "text": "VLIW processors are attractive for many embedded applications, but VLIW code scheduling, whether by hand or by compiler, is extremely challenging. In this paper, we extend previous work on automated verification of low-level software to handle the complexity of modern, aggressive VLIW designs, e.g., the exposed parallelism, pipelining, and resource constraints. We implement these ideas into a prototype tool for verifying short sequences of assembly code for TI's C62x family of VLIW DSPs, and demonstrate the effectiveness of the tool in quickly verifying, or finding bugs in, two difficult-to-analyze code segments."
            },
            "slug": "Automatic-formal-verification-for-scheduled-VLIW-Feng-Hu",
            "title": {
                "fragments": [],
                "text": "Automatic formal verification for scheduled VLIW code"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "This paper implements ideas from previous work on automated verification of low-level software into a prototype tool for verifying short sequences of assembly code for TI's C62x family of VLIW DSPs, and demonstrates the effectiveness of the tool in quickly verifying, or finding bugs in, two difficult-to-analyze code segments."
            },
            "venue": {
                "fragments": [],
                "text": "LCTES/SCOPES '02"
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790411"
                        ],
                        "name": "G. Necula",
                        "slug": "G.-Necula",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Necula",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Necula"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 133,
                                "start": 129
                            }
                        ],
                        "text": "For well-studied compilers such as gcc and COMPCERT, simple pattern matching heuristics are known to be well-suited to this task [25]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 429,
                                "start": 417
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 51
                            }
                        ],
                        "text": "Conceptually, if one tries to port the approach of Necula [25] to x86, then one will need to build a static analysis for x86 which is sound and precise enough to generate simulation relations."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 21,
                                "start": 17
                            }
                        ],
                        "text": "Following Necula [25], we consider invariants that are equalities over features, which are registers, stack locations and a finite set \u2206 of heap locations."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 15,
                                "start": 11
                            }
                        ],
                        "text": "Also as in [25], we replace reads from and writes to stack locations by reads and writes to named temporaries."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 70,
                                "start": 58
                            }
                        ],
                        "text": "Fractal symbolic analysis [24] and translation validation [14, 25, 29] are two techniques that reason about loops in general."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 40
                            }
                        ],
                        "text": "Previous work on translation validation [25, 36] makes the same assumption, which we find to be largely sufficient in practice (see Section 5)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 114,
                                "start": 110
                            }
                        ],
                        "text": "Our approach borrows a number of ideas from previous work on equivalence checking [6], translation validation [25], and software verification [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 59,
                                "start": 55
                            }
                        ],
                        "text": "We address this issue by borrowing an idea from Necula [25], who solves this problem by replacing spill slots with temporary registers that eliminate the possibility of aliasing between addresses passed as arguments and the stack frame."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 117
                            }
                        ],
                        "text": "However, for most interesting intra-procedural optimizations, simple equalities appear to be sufficiently expressive [25, 32, 36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 22,
                                "start": 16
                            }
                        ],
                        "text": "We thank George Necula, Jan Vitek, and the anonymous reviewers for their constructive comments."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 46,
                                "start": 42
                            }
                        ],
                        "text": "First, DDEC guesses a simulation relation [25]."
                    },
                    "intents": []
                }
            ],
            "corpusId": 2448939,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "011f7da0095ac8c0d4477eeda2728e5f80a35767",
            "isKey": true,
            "numCitedBy": 486,
            "numCiting": 36,
            "paperAbstract": {
                "fragments": [],
                "text": "We describe a translation validation infrastructure for the GNU C compiler. During the compilation the infrastructure compares the intermediate form of the program before and after each compiler pass and verifies the preservation of semantics. We discuss a general framework that the optimizer can use to communicate to the validator what transformations were performed. Our implementation however does not rely on help from the optimizer and it is quite successful by using instead a few heuristics to detect the transformations that take place.\nThe main message of this paper is that a practical translation validation infrastructure, able to check the correctness of many of the transformations performed by a realistic compiler, can be implemented with about the effort typically required to implement one compiler pass. We demonstrate this in the context of the GNU C compiler for a number of its optimizations while compiling realistic programs such as the compiler itself or the Linux kernel. We believe that the price of such an infrastructure is small considering the qualitative increase in the ability to isolate compilation errors during compiler testing and maintenance."
            },
            "slug": "Translation-validation-for-an-optimizing-compiler-Necula",
            "title": {
                "fragments": [],
                "text": "Translation validation for an optimizing compiler"
            },
            "tldr": {
                "abstractSimilarityScore": 52,
                "text": "A practical translation validation infrastructure, able to check the correctness of many of the transformations performed by a realistic compiler, can be implemented with about the effort typically required to implement one compiler pass."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI '00"
            },
            "year": 2000
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108314"
                        ],
                        "name": "Sumit Gulwani",
                        "slug": "Sumit-Gulwani",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Gulwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sumit Gulwani"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "37747652"
                        ],
                        "name": "Susmit Jha",
                        "slug": "Susmit-Jha",
                        "structuredName": {
                            "firstName": "Susmit",
                            "lastName": "Jha",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Susmit Jha"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145219494"
                        ],
                        "name": "A. Tiwari",
                        "slug": "A.-Tiwari",
                        "structuredName": {
                            "firstName": "Ashish",
                            "lastName": "Tiwari",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tiwari"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144002578"
                        ],
                        "name": "R. Venkatesan",
                        "slug": "R.-Venkatesan",
                        "structuredName": {
                            "firstName": "Ramarathnam",
                            "lastName": "Venkatesan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Venkatesan"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 110
                            }
                        ],
                        "text": "Superoptimizations and the related synthesis task have previously been limited to sequences of loop-free code [4, 16, 18, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6554381,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "ea93cf600859aa09b0b7b727c12f36543608e1cd",
            "isKey": false,
            "numCitedBy": 307,
            "numCiting": 59,
            "paperAbstract": {
                "fragments": [],
                "text": "We consider the problem of synthesizing loop-free programs that implement a desired functionality using components from a given library. Specifications of the desired functionality and the library components are provided as logical relations between their respective input and output variables. The library components can be used at most once, and hence the library is required to contain a reasonable overapproximation of the multiset of the components required.\n We solve the above component-based synthesis problem using a constraint-based approach that involves first generating a synthesis constraint, and then solving the constraint. The synthesis constraint is a first-order \u2203\u2200 logic formula whose size is quadratic in the number of components. We present a novel algorithm for solving such constraints. Our algorithm is based on counterexample guided iterative synthesis paradigm and uses off-the-shelf SMT solvers.\n We present experimental results that show that our tool Brahma can efficiently synthesize highly nontrivial 10-20 line loop-free bitvector programs. These programs represent a state space of approximately 2010 programs, and are beyond the reach of the other tools based on sketching and superoptimization."
            },
            "slug": "Synthesis-of-loop-free-programs-Gulwani-Jha",
            "title": {
                "fragments": [],
                "text": "Synthesis of loop-free programs"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Results are presented that show that the tool Brahma can efficiently synthesize highly nontrivial 10-20 line loop-free bitvector programs, and are beyond the reach of the other tools based on sketching and superoptimization."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI '11"
            },
            "year": 2011
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1707506"
                        ],
                        "name": "Patrice Godefroid",
                        "slug": "Patrice-Godefroid",
                        "structuredName": {
                            "firstName": "Patrice",
                            "lastName": "Godefroid",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Patrice Godefroid"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "40511120"
                        ],
                        "name": "Ankur Taly",
                        "slug": "Ankur-Taly",
                        "structuredName": {
                            "firstName": "Ankur",
                            "lastName": "Taly",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Ankur Taly"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 116,
                                "start": 112
                            }
                        ],
                        "text": "In the process we rediscovered known instances in which the x86 instruction set deviates from its specification [12]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 12970901,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "823925405e0544a9ceb0ad1232f0013873402094",
            "isKey": false,
            "numCitedBy": 66,
            "numCiting": 33,
            "paperAbstract": {
                "fragments": [],
                "text": "Symbolic execution is a key component of precise binary program analysis tools. We discuss how to automatically boot-strap the construction of a symbolic execution engine for a processor instruction set such as x86, x64 or ARM. We show how to automatically synthesize symbolic representations of individual processor instructions from input/output examples and express them as bit-vector constraints. We present and compare various synthesis algorithms and instruction sampling strategies. We introduce a new synthesis algorithm based on smart sampling which we show is one to two orders of magnitude faster than previous synthesis algorithms in our context. With this new algorithm, we can automatically synthesize bit-vector circuits for over 500 x86 instructions (8/16/32-bits, outputs, EFLAGS) using only 6 synthesis templates and in less than two hours using the Z3 SMT solver on a regular machine. During this work, we also discovered several inconsistencies across x86 processors, errors in the x86 Intel spec, and several bugs in previous manually-written x86 instruction handlers."
            },
            "slug": "Automated-synthesis-of-symbolic-instruction-from-Godefroid-Taly",
            "title": {
                "fragments": [],
                "text": "Automated synthesis of symbolic instruction encodings from I/O samples"
            },
            "tldr": {
                "abstractSimilarityScore": 39,
                "text": "A new synthesis algorithm based on smart sampling which is one to two orders of magnitude faster than previous synthesis algorithms in this context and discovered several inconsistencies across x86 processors, errors in the x86 Intel spec, and several bugs in previous manually-written x86 instruction handlers."
            },
            "venue": {
                "fragments": [],
                "text": "PLDI"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1790411"
                        ],
                        "name": "G. Necula",
                        "slug": "G.-Necula",
                        "structuredName": {
                            "firstName": "George",
                            "lastName": "Necula",
                            "middleNames": [
                                "C."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Necula"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2108314"
                        ],
                        "name": "Sumit Gulwani",
                        "slug": "Sumit-Gulwani",
                        "structuredName": {
                            "firstName": "Sumit",
                            "lastName": "Gulwani",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sumit Gulwani"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 89,
                                "start": 85
                            }
                        ],
                        "text": "Some of the most recent work on equivalence checking includes random interpretations [15] and equality saturation [36, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 985127,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "13389bdf2f9cc401f93939f4f1c8a3a68539f750",
            "isKey": false,
            "numCitedBy": 16,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "This dissertation describes a new program analysis technique called random interpretation that uses the power of randomization to verify and discover program properties. Random interpretation is inspired by, and combines the strengths of, the two complementary techniques for program analysis: random testing and abstract interpretation. Random testing is simple and finds real bugs in programs, but cannot prove absence of bugs. Abstract interpretation, on the other hand, is a class of sound and deterministic program analyses that find all bugs, but also report spurious bugs (false positives). Often these analyses are complicated and have long running time. In this dissertation, we describe few random interpretation based program analyses that are more efficient as well as simpler than their deterministic counterparts that had been state-of-the-art for almost 30 years. We then show how to extend these intra-procedural analyses to an inter-procedural setting, and how to combine these analyses. We also discuss our experience experimenting with some of these algorithms."
            },
            "slug": "Program-analysis-using-random-interpretation-Necula-Gulwani",
            "title": {
                "fragments": [],
                "text": "Program analysis using random interpretation"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "A new program analysis technique called random interpretation that uses the power of randomization to verify and discover program properties that are more efficient as well as simpler than their deterministic counterparts that had been state-of-the-art for almost 30 years are described."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2578198"
                        ],
                        "name": "Nimrod Partush",
                        "slug": "Nimrod-Partush",
                        "structuredName": {
                            "firstName": "Nimrod",
                            "lastName": "Partush",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Nimrod Partush"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1743232"
                        ],
                        "name": "Eran Yahav",
                        "slug": "Eran-Yahav",
                        "structuredName": {
                            "firstName": "Eran",
                            "lastName": "Yahav",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eran Yahav"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 198,
                                "start": 194
                            }
                        ],
                        "text": "Even if the composition step is successful, it is not clear how one would argue about the termination behaviors of the target and the rewrite (Definition 1) from the composed program and indeed [27] assumes terminating executions."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 32,
                                "start": 28
                            }
                        ],
                        "text": "However, the composition of [27] relies on syntactic heuristics that seem difficult to apply to binaries."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 131,
                                "start": 127
                            }
                        ],
                        "text": "As an alternative to DDEC, one can imagine composing two programs into a single program and then using an abstract interpreter [27]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1132017,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7cb1549ea79af30250c3b486822032fb6610930a",
            "isKey": true,
            "numCitedBy": 39,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "We address the problem of computing semantic differences between a program and a patched version of the program. Our goal is to obtain a precise characterization of the difference between program versions, or establish their equivalence when no difference exists."
            },
            "slug": "Abstract-Semantic-Differencing-for-Numerical-Partush-Yahav",
            "title": {
                "fragments": [],
                "text": "Abstract Semantic Differencing for Numerical Programs"
            },
            "tldr": {
                "abstractSimilarityScore": 93,
                "text": "This work addresses the problem of computing semantic differences between a program and a patched version of the program and obtains a precise characterization of the difference between program versions."
            },
            "venue": {
                "fragments": [],
                "text": "SAS"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6331083"
                        ],
                        "name": "R. Tate",
                        "slug": "R.-Tate",
                        "structuredName": {
                            "firstName": "Ross",
                            "lastName": "Tate",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "R. Tate"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30073551"
                        ],
                        "name": "M. Stepp",
                        "slug": "M.-Stepp",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Stepp",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Stepp"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2272813"
                        ],
                        "name": "Zachary Tatlock",
                        "slug": "Zachary-Tatlock",
                        "structuredName": {
                            "firstName": "Zachary",
                            "lastName": "Tatlock",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zachary Tatlock"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145655689"
                        ],
                        "name": "Sorin Lerner",
                        "slug": "Sorin-Lerner",
                        "structuredName": {
                            "firstName": "Sorin",
                            "lastName": "Lerner",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorin Lerner"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 355,
                                "start": 347
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 24,
                                "start": 20
                            }
                        ],
                        "text": "Equality saturation [36] is a state of the art technique for verifying compiler optimizations which relies on expert-written equality rules, such as \u201cmultiplication by two is equivalent to a bit shift\u201d."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 122,
                                "start": 114
                            }
                        ],
                        "text": "Some of the most recent work on equivalence checking includes random interpretations [15] and equality saturation [36, 38]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 109,
                                "start": 105
                            }
                        ],
                        "text": "We compare DDEC to equality saturation using the two motivating examples described in the original paper [36], the first of which appears in Section 2; these correspond to the first two rows in Table 1."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 113,
                                "start": 109
                            }
                        ],
                        "text": "Equality saturation, in its current form, is unable to handle important optimizations such as loop unrolling [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 40
                            }
                        ],
                        "text": "Previous work on translation validation [25, 36] makes the same assumption, which we find to be largely sufficient in practice (see Section 5)."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 129,
                                "start": 117
                            }
                        ],
                        "text": "However, for most interesting intra-procedural optimizations, simple equalities appear to be sufficiently expressive [25, 32, 36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 57,
                                "start": 53
                            }
                        ],
                        "text": "Figure 1 shows two versions of a function taken from [36]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 126,
                                "start": 118
                            }
                        ],
                        "text": "In this section, we attempt to provide a detailed comparison between DDEC and techniques based on equality saturation [18, 36]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 2138086,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f09d36476445c7f44d46555a753eae446cfed180",
            "isKey": true,
            "numCitedBy": 157,
            "numCiting": 85,
            "paperAbstract": {
                "fragments": [],
                "text": "Optimizations in a traditional compiler are applied sequentially, with each optimization destructively modifying the program to produce a transformed program that is then passed to the next optimization. We present a new approach for structuring the optimization phase of a compiler. In our approach, optimizations take the form of equality analyses that add equality information to a common intermediate representation. The optimizer works by repeatedly applying these analyses to infer equivalences between program fragments, thus saturating the intermediate representation with equalities. Once saturated, the intermediate representation encodes multiple optimized versions of the input program. At this point, a profitability heuristic picks the final optimized program from the various programs represented in the saturated representation. Our proposed way of structuring optimizers has a variety of benefits over previous approaches: our approach obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than our own. We present our approach, formalize it, and describe our choice of intermediate representation. We also present experimental results showing that our approach is practical in terms of time and space overhead, is effective at discovering intricate optimization opportunities, and is effective at performing translation validation for a realistic optimizer."
            },
            "slug": "Equality-saturation:-a-new-approach-to-optimization-Tate-Stepp",
            "title": {
                "fragments": [],
                "text": "Equality saturation: a new approach to optimization"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "The proposed way of structuring optimizers has a variety of benefits over previous approaches: it obviates the need to worry about optimization ordering, enables the use of a global optimization heuristic that selects among fully optimized programs, and can be used to perform translation validation, even on compilers other than the authors' own."
            },
            "venue": {
                "fragments": [],
                "text": "POPL '09"
            },
            "year": 2009
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2360140"
                        ],
                        "name": "T. Arons",
                        "slug": "T.-Arons",
                        "structuredName": {
                            "firstName": "Tamarah",
                            "lastName": "Arons",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Arons"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2266665"
                        ],
                        "name": "E. Elster",
                        "slug": "E.-Elster",
                        "structuredName": {
                            "firstName": "Elad",
                            "lastName": "Elster",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "E. Elster"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2322524"
                        ],
                        "name": "L. Fix",
                        "slug": "L.-Fix",
                        "structuredName": {
                            "firstName": "Limor",
                            "lastName": "Fix",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Fix"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1403805932"
                        ],
                        "name": "Sela Mador-Haim",
                        "slug": "Sela-Mador-Haim",
                        "structuredName": {
                            "firstName": "Sela",
                            "lastName": "Mador-Haim",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sela Mador-Haim"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2095320994"
                        ],
                        "name": "Michael Mishaeli",
                        "slug": "Michael-Mishaeli",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Mishaeli",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael Mishaeli"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2076011117"
                        ],
                        "name": "Jonathan Shalev",
                        "slug": "Jonathan-Shalev",
                        "structuredName": {
                            "firstName": "Jonathan",
                            "lastName": "Shalev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Jonathan Shalev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1774977"
                        ],
                        "name": "Eli Singerman",
                        "slug": "Eli-Singerman",
                        "structuredName": {
                            "firstName": "Eli",
                            "lastName": "Singerman",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eli Singerman"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3055223"
                        ],
                        "name": "A. Tiemeyer",
                        "slug": "A.-Tiemeyer",
                        "structuredName": {
                            "firstName": "Andreas",
                            "lastName": "Tiemeyer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Tiemeyer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "9083969"
                        ],
                        "name": "Moshe Y. Vardi",
                        "slug": "Moshe-Y.-Vardi",
                        "structuredName": {
                            "firstName": "Moshe",
                            "lastName": "Vardi",
                            "middleNames": [
                                "Y."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Moshe Y. Vardi"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1688414"
                        ],
                        "name": "L. Zuck",
                        "slug": "L.-Zuck",
                        "structuredName": {
                            "firstName": "Lenore",
                            "lastName": "Zuck",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "L. Zuck"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 6654069,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "50b8e5fc4f4d9f1c2f84efa04ecab75d0eeef8fb",
            "isKey": false,
            "numCitedBy": 43,
            "numCiting": 23,
            "paperAbstract": {
                "fragments": [],
                "text": "Microcode is used to facilitate new technologies in Intel CPU designs. A critical requirement is that new designs be backwardly compatible with legacy code when new functionalities are disabled. Several features distinguish microcode from other software systems, such as: interaction with the external environment, sensitivity to exceptions, and the complexity of instructions. This work describes the ideas behind MICROFORMAL,, a technology for fully automated formal verification of functional backward compatibility of microcode."
            },
            "slug": "Formal-Verification-of-Backward-Compatibility-of-Arons-Elster",
            "title": {
                "fragments": [],
                "text": "Formal Verification of Backward Compatibility of Microcode"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The ideas behind MICROFORMAL, a technology for fully automated formal verification of functional backward compatibility of microcode, are described."
            },
            "venue": {
                "fragments": [],
                "text": "CAV"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1810147"
                        ],
                        "name": "ThanhVu Nguyen",
                        "slug": "ThanhVu-Nguyen",
                        "structuredName": {
                            "firstName": "ThanhVu",
                            "lastName": "Nguyen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "ThanhVu Nguyen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144698995"
                        ],
                        "name": "D. Kapur",
                        "slug": "D.-Kapur",
                        "structuredName": {
                            "firstName": "Deepak",
                            "lastName": "Kapur",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Kapur"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "47211092"
                        ],
                        "name": "Westley Weimer",
                        "slug": "Westley-Weimer",
                        "structuredName": {
                            "firstName": "Westley",
                            "lastName": "Weimer",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Westley Weimer"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "30123380"
                        ],
                        "name": "S. Forrest",
                        "slug": "S.-Forrest",
                        "structuredName": {
                            "firstName": "Stephanie",
                            "lastName": "Forrest",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Forrest"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 239,
                                "start": 231
                            }
                        ],
                        "text": "Our approach easily extends to generate non-linear equalities of a given degree d for invariants using ideas from invariant inference: We simply create a new feature for every monomial up to the degree d from the existing features [26, 34]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 61,
                                "start": 57
                            }
                        ],
                        "text": "The process may generate spurious equality relationships [26], however these can be systematically eliminated using a theorem prover [34]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 497438,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "dc63fd57459d265ff10108790eb5db6cf4bf2724",
            "isKey": false,
            "numCitedBy": 71,
            "numCiting": 38,
            "paperAbstract": {
                "fragments": [],
                "text": "Dynamic invariant analysis identifies likely properties over variables from observed program traces. These properties can aid programmers in refactoring, documenting, and debugging tasks by making dynamic patterns visible statically. Two useful forms of invariants involve relations among polynomials over program variables and relations among array variables. Current dynamic analysis methods support such invariants in only very limited forms. We combine mathematical techniques that have not previously been applied to this problem, namely equation solving, polyhedra construction, and SMT solving, to bring new capabilities to dynamic invariant detection. Using these methods, we show how to find equalities and inequalities among nonlinear polynomials over program variables, and linear relations among array variables of multiple dimensions. Preliminary experiments on 24 mathematical algorithms and an implementation of AES encryption provide evidence that the approach is effective at finding these invariants."
            },
            "slug": "Using-dynamic-analysis-to-discover-polynomial-and-Nguyen-Kapur",
            "title": {
                "fragments": [],
                "text": "Using dynamic analysis to discover polynomial and array invariants"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "This work combines mathematical techniques that have not previously been applied to this problem, namely equation solving, polyhedra construction, and SMT solving, to bring new capabilities to dynamic invariant detection."
            },
            "venue": {
                "fragments": [],
                "text": "2012 34th International Conference on Software Engineering (ICSE)"
            },
            "year": 2012
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2560300"
                        ],
                        "name": "G. Balakrishnan",
                        "slug": "G.-Balakrishnan",
                        "structuredName": {
                            "firstName": "Gogul",
                            "lastName": "Balakrishnan",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "G. Balakrishnan"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1703130"
                        ],
                        "name": "T. Reps",
                        "slug": "T.-Reps",
                        "structuredName": {
                            "firstName": "T.",
                            "lastName": "Reps",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "T. Reps"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 186,
                                "start": 183
                            }
                        ],
                        "text": "For example, a bug in the code generator of a compiler can invalidate a proof performed at the RTL level or we can have a \u201cwhat you see is not what you execute\u201d (WYSINWYX) phenomenon [3] and the generated binary can deviate from what is intended in the source."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 2279802,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f19306e70c7374a6d9e9133bc419a2ef9678e7c2",
            "isKey": false,
            "numCitedBy": 245,
            "numCiting": 221,
            "paperAbstract": {
                "fragments": [],
                "text": "Over the last seven years, we have developed static-analysis methods to recover a good approximation to the variables and dynamically allocated memory objects of a stripped executable, and to track the flow of values through them. The article presents the algorithms that we developed, explains how they are used to recover Intermediate Representations (IRs) from executables that are similar to the IRs that would be available if one started from source code, and describes their application in the context of program understanding and automated bug hunting.\n Unlike algorithms for analyzing executables that existed prior to our work, the ones presented in this article provide useful information about memory accesses, even in the absence of debugging information. The ideas described in the article are incorporated in a tool for analyzing Intel x86 executables, called CodeSurfer/x86. CodeSurfer/x86 builds a system dependence graph for the program, and provides a GUI for exploring the graph by (i) navigating its edges, and (ii) invoking operations, such as forward slicing, backward slicing, and chopping, to discover how parts of the program can impact other parts.\n To assess the usefulness of the IRs recovered by CodeSurfer/x86 in the context of automated bug hunting, we built a tool on top of CodeSurfer/x86, called Device-Driver Analyzer for x86 (DDA/x86), which analyzes device-driver executables for bugs. Without the benefit of either source code or symbol-table/debugging information, DDA/x86 was able to find known bugs (that had been discovered previously by source-code analysis tools), along with useful error traces, while having a low false-positive rate. DDA/x86 is the first known application of program analysis/verification techniques to industrial executables."
            },
            "slug": "WYSINWYX:-What-you-see-is-not-what-you-eXecute-Balakrishnan-Reps",
            "title": {
                "fragments": [],
                "text": "WYSINWYX: What you see is not what you eXecute"
            },
            "tldr": {
                "abstractSimilarityScore": 44,
                "text": "The article presents the algorithms that were developed, explains how they are used to recover Intermediate Representations from executables that are similar to the IRs that would be available if one started from source code, and describes their application in the context of program understanding and automated bug hunting."
            },
            "venue": {
                "fragments": [],
                "text": "TOPL"
            },
            "year": 2010
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1678228"
                        ],
                        "name": "D. Bacon",
                        "slug": "D.-Bacon",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Bacon",
                            "middleNames": [
                                "F."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Bacon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111178"
                        ],
                        "name": "S. Graham",
                        "slug": "S.-Graham",
                        "structuredName": {
                            "firstName": "Susan",
                            "lastName": "Graham",
                            "middleNames": [
                                "L."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "S. Graham"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2990426"
                        ],
                        "name": "Oliver J. Sharp",
                        "slug": "Oliver-J.-Sharp",
                        "structuredName": {
                            "firstName": "Oliver",
                            "lastName": "Sharp",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Oliver J. Sharp"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 80,
                                "start": 77
                            }
                        ],
                        "text": "The straightforward implementation X is optimized using a strength reduction [2] to produce the code Y ; corresponding x86 assembly codes T andR are shown beneath each source code function."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 43,
                                "start": 40
                            }
                        ],
                        "text": "Specifically, an access to a spill slot [2] will appear indistinguishable from a memory dereference, and Z3 will dis-"
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 1338971,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "89dddec47974ddf33fe8eb16c0232ab255479807",
            "isKey": false,
            "numCitedBy": 911,
            "numCiting": 341,
            "paperAbstract": {
                "fragments": [],
                "text": "In the last three decades a large number of compiler transformations for optimizing programs have been implemented. Most optimizations for uniprocessors reduce the number of instructions executed by the program using transformations based on the analysis of scalar quantities and data-flow techniques. In contrast, optimizations for high-performance superscalar, vector, and parallel processors maximize parallelism and memory locality with transformations that rely on tracking the properties of arrays using loop dependence analysis.\nThis survey is a comprehensive overview of the important high-level program restructuring techniques for imperative languages, such as C and Fortran. Transformations for both sequential and various types of parallel architectures are covered in depth. We describe the purpose of each transformation, explain how to determine if it is legal, and give an example of its application.\nProgrammers wishing to enhance the performance of their code can use this survey to improve their understanding of the optimizations that compilers can perform, or as a reference for techniques to be applied manually. Students can obtain an overview of optimizing compiler technology. Compiler writers can use this survey as a reference for most of the important optimizations developed to date, and as bibliographic reference for the details of each optimization. Readers are expected to be familiar with modern computer architecture and basic program compilation techniques."
            },
            "slug": "Compiler-transformations-for-high-performance-Bacon-Graham",
            "title": {
                "fragments": [],
                "text": "Compiler transformations for high-performance computing"
            },
            "tldr": {
                "abstractSimilarityScore": 41,
                "text": "This survey is a comprehensive overview of the important high-level program restructuring techniques for imperative languages, such as C and Fortran, and describes the purpose of each transformation, how to determine if it is legal, and an example of its application."
            },
            "venue": {
                "fragments": [],
                "text": "CSUR"
            },
            "year": 1994
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1945094"
                        ],
                        "name": "Eric Schkufza",
                        "slug": "Eric-Schkufza",
                        "structuredName": {
                            "firstName": "Eric",
                            "lastName": "Schkufza",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Eric Schkufza"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111333766"
                        ],
                        "name": "Rahul Sharma",
                        "slug": "Rahul-Sharma",
                        "structuredName": {
                            "firstName": "Rahul",
                            "lastName": "Sharma",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Rahul Sharma"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 65,
                                "start": 61
                            }
                        ],
                        "text": "We now discuss the failure cases of STOKE: the benchmarks of [33] for which STOKE produces code inferior to gcc -O3."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 119,
                                "start": 115
                            }
                        ],
                        "text": "Our version of STOKE is able to produce optimizations for the loop benchmarks that could not be fully optimized in [33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 157,
                                "start": 153
                            }
                        ],
                        "text": "Given a suitable cost function and run for long enough (which may be an extremely long time), STOKE is guaranteed to produce a code with the lowest cost [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 47,
                                "start": 43
                            }
                        ],
                        "text": "We leave the correctness term described by [33] unmodified."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 28,
                                "start": 24
                            }
                        ],
                        "text": "The other benchmarks of [33] are loop free and are not relevant here."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 103,
                                "start": 99
                            }
                        ],
                        "text": "Performance results for gcc and STOKE+DDEC equivalence checking for the loop failure benchmarks in [33]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 10,
                                "start": 6
                            }
                        ],
                        "text": "STOKE [33] is an x86 binary superoptimizer based on the principle that program optimization can be cast as a stochastic search problem."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 92,
                                "start": 88
                            }
                        ],
                        "text": "This situation arises, for example, in verifying the correctness of STOKE optimizations [33]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 55,
                                "start": 51
                            }
                        ],
                        "text": "Additionally, we extend the applicability of STOKE [33], a superoptimizer for straightline programs, to loops."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 683646,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "308388616c12158423fbf8bd8c441d11d1f432a2",
            "isKey": false,
            "numCitedBy": 256,
            "numCiting": 27,
            "paperAbstract": {
                "fragments": [],
                "text": "We formulate the loop-free binary superoptimization task as a stochastic search problem. The competing constraints of transformation correctness and performance improvement are encoded as terms in a cost function, and a Markov Chain Monte Carlo sampler is used to rapidly explore the space of all possible programs to find one that is an optimization of a given target program. Although our method sacrifices completeness, the scope of programs we are able to consider, and the resulting quality of the programs that we produce, far exceed those of existing superoptimizers. Beginning from binaries compiled by llvm -O0 for 64-bit x86, our prototype implementation, STOKE, is able to produce programs which either match or outperform the code produced by gcc -O3, icc -O3, and in some cases, expert handwritten assembly."
            },
            "slug": "Stochastic-superoptimization-Schkufza-Sharma",
            "title": {
                "fragments": [],
                "text": "Stochastic superoptimization"
            },
            "tldr": {
                "abstractSimilarityScore": 59,
                "text": "This work forms the loop-free binary superoptimization task as a stochastic search problem, and a Markov Chain Monte Carlo sampler is used to rapidly explore the space of all possible programs to find one that is an optimization of a given target program."
            },
            "venue": {
                "fragments": [],
                "text": "ASPLOS '13"
            },
            "year": 2013
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "49392754"
                        ],
                        "name": "X. Leroy",
                        "slug": "X.-Leroy",
                        "structuredName": {
                            "firstName": "Xavier",
                            "lastName": "Leroy",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "X. Leroy"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "12, the current version, does not perform loop optimizations and its compilation model does not fit well with CISC architectures such as x86, due to their \u201cpaucity of [general purpose] registers\u201d [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 64555543,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "abb294786960216f6714dfde1431e9e45a63ad21",
            "isKey": false,
            "numCitedBy": 37,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "This document is the user\u2019s manual for the CompCert C verified compiler. It is organized as follows: Chapter 1 gives an overview of the CompCert C compiler and of the formal verification of compilers. Chapter 2 explains how to install CompCert C. Chapter 3 explains how to use the CompCert C compiler. Chapter 4 explains how to use the CompCert C reference interpreter. Chapter 5 describes the subset of the ISO C99 language that is implemented by CompCert. Chapter 6 describes the supported language extensions: pragmas, attributes, built-in functions."
            },
            "slug": "The-CompCert-C-verified-compiler:-Documentation-and-Leroy",
            "title": {
                "fragments": [],
                "text": "The CompCert C verified compiler: Documentation and user\u2019s manual"
            },
            "tldr": {
                "abstractSimilarityScore": 69,
                "text": "This document is the user\u2019s manual for the CompCert C verified compiler, and describes the subset of the ISO C99 language that is implemented by CompCert."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2015
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2082718"
                        ],
                        "name": "H. Massalin",
                        "slug": "H.-Massalin",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Massalin",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. Massalin"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 110
                            }
                        ],
                        "text": "Superoptimizations and the related synthesis task have previously been limited to sequences of loop-free code [4, 16, 18, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 6074260,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "b9addc8ce998f6892120c2c8b23ae183312bfa6c",
            "isKey": false,
            "numCitedBy": 230,
            "numCiting": 5,
            "paperAbstract": {
                "fragments": [],
                "text": "Given an instruction set, the superoptimizer finds the shortest program to compute a function. Startling programs have been generated, many of them engaging in convoluted bit-fiddling bearing little resemblance to the source programs which defined the functions. The key idea in the superoptimizer is a probabilistic test that makes exhaustive searches practical for programs of useful size. The search space is defined by the processor's instruction set, which may include the whole set, but it is typically restricted to a subset. By constraining the instructions and observing the effect on the output program, one can gain insight into the design of instruction sets. In addition, superoptimized programs may be used by peephole optimizers to improve the quality of generated code, or by assembly language programmers to improve manually written code."
            },
            "slug": "Superoptimizer:-a-look-at-the-smallest-program-Massalin",
            "title": {
                "fragments": [],
                "text": "Superoptimizer: a look at the smallest program"
            },
            "tldr": {
                "abstractSimilarityScore": 68,
                "text": "Given an instruction set, the superoptimizer finds the shortest program to compute a function, a probabilistic test that makes exhaustive searches practical for programs of useful size."
            },
            "venue": {
                "fragments": [],
                "text": "ASPLOS"
            },
            "year": 1987
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "145083365"
                        ],
                        "name": "Sorav Bansal",
                        "slug": "Sorav-Bansal",
                        "structuredName": {
                            "firstName": "Sorav",
                            "lastName": "Bansal",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Sorav Bansal"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144653825"
                        ],
                        "name": "A. Aiken",
                        "slug": "A.-Aiken",
                        "structuredName": {
                            "firstName": "Alexander",
                            "lastName": "Aiken",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Aiken"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 110
                            }
                        ],
                        "text": "Superoptimizations and the related synthesis task have previously been limited to sequences of loop-free code [4, 16, 18, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "corpusId": 990671,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "25a77652204ae3e524a1ca25cca7a44c72d37d6d",
            "isKey": false,
            "numCitedBy": 151,
            "numCiting": 14,
            "paperAbstract": {
                "fragments": [],
                "text": "Peephole optimizers are typically constructed using human-written pattern matching rules, an approach that requires expertise and time, as well as being less than systematic at exploiting all opportunities for optimization. We explore fully automatic construction of peephole optimizers using brute force superoptimization. While the optimizations discovered by our automatic system may be less general than human-written counterparts, our approach has the potential to automatically learn a database of thousands to millions of optimizations, in contrast to the hundreds found in current peephole optimizers. We show experimentally that our optimizer is able to exploit performance opportunities not found by existing compilers; in particular, we show speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler."
            },
            "slug": "Automatic-generation-of-peephole-superoptimizers-Bansal-Aiken",
            "title": {
                "fragments": [],
                "text": "Automatic generation of peephole superoptimizers"
            },
            "tldr": {
                "abstractSimilarityScore": 43,
                "text": "It is shown experimentally that the fully automatic construction of peephole optimizers using brute force superoptimization is able to exploit performance opportunities not found by existing compilers, and speedups from 1.7 to a factor of 10 on some compute intensive kernels over a conventional optimizing compiler are shown."
            },
            "venue": {
                "fragments": [],
                "text": "ASPLOS XII"
            },
            "year": 2006
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "6698059"
                        ],
                        "name": "Michael D. Ernst",
                        "slug": "Michael-D.-Ernst",
                        "structuredName": {
                            "firstName": "Michael",
                            "lastName": "Ernst",
                            "middleNames": [
                                "D."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Michael D. Ernst"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "38156943"
                        ],
                        "name": "J. Perkins",
                        "slug": "J.-Perkins",
                        "structuredName": {
                            "firstName": "Jeff",
                            "lastName": "Perkins",
                            "middleNames": [
                                "H."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "J. Perkins"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "33270118"
                        ],
                        "name": "Philip J. Guo",
                        "slug": "Philip-J.-Guo",
                        "structuredName": {
                            "firstName": "Philip",
                            "lastName": "Guo",
                            "middleNames": [
                                "J."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Philip J. Guo"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1736680"
                        ],
                        "name": "Stephen McCamant",
                        "slug": "Stephen-McCamant",
                        "structuredName": {
                            "firstName": "Stephen",
                            "lastName": "McCamant",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Stephen McCamant"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "143698348"
                        ],
                        "name": "Carlos Pacheco",
                        "slug": "Carlos-Pacheco",
                        "structuredName": {
                            "firstName": "Carlos",
                            "lastName": "Pacheco",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Carlos Pacheco"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3054864"
                        ],
                        "name": "Matthew S. Tschantz",
                        "slug": "Matthew-S.-Tschantz",
                        "structuredName": {
                            "firstName": "Matthew",
                            "lastName": "Tschantz",
                            "middleNames": [
                                "S."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Matthew S. Tschantz"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2075318802"
                        ],
                        "name": "Chen Xiao",
                        "slug": "Chen-Xiao",
                        "structuredName": {
                            "firstName": "Chen",
                            "lastName": "Xiao",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Chen Xiao"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 84,
                                "start": 81
                            }
                        ],
                        "text": "Generation of invariants from test data for verification was pioneered by Daikon [9]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 17620776,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "a99575d60f83cf28a01fd177465061ffc6b0021c",
            "isKey": false,
            "numCitedBy": 1001,
            "numCiting": 51,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Daikon-system-for-dynamic-detection-of-likely-Ernst-Perkins",
            "title": {
                "fragments": [],
                "text": "The Daikon system for dynamic detection of likely invariants"
            },
            "venue": {
                "fragments": [],
                "text": "Sci. Comput. Program."
            },
            "year": 2007
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "144827979"
                        ],
                        "name": "N. Mateev",
                        "slug": "N.-Mateev",
                        "structuredName": {
                            "firstName": "Nikolay",
                            "lastName": "Mateev",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "N. Mateev"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "39551644"
                        ],
                        "name": "V. Menon",
                        "slug": "V.-Menon",
                        "structuredName": {
                            "firstName": "Vijay",
                            "lastName": "Menon",
                            "middleNames": [
                                "Krishna"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "V. Menon"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1776186"
                        ],
                        "name": "K. Pingali",
                        "slug": "K.-Pingali",
                        "structuredName": {
                            "firstName": "Keshav",
                            "lastName": "Pingali",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "K. Pingali"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "UNPAYWALL"
                },
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 30,
                                "start": 26
                            }
                        ],
                        "text": "Fractal symbolic analysis [24] and translation validation [14, 25, 29] are two techniques that reason about loops in general."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 355,
                                "start": 347
                            }
                        ],
                        "text": "Existing techniques for proving equivalence can be classified into three categories: sound algorithms for loop-free code [1, 6, 10, 11, 23]; algorithms that analyze finite unwindings of loops or finite spaces of inputs [17, 20, 28, 30]; algorithms that require knowledge of the particular transformations used for turning one program into another [24, 36] and the order in which the transformations have been applied [14, 25, 29]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 1367,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "7433f7630f96ce80eb2bc7fdb7b9a3d920bd7498",
            "isKey": false,
            "numCitedBy": 34,
            "numCiting": 76,
            "paperAbstract": {
                "fragments": [],
                "text": "Modern compilers restructure programs to improve their efficiency. Dependence analysis is the most widely used technique for proving the correctness of such transformations, but it suffers from the limitation that it considers only the memory locations read and written by a statement without considering what is being computed by that statement. Exploiting the semantics of program statements permits more transformations to be proved correct, and is critical for automatic restructuring of codes such as LU with partial pivoting.One approach to exploiting the semantics of program statements is symbolic analysis and comparison of programs.In principle, this technique is very powerful, but in practice, it is intractable for all but the simplest programs.In this paper, we propose a new form of symbolic analysis and comparison of programs which is appropriate for use in restructuring compilers. Fractal symbolic analysis is an approximate symbolic analysis that compares a program and its transformed version by repeatedly simplifying these programs until symbolic analysis becomes tractable while ensuring that equality of the simplified programs is sufficient to guarantee equality of the original programs.Fractal symbolic analysis combines some of the power of symbolic analysis with the tractability of dependence analysis. We discuss a prototype implementation of fractal symbolic analysis, and show how it can be used to solve the long-open problem of verifying the correctness of transformations required to improve the cache performance of LU factorization with partial pivoting."
            },
            "slug": "Fractal-symbolic-analysis-Mateev-Menon",
            "title": {
                "fragments": [],
                "text": "Fractal symbolic analysis"
            },
            "tldr": {
                "abstractSimilarityScore": 50,
                "text": "Fractal symbolic analysis combines some of the power of symbolic analysis with the tractability of dependence analysis and can be used to solve the long-open problem of verifying the correctness of transformations required to improve the cache performance of LU factorization with partial pivoting."
            },
            "venue": {
                "fragments": [],
                "text": "TOPL"
            },
            "year": 2003
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "34939726"
                        ],
                        "name": "H. S. Warren",
                        "slug": "H.-S.-Warren",
                        "structuredName": {
                            "firstName": "Henry",
                            "lastName": "Warren",
                            "middleNames": [
                                "S."
                            ],
                            "suffix": "Jr."
                        }
                    },
                    {
                        "fragments": [],
                        "text": "H. S. Warren"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 106,
                                "start": 102
                            }
                        ],
                        "text": "Wherever possible, we deferred to corresponding loop-free implementations given in A Hacker\u2019s Delight [40]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 53918233,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f5bf32bade6ae00d33d99d5f53fcc7b4c466c68c",
            "isKey": false,
            "numCitedBy": 383,
            "numCiting": 68,
            "paperAbstract": {
                "fragments": [],
                "text": "From the Book: \nCaveat Emptor: The cost of software maintenance increases with the square of the programmer's creativity. \nFirst Law of Programmer Creativity, \nRobert D. Bliss, 1992 \n \nThis is a collection of small programming tricks which the author has comeacross over many years. Most of them will work only on computers that representintegers in two's-complement form. Although a 32-bit machine is assumedwhen the register length is relevant, most of the tricks are easily adapted tomachines with other register sizes. \n \nThis book does not deal with \"large\" tricks such as sophisticated sorting andcompiler optimization techniques. Rather, it deals with \"small\" tricks that usuallyinvolve individual computer words or instructions, such as counting thenumber of 1-bits in a word. Such tricks often use a mixture of arithmetic and logicalinstructions. \n \nIt is assumed throughout that integer overflow interrupts have been maskedoff, so they cannot occur. C, Fortran, and even Java programs run in thisenvironment, but Pascal and ADA users beware! \n \nThe presentation is informal. Proofs are given only when the algorithm is definitelynot obvious, and sometimes not even then. The methods use computer-arithmetic, \"floor\" functions, mixtures of arithmetic and logical operations, etc.Proofs in this domain are often difficult and awkward to express.To reduce typographical errors and oversights, many of the algorithms havebeen executed. That is why they are given in a real programming language eventhough it, like every computer language, has some ugly features. For the highlevel language C is used, because it is widely known, it allows the straightforwardmixture of integer and bit-stringoperations, and C compilers are availablethat produce high quality object code. \n \nOccasionally machine language is used. It employs a 3-address format, mainlyfor ease of readability. The assembly language used is that of a fictitiousmachine that is representative of today's RISC computers. \n \nBranch-free code is favored. This is because on many computers branchesslow down instruction fetching and inhibit executing instructions in parallel.Another problem with branches is that they may inhibit compiler optimizationssuch as instruction scheduling, commoning, and register allocation. That is, thecompiler may be more effective at these optimizations with a program that consistsof a few large basic blocks, rather than many small ones. \n \nThe code sequences also tend to favor small immediate values, comparisons tozero (rather than to some other number), and instruction-level parallelism.Although much of the code would become more concise by using table lookups(from memory), this is not often mentioned. This is because loads are becomingmore expensive relative to arithmetic instructions, and the table lookup methodsare often not very interesting (although they are often practical). But there areexceptional cases. \n \nFinally, I should mention that the term \"hacker\" in the title is meant in the originalsense of an aficionado of computers\u0097someone who enjoys making computersdo new things, or do old things in a new and clever way. The hacker isusually quite good at his craft, but may very well not be a professional computerprogrammer or designer. The hacker's work may be useful or may be just agame. As an example of the latter, more than one determined hacker has writtena program which, when executed, writes out an exact copy of itself. 1 This is thesense in which we use \"hacker.\" If you're looking for tips on how to break intoother's computers, you won't find them here. \nH. S. Warren, Jr. \nFebruary 2002"
            },
            "slug": "Hacker's-Delight-Warren",
            "title": {
                "fragments": [],
                "text": "Hacker's Delight"
            },
            "tldr": {
                "abstractSimilarityScore": 37,
                "text": "The term \"hacker\" in the title is meant in the originalsense of an aficionado of computers\u0097someone who enjoys making computers do new things, or do old things in a new and clever way."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2002
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "152925315"
                        ],
                        "name": "A. Hunt",
                        "slug": "A.-Hunt",
                        "structuredName": {
                            "firstName": "Andy",
                            "lastName": "Hunt",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Hunt"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2111204539"
                        ],
                        "name": "David Thomas",
                        "slug": "David-Thomas",
                        "structuredName": {
                            "firstName": "David",
                            "lastName": "Thomas",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "David Thomas"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 59628087,
            "fieldsOfStudy": [
                "Art"
            ],
            "id": "ed1565a97c557bd168d975adb5c8a3d4ef9230c4",
            "isKey": false,
            "numCitedBy": 8127,
            "numCiting": 6,
            "paperAbstract": {
                "fragments": [],
                "text": "Writers face the blank page, painters face the empty canvas, and programmers face the empty editor buffer. Perhaps it\u2019s not literally empty\u2014an IDE may want us to specify a few things first. Here we haven\u2019t even started the project yet, and already we\u2019re forced to answer many questions: what will this thing be named, what directory will it be in, what type of module is it, how should it be compiled, and so on."
            },
            "slug": "The-Art-in-Computer-Programming-Hunt-Thomas",
            "title": {
                "fragments": [],
                "text": "The Art in Computer Programming"
            },
            "tldr": {
                "abstractSimilarityScore": 40,
                "text": "Here the authors haven\u2019t even started the project yet, and already they\u2019re forced to answer many questions: what will this thing be named, what directory will it be in, what type of module is it, how should it be compiled, and so on."
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2001
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2293005"
                        ],
                        "name": "Zhuliang Chen",
                        "slug": "Zhuliang-Chen",
                        "structuredName": {
                            "firstName": "Zhuliang",
                            "lastName": "Chen",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "Zhuliang Chen"
                    }
                ],
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "3286009"
                        ],
                        "name": "A. Storjohann",
                        "slug": "A.-Storjohann",
                        "structuredName": {
                            "firstName": "Arne",
                            "lastName": "Storjohann",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Storjohann"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 86,
                                "start": 83
                            }
                        ],
                        "text": "Invariants are computed using the nullspace function of the Integer Matrix Library [5] which is specialized for computing the nullspace of integer and rational matrices using padic arithmetic."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 7380429,
            "fieldsOfStudy": [
                "Computer Science",
                "Mathematics"
            ],
            "id": "9c0019f9a549d030f90be29d4e0161c7a9733c71",
            "isKey": false,
            "numCitedBy": 62,
            "numCiting": 40,
            "paperAbstract": {
                "fragments": [],
                "text": "Algorithms for solving linear systems of equations over the integers are designed and implemented. The implementations are based on the highly optimized and portable ATLAS/BLAS library for numerical linear algebra and the GNU Multiple Precision library (GMP) for large integer arithmetic."
            },
            "slug": "A-BLAS-based-C-library-for-exact-linear-algebra-on-Chen-Storjohann",
            "title": {
                "fragments": [],
                "text": "A BLAS based C library for exact linear algebra on integer matrices"
            },
            "tldr": {
                "abstractSimilarityScore": 89,
                "text": "Algorithms for solving linear systems of equations over the integers are designed and implemented based on the highly optimized and portable ATLAS/BLAS library for numerical linear algebra and the GNU Multiple Precision library for large integer arithmetic."
            },
            "venue": {
                "fragments": [],
                "text": "ISSAC"
            },
            "year": 2005
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1380483709"
                        ],
                        "name": "M. Medard",
                        "slug": "M.-Medard",
                        "structuredName": {
                            "firstName": "Muriel",
                            "lastName": "Medard",
                            "middleNames": []
                        }
                    },
                    {
                        "fragments": [],
                        "text": "M. Medard"
                    }
                ]
            ],
            "badges": [
                {
                    "id": "OPEN_ACCESS"
                }
            ],
            "citationContexts": [],
            "corpusId": 4153826,
            "fieldsOfStudy": [
                "Law"
            ],
            "id": "defd66114b63aae75e0af2bcff2e52ae8fd2c873",
            "isKey": false,
            "numCitedBy": 1778,
            "numCiting": 3,
            "paperAbstract": {
                "fragments": [],
                "text": "WARNING NOTICE: The experiments described in these materials are potentially hazardous and require a high level ofsafety training, special facilities and equipment, and supervision by appropriate individuals. You bear the sole responsibility, liability, and risk for the implementation of such safety procedures and measures. MIT shall have no responsibility, liability, or risk for the content or implementation of any of the material presented. Legal Notices"
            },
            "slug": "Massachusetts-Institute-of-Technology-Medard",
            "title": {
                "fragments": [],
                "text": "Massachusetts Institute of Technology"
            },
            "venue": {
                "fragments": [],
                "text": "Nature"
            },
            "year": 1964
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 124,
                                "start": 121
                            }
                        ],
                        "text": "Second, DDEC generates verification conditions encoding the x86 instructions contained in each loop-free fragment as SMT [7] constraints."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 45,
                                "start": 42
                            }
                        ],
                        "text": "Proof obligations are discharged using Z3 [7]."
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 303,
                                "start": 300
                            }
                        ],
                        "text": "Categories and Subject Descriptors D.1.2 [Automatic Programming]: Program Transformation; D.2.4 [Program Verification]: Correctness proofs; D.3.4 [Processors]: Compilers; D.3.4 [Processors]: Optimization\nKeywords Binary Analysis; Compilers; Markov Chain Monte Carlo; Optimization; Superoptimization; SMT; Verification; x86"
                    },
                    "intents": []
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 132,
                                "start": 129
                            }
                        ],
                        "text": "DDEC is also currently unable to reason about floating point computations, simply because the current generation of offthe-shelf SMT solvers do not support floating point reasoning."
                    },
                    "intents": []
                }
            ],
            "fieldsOfStudy": [],
            "isKey": true,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "An efficient SMT solver"
            },
            "venue": {
                "fragments": [],
                "text": "In TACAS,"
            },
            "year": 2008
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "2262347"
                        ],
                        "name": "A. Turing",
                        "slug": "A.-Turing",
                        "structuredName": {
                            "firstName": "Alan",
                            "lastName": "Turing",
                            "middleNames": [
                                "M."
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "A. Turing"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [],
            "corpusId": 56518797,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "f50fef843592d31124e629420a5d7df39a51fcd7",
            "isKey": false,
            "numCitedBy": 393,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "Checking-a-large-routine-Turing",
            "title": {
                "fragments": [],
                "text": "Checking a large routine"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [
                [
                    {
                        "bitmap$0": false,
                        "ids": [
                            "1717349"
                        ],
                        "name": "D. Knuth",
                        "slug": "D.-Knuth",
                        "structuredName": {
                            "firstName": "Donald",
                            "lastName": "Knuth",
                            "middleNames": [
                                "Ervin"
                            ]
                        }
                    },
                    {
                        "fragments": [],
                        "text": "D. Knuth"
                    }
                ]
            ],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 42,
                                "start": 38
                            }
                        ],
                        "text": "Say the target uses Karatsuba\u2019s trick [19] and performs three 32-bit signed multiplications to obtain a 64-bit result, whereas the rewrite uses a special x86 instruction that performs an unsigned multiplication of two 32-bit numbers and directly produces a 64-bit result."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "corpusId": 298987,
            "fieldsOfStudy": [
                "Computer Science"
            ],
            "id": "2f05f96a53f94c9da24fbcca5c3df1d7e159c63d",
            "isKey": false,
            "numCitedBy": 2096,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "The-Art-of-Computer-Programming,-Volume-II:-Knuth",
            "title": {
                "fragments": [],
                "text": "The Art of Computer Programming, Volume II: Seminumerical Algorithms"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1969
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 82,
                                "start": 78
                            }
                        ],
                        "text": "We generate corresponding paths t and r for proof obligations using cutpoints [39]."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                },
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 48,
                                "start": 44
                            }
                        ],
                        "text": "We use the well-known concept of a cutpoint [39] to decompose equivalence checking of two loops into manageable sub-parts."
                    },
                    "intents": [
                        {
                            "id": "methodology"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Checking a large routine. In The early British computer conferences, pages 70\u201372"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 1989
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 200,
                                "start": 196
                            }
                        ],
                        "text": "12, the current version, does not perform loop optimizations and its compilation model does not fit well with CISC architectures such as x86, due to their \u201cpaucity of [general purpose] registers\u201d [21]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "The CompCert C verified compiler documentation and users manual, 2013.  URL http://compcert.inria.fr/man/manual.pdf"
            },
            "venue": {
                "fragments": [],
                "text": ""
            },
            "year": 2013
        },
        {
            "authors": [],
            "badges": [],
            "citationContexts": [
                {
                    "context": {
                        "fragments": [
                            {
                                "end": 125,
                                "start": 110
                            }
                        ],
                        "text": "Superoptimizations and the related synthesis task have previously been limited to sequences of loop-free code [4, 16, 18, 22]."
                    },
                    "intents": [
                        {
                            "id": "background"
                        }
                    ]
                }
            ],
            "fieldsOfStudy": [],
            "isKey": false,
            "numCitedBy": 0,
            "numCiting": 0,
            "paperAbstract": {
                "fragments": [],
                "text": ""
            },
            "slug": "+",
            "title": {
                "fragments": [],
                "text": "Synthesis of loopfree programs"
            },
            "venue": {
                "fragments": [],
                "text": "In PLDI,"
            },
            "year": 2011
        }
    ],
    "meta_info": {
        "citationIntent": "all",
        "citationIntentCount": {
            "background": 29,
            "methodology": 26
        },
        "citationType": "citedPapers",
        "pageNumber": 1,
        "requestedPageSize": 10,
        "sort": "relevance",
        "totalCitations": 48,
        "totalPages": 5
    },
    "page_url": "https://www.semanticscholar.org/paper/Data-driven-equivalence-checking-Sharma-Schkufza/1e52a2e8535509ab0111c0c5d89a88d3bb10b34c?sort=total-citations"
}